2025-12-21 12:12:27,829 INFO __main__: Starting benchmark for dataset=ml_arxiv
2025-12-21 12:12:32,656 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-21 12:12:33,186 INFO gensim.corpora.dictionary: built Dictionary<25044 unique tokens: ['achievable', 'admissible', 'allowable', 'allowed', 'approach']...> from 10000 documents (total 970327 corpus positions)
2025-12-21 12:12:33,191 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<25044 unique tokens: ['achievable', 'admissible', 'allowable', 'allowed', 'approach']...> from 10000 documents (total 970327 corpus positions)", 'datetime': '2025-12-21T12:12:33.186948', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-187-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-21 12:12:35,382 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-21 12:12:35,382 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-21 12:12:41,027 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10000 docs
2025-12-21 12:14:44,364 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:14:46,416 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (-70568 virtual)
2025-12-21 12:14:46,582 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (-99218 virtual)
2025-12-21 12:14:46,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,885 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,889 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,894 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:46,900 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,924 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,936 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,944 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,945 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,946 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:46,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:47,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:47,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:48,679 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-21 12:14:48,709 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 86452 virtual documents
2025-12-21 12:14:48,806 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:14:50,816 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5038 virtual)
2025-12-21 12:14:50,818 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (9939 virtual)
2025-12-21 12:14:50,819 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15346 virtual)
2025-12-21 12:14:50,820 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19775 virtual)
2025-12-21 12:14:50,822 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (24513 virtual)
2025-12-21 12:14:50,823 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30004 virtual)
2025-12-21 12:14:50,824 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35561 virtual)
2025-12-21 12:14:50,826 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (40599 virtual)
2025-12-21 12:14:50,827 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45821 virtual)
2025-12-21 12:14:50,829 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51004 virtual)
2025-12-21 12:14:50,830 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (56880 virtual)
2025-12-21 12:14:50,832 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62578 virtual)
2025-12-21 12:14:50,833 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (68501 virtual)
2025-12-21 12:14:50,835 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (74285 virtual)
2025-12-21 12:14:50,836 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (78940 virtual)
2025-12-21 12:14:50,838 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (84436 virtual)
2025-12-21 12:14:50,839 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (90137 virtual)
2025-12-21 12:14:50,841 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (95470 virtual)
2025-12-21 12:14:50,842 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (100900 virtual)
2025-12-21 12:14:50,843 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (106659 virtual)
2025-12-21 12:14:50,845 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (111500 virtual)
2025-12-21 12:14:50,846 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (116869 virtual)
2025-12-21 12:14:50,848 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (122293 virtual)
2025-12-21 12:14:50,849 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (127898 virtual)
2025-12-21 12:14:50,851 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (132828 virtual)
2025-12-21 12:14:50,852 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (138085 virtual)
2025-12-21 12:14:50,853 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (143343 virtual)
2025-12-21 12:14:50,855 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (148098 virtual)
2025-12-21 12:14:50,856 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (152948 virtual)
2025-12-21 12:14:50,857 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (157644 virtual)
2025-12-21 12:14:50,859 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (162784 virtual)
2025-12-21 12:14:50,860 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (167970 virtual)
2025-12-21 12:14:50,862 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (173334 virtual)
2025-12-21 12:14:50,863 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (179046 virtual)
2025-12-21 12:14:50,864 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (184503 virtual)
2025-12-21 12:14:50,866 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (190482 virtual)
2025-12-21 12:14:50,867 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (195734 virtual)
2025-12-21 12:14:50,892 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (201130 virtual)
2025-12-21 12:14:50,894 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (206629 virtual)
2025-12-21 12:14:50,895 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (212054 virtual)
2025-12-21 12:14:50,897 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (217483 virtual)
2025-12-21 12:14:50,904 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (222543 virtual)
2025-12-21 12:14:50,905 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (227517 virtual)
2025-12-21 12:14:50,906 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (232518 virtual)
2025-12-21 12:14:50,916 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (237974 virtual)
2025-12-21 12:14:50,917 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (243778 virtual)
2025-12-21 12:14:50,919 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (249684 virtual)
2025-12-21 12:14:50,920 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (255577 virtual)
2025-12-21 12:14:50,932 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (261521 virtual)
2025-12-21 12:14:50,934 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (267029 virtual)
2025-12-21 12:14:50,935 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (272359 virtual)
2025-12-21 12:14:50,940 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (278302 virtual)
2025-12-21 12:14:50,952 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (284138 virtual)
2025-12-21 12:14:50,953 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (289694 virtual)
2025-12-21 12:14:50,955 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (295460 virtual)
2025-12-21 12:14:50,956 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (301061 virtual)
2025-12-21 12:14:50,968 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (306311 virtual)
2025-12-21 12:14:50,971 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (312194 virtual)
2025-12-21 12:14:50,973 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (318140 virtual)
2025-12-21 12:14:50,976 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (323661 virtual)
2025-12-21 12:14:50,977 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (328872 virtual)
2025-12-21 12:14:50,984 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (334616 virtual)
2025-12-21 12:14:50,986 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (340760 virtual)
2025-12-21 12:14:50,992 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (347024 virtual)
2025-12-21 12:14:51,001 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (353005 virtual)
2025-12-21 12:14:51,008 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (358895 virtual)
2025-12-21 12:14:51,040 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (364790 virtual)
2025-12-21 12:14:51,041 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (370572 virtual)
2025-12-21 12:14:51,043 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (376025 virtual)
2025-12-21 12:14:51,044 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (381884 virtual)
2025-12-21 12:14:51,076 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (387336 virtual)
2025-12-21 12:14:51,092 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (393046 virtual)
2025-12-21 12:14:51,212 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (398587 virtual)
2025-12-21 12:14:51,214 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404515 virtual)
2025-12-21 12:14:51,215 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (409840 virtual)
2025-12-21 12:14:51,220 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (415807 virtual)
2025-12-21 12:14:51,360 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (422232 virtual)
2025-12-21 12:14:51,362 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427794 virtual)
2025-12-21 12:14:51,363 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (433810 virtual)
2025-12-21 12:14:51,432 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (439773 virtual)
2025-12-21 12:14:51,434 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (445625 virtual)
2025-12-21 12:14:51,435 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (451425 virtual)
2025-12-21 12:14:51,437 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (457024 virtual)
2025-12-21 12:14:51,448 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (462670 virtual)
2025-12-21 12:14:51,532 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (468463 virtual)
2025-12-21 12:14:51,534 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (474212 virtual)
2025-12-21 12:14:51,535 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (479446 virtual)
2025-12-21 12:14:51,556 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (484627 virtual)
2025-12-21 12:14:51,636 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (490059 virtual)
2025-12-21 12:14:51,652 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (495865 virtual)
2025-12-21 12:14:51,653 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (501609 virtual)
2025-12-21 12:14:51,655 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (507454 virtual)
2025-12-21 12:14:51,660 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (512888 virtual)
2025-12-21 12:14:51,784 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (518689 virtual)
2025-12-21 12:14:51,786 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (524392 virtual)
2025-12-21 12:14:51,787 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (530209 virtual)
2025-12-21 12:14:51,788 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (536088 virtual)
2025-12-21 12:14:51,796 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (542046 virtual)
2025-12-21 12:14:51,816 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (548028 virtual)
2025-12-21 12:14:51,908 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (553932 virtual)
2025-12-21 12:14:51,910 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (560021 virtual)
2025-12-21 12:14:51,956 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (566327 virtual)
2025-12-21 12:14:51,958 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (572119 virtual)
2025-12-21 12:14:51,960 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (577590 virtual)
2025-12-21 12:14:51,962 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (583031 virtual)
2025-12-21 12:14:52,039 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (588967 virtual)
2025-12-21 12:14:52,052 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (594321 virtual)
2025-12-21 12:14:52,105 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (599788 virtual)
2025-12-21 12:14:52,107 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (605199 virtual)
2025-12-21 12:14:52,109 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (611045 virtual)
2025-12-21 12:14:52,111 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (616902 virtual)
2025-12-21 12:14:52,179 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (622705 virtual)
2025-12-21 12:14:52,181 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (628573 virtual)
2025-12-21 12:14:52,231 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (634432 virtual)
2025-12-21 12:14:52,233 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (640090 virtual)
2025-12-21 12:14:52,234 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (645815 virtual)
2025-12-21 12:14:52,236 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (651571 virtual)
2025-12-21 12:14:52,314 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (657387 virtual)
2025-12-21 12:14:52,316 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (663236 virtual)
2025-12-21 12:14:52,318 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (669362 virtual)
2025-12-21 12:14:52,320 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (675610 virtual)
2025-12-21 12:14:52,372 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (681771 virtual)
2025-12-21 12:14:52,374 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (687925 virtual)
2025-12-21 12:14:52,376 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (694382 virtual)
2025-12-21 12:14:52,443 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (700400 virtual)
2025-12-21 12:14:52,445 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (706047 virtual)
2025-12-21 12:14:52,446 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (712150 virtual)
2025-12-21 12:14:52,489 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (718089 virtual)
2025-12-21 12:14:52,491 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (723908 virtual)
2025-12-21 12:14:52,504 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (729792 virtual)
2025-12-21 12:14:52,572 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (735737 virtual)
2025-12-21 12:14:52,574 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (740988 virtual)
2025-12-21 12:14:52,587 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (746714 virtual)
2025-12-21 12:14:52,589 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (752115 virtual)
2025-12-21 12:14:52,591 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (757747 virtual)
2025-12-21 12:14:52,658 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (763660 virtual)
2025-12-21 12:14:52,661 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (769419 virtual)
2025-12-21 12:14:52,662 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (774931 virtual)
2025-12-21 12:14:52,679 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (780589 virtual)
2025-12-21 12:14:52,692 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (786081 virtual)
2025-12-21 12:14:52,739 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (791777 virtual)
2025-12-21 12:14:52,744 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (798138 virtual)
2025-12-21 12:14:52,746 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (803809 virtual)
2025-12-21 12:14:52,784 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (809775 virtual)
2025-12-21 12:14:52,786 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (815761 virtual)
2025-12-21 12:14:52,788 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (821960 virtual)
2025-12-21 12:14:52,836 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (827580 virtual)
2025-12-21 12:14:52,848 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (833163 virtual)
2025-12-21 12:14:52,854 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (838905 virtual)
2025-12-21 12:14:52,856 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (844537 virtual)
2025-12-21 12:14:52,909 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (850171 virtual)
2025-12-21 12:14:52,911 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (855698 virtual)
2025-12-21 12:14:52,912 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (861596 virtual)
2025-12-21 12:14:52,914 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (867137 virtual)
2025-12-21 12:14:52,916 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (873138 virtual)
2025-12-21 12:14:52,967 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (878914 virtual)
2025-12-21 12:14:52,968 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (880327 virtual)
2025-12-21 12:14:52,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:52,990 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,029 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,052 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,061 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,109 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,133 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,134 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,162 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,620 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:53,679 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:14:53,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:14:54,947 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-21 12:14:54,966 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 880332 virtual documents
2025-12-21 12:14:55,098 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10000 docs
2025-12-21 12:16:46,063 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:16:48,711 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (-70568 virtual)
2025-12-21 12:16:49,988 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (-99218 virtual)
2025-12-21 12:16:50,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,868 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,922 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,942 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,974 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:50,992 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:50,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,265 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,274 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,683 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,700 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:51,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:51,992 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:16:52,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:16:54,229 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-21 12:16:54,344 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 86452 virtual documents
2025-12-21 12:16:54,792 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:16:57,239 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5038 virtual)
2025-12-21 12:16:57,242 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (9939 virtual)
2025-12-21 12:16:57,243 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15346 virtual)
2025-12-21 12:16:57,245 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19775 virtual)
2025-12-21 12:16:57,246 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (24513 virtual)
2025-12-21 12:16:57,248 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30004 virtual)
2025-12-21 12:16:57,250 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35561 virtual)
2025-12-21 12:16:57,252 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (40599 virtual)
2025-12-21 12:16:57,254 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45821 virtual)
2025-12-21 12:16:57,255 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51004 virtual)
2025-12-21 12:16:57,257 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (56880 virtual)
2025-12-21 12:16:57,259 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62578 virtual)
2025-12-21 12:16:57,261 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (68501 virtual)
2025-12-21 12:16:57,264 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (74285 virtual)
2025-12-21 12:16:57,265 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (78940 virtual)
2025-12-21 12:16:57,267 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (84436 virtual)
2025-12-21 12:16:57,269 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (90137 virtual)
2025-12-21 12:16:57,271 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (95470 virtual)
2025-12-21 12:16:57,273 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (100900 virtual)
2025-12-21 12:16:57,275 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (106659 virtual)
2025-12-21 12:16:57,276 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (111500 virtual)
2025-12-21 12:16:57,278 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (116869 virtual)
2025-12-21 12:16:57,280 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (122293 virtual)
2025-12-21 12:16:57,282 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (127898 virtual)
2025-12-21 12:16:57,284 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (132828 virtual)
2025-12-21 12:16:57,286 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (138085 virtual)
2025-12-21 12:16:57,288 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (143343 virtual)
2025-12-21 12:16:57,289 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (148098 virtual)
2025-12-21 12:16:57,291 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (152948 virtual)
2025-12-21 12:16:57,292 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (157644 virtual)
2025-12-21 12:16:57,294 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (162784 virtual)
2025-12-21 12:16:57,296 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (167970 virtual)
2025-12-21 12:16:57,298 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (173334 virtual)
2025-12-21 12:16:57,300 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (179046 virtual)
2025-12-21 12:16:57,302 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (184503 virtual)
2025-12-21 12:16:57,304 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (190482 virtual)
2025-12-21 12:16:57,306 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (195734 virtual)
2025-12-21 12:16:57,308 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (201130 virtual)
2025-12-21 12:16:57,310 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (206629 virtual)
2025-12-21 12:16:57,312 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (212054 virtual)
2025-12-21 12:16:57,314 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (217483 virtual)
2025-12-21 12:16:57,316 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (222543 virtual)
2025-12-21 12:16:57,318 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (227517 virtual)
2025-12-21 12:16:57,320 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (232518 virtual)
2025-12-21 12:16:57,322 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (237974 virtual)
2025-12-21 12:16:57,324 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (243778 virtual)
2025-12-21 12:16:57,327 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (249684 virtual)
2025-12-21 12:16:57,329 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (255577 virtual)
2025-12-21 12:16:57,331 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (261521 virtual)
2025-12-21 12:16:57,333 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (267029 virtual)
2025-12-21 12:16:57,335 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (272359 virtual)
2025-12-21 12:16:57,338 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (278302 virtual)
2025-12-21 12:16:57,340 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (284138 virtual)
2025-12-21 12:16:57,342 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (289694 virtual)
2025-12-21 12:16:57,344 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (295460 virtual)
2025-12-21 12:16:57,346 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (301061 virtual)
2025-12-21 12:16:57,348 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (306311 virtual)
2025-12-21 12:16:57,350 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (312194 virtual)
2025-12-21 12:16:57,352 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (318140 virtual)
2025-12-21 12:16:57,354 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (323661 virtual)
2025-12-21 12:16:57,356 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (328872 virtual)
2025-12-21 12:16:57,359 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (334616 virtual)
2025-12-21 12:16:57,361 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (340760 virtual)
2025-12-21 12:16:57,363 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (347024 virtual)
2025-12-21 12:16:57,366 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (353005 virtual)
2025-12-21 12:16:57,368 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (358895 virtual)
2025-12-21 12:16:57,370 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (364790 virtual)
2025-12-21 12:16:57,372 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (370572 virtual)
2025-12-21 12:16:57,374 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (376025 virtual)
2025-12-21 12:16:57,376 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (381884 virtual)
2025-12-21 12:16:57,378 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (387336 virtual)
2025-12-21 12:16:57,380 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (393046 virtual)
2025-12-21 12:16:57,382 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (398587 virtual)
2025-12-21 12:16:57,390 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404515 virtual)
2025-12-21 12:16:57,392 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (409840 virtual)
2025-12-21 12:16:57,394 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (415807 virtual)
2025-12-21 12:16:57,397 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (422232 virtual)
2025-12-21 12:16:57,405 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427794 virtual)
2025-12-21 12:16:57,407 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (433810 virtual)
2025-12-21 12:16:57,561 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (439773 virtual)
2025-12-21 12:16:57,563 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (445625 virtual)
2025-12-21 12:16:57,588 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (451425 virtual)
2025-12-21 12:16:57,596 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (457024 virtual)
2025-12-21 12:16:57,624 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (462670 virtual)
2025-12-21 12:16:57,660 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (468463 virtual)
2025-12-21 12:16:57,692 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (474212 virtual)
2025-12-21 12:16:57,728 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (479446 virtual)
2025-12-21 12:16:57,760 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (484627 virtual)
2025-12-21 12:16:57,788 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (490059 virtual)
2025-12-21 12:16:57,820 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (495865 virtual)
2025-12-21 12:16:57,852 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (501609 virtual)
2025-12-21 12:16:57,888 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (507454 virtual)
2025-12-21 12:16:57,920 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (512888 virtual)
2025-12-21 12:16:57,956 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (518689 virtual)
2025-12-21 12:16:57,984 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (524392 virtual)
2025-12-21 12:16:58,020 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (530209 virtual)
2025-12-21 12:16:58,048 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (536088 virtual)
2025-12-21 12:16:58,088 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (542046 virtual)
2025-12-21 12:16:58,116 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (548028 virtual)
2025-12-21 12:16:58,156 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (553932 virtual)
2025-12-21 12:16:58,184 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (560021 virtual)
2025-12-21 12:16:58,224 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (566327 virtual)
2025-12-21 12:16:58,252 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (572119 virtual)
2025-12-21 12:16:58,288 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (577590 virtual)
2025-12-21 12:16:58,320 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (583031 virtual)
2025-12-21 12:16:58,352 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (588967 virtual)
2025-12-21 12:16:58,388 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (594321 virtual)
2025-12-21 12:16:58,416 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (599788 virtual)
2025-12-21 12:16:58,456 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (605199 virtual)
2025-12-21 12:16:58,480 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (611045 virtual)
2025-12-21 12:16:58,520 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (616902 virtual)
2025-12-21 12:16:58,544 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (622705 virtual)
2025-12-21 12:16:58,585 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (628573 virtual)
2025-12-21 12:16:58,608 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (634432 virtual)
2025-12-21 12:16:58,708 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (640090 virtual)
2025-12-21 12:16:58,711 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (645815 virtual)
2025-12-21 12:16:58,712 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (651571 virtual)
2025-12-21 12:16:58,736 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (657387 virtual)
2025-12-21 12:16:58,776 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (663236 virtual)
2025-12-21 12:16:58,800 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (669362 virtual)
2025-12-21 12:16:58,841 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (675610 virtual)
2025-12-21 12:16:58,928 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (681771 virtual)
2025-12-21 12:16:58,931 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (687925 virtual)
2025-12-21 12:16:58,933 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (694382 virtual)
2025-12-21 12:16:58,960 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (700400 virtual)
2025-12-21 12:16:59,056 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (706047 virtual)
2025-12-21 12:16:59,059 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (712150 virtual)
2025-12-21 12:16:59,060 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (718089 virtual)
2025-12-21 12:16:59,096 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (723908 virtual)
2025-12-21 12:16:59,124 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (729792 virtual)
2025-12-21 12:16:59,161 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (735737 virtual)
2025-12-21 12:16:59,240 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (740988 virtual)
2025-12-21 12:16:59,242 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (746714 virtual)
2025-12-21 12:16:59,244 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (752115 virtual)
2025-12-21 12:16:59,272 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (757747 virtual)
2025-12-21 12:16:59,296 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (763660 virtual)
2025-12-21 12:16:59,334 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (769419 virtual)
2025-12-21 12:16:59,364 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (774931 virtual)
2025-12-21 12:16:59,396 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (780589 virtual)
2025-12-21 12:16:59,424 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (786081 virtual)
2025-12-21 12:16:59,460 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (791777 virtual)
2025-12-21 12:16:59,492 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (798138 virtual)
2025-12-21 12:16:59,524 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (803809 virtual)
2025-12-21 12:16:59,556 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (809775 virtual)
2025-12-21 12:16:59,592 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (815761 virtual)
2025-12-21 12:16:59,624 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (821960 virtual)
2025-12-21 12:16:59,660 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (827580 virtual)
2025-12-21 12:16:59,688 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (833163 virtual)
2025-12-21 12:16:59,724 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (838905 virtual)
2025-12-21 12:16:59,752 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (844537 virtual)
2025-12-21 12:16:59,788 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (850171 virtual)
2025-12-21 12:16:59,816 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (855698 virtual)
2025-12-21 12:16:59,852 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (861596 virtual)
2025-12-21 12:16:59,884 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (867137 virtual)
2025-12-21 12:16:59,916 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (873138 virtual)
2025-12-21 12:16:59,945 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (878914 virtual)
2025-12-21 12:16:59,979 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (880327 virtual)
2025-12-21 12:17:00,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,866 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,867 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,869 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,915 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,950 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,956 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:00,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:00,966 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,040 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,070 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,070 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,139 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,846 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:01,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:01,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,070 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,126 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,206 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,208 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,241 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,258 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,274 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:17:02,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:02,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:17:04,228 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-21 12:17:04,331 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 880332 virtual documents
2025-12-21 12:17:04,659 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10000 docs
Training CobwebTree:   0%|          | 0/10000 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 20/10000 [00:00<00:51, 195.00it/s]Training CobwebTree:   0%|          | 40/10000 [00:00<01:22, 120.55it/s]Training CobwebTree:   1%|          | 54/10000 [00:00<01:33, 106.51it/s]Training CobwebTree:   1%|          | 66/10000 [00:00<01:39, 99.64it/s] Training CobwebTree:   1%|          | 77/10000 [00:00<01:58, 84.07it/s]Training CobwebTree:   1%|          | 86/10000 [00:00<02:03, 80.15it/s]Training CobwebTree:   1%|          | 95/10000 [00:01<02:02, 81.05it/s]Training CobwebTree:   1%|          | 104/10000 [00:01<02:09, 76.14it/s]Training CobwebTree:   1%|          | 112/10000 [00:01<02:20, 70.48it/s]Training CobwebTree:   1%|          | 120/10000 [00:01<02:22, 69.55it/s]Training CobwebTree:   1%|         | 128/10000 [00:01<02:20, 70.12it/s]Training CobwebTree:   1%|         | 136/10000 [00:01<02:18, 71.48it/s]Training CobwebTree:   1%|         | 144/10000 [00:01<02:18, 71.41it/s]Training CobwebTree:   2%|         | 152/10000 [00:01<02:19, 70.38it/s]Training CobwebTree:   2%|         | 160/10000 [00:01<02:20, 69.91it/s]Training CobwebTree:   2%|         | 168/10000 [00:02<02:20, 69.82it/s]Training CobwebTree:   2%|         | 175/10000 [00:02<02:24, 67.86it/s]Training CobwebTree:   2%|         | 182/10000 [00:02<02:35, 63.16it/s]Training CobwebTree:   2%|         | 189/10000 [00:02<02:37, 62.18it/s]Training CobwebTree:   2%|         | 196/10000 [00:02<02:39, 61.46it/s]Training CobwebTree:   2%|         | 203/10000 [00:02<02:38, 61.74it/s]Training CobwebTree:   2%|         | 210/10000 [00:02<02:43, 59.88it/s]Training CobwebTree:   2%|         | 217/10000 [00:02<02:41, 60.39it/s]Training CobwebTree:   2%|         | 224/10000 [00:03<02:44, 59.59it/s]Training CobwebTree:   2%|         | 230/10000 [00:03<02:44, 59.42it/s]Training CobwebTree:   2%|         | 236/10000 [00:03<03:02, 53.54it/s]Training CobwebTree:   2%|         | 243/10000 [00:03<02:54, 55.87it/s]Training CobwebTree:   2%|         | 250/10000 [00:03<02:45, 58.93it/s]Training CobwebTree:   3%|         | 257/10000 [00:03<02:43, 59.49it/s]Training CobwebTree:   3%|         | 264/10000 [00:03<02:46, 58.49it/s]Training CobwebTree:   3%|         | 271/10000 [00:03<02:39, 61.03it/s]Training CobwebTree:   3%|         | 278/10000 [00:03<02:38, 61.22it/s]Training CobwebTree:   3%|         | 285/10000 [00:04<02:41, 60.28it/s]Training CobwebTree:   3%|         | 292/10000 [00:04<02:49, 57.33it/s]Training CobwebTree:   3%|         | 298/10000 [00:04<02:47, 57.83it/s]Training CobwebTree:   3%|         | 304/10000 [00:04<02:53, 56.03it/s]Training CobwebTree:   3%|         | 311/10000 [00:04<02:44, 58.97it/s]Training CobwebTree:   3%|         | 318/10000 [00:04<02:40, 60.33it/s]Training CobwebTree:   3%|         | 325/10000 [00:04<02:37, 61.26it/s]Training CobwebTree:   3%|         | 332/10000 [00:04<02:45, 58.27it/s]Training CobwebTree:   3%|         | 338/10000 [00:05<02:55, 55.02it/s]Training CobwebTree:   3%|         | 345/10000 [00:05<02:50, 56.63it/s]Training CobwebTree:   4%|         | 351/10000 [00:05<02:49, 56.98it/s]Training CobwebTree:   4%|         | 357/10000 [00:05<02:47, 57.51it/s]Training CobwebTree:   4%|         | 363/10000 [00:05<03:00, 53.46it/s]Training CobwebTree:   4%|         | 369/10000 [00:05<02:55, 54.83it/s]Training CobwebTree:   4%|         | 375/10000 [00:05<02:51, 55.97it/s]Training CobwebTree:   4%|         | 381/10000 [00:05<02:55, 54.69it/s]Training CobwebTree:   4%|         | 387/10000 [00:05<02:52, 55.75it/s]Training CobwebTree:   4%|         | 393/10000 [00:06<03:04, 52.05it/s]Training CobwebTree:   4%|         | 399/10000 [00:06<03:02, 52.52it/s]Training CobwebTree:   4%|         | 405/10000 [00:06<03:01, 52.85it/s]Training CobwebTree:   4%|         | 412/10000 [00:06<02:55, 54.52it/s]Training CobwebTree:   4%|         | 418/10000 [00:06<02:56, 54.33it/s]Training CobwebTree:   4%|         | 424/10000 [00:06<02:58, 53.56it/s]Training CobwebTree:   4%|         | 430/10000 [00:06<02:58, 53.55it/s]Training CobwebTree:   4%|         | 436/10000 [00:06<03:00, 53.07it/s]Training CobwebTree:   4%|         | 442/10000 [00:06<02:54, 54.67it/s]Training CobwebTree:   4%|         | 448/10000 [00:07<03:02, 52.22it/s]Training CobwebTree:   5%|         | 454/10000 [00:07<02:59, 53.33it/s]Training CobwebTree:   5%|         | 460/10000 [00:07<03:01, 52.59it/s]Training CobwebTree:   5%|         | 466/10000 [00:07<03:01, 52.57it/s]Training CobwebTree:   5%|         | 472/10000 [00:07<02:55, 54.17it/s]Training CobwebTree:   5%|         | 478/10000 [00:07<02:51, 55.37it/s]Training CobwebTree:   5%|         | 485/10000 [00:07<02:48, 56.59it/s]Training CobwebTree:   5%|         | 491/10000 [00:07<02:48, 56.44it/s]Training CobwebTree:   5%|         | 497/10000 [00:07<02:46, 57.22it/s]Training CobwebTree:   5%|         | 503/10000 [00:08<02:55, 54.11it/s]Training CobwebTree:   5%|         | 509/10000 [00:08<02:55, 54.13it/s]Training CobwebTree:   5%|         | 516/10000 [00:08<02:44, 57.62it/s]Training CobwebTree:   5%|         | 522/10000 [00:08<02:49, 55.85it/s]Training CobwebTree:   5%|         | 528/10000 [00:08<02:52, 54.98it/s]Training CobwebTree:   5%|         | 534/10000 [00:08<02:55, 54.07it/s]Training CobwebTree:   5%|         | 541/10000 [00:08<02:46, 56.90it/s]Training CobwebTree:   5%|         | 548/10000 [00:08<02:44, 57.53it/s]Training CobwebTree:   6%|         | 554/10000 [00:08<02:48, 56.14it/s]Training CobwebTree:   6%|         | 560/10000 [00:09<02:51, 54.89it/s]Training CobwebTree:   6%|         | 566/10000 [00:09<03:05, 50.81it/s]Training CobwebTree:   6%|         | 572/10000 [00:09<03:06, 50.64it/s]Training CobwebTree:   6%|         | 578/10000 [00:09<03:14, 48.53it/s]Training CobwebTree:   6%|         | 584/10000 [00:09<03:10, 49.33it/s]Training CobwebTree:   6%|         | 590/10000 [00:09<03:03, 51.41it/s]Training CobwebTree:   6%|         | 596/10000 [00:09<03:03, 51.26it/s]Training CobwebTree:   6%|         | 602/10000 [00:09<03:06, 50.29it/s]Training CobwebTree:   6%|         | 608/10000 [00:10<03:10, 49.22it/s]Training CobwebTree:   6%|         | 613/10000 [00:10<03:19, 47.11it/s]Training CobwebTree:   6%|         | 619/10000 [00:10<03:07, 49.90it/s]Training CobwebTree:   6%|         | 625/10000 [00:10<03:16, 47.78it/s]Training CobwebTree:   6%|         | 630/10000 [00:10<03:21, 46.46it/s]Training CobwebTree:   6%|         | 635/10000 [00:10<03:22, 46.16it/s]Training CobwebTree:   6%|         | 640/10000 [00:10<03:25, 45.50it/s]Training CobwebTree:   6%|         | 646/10000 [00:10<03:16, 47.62it/s]Training CobwebTree:   7%|         | 652/10000 [00:10<03:06, 50.07it/s]Training CobwebTree:   7%|         | 658/10000 [00:11<03:02, 51.08it/s]Training CobwebTree:   7%|         | 664/10000 [00:11<03:00, 51.69it/s]Training CobwebTree:   7%|         | 670/10000 [00:11<02:58, 52.22it/s]Training CobwebTree:   7%|         | 676/10000 [00:11<02:54, 53.39it/s]Training CobwebTree:   7%|         | 682/10000 [00:11<02:56, 52.89it/s]Training CobwebTree:   7%|         | 688/10000 [00:11<02:59, 51.83it/s]Training CobwebTree:   7%|         | 694/10000 [00:11<02:57, 52.38it/s]Training CobwebTree:   7%|         | 700/10000 [00:11<02:56, 52.70it/s]Training CobwebTree:   7%|         | 706/10000 [00:11<02:55, 52.98it/s]Training CobwebTree:   7%|         | 712/10000 [00:12<02:52, 53.89it/s]Training CobwebTree:   7%|         | 718/10000 [00:12<02:55, 52.96it/s]Training CobwebTree:   7%|         | 725/10000 [00:12<02:45, 56.19it/s]Training CobwebTree:   7%|         | 731/10000 [00:12<02:57, 52.33it/s]Training CobwebTree:   7%|         | 737/10000 [00:12<02:55, 52.83it/s]Training CobwebTree:   7%|         | 743/10000 [00:12<02:55, 52.62it/s]Training CobwebTree:   7%|         | 749/10000 [00:12<02:58, 51.73it/s]Training CobwebTree:   8%|         | 755/10000 [00:12<03:04, 50.19it/s]Training CobwebTree:   8%|         | 761/10000 [00:13<03:00, 51.07it/s]Training CobwebTree:   8%|         | 767/10000 [00:13<03:01, 50.87it/s]Training CobwebTree:   8%|         | 773/10000 [00:13<02:58, 51.56it/s]Training CobwebTree:   8%|         | 779/10000 [00:13<03:08, 48.94it/s]Training CobwebTree:   8%|         | 785/10000 [00:13<03:06, 49.28it/s]Training CobwebTree:   8%|         | 790/10000 [00:13<03:06, 49.42it/s]Training CobwebTree:   8%|         | 795/10000 [00:13<03:08, 48.95it/s]Training CobwebTree:   8%|         | 801/10000 [00:13<03:05, 49.67it/s]Training CobwebTree:   8%|         | 808/10000 [00:13<02:57, 51.85it/s]Training CobwebTree:   8%|         | 814/10000 [00:14<03:03, 49.94it/s]Training CobwebTree:   8%|         | 820/10000 [00:14<02:59, 51.15it/s]Training CobwebTree:   8%|         | 826/10000 [00:14<02:57, 51.68it/s]Training CobwebTree:   8%|         | 832/10000 [00:14<03:01, 50.48it/s]Training CobwebTree:   8%|         | 838/10000 [00:14<03:00, 50.70it/s]Training CobwebTree:   8%|         | 844/10000 [00:14<03:00, 50.75it/s]Training CobwebTree:   8%|         | 850/10000 [00:14<02:57, 51.52it/s]Training CobwebTree:   9%|         | 856/10000 [00:14<02:59, 50.94it/s]Training CobwebTree:   9%|         | 862/10000 [00:15<03:04, 49.40it/s]Training CobwebTree:   9%|         | 867/10000 [00:15<03:07, 48.62it/s]Training CobwebTree:   9%|         | 873/10000 [00:15<02:58, 51.05it/s]Training CobwebTree:   9%|         | 879/10000 [00:15<03:00, 50.61it/s]Training CobwebTree:   9%|         | 885/10000 [00:15<03:01, 50.26it/s]Training CobwebTree:   9%|         | 891/10000 [00:15<02:59, 50.71it/s]Training CobwebTree:   9%|         | 897/10000 [00:15<03:00, 50.43it/s]Training CobwebTree:   9%|         | 903/10000 [00:15<03:05, 48.92it/s]Training CobwebTree:   9%|         | 908/10000 [00:15<03:06, 48.81it/s]Training CobwebTree:   9%|         | 914/10000 [00:16<03:01, 50.10it/s]Training CobwebTree:   9%|         | 920/10000 [00:16<03:02, 49.85it/s]Training CobwebTree:   9%|         | 926/10000 [00:16<02:57, 51.14it/s]Training CobwebTree:   9%|         | 932/10000 [00:16<03:05, 48.79it/s]Training CobwebTree:   9%|         | 937/10000 [00:16<03:07, 48.25it/s]Training CobwebTree:   9%|         | 942/10000 [00:16<03:06, 48.46it/s]Training CobwebTree:   9%|         | 948/10000 [00:16<03:00, 50.13it/s]Training CobwebTree:  10%|         | 954/10000 [00:16<02:59, 50.34it/s]Training CobwebTree:  10%|         | 960/10000 [00:17<03:00, 49.95it/s]Training CobwebTree:  10%|         | 966/10000 [00:17<02:58, 50.66it/s]Training CobwebTree:  10%|         | 972/10000 [00:17<03:26, 43.80it/s]Training CobwebTree:  10%|         | 977/10000 [00:17<03:20, 44.98it/s]Training CobwebTree:  10%|         | 982/10000 [00:17<03:18, 45.33it/s]Training CobwebTree:  10%|         | 988/10000 [00:17<03:11, 47.05it/s]Training CobwebTree:  10%|         | 994/10000 [00:17<03:02, 49.23it/s]Training CobwebTree:  10%|         | 1000/10000 [00:17<02:56, 51.13it/s]Training CobwebTree:  10%|         | 1006/10000 [00:18<03:03, 49.00it/s]Training CobwebTree:  10%|         | 1011/10000 [00:18<03:11, 46.83it/s]Training CobwebTree:  10%|         | 1016/10000 [00:18<03:10, 47.08it/s]Training CobwebTree:  10%|         | 1022/10000 [00:18<03:06, 48.03it/s]Training CobwebTree:  10%|         | 1027/10000 [00:18<03:04, 48.53it/s]Training CobwebTree:  10%|         | 1032/10000 [00:18<03:05, 48.41it/s]Training CobwebTree:  10%|         | 1038/10000 [00:18<02:55, 51.04it/s]Training CobwebTree:  10%|         | 1044/10000 [00:18<02:48, 53.23it/s]Training CobwebTree:  10%|         | 1050/10000 [00:18<02:59, 49.94it/s]Training CobwebTree:  11%|         | 1056/10000 [00:19<02:56, 50.69it/s]Training CobwebTree:  11%|         | 1062/10000 [00:19<03:00, 49.63it/s]Training CobwebTree:  11%|         | 1068/10000 [00:19<02:57, 50.20it/s]Training CobwebTree:  11%|         | 1074/10000 [00:19<03:02, 49.00it/s]Training CobwebTree:  11%|         | 1079/10000 [00:19<03:01, 49.06it/s]Training CobwebTree:  11%|         | 1085/10000 [00:19<02:57, 50.26it/s]Training CobwebTree:  11%|         | 1091/10000 [00:19<03:01, 49.00it/s]Training CobwebTree:  11%|         | 1097/10000 [00:19<02:56, 50.40it/s]Training CobwebTree:  11%|         | 1103/10000 [00:19<03:05, 47.99it/s]Training CobwebTree:  11%|         | 1109/10000 [00:20<02:59, 49.59it/s]Training CobwebTree:  11%|         | 1114/10000 [00:20<03:04, 48.05it/s]Training CobwebTree:  11%|         | 1119/10000 [00:20<03:08, 46.99it/s]Training CobwebTree:  11%|        | 1125/10000 [00:20<03:01, 48.88it/s]Training CobwebTree:  11%|        | 1131/10000 [00:20<02:51, 51.63it/s]Training CobwebTree:  11%|        | 1137/10000 [00:20<02:52, 51.32it/s]Training CobwebTree:  11%|        | 1143/10000 [00:20<02:52, 51.28it/s]Training CobwebTree:  11%|        | 1149/10000 [00:20<02:50, 51.90it/s]Training CobwebTree:  12%|        | 1155/10000 [00:20<02:52, 51.40it/s]Training CobwebTree:  12%|        | 1161/10000 [00:21<02:49, 52.14it/s]Training CobwebTree:  12%|        | 1167/10000 [00:21<02:45, 53.45it/s]Training CobwebTree:  12%|        | 1173/10000 [00:21<02:49, 52.01it/s]Training CobwebTree:  12%|        | 1179/10000 [00:21<02:59, 49.18it/s]Training CobwebTree:  12%|        | 1184/10000 [00:21<03:01, 48.44it/s]Training CobwebTree:  12%|        | 1189/10000 [00:21<03:01, 48.43it/s]Training CobwebTree:  12%|        | 1194/10000 [00:21<03:15, 45.13it/s]Training CobwebTree:  12%|        | 1199/10000 [00:21<03:09, 46.39it/s]Training CobwebTree:  12%|        | 1205/10000 [00:22<03:02, 48.28it/s]Training CobwebTree:  12%|        | 1210/10000 [00:22<03:06, 47.22it/s]Training CobwebTree:  12%|        | 1215/10000 [00:22<03:15, 44.93it/s]Training CobwebTree:  12%|        | 1220/10000 [00:22<03:13, 45.49it/s]Training CobwebTree:  12%|        | 1225/10000 [00:22<03:11, 45.94it/s]Training CobwebTree:  12%|        | 1230/10000 [00:22<03:15, 44.94it/s]Training CobwebTree:  12%|        | 1235/10000 [00:22<03:10, 45.95it/s]Training CobwebTree:  12%|        | 1240/10000 [00:22<03:08, 46.55it/s]Training CobwebTree:  12%|        | 1245/10000 [00:22<03:12, 45.40it/s]Training CobwebTree:  12%|        | 1250/10000 [00:23<03:19, 43.87it/s]Training CobwebTree:  13%|        | 1256/10000 [00:23<03:11, 45.57it/s]Training CobwebTree:  13%|        | 1261/10000 [00:23<03:13, 45.05it/s]Training CobwebTree:  13%|        | 1266/10000 [00:23<03:09, 46.12it/s]Training CobwebTree:  13%|        | 1271/10000 [00:23<03:11, 45.66it/s]Training CobwebTree:  13%|        | 1276/10000 [00:23<03:10, 45.71it/s]Training CobwebTree:  13%|        | 1282/10000 [00:23<03:03, 47.63it/s]Training CobwebTree:  13%|        | 1288/10000 [00:23<02:55, 49.53it/s]Training CobwebTree:  13%|        | 1293/10000 [00:23<02:59, 48.47it/s]Training CobwebTree:  13%|        | 1299/10000 [00:24<02:58, 48.67it/s]Training CobwebTree:  13%|        | 1305/10000 [00:24<02:55, 49.53it/s]Training CobwebTree:  13%|        | 1310/10000 [00:24<02:58, 48.70it/s]Training CobwebTree:  13%|        | 1315/10000 [00:24<03:09, 45.94it/s]Training CobwebTree:  13%|        | 1321/10000 [00:24<03:05, 46.69it/s]Training CobwebTree:  13%|        | 1326/10000 [00:24<03:08, 46.13it/s]Training CobwebTree:  13%|        | 1331/10000 [00:24<03:08, 45.93it/s]Training CobwebTree:  13%|        | 1336/10000 [00:24<03:09, 45.70it/s]Training CobwebTree:  13%|        | 1341/10000 [00:24<03:05, 46.58it/s]Training CobwebTree:  13%|        | 1346/10000 [00:25<03:02, 47.31it/s]Training CobwebTree:  14%|        | 1351/10000 [00:25<03:02, 47.46it/s]Training CobwebTree:  14%|        | 1356/10000 [00:25<03:02, 47.42it/s]Training CobwebTree:  14%|        | 1361/10000 [00:25<03:04, 46.94it/s]Training CobwebTree:  14%|        | 1366/10000 [00:25<03:15, 44.19it/s]Training CobwebTree:  14%|        | 1371/10000 [00:25<03:10, 45.20it/s]Training CobwebTree:  14%|        | 1376/10000 [00:25<03:20, 42.91it/s]Training CobwebTree:  14%|        | 1381/10000 [00:25<03:20, 42.89it/s]Training CobwebTree:  14%|        | 1386/10000 [00:25<03:26, 41.75it/s]Training CobwebTree:  14%|        | 1392/10000 [00:26<03:15, 44.07it/s]Training CobwebTree:  14%|        | 1397/10000 [00:26<03:15, 43.92it/s]Training CobwebTree:  14%|        | 1402/10000 [00:26<03:19, 43.12it/s]Training CobwebTree:  14%|        | 1407/10000 [00:26<03:21, 42.64it/s]Training CobwebTree:  14%|        | 1412/10000 [00:26<03:19, 43.12it/s]Training CobwebTree:  14%|        | 1417/10000 [00:26<03:13, 44.45it/s]Training CobwebTree:  14%|        | 1422/10000 [00:26<03:27, 41.38it/s]Training CobwebTree:  14%|        | 1427/10000 [00:26<03:19, 43.08it/s]Training CobwebTree:  14%|        | 1432/10000 [00:27<03:23, 42.13it/s]Training CobwebTree:  14%|        | 1437/10000 [00:27<03:17, 43.30it/s]Training CobwebTree:  14%|        | 1442/10000 [00:27<03:18, 43.18it/s]Training CobwebTree:  14%|        | 1447/10000 [00:27<03:20, 42.73it/s]Training CobwebTree:  15%|        | 1452/10000 [00:27<03:21, 42.42it/s]Training CobwebTree:  15%|        | 1457/10000 [00:27<03:18, 43.02it/s]Training CobwebTree:  15%|        | 1462/10000 [00:27<03:22, 42.09it/s]Training CobwebTree:  15%|        | 1467/10000 [00:27<03:16, 43.44it/s]Training CobwebTree:  15%|        | 1472/10000 [00:27<03:15, 43.68it/s]Training CobwebTree:  15%|        | 1478/10000 [00:28<03:01, 46.95it/s]Training CobwebTree:  15%|        | 1483/10000 [00:28<03:07, 45.54it/s]Training CobwebTree:  15%|        | 1488/10000 [00:28<03:09, 44.90it/s]Training CobwebTree:  15%|        | 1494/10000 [00:28<03:03, 46.36it/s]Training CobwebTree:  15%|        | 1499/10000 [00:28<03:03, 46.24it/s]Training CobwebTree:  15%|        | 1504/10000 [00:28<03:09, 44.91it/s]Training CobwebTree:  15%|        | 1509/10000 [00:28<03:08, 45.03it/s]Training CobwebTree:  15%|        | 1514/10000 [00:28<03:13, 43.80it/s]Training CobwebTree:  15%|        | 1519/10000 [00:29<03:08, 44.92it/s]Training CobwebTree:  15%|        | 1524/10000 [00:29<03:07, 45.17it/s]Training CobwebTree:  15%|        | 1529/10000 [00:29<03:12, 44.01it/s]Training CobwebTree:  15%|        | 1534/10000 [00:29<03:13, 43.77it/s]Training CobwebTree:  15%|        | 1539/10000 [00:29<03:13, 43.69it/s]Training CobwebTree:  15%|        | 1544/10000 [00:29<03:14, 43.53it/s]Training CobwebTree:  15%|        | 1549/10000 [00:29<03:10, 44.34it/s]Training CobwebTree:  16%|        | 1554/10000 [00:29<03:13, 43.74it/s]Training CobwebTree:  16%|        | 1559/10000 [00:29<03:24, 41.18it/s]Training CobwebTree:  16%|        | 1564/10000 [00:30<03:16, 42.85it/s]Training CobwebTree:  16%|        | 1569/10000 [00:30<03:11, 43.97it/s]Training CobwebTree:  16%|        | 1574/10000 [00:30<03:07, 44.94it/s]Training CobwebTree:  16%|        | 1579/10000 [00:30<03:08, 44.78it/s]Training CobwebTree:  16%|        | 1584/10000 [00:30<03:08, 44.54it/s]Training CobwebTree:  16%|        | 1589/10000 [00:30<03:06, 45.17it/s]Training CobwebTree:  16%|        | 1595/10000 [00:30<03:02, 46.13it/s]Training CobwebTree:  16%|        | 1600/10000 [00:30<03:06, 45.11it/s]Training CobwebTree:  16%|        | 1605/10000 [00:30<03:05, 45.16it/s]Training CobwebTree:  16%|        | 1610/10000 [00:31<03:19, 42.06it/s]Training CobwebTree:  16%|        | 1615/10000 [00:31<03:18, 42.32it/s]Training CobwebTree:  16%|        | 1620/10000 [00:31<03:21, 41.52it/s]Training CobwebTree:  16%|        | 1626/10000 [00:31<03:10, 44.00it/s]Training CobwebTree:  16%|        | 1631/10000 [00:31<03:12, 43.53it/s]Training CobwebTree:  16%|        | 1636/10000 [00:31<03:14, 42.92it/s]Training CobwebTree:  16%|        | 1641/10000 [00:31<03:10, 43.95it/s]Training CobwebTree:  16%|        | 1647/10000 [00:31<02:56, 47.20it/s]Training CobwebTree:  17%|        | 1652/10000 [00:32<02:55, 47.63it/s]Training CobwebTree:  17%|        | 1657/10000 [00:32<03:04, 45.16it/s]Training CobwebTree:  17%|        | 1662/10000 [00:32<03:08, 44.22it/s]Training CobwebTree:  17%|        | 1667/10000 [00:32<03:18, 42.01it/s]Training CobwebTree:  17%|        | 1672/10000 [00:32<03:23, 40.98it/s]Training CobwebTree:  17%|        | 1677/10000 [00:32<03:18, 41.96it/s]Training CobwebTree:  17%|        | 1682/10000 [00:32<03:14, 42.67it/s]Training CobwebTree:  17%|        | 1687/10000 [00:32<03:13, 42.94it/s]Training CobwebTree:  17%|        | 1692/10000 [00:32<03:13, 42.95it/s]Training CobwebTree:  17%|        | 1697/10000 [00:33<03:07, 44.35it/s]Training CobwebTree:  17%|        | 1702/10000 [00:33<03:15, 42.35it/s]Training CobwebTree:  17%|        | 1707/10000 [00:33<03:17, 41.92it/s]Training CobwebTree:  17%|        | 1712/10000 [00:33<03:16, 42.27it/s]Training CobwebTree:  17%|        | 1717/10000 [00:33<03:22, 40.81it/s]Training CobwebTree:  17%|        | 1722/10000 [00:33<03:16, 42.08it/s]Training CobwebTree:  17%|        | 1727/10000 [00:33<03:09, 43.55it/s]Training CobwebTree:  17%|        | 1732/10000 [00:33<03:08, 43.91it/s]Training CobwebTree:  17%|        | 1737/10000 [00:34<03:24, 40.45it/s]Training CobwebTree:  17%|        | 1742/10000 [00:34<03:26, 39.96it/s]Training CobwebTree:  17%|        | 1747/10000 [00:34<03:18, 41.67it/s]Training CobwebTree:  18%|        | 1752/10000 [00:34<03:21, 40.99it/s]Training CobwebTree:  18%|        | 1757/10000 [00:34<03:22, 40.80it/s]Training CobwebTree:  18%|        | 1762/10000 [00:34<03:18, 41.52it/s]Training CobwebTree:  18%|        | 1767/10000 [00:34<03:14, 42.30it/s]Training CobwebTree:  18%|        | 1772/10000 [00:34<03:13, 42.52it/s]Training CobwebTree:  18%|        | 1777/10000 [00:35<03:15, 42.10it/s]Training CobwebTree:  18%|        | 1782/10000 [00:35<03:15, 42.13it/s]Training CobwebTree:  18%|        | 1787/10000 [00:35<03:15, 42.06it/s]Training CobwebTree:  18%|        | 1792/10000 [00:35<03:11, 42.86it/s]Training CobwebTree:  18%|        | 1797/10000 [00:35<03:14, 42.19it/s]Training CobwebTree:  18%|        | 1802/10000 [00:35<03:20, 40.92it/s]Training CobwebTree:  18%|        | 1808/10000 [00:35<03:07, 43.61it/s]Training CobwebTree:  18%|        | 1813/10000 [00:35<03:04, 44.40it/s]Training CobwebTree:  18%|        | 1818/10000 [00:35<03:06, 43.85it/s]Training CobwebTree:  18%|        | 1823/10000 [00:36<03:11, 42.77it/s]Training CobwebTree:  18%|        | 1829/10000 [00:36<02:58, 45.88it/s]Training CobwebTree:  18%|        | 1834/10000 [00:36<02:56, 46.39it/s]Training CobwebTree:  18%|        | 1839/10000 [00:36<02:53, 46.92it/s]Training CobwebTree:  18%|        | 1845/10000 [00:36<02:49, 48.08it/s]Training CobwebTree:  18%|        | 1850/10000 [00:36<02:51, 47.46it/s]Training CobwebTree:  19%|        | 1855/10000 [00:36<02:50, 47.76it/s]Training CobwebTree:  19%|        | 1860/10000 [00:36<02:53, 47.05it/s]Training CobwebTree:  19%|        | 1865/10000 [00:36<03:02, 44.69it/s]Training CobwebTree:  19%|        | 1870/10000 [00:37<03:07, 43.34it/s]Training CobwebTree:  19%|        | 1876/10000 [00:37<03:00, 44.94it/s]Training CobwebTree:  19%|        | 1882/10000 [00:37<02:55, 46.24it/s]Training CobwebTree:  19%|        | 1888/10000 [00:37<02:49, 47.76it/s]Training CobwebTree:  19%|        | 1894/10000 [00:37<02:42, 49.77it/s]Training CobwebTree:  19%|        | 1899/10000 [00:37<02:53, 46.80it/s]Training CobwebTree:  19%|        | 1904/10000 [00:37<02:59, 45.16it/s]Training CobwebTree:  19%|        | 1909/10000 [00:37<03:04, 43.94it/s]Training CobwebTree:  19%|        | 1914/10000 [00:38<03:08, 42.82it/s]Training CobwebTree:  19%|        | 1919/10000 [00:38<03:08, 42.96it/s]Training CobwebTree:  19%|        | 1924/10000 [00:38<03:11, 42.17it/s]Training CobwebTree:  19%|        | 1929/10000 [00:38<03:08, 42.76it/s]Training CobwebTree:  19%|        | 1934/10000 [00:38<03:08, 42.85it/s]Training CobwebTree:  19%|        | 1939/10000 [00:38<03:16, 40.97it/s]Training CobwebTree:  19%|        | 1944/10000 [00:38<03:11, 42.03it/s]Training CobwebTree:  19%|        | 1949/10000 [00:38<03:13, 41.68it/s]Training CobwebTree:  20%|        | 1954/10000 [00:39<03:14, 41.46it/s]Training CobwebTree:  20%|        | 1959/10000 [00:39<03:19, 40.23it/s]Training CobwebTree:  20%|        | 1964/10000 [00:39<03:17, 40.74it/s]Training CobwebTree:  20%|        | 1969/10000 [00:39<03:12, 41.82it/s]Training CobwebTree:  20%|        | 1974/10000 [00:39<03:13, 41.44it/s]Training CobwebTree:  20%|        | 1979/10000 [00:39<03:14, 41.29it/s]Training CobwebTree:  20%|        | 1984/10000 [00:39<03:27, 38.61it/s]Training CobwebTree:  20%|        | 1989/10000 [00:39<03:19, 40.24it/s]Training CobwebTree:  20%|        | 1994/10000 [00:39<03:08, 42.47it/s]Training CobwebTree:  20%|        | 1999/10000 [00:40<03:09, 42.14it/s]Training CobwebTree:  20%|        | 2004/10000 [00:40<03:02, 43.86it/s]Training CobwebTree:  20%|        | 2009/10000 [00:40<03:06, 42.88it/s]Training CobwebTree:  20%|        | 2014/10000 [00:40<03:07, 42.50it/s]Training CobwebTree:  20%|        | 2019/10000 [00:40<03:05, 42.94it/s]Training CobwebTree:  20%|        | 2024/10000 [00:40<03:08, 42.29it/s]Training CobwebTree:  20%|        | 2030/10000 [00:40<02:58, 44.57it/s]Training CobwebTree:  20%|        | 2035/10000 [00:40<03:04, 43.17it/s]Training CobwebTree:  20%|        | 2040/10000 [00:41<03:03, 43.30it/s]Training CobwebTree:  20%|        | 2045/10000 [00:41<03:05, 42.84it/s]Training CobwebTree:  20%|        | 2050/10000 [00:41<03:03, 43.37it/s]Training CobwebTree:  21%|        | 2055/10000 [00:41<03:01, 43.85it/s]Training CobwebTree:  21%|        | 2060/10000 [00:41<03:03, 43.25it/s]Training CobwebTree:  21%|        | 2065/10000 [00:41<03:00, 43.99it/s]Training CobwebTree:  21%|        | 2070/10000 [00:41<02:56, 44.93it/s]Training CobwebTree:  21%|        | 2075/10000 [00:41<03:04, 43.01it/s]Training CobwebTree:  21%|        | 2080/10000 [00:41<02:58, 44.49it/s]Training CobwebTree:  21%|        | 2085/10000 [00:42<02:57, 44.69it/s]Training CobwebTree:  21%|        | 2090/10000 [00:42<02:58, 44.31it/s]Training CobwebTree:  21%|        | 2095/10000 [00:42<03:03, 43.02it/s]Training CobwebTree:  21%|        | 2100/10000 [00:42<03:08, 41.90it/s]Training CobwebTree:  21%|        | 2106/10000 [00:42<02:54, 45.17it/s]Training CobwebTree:  21%|        | 2111/10000 [00:42<03:09, 41.54it/s]Training CobwebTree:  21%|        | 2116/10000 [00:42<03:01, 43.42it/s]Training CobwebTree:  21%|        | 2121/10000 [00:42<02:58, 44.03it/s]Training CobwebTree:  21%|       | 2126/10000 [00:43<02:56, 44.69it/s]Training CobwebTree:  21%|       | 2131/10000 [00:43<02:55, 44.83it/s]Training CobwebTree:  21%|       | 2136/10000 [00:43<03:04, 42.58it/s]Training CobwebTree:  21%|       | 2141/10000 [00:43<03:05, 42.33it/s]Training CobwebTree:  21%|       | 2146/10000 [00:43<03:08, 41.62it/s]Training CobwebTree:  22%|       | 2151/10000 [00:43<03:16, 39.91it/s]Training CobwebTree:  22%|       | 2156/10000 [00:43<03:20, 39.14it/s]Training CobwebTree:  22%|       | 2161/10000 [00:43<03:13, 40.46it/s]Training CobwebTree:  22%|       | 2166/10000 [00:44<03:17, 39.64it/s]Training CobwebTree:  22%|       | 2170/10000 [00:44<03:18, 39.39it/s]Training CobwebTree:  22%|       | 2175/10000 [00:44<03:14, 40.31it/s]Training CobwebTree:  22%|       | 2180/10000 [00:44<03:07, 41.75it/s]Training CobwebTree:  22%|       | 2185/10000 [00:44<03:04, 42.37it/s]Training CobwebTree:  22%|       | 2190/10000 [00:44<03:07, 41.66it/s]Training CobwebTree:  22%|       | 2195/10000 [00:44<03:12, 40.50it/s]Training CobwebTree:  22%|       | 2200/10000 [00:44<03:09, 41.10it/s]Training CobwebTree:  22%|       | 2205/10000 [00:44<03:11, 40.61it/s]Training CobwebTree:  22%|       | 2210/10000 [00:45<03:15, 39.82it/s]Training CobwebTree:  22%|       | 2214/10000 [00:45<03:16, 39.64it/s]Training CobwebTree:  22%|       | 2218/10000 [00:45<03:16, 39.58it/s]Training CobwebTree:  22%|       | 2223/10000 [00:45<03:08, 41.33it/s]Training CobwebTree:  22%|       | 2229/10000 [00:45<02:55, 44.36it/s]Training CobwebTree:  22%|       | 2234/10000 [00:45<02:59, 43.15it/s]Training CobwebTree:  22%|       | 2239/10000 [00:45<02:53, 44.75it/s]Training CobwebTree:  22%|       | 2244/10000 [00:45<03:04, 41.97it/s]Training CobwebTree:  22%|       | 2249/10000 [00:45<03:05, 41.76it/s]Training CobwebTree:  23%|       | 2254/10000 [00:46<03:11, 40.54it/s]Training CobwebTree:  23%|       | 2259/10000 [00:46<03:14, 39.75it/s]Training CobwebTree:  23%|       | 2264/10000 [00:46<03:05, 41.67it/s]Training CobwebTree:  23%|       | 2269/10000 [00:46<02:57, 43.68it/s]Training CobwebTree:  23%|       | 2274/10000 [00:46<02:56, 43.65it/s]Training CobwebTree:  23%|       | 2279/10000 [00:46<03:02, 42.23it/s]Training CobwebTree:  23%|       | 2284/10000 [00:46<03:03, 42.01it/s]Training CobwebTree:  23%|       | 2289/10000 [00:46<03:10, 40.55it/s]Training CobwebTree:  23%|       | 2294/10000 [00:47<03:04, 41.68it/s]Training CobwebTree:  23%|       | 2299/10000 [00:47<03:03, 41.95it/s]Training CobwebTree:  23%|       | 2304/10000 [00:47<03:01, 42.43it/s]Training CobwebTree:  23%|       | 2309/10000 [00:47<02:55, 43.88it/s]Training CobwebTree:  23%|       | 2314/10000 [00:47<02:55, 43.92it/s]Training CobwebTree:  23%|       | 2319/10000 [00:47<02:53, 44.39it/s]Training CobwebTree:  23%|       | 2324/10000 [00:47<02:56, 43.57it/s]Training CobwebTree:  23%|       | 2330/10000 [00:47<02:52, 44.50it/s]Training CobwebTree:  23%|       | 2335/10000 [00:47<02:50, 44.90it/s]Training CobwebTree:  23%|       | 2340/10000 [00:48<02:51, 44.55it/s]Training CobwebTree:  23%|       | 2345/10000 [00:48<02:48, 45.49it/s]Training CobwebTree:  24%|       | 2352/10000 [00:48<02:27, 51.83it/s]Training CobwebTree:  24%|       | 2358/10000 [00:48<02:27, 51.66it/s]Training CobwebTree:  24%|       | 2364/10000 [00:48<02:32, 50.23it/s]Training CobwebTree:  24%|       | 2370/10000 [00:48<02:33, 49.55it/s]Training CobwebTree:  24%|       | 2375/10000 [00:48<02:49, 45.01it/s]Training CobwebTree:  24%|       | 2380/10000 [00:48<02:54, 43.68it/s]Training CobwebTree:  24%|       | 2385/10000 [00:49<02:52, 44.09it/s]Training CobwebTree:  24%|       | 2390/10000 [00:49<02:56, 43.03it/s]Training CobwebTree:  24%|       | 2395/10000 [00:49<02:53, 43.77it/s]Training CobwebTree:  24%|       | 2400/10000 [00:49<02:50, 44.65it/s]Training CobwebTree:  24%|       | 2405/10000 [00:49<02:49, 44.90it/s]Training CobwebTree:  24%|       | 2410/10000 [00:49<02:48, 45.00it/s]Training CobwebTree:  24%|       | 2415/10000 [00:49<02:54, 43.48it/s]Training CobwebTree:  24%|       | 2420/10000 [00:49<02:51, 44.17it/s]Training CobwebTree:  24%|       | 2425/10000 [00:49<02:51, 44.27it/s]Training CobwebTree:  24%|       | 2430/10000 [00:50<02:52, 43.92it/s]Training CobwebTree:  24%|       | 2435/10000 [00:50<02:49, 44.68it/s]Training CobwebTree:  24%|       | 2440/10000 [00:50<02:46, 45.54it/s]Training CobwebTree:  24%|       | 2445/10000 [00:50<02:49, 44.61it/s]Training CobwebTree:  24%|       | 2450/10000 [00:50<02:55, 42.96it/s]Training CobwebTree:  25%|       | 2455/10000 [00:50<02:50, 44.30it/s]Training CobwebTree:  25%|       | 2460/10000 [00:50<03:00, 41.72it/s]Training CobwebTree:  25%|       | 2465/10000 [00:50<02:52, 43.61it/s]Training CobwebTree:  25%|       | 2470/10000 [00:50<02:55, 42.90it/s]Training CobwebTree:  25%|       | 2475/10000 [00:51<02:57, 42.28it/s]Training CobwebTree:  25%|       | 2480/10000 [00:51<02:55, 42.86it/s]Training CobwebTree:  25%|       | 2485/10000 [00:51<02:54, 43.06it/s]Training CobwebTree:  25%|       | 2490/10000 [00:51<03:01, 41.34it/s]Training CobwebTree:  25%|       | 2495/10000 [00:51<02:56, 42.48it/s]Training CobwebTree:  25%|       | 2500/10000 [00:51<03:14, 38.65it/s]Training CobwebTree:  25%|       | 2505/10000 [00:51<03:01, 41.39it/s]Training CobwebTree:  25%|       | 2510/10000 [00:51<03:02, 41.10it/s]Training CobwebTree:  25%|       | 2515/10000 [00:52<02:53, 43.15it/s]Training CobwebTree:  25%|       | 2520/10000 [00:52<02:53, 43.17it/s]Training CobwebTree:  25%|       | 2525/10000 [00:52<02:47, 44.61it/s]Training CobwebTree:  25%|       | 2530/10000 [00:52<02:53, 43.11it/s]Training CobwebTree:  25%|       | 2535/10000 [00:52<02:51, 43.51it/s]Training CobwebTree:  25%|       | 2540/10000 [00:52<02:58, 41.89it/s]Training CobwebTree:  25%|       | 2545/10000 [00:52<03:02, 40.93it/s]Training CobwebTree:  26%|       | 2550/10000 [00:52<03:00, 41.29it/s]Training CobwebTree:  26%|       | 2555/10000 [00:53<03:05, 40.09it/s]Training CobwebTree:  26%|       | 2560/10000 [00:53<03:03, 40.49it/s]Training CobwebTree:  26%|       | 2565/10000 [00:53<02:56, 42.13it/s]Training CobwebTree:  26%|       | 2570/10000 [00:53<02:58, 41.56it/s]Training CobwebTree:  26%|       | 2575/10000 [00:53<03:01, 40.94it/s]Training CobwebTree:  26%|       | 2580/10000 [00:53<03:02, 40.73it/s]Training CobwebTree:  26%|       | 2585/10000 [00:53<03:05, 39.92it/s]Training CobwebTree:  26%|       | 2590/10000 [00:53<03:04, 40.24it/s]Training CobwebTree:  26%|       | 2595/10000 [00:54<03:00, 41.10it/s]Training CobwebTree:  26%|       | 2600/10000 [00:54<02:55, 42.05it/s]Training CobwebTree:  26%|       | 2605/10000 [00:54<02:55, 42.14it/s]Training CobwebTree:  26%|       | 2610/10000 [00:54<03:01, 40.63it/s]Training CobwebTree:  26%|       | 2615/10000 [00:54<02:53, 42.46it/s]Training CobwebTree:  26%|       | 2620/10000 [00:54<02:51, 43.15it/s]Training CobwebTree:  26%|       | 2625/10000 [00:54<02:54, 42.24it/s]Training CobwebTree:  26%|       | 2630/10000 [00:54<02:55, 41.91it/s]Training CobwebTree:  26%|       | 2635/10000 [00:54<02:55, 41.96it/s]Training CobwebTree:  26%|       | 2640/10000 [00:55<02:53, 42.37it/s]Training CobwebTree:  26%|       | 2645/10000 [00:55<02:56, 41.79it/s]Training CobwebTree:  26%|       | 2650/10000 [00:55<02:49, 43.24it/s]Training CobwebTree:  27%|       | 2655/10000 [00:55<02:45, 44.51it/s]Training CobwebTree:  27%|       | 2660/10000 [00:55<02:46, 44.02it/s]Training CobwebTree:  27%|       | 2665/10000 [00:55<02:49, 43.24it/s]Training CobwebTree:  27%|       | 2670/10000 [00:55<02:53, 42.34it/s]Training CobwebTree:  27%|       | 2675/10000 [00:55<02:52, 42.43it/s]Training CobwebTree:  27%|       | 2680/10000 [00:56<02:55, 41.64it/s]Training CobwebTree:  27%|       | 2685/10000 [00:56<02:59, 40.69it/s]Training CobwebTree:  27%|       | 2690/10000 [00:56<02:54, 41.86it/s]Training CobwebTree:  27%|       | 2695/10000 [00:56<02:52, 42.38it/s]Training CobwebTree:  27%|       | 2700/10000 [00:56<03:00, 40.36it/s]Training CobwebTree:  27%|       | 2705/10000 [00:56<02:56, 41.33it/s]Training CobwebTree:  27%|       | 2710/10000 [00:56<02:54, 41.70it/s]Training CobwebTree:  27%|       | 2715/10000 [00:56<02:53, 41.91it/s]Training CobwebTree:  27%|       | 2720/10000 [00:56<03:02, 39.99it/s]Training CobwebTree:  27%|       | 2725/10000 [00:57<02:58, 40.75it/s]Training CobwebTree:  27%|       | 2730/10000 [00:57<02:50, 42.58it/s]Training CobwebTree:  27%|       | 2735/10000 [00:57<02:48, 43.13it/s]Training CobwebTree:  27%|       | 2740/10000 [00:57<02:47, 43.46it/s]Training CobwebTree:  27%|       | 2745/10000 [00:57<02:48, 42.95it/s]Training CobwebTree:  28%|       | 2750/10000 [00:57<02:51, 42.33it/s]Training CobwebTree:  28%|       | 2755/10000 [00:57<02:56, 41.02it/s]Training CobwebTree:  28%|       | 2760/10000 [00:57<03:21, 35.85it/s]Training CobwebTree:  28%|       | 2764/10000 [00:58<03:18, 36.38it/s]Training CobwebTree:  28%|       | 2769/10000 [00:58<03:08, 38.29it/s]Training CobwebTree:  28%|       | 2774/10000 [00:58<03:03, 39.47it/s]Training CobwebTree:  28%|       | 2779/10000 [00:58<03:03, 39.43it/s]Training CobwebTree:  28%|       | 2783/10000 [00:58<03:09, 38.09it/s]Training CobwebTree:  28%|       | 2788/10000 [00:58<03:03, 39.34it/s]Training CobwebTree:  28%|       | 2793/10000 [00:58<02:58, 40.27it/s]Training CobwebTree:  28%|       | 2798/10000 [00:58<02:55, 41.09it/s]Training CobwebTree:  28%|       | 2803/10000 [00:59<02:56, 40.74it/s]Training CobwebTree:  28%|       | 2808/10000 [00:59<02:49, 42.39it/s]Training CobwebTree:  28%|       | 2813/10000 [00:59<02:44, 43.63it/s]Training CobwebTree:  28%|       | 2818/10000 [00:59<02:44, 43.78it/s]Training CobwebTree:  28%|       | 2823/10000 [00:59<02:41, 44.50it/s]Training CobwebTree:  28%|       | 2828/10000 [00:59<02:54, 41.11it/s]Training CobwebTree:  28%|       | 2833/10000 [00:59<02:58, 40.08it/s]Training CobwebTree:  28%|       | 2838/10000 [00:59<02:53, 41.29it/s]Training CobwebTree:  28%|       | 2843/10000 [00:59<02:50, 41.94it/s]Training CobwebTree:  28%|       | 2848/10000 [01:00<02:47, 42.67it/s]Training CobwebTree:  29%|       | 2853/10000 [01:00<02:47, 42.72it/s]Training CobwebTree:  29%|       | 2858/10000 [01:00<02:48, 42.51it/s]Training CobwebTree:  29%|       | 2863/10000 [01:00<02:56, 40.33it/s]Training CobwebTree:  29%|       | 2868/10000 [01:00<02:58, 40.06it/s]Training CobwebTree:  29%|       | 2873/10000 [01:00<02:57, 40.27it/s]Training CobwebTree:  29%|       | 2879/10000 [01:00<02:45, 43.02it/s]Training CobwebTree:  29%|       | 2884/10000 [01:00<02:43, 43.49it/s]Training CobwebTree:  29%|       | 2889/10000 [01:01<02:46, 42.58it/s]Training CobwebTree:  29%|       | 2894/10000 [01:01<02:45, 42.90it/s]Training CobwebTree:  29%|       | 2899/10000 [01:01<02:51, 41.37it/s]Training CobwebTree:  29%|       | 2904/10000 [01:01<02:53, 40.99it/s]Training CobwebTree:  29%|       | 2909/10000 [01:01<02:49, 41.93it/s]Training CobwebTree:  29%|       | 2914/10000 [01:01<02:55, 40.48it/s]Training CobwebTree:  29%|       | 2919/10000 [01:01<02:52, 41.12it/s]Training CobwebTree:  29%|       | 2924/10000 [01:01<02:45, 42.64it/s]Training CobwebTree:  29%|       | 2929/10000 [01:02<02:51, 41.22it/s]Training CobwebTree:  29%|       | 2934/10000 [01:02<02:48, 41.96it/s]Training CobwebTree:  29%|       | 2939/10000 [01:02<02:48, 41.84it/s]Training CobwebTree:  29%|       | 2944/10000 [01:02<02:52, 40.94it/s]Training CobwebTree:  29%|       | 2949/10000 [01:02<02:48, 41.74it/s]Training CobwebTree:  30%|       | 2954/10000 [01:02<02:45, 42.45it/s]Training CobwebTree:  30%|       | 2959/10000 [01:02<02:43, 43.04it/s]Training CobwebTree:  30%|       | 2964/10000 [01:02<02:40, 43.92it/s]Training CobwebTree:  30%|       | 2969/10000 [01:02<02:34, 45.43it/s]Training CobwebTree:  30%|       | 2974/10000 [01:03<02:34, 45.45it/s]Training CobwebTree:  30%|       | 2979/10000 [01:03<02:37, 44.64it/s]Training CobwebTree:  30%|       | 2984/10000 [01:03<02:38, 44.19it/s]Training CobwebTree:  30%|       | 2989/10000 [01:03<02:40, 43.61it/s]Training CobwebTree:  30%|       | 2994/10000 [01:03<02:46, 42.17it/s]Training CobwebTree:  30%|       | 2999/10000 [01:03<02:46, 41.96it/s]Training CobwebTree:  30%|       | 3005/10000 [01:03<02:36, 44.81it/s]Training CobwebTree:  30%|       | 3010/10000 [01:03<02:42, 43.06it/s]Training CobwebTree:  30%|       | 3015/10000 [01:04<02:46, 41.95it/s]Training CobwebTree:  30%|       | 3020/10000 [01:04<02:53, 40.16it/s]Training CobwebTree:  30%|       | 3025/10000 [01:04<02:53, 40.26it/s]Training CobwebTree:  30%|       | 3030/10000 [01:04<02:53, 40.10it/s]Training CobwebTree:  30%|       | 3035/10000 [01:04<02:47, 41.48it/s]Training CobwebTree:  30%|       | 3040/10000 [01:04<02:53, 40.06it/s]Training CobwebTree:  30%|       | 3045/10000 [01:04<02:49, 40.94it/s]Training CobwebTree:  30%|       | 3050/10000 [01:04<02:52, 40.31it/s]Training CobwebTree:  31%|       | 3055/10000 [01:05<03:01, 38.29it/s]Training CobwebTree:  31%|       | 3059/10000 [01:05<03:00, 38.55it/s]Training CobwebTree:  31%|       | 3063/10000 [01:05<02:58, 38.82it/s]Training CobwebTree:  31%|       | 3068/10000 [01:05<02:52, 40.17it/s]Training CobwebTree:  31%|       | 3073/10000 [01:05<02:52, 40.09it/s]Training CobwebTree:  31%|       | 3078/10000 [01:05<02:57, 39.07it/s]Training CobwebTree:  31%|       | 3082/10000 [01:05<02:56, 39.24it/s]Training CobwebTree:  31%|       | 3087/10000 [01:05<02:51, 40.37it/s]Training CobwebTree:  31%|       | 3092/10000 [01:05<02:44, 42.08it/s]Training CobwebTree:  31%|       | 3097/10000 [01:06<02:45, 41.65it/s]Training CobwebTree:  31%|       | 3102/10000 [01:06<02:50, 40.35it/s]Training CobwebTree:  31%|       | 3107/10000 [01:06<02:48, 40.91it/s]Training CobwebTree:  31%|       | 3112/10000 [01:06<02:41, 42.53it/s]Training CobwebTree:  31%|       | 3117/10000 [01:06<02:47, 41.18it/s]Training CobwebTree:  31%|       | 3122/10000 [01:06<02:42, 42.34it/s]Training CobwebTree:  31%|      | 3127/10000 [01:06<02:41, 42.58it/s]Training CobwebTree:  31%|      | 3132/10000 [01:06<02:44, 41.69it/s]Training CobwebTree:  31%|      | 3138/10000 [01:07<02:36, 43.86it/s]Training CobwebTree:  31%|      | 3143/10000 [01:07<02:34, 44.35it/s]Training CobwebTree:  31%|      | 3148/10000 [01:07<02:34, 44.44it/s]Training CobwebTree:  32%|      | 3153/10000 [01:07<02:41, 42.48it/s]Training CobwebTree:  32%|      | 3158/10000 [01:07<02:36, 43.66it/s]Training CobwebTree:  32%|      | 3163/10000 [01:07<02:41, 42.33it/s]Training CobwebTree:  32%|      | 3168/10000 [01:07<02:41, 42.33it/s]Training CobwebTree:  32%|      | 3173/10000 [01:07<02:45, 41.25it/s]Training CobwebTree:  32%|      | 3178/10000 [01:08<02:49, 40.35it/s]Training CobwebTree:  32%|      | 3183/10000 [01:08<02:42, 42.01it/s]Training CobwebTree:  32%|      | 3188/10000 [01:08<02:39, 42.81it/s]Training CobwebTree:  32%|      | 3193/10000 [01:08<02:45, 41.08it/s]Training CobwebTree:  32%|      | 3198/10000 [01:08<02:55, 38.81it/s]Training CobwebTree:  32%|      | 3202/10000 [01:08<02:54, 38.97it/s]Training CobwebTree:  32%|      | 3207/10000 [01:08<02:50, 39.83it/s]Training CobwebTree:  32%|      | 3212/10000 [01:08<02:42, 41.83it/s]Training CobwebTree:  32%|      | 3217/10000 [01:08<02:36, 43.22it/s]Training CobwebTree:  32%|      | 3222/10000 [01:09<02:37, 42.96it/s]Training CobwebTree:  32%|      | 3227/10000 [01:09<02:31, 44.64it/s]Training CobwebTree:  32%|      | 3232/10000 [01:09<02:44, 41.19it/s]Training CobwebTree:  32%|      | 3237/10000 [01:09<02:42, 41.74it/s]Training CobwebTree:  32%|      | 3242/10000 [01:09<02:45, 40.72it/s]Training CobwebTree:  32%|      | 3247/10000 [01:09<02:41, 41.70it/s]Training CobwebTree:  33%|      | 3252/10000 [01:09<02:53, 38.96it/s]Training CobwebTree:  33%|      | 3257/10000 [01:09<02:50, 39.51it/s]Training CobwebTree:  33%|      | 3261/10000 [01:10<02:50, 39.57it/s]Training CobwebTree:  33%|      | 3265/10000 [01:10<02:54, 38.68it/s]Training CobwebTree:  33%|      | 3270/10000 [01:10<02:49, 39.61it/s]Training CobwebTree:  33%|      | 3274/10000 [01:10<02:50, 39.54it/s]Training CobwebTree:  33%|      | 3279/10000 [01:10<02:47, 40.05it/s]Training CobwebTree:  33%|      | 3284/10000 [01:10<02:54, 38.58it/s]Training CobwebTree:  33%|      | 3288/10000 [01:10<02:58, 37.61it/s]Training CobwebTree:  33%|      | 3292/10000 [01:10<02:55, 38.12it/s]Training CobwebTree:  33%|      | 3297/10000 [01:10<02:50, 39.40it/s]Training CobwebTree:  33%|      | 3302/10000 [01:11<02:47, 39.95it/s]Training CobwebTree:  33%|      | 3306/10000 [01:11<02:48, 39.62it/s]Training CobwebTree:  33%|      | 3311/10000 [01:11<02:46, 40.26it/s]Training CobwebTree:  33%|      | 3316/10000 [01:11<02:49, 39.51it/s]Training CobwebTree:  33%|      | 3321/10000 [01:11<02:47, 39.80it/s]Training CobwebTree:  33%|      | 3326/10000 [01:11<02:41, 41.34it/s]Training CobwebTree:  33%|      | 3331/10000 [01:11<02:46, 40.11it/s]Training CobwebTree:  33%|      | 3336/10000 [01:11<02:40, 41.44it/s]Training CobwebTree:  33%|      | 3341/10000 [01:12<02:40, 41.49it/s]Training CobwebTree:  33%|      | 3346/10000 [01:12<02:42, 40.98it/s]Training CobwebTree:  34%|      | 3351/10000 [01:12<02:41, 41.23it/s]Training CobwebTree:  34%|      | 3356/10000 [01:12<02:45, 40.20it/s]Training CobwebTree:  34%|      | 3361/10000 [01:12<02:49, 39.16it/s]Training CobwebTree:  34%|      | 3365/10000 [01:12<02:51, 38.65it/s]Training CobwebTree:  34%|      | 3369/10000 [01:12<02:50, 38.94it/s]Training CobwebTree:  34%|      | 3373/10000 [01:12<02:51, 38.58it/s]Training CobwebTree:  34%|      | 3377/10000 [01:12<02:54, 37.89it/s]Training CobwebTree:  34%|      | 3382/10000 [01:13<02:46, 39.63it/s]Training CobwebTree:  34%|      | 3387/10000 [01:13<02:43, 40.48it/s]Training CobwebTree:  34%|      | 3392/10000 [01:13<02:37, 41.89it/s]Training CobwebTree:  34%|      | 3397/10000 [01:13<02:37, 41.99it/s]Training CobwebTree:  34%|      | 3402/10000 [01:13<02:32, 43.27it/s]Training CobwebTree:  34%|      | 3407/10000 [01:13<02:33, 42.84it/s]Training CobwebTree:  34%|      | 3412/10000 [01:13<02:33, 42.87it/s]Training CobwebTree:  34%|      | 3417/10000 [01:13<02:40, 41.01it/s]Training CobwebTree:  34%|      | 3422/10000 [01:14<02:37, 41.85it/s]Training CobwebTree:  34%|      | 3427/10000 [01:14<02:44, 39.84it/s]Training CobwebTree:  34%|      | 3432/10000 [01:14<02:40, 40.85it/s]Training CobwebTree:  34%|      | 3437/10000 [01:14<02:42, 40.37it/s]Training CobwebTree:  34%|      | 3442/10000 [01:14<02:44, 39.90it/s]Training CobwebTree:  34%|      | 3447/10000 [01:14<02:43, 40.19it/s]Training CobwebTree:  35%|      | 3452/10000 [01:14<02:40, 40.88it/s]Training CobwebTree:  35%|      | 3457/10000 [01:14<02:47, 39.06it/s]Training CobwebTree:  35%|      | 3462/10000 [01:15<02:41, 40.55it/s]Training CobwebTree:  35%|      | 3467/10000 [01:15<02:36, 41.65it/s]Training CobwebTree:  35%|      | 3472/10000 [01:15<02:38, 41.12it/s]Training CobwebTree:  35%|      | 3477/10000 [01:15<02:39, 41.03it/s]Training CobwebTree:  35%|      | 3482/10000 [01:15<02:42, 40.06it/s]Training CobwebTree:  35%|      | 3487/10000 [01:15<02:44, 39.49it/s]Training CobwebTree:  35%|      | 3491/10000 [01:15<02:45, 39.25it/s]Training CobwebTree:  35%|      | 3496/10000 [01:15<02:38, 41.12it/s]Training CobwebTree:  35%|      | 3502/10000 [01:15<02:28, 43.72it/s]Training CobwebTree:  35%|      | 3507/10000 [01:16<02:29, 43.40it/s]Training CobwebTree:  35%|      | 3512/10000 [01:16<02:29, 43.35it/s]Training CobwebTree:  35%|      | 3517/10000 [01:16<02:37, 41.05it/s]Training CobwebTree:  35%|      | 3522/10000 [01:16<02:41, 40.05it/s]Training CobwebTree:  35%|      | 3527/10000 [01:16<02:39, 40.61it/s]Training CobwebTree:  35%|      | 3532/10000 [01:16<02:44, 39.21it/s]Training CobwebTree:  35%|      | 3537/10000 [01:16<02:38, 40.74it/s]Training CobwebTree:  35%|      | 3542/10000 [01:16<02:39, 40.50it/s]Training CobwebTree:  35%|      | 3547/10000 [01:17<02:32, 42.18it/s]Training CobwebTree:  36%|      | 3552/10000 [01:17<02:32, 42.35it/s]Training CobwebTree:  36%|      | 3557/10000 [01:17<02:40, 40.26it/s]Training CobwebTree:  36%|      | 3562/10000 [01:17<02:41, 39.84it/s]Training CobwebTree:  36%|      | 3567/10000 [01:17<02:39, 40.30it/s]Training CobwebTree:  36%|      | 3572/10000 [01:17<02:38, 40.44it/s]Training CobwebTree:  36%|      | 3577/10000 [01:17<02:35, 41.41it/s]Training CobwebTree:  36%|      | 3582/10000 [01:17<02:39, 40.12it/s]Training CobwebTree:  36%|      | 3587/10000 [01:18<02:42, 39.42it/s]Training CobwebTree:  36%|      | 3591/10000 [01:18<02:43, 39.24it/s]Training CobwebTree:  36%|      | 3595/10000 [01:18<02:45, 38.73it/s]Training CobwebTree:  36%|      | 3600/10000 [01:18<02:41, 39.67it/s]Training CobwebTree:  36%|      | 3605/10000 [01:18<02:35, 41.04it/s]Training CobwebTree:  36%|      | 3610/10000 [01:18<02:39, 40.10it/s]Training CobwebTree:  36%|      | 3615/10000 [01:18<02:43, 39.05it/s]Training CobwebTree:  36%|      | 3619/10000 [01:18<02:51, 37.26it/s]Training CobwebTree:  36%|      | 3623/10000 [01:19<02:54, 36.48it/s]Training CobwebTree:  36%|      | 3627/10000 [01:19<02:53, 36.73it/s]Training CobwebTree:  36%|      | 3632/10000 [01:19<02:45, 38.47it/s]Training CobwebTree:  36%|      | 3636/10000 [01:19<02:45, 38.51it/s]Training CobwebTree:  36%|      | 3640/10000 [01:19<02:47, 37.86it/s]Training CobwebTree:  36%|      | 3645/10000 [01:19<02:38, 40.12it/s]Training CobwebTree:  36%|      | 3650/10000 [01:19<02:34, 41.07it/s]Training CobwebTree:  37%|      | 3655/10000 [01:19<02:35, 40.71it/s]Training CobwebTree:  37%|      | 3660/10000 [01:19<02:31, 41.81it/s]Training CobwebTree:  37%|      | 3665/10000 [01:20<02:30, 42.03it/s]Training CobwebTree:  37%|      | 3670/10000 [01:20<02:26, 43.18it/s]Training CobwebTree:  37%|      | 3675/10000 [01:20<02:36, 40.33it/s]Training CobwebTree:  37%|      | 3680/10000 [01:20<02:34, 41.01it/s]Training CobwebTree:  37%|      | 3685/10000 [01:20<02:29, 42.35it/s]Training CobwebTree:  37%|      | 3690/10000 [01:20<02:30, 41.92it/s]Training CobwebTree:  37%|      | 3695/10000 [01:20<02:28, 42.42it/s]Training CobwebTree:  37%|      | 3700/10000 [01:20<02:33, 40.92it/s]Training CobwebTree:  37%|      | 3705/10000 [01:21<02:28, 42.42it/s]Training CobwebTree:  37%|      | 3710/10000 [01:21<02:33, 41.05it/s]Training CobwebTree:  37%|      | 3715/10000 [01:21<02:28, 42.36it/s]Training CobwebTree:  37%|      | 3720/10000 [01:21<02:30, 41.79it/s]Training CobwebTree:  37%|      | 3725/10000 [01:21<02:23, 43.76it/s]Training CobwebTree:  37%|      | 3730/10000 [01:21<02:24, 43.47it/s]Training CobwebTree:  37%|      | 3735/10000 [01:21<02:32, 41.14it/s]Training CobwebTree:  37%|      | 3740/10000 [01:21<02:40, 39.12it/s]Training CobwebTree:  37%|      | 3744/10000 [01:21<02:40, 39.08it/s]Training CobwebTree:  37%|      | 3748/10000 [01:22<02:39, 39.22it/s]Training CobwebTree:  38%|      | 3752/10000 [01:22<02:42, 38.43it/s]Training CobwebTree:  38%|      | 3756/10000 [01:22<02:43, 38.20it/s]Training CobwebTree:  38%|      | 3760/10000 [01:22<02:43, 38.21it/s]Training CobwebTree:  38%|      | 3765/10000 [01:22<02:31, 41.24it/s]Training CobwebTree:  38%|      | 3770/10000 [01:22<02:33, 40.52it/s]Training CobwebTree:  38%|      | 3776/10000 [01:22<02:24, 42.92it/s]Training CobwebTree:  38%|      | 3781/10000 [01:22<02:21, 43.95it/s]Training CobwebTree:  38%|      | 3786/10000 [01:23<02:29, 41.45it/s]Training CobwebTree:  38%|      | 3791/10000 [01:23<02:29, 41.48it/s]Training CobwebTree:  38%|      | 3796/10000 [01:23<02:28, 41.87it/s]Training CobwebTree:  38%|      | 3801/10000 [01:23<02:26, 42.36it/s]Training CobwebTree:  38%|      | 3806/10000 [01:23<02:24, 42.78it/s]Training CobwebTree:  38%|      | 3811/10000 [01:23<02:25, 42.64it/s]Training CobwebTree:  38%|      | 3816/10000 [01:23<02:29, 41.23it/s]Training CobwebTree:  38%|      | 3821/10000 [01:23<02:24, 42.90it/s]Training CobwebTree:  38%|      | 3826/10000 [01:23<02:28, 41.46it/s]Training CobwebTree:  38%|      | 3831/10000 [01:24<02:29, 41.35it/s]Training CobwebTree:  38%|      | 3836/10000 [01:24<02:35, 39.63it/s]Training CobwebTree:  38%|      | 3841/10000 [01:24<02:33, 40.11it/s]Training CobwebTree:  38%|      | 3846/10000 [01:24<02:33, 39.98it/s]Training CobwebTree:  39%|      | 3851/10000 [01:24<02:32, 40.24it/s]Training CobwebTree:  39%|      | 3856/10000 [01:24<02:29, 41.02it/s]Training CobwebTree:  39%|      | 3861/10000 [01:24<02:33, 39.93it/s]Training CobwebTree:  39%|      | 3866/10000 [01:24<02:39, 38.40it/s]Training CobwebTree:  39%|      | 3871/10000 [01:25<02:34, 39.77it/s]Training CobwebTree:  39%|      | 3876/10000 [01:25<02:28, 41.34it/s]Training CobwebTree:  39%|      | 3881/10000 [01:25<02:32, 40.19it/s]Training CobwebTree:  39%|      | 3886/10000 [01:25<02:26, 41.81it/s]Training CobwebTree:  39%|      | 3891/10000 [01:25<02:25, 41.85it/s]Training CobwebTree:  39%|      | 3896/10000 [01:25<02:35, 39.33it/s]Training CobwebTree:  39%|      | 3900/10000 [01:25<02:37, 38.78it/s]Training CobwebTree:  39%|      | 3904/10000 [01:25<02:42, 37.50it/s]Training CobwebTree:  39%|      | 3908/10000 [01:26<02:39, 38.11it/s]Training CobwebTree:  39%|      | 3912/10000 [01:26<02:38, 38.36it/s]Training CobwebTree:  39%|      | 3917/10000 [01:26<02:30, 40.50it/s]Training CobwebTree:  39%|      | 3923/10000 [01:26<02:21, 42.92it/s]Training CobwebTree:  39%|      | 3928/10000 [01:26<02:25, 41.59it/s]Training CobwebTree:  39%|      | 3933/10000 [01:26<02:24, 42.13it/s]Training CobwebTree:  39%|      | 3938/10000 [01:26<02:19, 43.61it/s]Training CobwebTree:  39%|      | 3943/10000 [01:26<02:23, 42.24it/s]Training CobwebTree:  39%|      | 3948/10000 [01:26<02:34, 39.20it/s]Training CobwebTree:  40%|      | 3952/10000 [01:27<02:36, 38.57it/s]Training CobwebTree:  40%|      | 3956/10000 [01:27<02:42, 37.30it/s]Training CobwebTree:  40%|      | 3960/10000 [01:27<02:42, 37.26it/s]Training CobwebTree:  40%|      | 3964/10000 [01:27<02:40, 37.64it/s]Training CobwebTree:  40%|      | 3969/10000 [01:27<02:37, 38.36it/s]Training CobwebTree:  40%|      | 3974/10000 [01:27<02:36, 38.58it/s]Training CobwebTree:  40%|      | 3978/10000 [01:27<02:38, 37.91it/s]Training CobwebTree:  40%|      | 3982/10000 [01:27<02:46, 36.12it/s]Training CobwebTree:  40%|      | 3986/10000 [01:28<02:45, 36.44it/s]Training CobwebTree:  40%|      | 3990/10000 [01:28<02:42, 36.96it/s]Training CobwebTree:  40%|      | 3995/10000 [01:28<02:33, 39.03it/s]Training CobwebTree:  40%|      | 3999/10000 [01:28<02:33, 39.13it/s]Training CobwebTree:  40%|      | 4003/10000 [01:28<02:38, 37.84it/s]Training CobwebTree:  40%|      | 4007/10000 [01:28<02:36, 38.25it/s]Training CobwebTree:  40%|      | 4011/10000 [01:28<02:40, 37.40it/s]Training CobwebTree:  40%|      | 4016/10000 [01:28<02:38, 37.69it/s]Training CobwebTree:  40%|      | 4021/10000 [01:28<02:32, 39.31it/s]Training CobwebTree:  40%|      | 4026/10000 [01:29<02:30, 39.62it/s]Training CobwebTree:  40%|      | 4030/10000 [01:29<02:31, 39.43it/s]Training CobwebTree:  40%|      | 4035/10000 [01:29<02:31, 39.42it/s]Training CobwebTree:  40%|      | 4039/10000 [01:29<02:31, 39.46it/s]Training CobwebTree:  40%|      | 4043/10000 [01:29<02:34, 38.59it/s]Training CobwebTree:  40%|      | 4047/10000 [01:29<02:35, 38.40it/s]Training CobwebTree:  41%|      | 4052/10000 [01:29<02:26, 40.60it/s]Training CobwebTree:  41%|      | 4057/10000 [01:29<02:30, 39.59it/s]Training CobwebTree:  41%|      | 4061/10000 [01:29<02:31, 39.30it/s]Training CobwebTree:  41%|      | 4065/10000 [01:30<02:30, 39.36it/s]Training CobwebTree:  41%|      | 4070/10000 [01:30<02:29, 39.77it/s]Training CobwebTree:  41%|      | 4074/10000 [01:30<02:31, 39.21it/s]Training CobwebTree:  41%|      | 4078/10000 [01:30<02:38, 37.44it/s]Training CobwebTree:  41%|      | 4082/10000 [01:30<02:41, 36.59it/s]Training CobwebTree:  41%|      | 4087/10000 [01:30<02:30, 39.35it/s]Training CobwebTree:  41%|      | 4091/10000 [01:30<02:29, 39.48it/s]Training CobwebTree:  41%|      | 4095/10000 [01:30<02:34, 38.15it/s]Training CobwebTree:  41%|      | 4100/10000 [01:30<02:29, 39.54it/s]Training CobwebTree:  41%|      | 4105/10000 [01:31<02:26, 40.32it/s]Training CobwebTree:  41%|      | 4110/10000 [01:31<02:20, 41.91it/s]Training CobwebTree:  41%|      | 4115/10000 [01:31<02:19, 42.14it/s]Training CobwebTree:  41%|      | 4120/10000 [01:31<02:24, 40.59it/s]Training CobwebTree:  41%|     | 4125/10000 [01:31<02:25, 40.26it/s]Training CobwebTree:  41%|     | 4130/10000 [01:31<02:25, 40.28it/s]Training CobwebTree:  41%|     | 4135/10000 [01:31<02:27, 39.67it/s]Training CobwebTree:  41%|     | 4140/10000 [01:31<02:22, 41.22it/s]Training CobwebTree:  41%|     | 4145/10000 [01:32<02:24, 40.59it/s]Training CobwebTree:  42%|     | 4150/10000 [01:32<02:25, 40.15it/s]Training CobwebTree:  42%|     | 4155/10000 [01:32<02:24, 40.32it/s]Training CobwebTree:  42%|     | 4160/10000 [01:32<02:23, 40.75it/s]Training CobwebTree:  42%|     | 4165/10000 [01:32<02:24, 40.50it/s]Training CobwebTree:  42%|     | 4170/10000 [01:32<02:21, 41.13it/s]Training CobwebTree:  42%|     | 4175/10000 [01:32<02:23, 40.62it/s]Training CobwebTree:  42%|     | 4180/10000 [01:32<02:28, 39.20it/s]Training CobwebTree:  42%|     | 4185/10000 [01:33<02:23, 40.54it/s]Training CobwebTree:  42%|     | 4190/10000 [01:33<02:28, 39.21it/s]Training CobwebTree:  42%|     | 4195/10000 [01:33<02:21, 41.00it/s]Training CobwebTree:  42%|     | 4200/10000 [01:33<02:24, 40.21it/s]Training CobwebTree:  42%|     | 4205/10000 [01:33<02:21, 40.96it/s]Training CobwebTree:  42%|     | 4210/10000 [01:33<02:23, 40.22it/s]Training CobwebTree:  42%|     | 4215/10000 [01:33<02:23, 40.32it/s]Training CobwebTree:  42%|     | 4220/10000 [01:33<02:19, 41.48it/s]Training CobwebTree:  42%|     | 4225/10000 [01:34<02:20, 41.11it/s]Training CobwebTree:  42%|     | 4230/10000 [01:34<02:22, 40.53it/s]Training CobwebTree:  42%|     | 4235/10000 [01:34<02:20, 41.05it/s]Training CobwebTree:  42%|     | 4240/10000 [01:34<02:21, 40.75it/s]Training CobwebTree:  42%|     | 4245/10000 [01:34<02:20, 41.01it/s]Training CobwebTree:  42%|     | 4250/10000 [01:34<02:21, 40.49it/s]Training CobwebTree:  43%|     | 4255/10000 [01:34<02:27, 38.83it/s]Training CobwebTree:  43%|     | 4259/10000 [01:34<02:26, 39.07it/s]Training CobwebTree:  43%|     | 4264/10000 [01:34<02:25, 39.52it/s]Training CobwebTree:  43%|     | 4268/10000 [01:35<02:30, 38.14it/s]Training CobwebTree:  43%|     | 4273/10000 [01:35<02:29, 38.43it/s]Training CobwebTree:  43%|     | 4278/10000 [01:35<02:26, 39.19it/s]Training CobwebTree:  43%|     | 4282/10000 [01:35<02:30, 38.12it/s]Training CobwebTree:  43%|     | 4286/10000 [01:35<02:30, 38.03it/s]Training CobwebTree:  43%|     | 4290/10000 [01:35<02:28, 38.46it/s]Training CobwebTree:  43%|     | 4294/10000 [01:35<02:35, 36.60it/s]Training CobwebTree:  43%|     | 4299/10000 [01:35<02:28, 38.46it/s]Training CobwebTree:  43%|     | 4303/10000 [01:36<02:32, 37.36it/s]Training CobwebTree:  43%|     | 4308/10000 [01:36<02:26, 38.92it/s]Training CobwebTree:  43%|     | 4312/10000 [01:36<02:26, 38.71it/s]Training CobwebTree:  43%|     | 4316/10000 [01:36<02:28, 38.23it/s]Training CobwebTree:  43%|     | 4320/10000 [01:36<02:33, 36.90it/s]Training CobwebTree:  43%|     | 4324/10000 [01:36<02:34, 36.68it/s]Training CobwebTree:  43%|     | 4328/10000 [01:36<02:43, 34.72it/s]Training CobwebTree:  43%|     | 4332/10000 [01:36<02:41, 35.06it/s]Training CobwebTree:  43%|     | 4336/10000 [01:36<02:40, 35.27it/s]Training CobwebTree:  43%|     | 4340/10000 [01:37<02:39, 35.44it/s]Training CobwebTree:  43%|     | 4344/10000 [01:37<02:37, 35.91it/s]Training CobwebTree:  43%|     | 4349/10000 [01:37<02:30, 37.49it/s]Training CobwebTree:  44%|     | 4354/10000 [01:37<02:19, 40.46it/s]Training CobwebTree:  44%|     | 4359/10000 [01:37<02:19, 40.52it/s]Training CobwebTree:  44%|     | 4364/10000 [01:37<02:18, 40.68it/s]Training CobwebTree:  44%|     | 4369/10000 [01:37<02:14, 41.76it/s]Training CobwebTree:  44%|     | 4374/10000 [01:37<02:23, 39.27it/s]Training CobwebTree:  44%|     | 4378/10000 [01:37<02:23, 39.18it/s]Training CobwebTree:  44%|     | 4383/10000 [01:38<02:20, 40.01it/s]Training CobwebTree:  44%|     | 4388/10000 [01:38<02:17, 40.75it/s]Training CobwebTree:  44%|     | 4393/10000 [01:38<02:16, 41.14it/s]Training CobwebTree:  44%|     | 4398/10000 [01:38<02:18, 40.36it/s]Training CobwebTree:  44%|     | 4403/10000 [01:38<02:13, 41.85it/s]Training CobwebTree:  44%|     | 4408/10000 [01:38<02:18, 40.52it/s]Training CobwebTree:  44%|     | 4413/10000 [01:38<02:10, 42.91it/s]Training CobwebTree:  44%|     | 4418/10000 [01:38<02:10, 42.66it/s]Training CobwebTree:  44%|     | 4423/10000 [01:39<02:18, 40.37it/s]Training CobwebTree:  44%|     | 4428/10000 [01:39<02:25, 38.41it/s]Training CobwebTree:  44%|     | 4432/10000 [01:39<02:25, 38.30it/s]Training CobwebTree:  44%|     | 4437/10000 [01:39<02:26, 38.03it/s]Training CobwebTree:  44%|     | 4441/10000 [01:39<02:30, 36.84it/s]Training CobwebTree:  44%|     | 4446/10000 [01:39<02:21, 39.34it/s]Training CobwebTree:  45%|     | 4451/10000 [01:39<02:16, 40.68it/s]Training CobwebTree:  45%|     | 4456/10000 [01:39<02:18, 39.97it/s]Training CobwebTree:  45%|     | 4461/10000 [01:40<02:16, 40.64it/s]Training CobwebTree:  45%|     | 4466/10000 [01:40<02:19, 39.55it/s]Training CobwebTree:  45%|     | 4470/10000 [01:40<02:22, 38.72it/s]Training CobwebTree:  45%|     | 4474/10000 [01:40<02:24, 38.32it/s]Training CobwebTree:  45%|     | 4479/10000 [01:40<02:17, 40.18it/s]Training CobwebTree:  45%|     | 4484/10000 [01:40<02:12, 41.48it/s]Training CobwebTree:  45%|     | 4489/10000 [01:40<02:15, 40.82it/s]Training CobwebTree:  45%|     | 4494/10000 [01:40<02:18, 39.82it/s]Training CobwebTree:  45%|     | 4498/10000 [01:40<02:20, 39.06it/s]Training CobwebTree:  45%|     | 4502/10000 [01:41<02:23, 38.33it/s]Training CobwebTree:  45%|     | 4507/10000 [01:41<02:19, 39.32it/s]Training CobwebTree:  45%|     | 4512/10000 [01:41<02:19, 39.36it/s]Training CobwebTree:  45%|     | 4517/10000 [01:41<02:14, 40.65it/s]Training CobwebTree:  45%|     | 4522/10000 [01:41<02:15, 40.51it/s]Training CobwebTree:  45%|     | 4527/10000 [01:41<02:12, 41.39it/s]Training CobwebTree:  45%|     | 4532/10000 [01:41<02:16, 39.95it/s]Training CobwebTree:  45%|     | 4537/10000 [01:41<02:17, 39.64it/s]Training CobwebTree:  45%|     | 4541/10000 [01:42<02:20, 38.83it/s]Training CobwebTree:  45%|     | 4545/10000 [01:42<02:25, 37.42it/s]Training CobwebTree:  46%|     | 4550/10000 [01:42<02:19, 39.03it/s]Training CobwebTree:  46%|     | 4554/10000 [01:42<02:19, 39.05it/s]Training CobwebTree:  46%|     | 4558/10000 [01:42<02:27, 36.78it/s]Training CobwebTree:  46%|     | 4562/10000 [01:42<02:24, 37.52it/s]Training CobwebTree:  46%|     | 4567/10000 [01:42<02:20, 38.74it/s]Training CobwebTree:  46%|     | 4571/10000 [01:42<02:19, 38.98it/s]Training CobwebTree:  46%|     | 4575/10000 [01:42<02:28, 36.48it/s]Training CobwebTree:  46%|     | 4580/10000 [01:43<02:20, 38.70it/s]Training CobwebTree:  46%|     | 4585/10000 [01:43<02:13, 40.63it/s]Training CobwebTree:  46%|     | 4590/10000 [01:43<02:14, 40.28it/s]Training CobwebTree:  46%|     | 4595/10000 [01:43<02:14, 40.13it/s]Training CobwebTree:  46%|     | 4600/10000 [01:43<02:17, 39.39it/s]Training CobwebTree:  46%|     | 4604/10000 [01:43<02:16, 39.45it/s]Training CobwebTree:  46%|     | 4608/10000 [01:43<02:18, 39.05it/s]Training CobwebTree:  46%|     | 4612/10000 [01:43<02:21, 38.09it/s]Training CobwebTree:  46%|     | 4617/10000 [01:44<02:19, 38.56it/s]Training CobwebTree:  46%|     | 4621/10000 [01:44<02:21, 38.00it/s]Training CobwebTree:  46%|     | 4626/10000 [01:44<02:19, 38.62it/s]Training CobwebTree:  46%|     | 4631/10000 [01:44<02:14, 39.81it/s]Training CobwebTree:  46%|     | 4636/10000 [01:44<02:13, 40.17it/s]Training CobwebTree:  46%|     | 4641/10000 [01:44<02:12, 40.33it/s]Training CobwebTree:  46%|     | 4646/10000 [01:44<02:17, 38.87it/s]Training CobwebTree:  47%|     | 4651/10000 [01:44<02:16, 39.28it/s]Training CobwebTree:  47%|     | 4655/10000 [01:45<02:18, 38.67it/s]Training CobwebTree:  47%|     | 4659/10000 [01:45<02:18, 38.55it/s]Training CobwebTree:  47%|     | 4663/10000 [01:45<02:17, 38.78it/s]Training CobwebTree:  47%|     | 4667/10000 [01:45<02:18, 38.51it/s]Training CobwebTree:  47%|     | 4671/10000 [01:45<02:19, 38.15it/s]Training CobwebTree:  47%|     | 4675/10000 [01:45<02:24, 36.84it/s]Training CobwebTree:  47%|     | 4679/10000 [01:45<02:26, 36.39it/s]Training CobwebTree:  47%|     | 4683/10000 [01:45<02:23, 36.95it/s]Training CobwebTree:  47%|     | 4688/10000 [01:45<02:12, 40.05it/s]Training CobwebTree:  47%|     | 4693/10000 [01:45<02:10, 40.65it/s]Training CobwebTree:  47%|     | 4698/10000 [01:46<02:11, 40.34it/s]Training CobwebTree:  47%|     | 4703/10000 [01:46<02:11, 40.41it/s]Training CobwebTree:  47%|     | 4708/10000 [01:46<02:11, 40.16it/s]Training CobwebTree:  47%|     | 4713/10000 [01:46<02:11, 40.30it/s]Training CobwebTree:  47%|     | 4718/10000 [01:46<02:08, 40.96it/s]Training CobwebTree:  47%|     | 4723/10000 [01:46<02:09, 40.77it/s]Training CobwebTree:  47%|     | 4728/10000 [01:46<02:15, 38.84it/s]Training CobwebTree:  47%|     | 4732/10000 [01:46<02:15, 38.94it/s]Training CobwebTree:  47%|     | 4737/10000 [01:47<02:09, 40.65it/s]Training CobwebTree:  47%|     | 4742/10000 [01:47<02:07, 41.28it/s]Training CobwebTree:  47%|     | 4747/10000 [01:47<02:06, 41.49it/s]Training CobwebTree:  48%|     | 4752/10000 [01:47<02:18, 38.00it/s]Training CobwebTree:  48%|     | 4757/10000 [01:47<02:13, 39.23it/s]Training CobwebTree:  48%|     | 4761/10000 [01:47<02:13, 39.27it/s]Training CobwebTree:  48%|     | 4765/10000 [01:47<02:14, 38.78it/s]Training CobwebTree:  48%|     | 4769/10000 [01:47<02:17, 38.02it/s]Training CobwebTree:  48%|     | 4774/10000 [01:48<02:12, 39.40it/s]Training CobwebTree:  48%|     | 4778/10000 [01:48<02:13, 39.21it/s]Training CobwebTree:  48%|     | 4782/10000 [01:48<02:22, 36.70it/s]Training CobwebTree:  48%|     | 4787/10000 [01:48<02:14, 38.66it/s]Training CobwebTree:  48%|     | 4791/10000 [01:48<02:23, 36.26it/s]Training CobwebTree:  48%|     | 4795/10000 [01:48<02:24, 36.14it/s]Training CobwebTree:  48%|     | 4800/10000 [01:48<02:18, 37.59it/s]Training CobwebTree:  48%|     | 4805/10000 [01:48<02:12, 39.24it/s]Training CobwebTree:  48%|     | 4810/10000 [01:48<02:12, 39.06it/s]Training CobwebTree:  48%|     | 4815/10000 [01:49<02:11, 39.49it/s]Training CobwebTree:  48%|     | 4820/10000 [01:49<02:06, 40.91it/s]Training CobwebTree:  48%|     | 4825/10000 [01:49<02:07, 40.43it/s]Training CobwebTree:  48%|     | 4830/10000 [01:49<02:10, 39.54it/s]Training CobwebTree:  48%|     | 4835/10000 [01:49<02:09, 39.73it/s]Training CobwebTree:  48%|     | 4840/10000 [01:49<02:10, 39.58it/s]Training CobwebTree:  48%|     | 4844/10000 [01:49<02:10, 39.65it/s]Training CobwebTree:  48%|     | 4849/10000 [01:49<02:07, 40.40it/s]Training CobwebTree:  49%|     | 4854/10000 [01:50<02:07, 40.23it/s]Training CobwebTree:  49%|     | 4859/10000 [01:50<02:13, 38.44it/s]Training CobwebTree:  49%|     | 4863/10000 [01:50<02:15, 37.80it/s]Training CobwebTree:  49%|     | 4868/10000 [01:50<02:12, 38.69it/s]Training CobwebTree:  49%|     | 4873/10000 [01:50<02:05, 40.85it/s]Training CobwebTree:  49%|     | 4878/10000 [01:50<02:04, 41.07it/s]Training CobwebTree:  49%|     | 4883/10000 [01:50<02:04, 41.19it/s]Training CobwebTree:  49%|     | 4888/10000 [01:50<02:07, 40.15it/s]Training CobwebTree:  49%|     | 4893/10000 [01:51<02:04, 41.07it/s]Training CobwebTree:  49%|     | 4898/10000 [01:51<02:01, 41.89it/s]Training CobwebTree:  49%|     | 4903/10000 [01:51<02:00, 42.33it/s]Training CobwebTree:  49%|     | 4908/10000 [01:51<02:00, 42.20it/s]Training CobwebTree:  49%|     | 4913/10000 [01:51<02:04, 40.74it/s]Training CobwebTree:  49%|     | 4918/10000 [01:51<02:03, 41.06it/s]Training CobwebTree:  49%|     | 4923/10000 [01:51<02:05, 40.53it/s]Training CobwebTree:  49%|     | 4928/10000 [01:51<02:06, 40.18it/s]Training CobwebTree:  49%|     | 4933/10000 [01:52<02:06, 39.96it/s]Training CobwebTree:  49%|     | 4938/10000 [01:52<02:09, 39.05it/s]Training CobwebTree:  49%|     | 4943/10000 [01:52<02:06, 39.96it/s]Training CobwebTree:  49%|     | 4948/10000 [01:52<02:08, 39.26it/s]Training CobwebTree:  50%|     | 4952/10000 [01:52<02:10, 38.63it/s]Training CobwebTree:  50%|     | 4956/10000 [01:52<02:10, 38.55it/s]Training CobwebTree:  50%|     | 4960/10000 [01:52<02:11, 38.33it/s]Training CobwebTree:  50%|     | 4964/10000 [01:52<02:11, 38.28it/s]Training CobwebTree:  50%|     | 4968/10000 [01:52<02:12, 37.98it/s]Training CobwebTree:  50%|     | 4973/10000 [01:53<02:11, 38.24it/s]Training CobwebTree:  50%|     | 4977/10000 [01:53<02:12, 37.83it/s]Training CobwebTree:  50%|     | 4981/10000 [01:53<02:11, 38.04it/s]Training CobwebTree:  50%|     | 4985/10000 [01:53<02:12, 37.83it/s]Training CobwebTree:  50%|     | 4989/10000 [01:53<02:13, 37.65it/s]Training CobwebTree:  50%|     | 4994/10000 [01:53<02:10, 38.30it/s]Training CobwebTree:  50%|     | 4999/10000 [01:53<02:10, 38.30it/s]Training CobwebTree:  50%|     | 5004/10000 [01:53<02:11, 38.06it/s]Training CobwebTree:  50%|     | 5009/10000 [01:54<02:06, 39.48it/s]Training CobwebTree:  50%|     | 5014/10000 [01:54<02:05, 39.66it/s]Training CobwebTree:  50%|     | 5019/10000 [01:54<02:03, 40.49it/s]Training CobwebTree:  50%|     | 5024/10000 [01:54<02:10, 38.24it/s]Training CobwebTree:  50%|     | 5029/10000 [01:54<02:06, 39.24it/s]Training CobwebTree:  50%|     | 5033/10000 [01:54<02:12, 37.59it/s]Training CobwebTree:  50%|     | 5038/10000 [01:54<02:08, 38.54it/s]Training CobwebTree:  50%|     | 5042/10000 [01:54<02:10, 38.06it/s]Training CobwebTree:  50%|     | 5047/10000 [01:54<02:05, 39.39it/s]Training CobwebTree:  51%|     | 5052/10000 [01:55<02:07, 38.82it/s]Training CobwebTree:  51%|     | 5056/10000 [01:55<02:12, 37.32it/s]Training CobwebTree:  51%|     | 5061/10000 [01:55<02:08, 38.51it/s]Training CobwebTree:  51%|     | 5065/10000 [01:55<02:07, 38.68it/s]Training CobwebTree:  51%|     | 5069/10000 [01:55<02:06, 39.03it/s]Training CobwebTree:  51%|     | 5073/10000 [01:55<02:07, 38.76it/s]Training CobwebTree:  51%|     | 5077/10000 [01:55<02:08, 38.27it/s]Training CobwebTree:  51%|     | 5081/10000 [01:55<02:13, 36.86it/s]Training CobwebTree:  51%|     | 5086/10000 [01:56<02:09, 37.92it/s]Training CobwebTree:  51%|     | 5090/10000 [01:56<02:15, 36.26it/s]Training CobwebTree:  51%|     | 5094/10000 [01:56<02:11, 37.17it/s]Training CobwebTree:  51%|     | 5099/10000 [01:56<02:06, 38.67it/s]Training CobwebTree:  51%|     | 5103/10000 [01:56<02:09, 37.95it/s]Training CobwebTree:  51%|     | 5107/10000 [01:56<02:12, 36.87it/s]Training CobwebTree:  51%|     | 5111/10000 [01:56<02:16, 35.85it/s]Training CobwebTree:  51%|     | 5115/10000 [01:56<02:12, 36.89it/s]Training CobwebTree:  51%|     | 5119/10000 [01:56<02:12, 36.89it/s]Training CobwebTree:  51%|     | 5124/10000 [01:57<02:04, 39.07it/s]Training CobwebTree:  51%|    | 5129/10000 [01:57<02:00, 40.39it/s]Training CobwebTree:  51%|    | 5134/10000 [01:57<02:01, 39.96it/s]Training CobwebTree:  51%|    | 5138/10000 [01:57<02:10, 37.12it/s]Training CobwebTree:  51%|    | 5142/10000 [01:57<02:24, 33.64it/s]Training CobwebTree:  51%|    | 5147/10000 [01:57<02:15, 35.69it/s]Training CobwebTree:  52%|    | 5152/10000 [01:57<02:10, 37.14it/s]Training CobwebTree:  52%|    | 5156/10000 [01:57<02:12, 36.49it/s]Training CobwebTree:  52%|    | 5160/10000 [01:58<02:12, 36.54it/s]Training CobwebTree:  52%|    | 5164/10000 [01:58<02:09, 37.45it/s]Training CobwebTree:  52%|    | 5169/10000 [01:58<02:04, 38.83it/s]Training CobwebTree:  52%|    | 5173/10000 [01:58<02:07, 37.82it/s]Training CobwebTree:  52%|    | 5177/10000 [01:58<02:06, 38.08it/s]Training CobwebTree:  52%|    | 5182/10000 [01:58<02:00, 39.91it/s]Training CobwebTree:  52%|    | 5186/10000 [01:58<02:02, 39.30it/s]Training CobwebTree:  52%|    | 5190/10000 [01:58<02:06, 37.94it/s]Training CobwebTree:  52%|    | 5195/10000 [01:58<02:03, 38.98it/s]Training CobwebTree:  52%|    | 5200/10000 [01:59<02:00, 39.86it/s]Training CobwebTree:  52%|    | 5205/10000 [01:59<01:59, 40.23it/s]Training CobwebTree:  52%|    | 5210/10000 [01:59<02:00, 39.85it/s]Training CobwebTree:  52%|    | 5215/10000 [01:59<02:00, 39.65it/s]Training CobwebTree:  52%|    | 5220/10000 [01:59<01:58, 40.47it/s]Training CobwebTree:  52%|    | 5225/10000 [01:59<01:55, 41.36it/s]Training CobwebTree:  52%|    | 5230/10000 [01:59<01:58, 40.16it/s]Training CobwebTree:  52%|    | 5235/10000 [01:59<02:02, 39.03it/s]Training CobwebTree:  52%|    | 5239/10000 [02:00<02:10, 36.60it/s]Training CobwebTree:  52%|    | 5243/10000 [02:00<02:14, 35.47it/s]Training CobwebTree:  52%|    | 5247/10000 [02:00<02:14, 35.30it/s]Training CobwebTree:  53%|    | 5251/10000 [02:00<02:11, 36.08it/s]Training CobwebTree:  53%|    | 5256/10000 [02:00<02:01, 38.91it/s]Training CobwebTree:  53%|    | 5261/10000 [02:00<01:55, 41.01it/s]Training CobwebTree:  53%|    | 5266/10000 [02:00<01:57, 40.28it/s]Training CobwebTree:  53%|    | 5271/10000 [02:00<01:56, 40.62it/s]Training CobwebTree:  53%|    | 5276/10000 [02:00<02:00, 39.18it/s]Training CobwebTree:  53%|    | 5281/10000 [02:01<01:57, 40.25it/s]Training CobwebTree:  53%|    | 5286/10000 [02:01<02:03, 38.27it/s]Training CobwebTree:  53%|    | 5291/10000 [02:01<01:58, 39.81it/s]Training CobwebTree:  53%|    | 5296/10000 [02:01<01:57, 40.10it/s]Training CobwebTree:  53%|    | 5301/10000 [02:01<01:56, 40.50it/s]Training CobwebTree:  53%|    | 5306/10000 [02:01<02:00, 38.92it/s]Training CobwebTree:  53%|    | 5311/10000 [02:01<01:56, 40.08it/s]Training CobwebTree:  53%|    | 5316/10000 [02:01<01:54, 40.93it/s]Training CobwebTree:  53%|    | 5321/10000 [02:02<01:58, 39.34it/s]Training CobwebTree:  53%|    | 5325/10000 [02:02<01:58, 39.39it/s]Training CobwebTree:  53%|    | 5329/10000 [02:02<02:01, 38.53it/s]Training CobwebTree:  53%|    | 5334/10000 [02:02<01:56, 40.05it/s]Training CobwebTree:  53%|    | 5339/10000 [02:02<01:58, 39.50it/s]Training CobwebTree:  53%|    | 5344/10000 [02:02<01:58, 39.38it/s]Training CobwebTree:  53%|    | 5349/10000 [02:02<01:55, 40.15it/s]Training CobwebTree:  54%|    | 5354/10000 [02:02<01:59, 39.01it/s]Training CobwebTree:  54%|    | 5358/10000 [02:03<02:05, 37.11it/s]Training CobwebTree:  54%|    | 5362/10000 [02:03<02:08, 36.22it/s]Training CobwebTree:  54%|    | 5366/10000 [02:03<02:06, 36.75it/s]Training CobwebTree:  54%|    | 5371/10000 [02:03<01:59, 38.84it/s]Training CobwebTree:  54%|    | 5376/10000 [02:03<01:54, 40.30it/s]Training CobwebTree:  54%|    | 5381/10000 [02:03<01:55, 39.96it/s]Training CobwebTree:  54%|    | 5386/10000 [02:03<01:52, 41.08it/s]Training CobwebTree:  54%|    | 5391/10000 [02:03<01:54, 40.38it/s]Training CobwebTree:  54%|    | 5396/10000 [02:04<02:01, 38.02it/s]Training CobwebTree:  54%|    | 5401/10000 [02:04<01:58, 38.97it/s]Training CobwebTree:  54%|    | 5405/10000 [02:04<01:58, 38.62it/s]Training CobwebTree:  54%|    | 5410/10000 [02:04<01:56, 39.33it/s]Training CobwebTree:  54%|    | 5414/10000 [02:04<01:58, 38.77it/s]Training CobwebTree:  54%|    | 5418/10000 [02:04<02:00, 38.09it/s]Training CobwebTree:  54%|    | 5422/10000 [02:04<02:04, 36.80it/s]Training CobwebTree:  54%|    | 5426/10000 [02:04<02:06, 36.05it/s]Training CobwebTree:  54%|    | 5430/10000 [02:04<02:05, 36.42it/s]Training CobwebTree:  54%|    | 5434/10000 [02:05<02:04, 36.78it/s]Training CobwebTree:  54%|    | 5439/10000 [02:05<01:59, 38.21it/s]Training CobwebTree:  54%|    | 5443/10000 [02:05<01:58, 38.59it/s]Training CobwebTree:  54%|    | 5447/10000 [02:05<01:57, 38.72it/s]Training CobwebTree:  55%|    | 5452/10000 [02:05<01:57, 38.74it/s]Training CobwebTree:  55%|    | 5457/10000 [02:05<01:57, 38.54it/s]Training CobwebTree:  55%|    | 5461/10000 [02:05<01:56, 38.85it/s]Training CobwebTree:  55%|    | 5465/10000 [02:05<02:01, 37.38it/s]Training CobwebTree:  55%|    | 5469/10000 [02:05<02:02, 37.12it/s]Training CobwebTree:  55%|    | 5474/10000 [02:06<01:54, 39.55it/s]Training CobwebTree:  55%|    | 5478/10000 [02:06<01:57, 38.57it/s]Training CobwebTree:  55%|    | 5482/10000 [02:06<01:56, 38.92it/s]Training CobwebTree:  55%|    | 5487/10000 [02:06<01:55, 39.17it/s]Training CobwebTree:  55%|    | 5492/10000 [02:06<01:52, 39.98it/s]Training CobwebTree:  55%|    | 5496/10000 [02:06<01:57, 38.48it/s]Training CobwebTree:  55%|    | 5500/10000 [02:06<01:56, 38.48it/s]Training CobwebTree:  55%|    | 5504/10000 [02:06<01:59, 37.76it/s]Training CobwebTree:  55%|    | 5508/10000 [02:06<02:03, 36.30it/s]Training CobwebTree:  55%|    | 5513/10000 [02:07<01:56, 38.60it/s]Training CobwebTree:  55%|    | 5517/10000 [02:07<01:55, 38.98it/s]Training CobwebTree:  55%|    | 5521/10000 [02:07<01:57, 38.13it/s]Training CobwebTree:  55%|    | 5526/10000 [02:07<01:54, 39.00it/s]Training CobwebTree:  55%|    | 5530/10000 [02:07<01:56, 38.44it/s]Training CobwebTree:  55%|    | 5534/10000 [02:07<01:55, 38.74it/s]Training CobwebTree:  55%|    | 5539/10000 [02:07<01:51, 39.96it/s]Training CobwebTree:  55%|    | 5543/10000 [02:07<01:54, 39.06it/s]Training CobwebTree:  55%|    | 5547/10000 [02:07<01:55, 38.60it/s]Training CobwebTree:  56%|    | 5551/10000 [02:08<01:56, 38.12it/s]Training CobwebTree:  56%|    | 5555/10000 [02:08<01:58, 37.57it/s]Training CobwebTree:  56%|    | 5560/10000 [02:08<01:53, 39.20it/s]Training CobwebTree:  56%|    | 5564/10000 [02:08<01:59, 37.25it/s]Training CobwebTree:  56%|    | 5568/10000 [02:08<01:56, 37.95it/s]Training CobwebTree:  56%|    | 5573/10000 [02:08<01:49, 40.36it/s]Training CobwebTree:  56%|    | 5578/10000 [02:08<01:49, 40.39it/s]Training CobwebTree:  56%|    | 5583/10000 [02:08<01:49, 40.19it/s]Training CobwebTree:  56%|    | 5588/10000 [02:09<01:50, 39.94it/s]Training CobwebTree:  56%|    | 5592/10000 [02:09<01:52, 39.28it/s]Training CobwebTree:  56%|    | 5596/10000 [02:09<01:53, 38.84it/s]Training CobwebTree:  56%|    | 5601/10000 [02:09<01:52, 39.04it/s]Training CobwebTree:  56%|    | 5605/10000 [02:09<01:51, 39.25it/s]Training CobwebTree:  56%|    | 5609/10000 [02:09<01:55, 37.97it/s]Training CobwebTree:  56%|    | 5614/10000 [02:09<01:52, 38.90it/s]Training CobwebTree:  56%|    | 5618/10000 [02:09<01:57, 37.43it/s]Training CobwebTree:  56%|    | 5622/10000 [02:09<01:56, 37.61it/s]Training CobwebTree:  56%|    | 5626/10000 [02:10<01:59, 36.50it/s]Training CobwebTree:  56%|    | 5630/10000 [02:10<01:59, 36.58it/s]Training CobwebTree:  56%|    | 5634/10000 [02:10<01:57, 37.16it/s]Training CobwebTree:  56%|    | 5638/10000 [02:10<01:56, 37.59it/s]Training CobwebTree:  56%|    | 5643/10000 [02:10<01:53, 38.24it/s]Training CobwebTree:  56%|    | 5647/10000 [02:10<01:58, 36.76it/s]Training CobwebTree:  57%|    | 5651/10000 [02:10<02:01, 35.72it/s]Training CobwebTree:  57%|    | 5655/10000 [02:10<02:01, 35.90it/s]Training CobwebTree:  57%|    | 5659/10000 [02:10<02:02, 35.35it/s]Training CobwebTree:  57%|    | 5664/10000 [02:11<01:54, 37.99it/s]Training CobwebTree:  57%|    | 5668/10000 [02:11<01:56, 37.18it/s]Training CobwebTree:  57%|    | 5672/10000 [02:11<01:55, 37.63it/s]Training CobwebTree:  57%|    | 5676/10000 [02:11<01:57, 36.88it/s]Training CobwebTree:  57%|    | 5680/10000 [02:11<01:54, 37.60it/s]Training CobwebTree:  57%|    | 5685/10000 [02:11<01:51, 38.68it/s]Training CobwebTree:  57%|    | 5690/10000 [02:11<01:50, 38.89it/s]Training CobwebTree:  57%|    | 5694/10000 [02:11<01:51, 38.52it/s]Training CobwebTree:  57%|    | 5698/10000 [02:11<01:53, 37.84it/s]Training CobwebTree:  57%|    | 5703/10000 [02:12<01:51, 38.70it/s]Training CobwebTree:  57%|    | 5707/10000 [02:12<01:52, 38.29it/s]Training CobwebTree:  57%|    | 5711/10000 [02:12<01:52, 38.17it/s]Training CobwebTree:  57%|    | 5715/10000 [02:12<01:57, 36.46it/s]Training CobwebTree:  57%|    | 5719/10000 [02:12<01:55, 37.12it/s]Training CobwebTree:  57%|    | 5723/10000 [02:12<01:52, 37.91it/s]Training CobwebTree:  57%|    | 5727/10000 [02:12<01:52, 38.07it/s]Training CobwebTree:  57%|    | 5731/10000 [02:12<01:55, 37.03it/s]Training CobwebTree:  57%|    | 5735/10000 [02:12<01:55, 37.01it/s]Training CobwebTree:  57%|    | 5739/10000 [02:13<01:55, 36.95it/s]Training CobwebTree:  57%|    | 5743/10000 [02:13<01:52, 37.71it/s]Training CobwebTree:  57%|    | 5747/10000 [02:13<01:51, 38.25it/s]Training CobwebTree:  58%|    | 5751/10000 [02:13<01:52, 37.84it/s]Training CobwebTree:  58%|    | 5755/10000 [02:13<01:52, 37.89it/s]Training CobwebTree:  58%|    | 5759/10000 [02:13<01:51, 38.20it/s]Training CobwebTree:  58%|    | 5763/10000 [02:13<02:00, 35.05it/s]Training CobwebTree:  58%|    | 5767/10000 [02:13<01:58, 35.67it/s]Training CobwebTree:  58%|    | 5771/10000 [02:13<01:59, 35.41it/s]Training CobwebTree:  58%|    | 5775/10000 [02:14<02:01, 34.66it/s]Training CobwebTree:  58%|    | 5780/10000 [02:14<01:52, 37.35it/s]Training CobwebTree:  58%|    | 5784/10000 [02:14<01:53, 37.16it/s]Training CobwebTree:  58%|    | 5788/10000 [02:14<01:55, 36.50it/s]Training CobwebTree:  58%|    | 5792/10000 [02:14<01:54, 36.76it/s]Training CobwebTree:  58%|    | 5796/10000 [02:14<01:51, 37.58it/s]Training CobwebTree:  58%|    | 5801/10000 [02:14<01:49, 38.38it/s]Training CobwebTree:  58%|    | 5806/10000 [02:14<01:45, 39.75it/s]Training CobwebTree:  58%|    | 5811/10000 [02:14<01:42, 40.99it/s]Training CobwebTree:  58%|    | 5816/10000 [02:15<01:42, 41.00it/s]Training CobwebTree:  58%|    | 5821/10000 [02:15<01:45, 39.49it/s]Training CobwebTree:  58%|    | 5825/10000 [02:15<01:47, 38.86it/s]Training CobwebTree:  58%|    | 5829/10000 [02:15<01:49, 38.19it/s]Training CobwebTree:  58%|    | 5834/10000 [02:15<01:44, 39.92it/s]Training CobwebTree:  58%|    | 5839/10000 [02:15<01:43, 40.35it/s]Training CobwebTree:  58%|    | 5844/10000 [02:15<01:41, 40.97it/s]Training CobwebTree:  58%|    | 5849/10000 [02:15<01:41, 40.88it/s]Training CobwebTree:  59%|    | 5854/10000 [02:16<01:45, 39.29it/s]Training CobwebTree:  59%|    | 5858/10000 [02:16<01:51, 37.10it/s]Training CobwebTree:  59%|    | 5862/10000 [02:16<01:53, 36.45it/s]Training CobwebTree:  59%|    | 5866/10000 [02:16<01:53, 36.46it/s]Training CobwebTree:  59%|    | 5870/10000 [02:16<01:52, 36.82it/s]Training CobwebTree:  59%|    | 5875/10000 [02:16<01:50, 37.46it/s]Training CobwebTree:  59%|    | 5879/10000 [02:16<01:48, 37.90it/s]Training CobwebTree:  59%|    | 5883/10000 [02:16<01:50, 37.37it/s]Training CobwebTree:  59%|    | 5887/10000 [02:16<01:49, 37.65it/s]Training CobwebTree:  59%|    | 5892/10000 [02:17<01:47, 38.17it/s]Training CobwebTree:  59%|    | 5896/10000 [02:17<01:47, 38.19it/s]Training CobwebTree:  59%|    | 5900/10000 [02:17<01:49, 37.35it/s]Training CobwebTree:  59%|    | 5905/10000 [02:17<01:46, 38.48it/s]Training CobwebTree:  59%|    | 5910/10000 [02:17<01:42, 39.75it/s]Training CobwebTree:  59%|    | 5914/10000 [02:17<01:46, 38.34it/s]Training CobwebTree:  59%|    | 5919/10000 [02:17<01:46, 38.36it/s]Training CobwebTree:  59%|    | 5923/10000 [02:17<01:47, 38.07it/s]Training CobwebTree:  59%|    | 5928/10000 [02:17<01:42, 39.86it/s]Training CobwebTree:  59%|    | 5933/10000 [02:18<01:40, 40.44it/s]Training CobwebTree:  59%|    | 5938/10000 [02:18<01:42, 39.71it/s]Training CobwebTree:  59%|    | 5942/10000 [02:18<01:46, 38.18it/s]Training CobwebTree:  59%|    | 5947/10000 [02:18<01:43, 39.06it/s]Training CobwebTree:  60%|    | 5951/10000 [02:18<01:47, 37.80it/s]Training CobwebTree:  60%|    | 5956/10000 [02:18<01:44, 38.78it/s]Training CobwebTree:  60%|    | 5960/10000 [02:18<01:45, 38.27it/s]Training CobwebTree:  60%|    | 5964/10000 [02:18<01:45, 38.37it/s]Training CobwebTree:  60%|    | 5968/10000 [02:19<01:47, 37.46it/s]Training CobwebTree:  60%|    | 5972/10000 [02:19<01:49, 36.80it/s]Training CobwebTree:  60%|    | 5976/10000 [02:19<01:47, 37.28it/s]Training CobwebTree:  60%|    | 5981/10000 [02:19<01:43, 38.74it/s]Training CobwebTree:  60%|    | 5986/10000 [02:19<01:42, 39.31it/s]Training CobwebTree:  60%|    | 5990/10000 [02:19<01:43, 38.78it/s]Training CobwebTree:  60%|    | 5995/10000 [02:19<01:42, 39.05it/s]Training CobwebTree:  60%|    | 5999/10000 [02:19<01:42, 39.07it/s]Training CobwebTree:  60%|    | 6003/10000 [02:19<01:42, 39.12it/s]Training CobwebTree:  60%|    | 6007/10000 [02:20<01:43, 38.46it/s]Training CobwebTree:  60%|    | 6011/10000 [02:20<01:45, 37.81it/s]Training CobwebTree:  60%|    | 6016/10000 [02:20<01:42, 38.83it/s]Training CobwebTree:  60%|    | 6021/10000 [02:20<01:39, 40.12it/s]Training CobwebTree:  60%|    | 6026/10000 [02:20<01:39, 39.87it/s]Training CobwebTree:  60%|    | 6030/10000 [02:20<01:40, 39.32it/s]Training CobwebTree:  60%|    | 6034/10000 [02:20<01:41, 38.97it/s]Training CobwebTree:  60%|    | 6038/10000 [02:20<01:42, 38.64it/s]Training CobwebTree:  60%|    | 6043/10000 [02:20<01:39, 39.92it/s]Training CobwebTree:  60%|    | 6047/10000 [02:21<01:40, 39.35it/s]Training CobwebTree:  61%|    | 6052/10000 [02:21<01:39, 39.85it/s]Training CobwebTree:  61%|    | 6056/10000 [02:21<01:40, 39.40it/s]Training CobwebTree:  61%|    | 6060/10000 [02:21<01:39, 39.46it/s]Training CobwebTree:  61%|    | 6065/10000 [02:21<01:37, 40.17it/s]Training CobwebTree:  61%|    | 6070/10000 [02:21<01:36, 40.88it/s]Training CobwebTree:  61%|    | 6075/10000 [02:21<01:38, 39.72it/s]Training CobwebTree:  61%|    | 6080/10000 [02:21<01:38, 39.82it/s]Training CobwebTree:  61%|    | 6084/10000 [02:22<01:42, 38.36it/s]Training CobwebTree:  61%|    | 6088/10000 [02:22<01:45, 37.06it/s]Training CobwebTree:  61%|    | 6093/10000 [02:22<01:42, 38.10it/s]Training CobwebTree:  61%|    | 6097/10000 [02:22<01:43, 37.66it/s]Training CobwebTree:  61%|    | 6101/10000 [02:22<01:43, 37.79it/s]Training CobwebTree:  61%|    | 6105/10000 [02:22<01:43, 37.65it/s]Training CobwebTree:  61%|    | 6110/10000 [02:22<01:41, 38.29it/s]Training CobwebTree:  61%|    | 6115/10000 [02:22<01:40, 38.56it/s]Training CobwebTree:  61%|    | 6119/10000 [02:22<01:41, 38.35it/s]Training CobwebTree:  61%|    | 6124/10000 [02:23<01:39, 39.06it/s]Training CobwebTree:  61%|   | 6128/10000 [02:23<01:44, 37.15it/s]Training CobwebTree:  61%|   | 6133/10000 [02:23<01:41, 37.97it/s]Training CobwebTree:  61%|   | 6137/10000 [02:23<01:41, 38.03it/s]Training CobwebTree:  61%|   | 6141/10000 [02:23<01:41, 38.09it/s]Training CobwebTree:  61%|   | 6146/10000 [02:23<01:39, 38.61it/s]Training CobwebTree:  62%|   | 6150/10000 [02:23<01:43, 37.18it/s]Training CobwebTree:  62%|   | 6155/10000 [02:23<01:38, 39.02it/s]Training CobwebTree:  62%|   | 6160/10000 [02:23<01:33, 40.99it/s]Training CobwebTree:  62%|   | 6165/10000 [02:24<01:37, 39.27it/s]Training CobwebTree:  62%|   | 6169/10000 [02:24<01:39, 38.52it/s]Training CobwebTree:  62%|   | 6174/10000 [02:24<01:38, 38.92it/s]Training CobwebTree:  62%|   | 6178/10000 [02:24<01:37, 39.10it/s]Training CobwebTree:  62%|   | 6182/10000 [02:24<01:42, 37.35it/s]Training CobwebTree:  62%|   | 6186/10000 [02:24<01:43, 36.98it/s]Training CobwebTree:  62%|   | 6191/10000 [02:24<01:41, 37.46it/s]Training CobwebTree:  62%|   | 6195/10000 [02:24<01:41, 37.65it/s]Training CobwebTree:  62%|   | 6199/10000 [02:25<01:40, 37.69it/s]Training CobwebTree:  62%|   | 6203/10000 [02:25<01:42, 37.21it/s]Training CobwebTree:  62%|   | 6207/10000 [02:25<01:46, 35.77it/s]Training CobwebTree:  62%|   | 6211/10000 [02:25<01:44, 36.32it/s]Training CobwebTree:  62%|   | 6215/10000 [02:25<01:49, 34.58it/s]Training CobwebTree:  62%|   | 6220/10000 [02:25<01:41, 37.32it/s]Training CobwebTree:  62%|   | 6224/10000 [02:25<01:39, 37.77it/s]Training CobwebTree:  62%|   | 6228/10000 [02:25<01:44, 36.10it/s]Training CobwebTree:  62%|   | 6233/10000 [02:25<01:39, 37.87it/s]Training CobwebTree:  62%|   | 6238/10000 [02:26<01:37, 38.69it/s]Training CobwebTree:  62%|   | 6242/10000 [02:26<01:37, 38.73it/s]Training CobwebTree:  62%|   | 6246/10000 [02:26<01:39, 37.73it/s]Training CobwebTree:  62%|   | 6250/10000 [02:26<01:40, 37.27it/s]Training CobwebTree:  63%|   | 6255/10000 [02:26<01:37, 38.26it/s]Training CobwebTree:  63%|   | 6259/10000 [02:26<01:39, 37.42it/s]Training CobwebTree:  63%|   | 6264/10000 [02:26<01:39, 37.65it/s]Training CobwebTree:  63%|   | 6268/10000 [02:26<01:40, 37.19it/s]Training CobwebTree:  63%|   | 6273/10000 [02:26<01:36, 38.70it/s]Training CobwebTree:  63%|   | 6277/10000 [02:27<01:41, 36.83it/s]Training CobwebTree:  63%|   | 6281/10000 [02:27<01:44, 35.65it/s]Training CobwebTree:  63%|   | 6285/10000 [02:27<01:46, 35.03it/s]Training CobwebTree:  63%|   | 6289/10000 [02:27<01:42, 36.08it/s]Training CobwebTree:  63%|   | 6293/10000 [02:27<01:42, 36.28it/s]Training CobwebTree:  63%|   | 6297/10000 [02:27<01:41, 36.48it/s]Training CobwebTree:  63%|   | 6302/10000 [02:27<01:39, 37.10it/s]Training CobwebTree:  63%|   | 6306/10000 [02:27<01:38, 37.47it/s]Training CobwebTree:  63%|   | 6310/10000 [02:28<01:41, 36.19it/s]Training CobwebTree:  63%|   | 6314/10000 [02:28<01:44, 35.39it/s]Training CobwebTree:  63%|   | 6318/10000 [02:28<01:42, 36.01it/s]Training CobwebTree:  63%|   | 6322/10000 [02:28<01:41, 36.25it/s]Training CobwebTree:  63%|   | 6326/10000 [02:28<01:41, 36.24it/s]Training CobwebTree:  63%|   | 6331/10000 [02:28<01:36, 37.93it/s]Training CobwebTree:  63%|   | 6336/10000 [02:28<01:31, 40.20it/s]Training CobwebTree:  63%|   | 6341/10000 [02:28<01:31, 39.85it/s]Training CobwebTree:  63%|   | 6345/10000 [02:28<01:35, 38.22it/s]Training CobwebTree:  63%|   | 6349/10000 [02:29<01:37, 37.54it/s]Training CobwebTree:  64%|   | 6353/10000 [02:29<01:39, 36.53it/s]Training CobwebTree:  64%|   | 6357/10000 [02:29<01:41, 35.96it/s]Training CobwebTree:  64%|   | 6361/10000 [02:29<01:39, 36.44it/s]Training CobwebTree:  64%|   | 6365/10000 [02:29<01:37, 37.30it/s]Training CobwebTree:  64%|   | 6369/10000 [02:29<01:35, 38.00it/s]Training CobwebTree:  64%|   | 6373/10000 [02:29<01:38, 36.71it/s]Training CobwebTree:  64%|   | 6378/10000 [02:29<01:31, 39.75it/s]Training CobwebTree:  64%|   | 6382/10000 [02:29<01:31, 39.36it/s]Training CobwebTree:  64%|   | 6386/10000 [02:30<01:35, 38.00it/s]Training CobwebTree:  64%|   | 6391/10000 [02:30<01:32, 38.84it/s]Training CobwebTree:  64%|   | 6396/10000 [02:30<01:32, 39.16it/s]Training CobwebTree:  64%|   | 6400/10000 [02:30<01:34, 37.95it/s]Training CobwebTree:  64%|   | 6404/10000 [02:30<01:35, 37.79it/s]Training CobwebTree:  64%|   | 6408/10000 [02:30<01:38, 36.40it/s]Training CobwebTree:  64%|   | 6412/10000 [02:30<01:38, 36.48it/s]Training CobwebTree:  64%|   | 6417/10000 [02:30<01:30, 39.73it/s]Training CobwebTree:  64%|   | 6421/10000 [02:30<01:31, 39.23it/s]Training CobwebTree:  64%|   | 6425/10000 [02:31<01:37, 36.67it/s]Training CobwebTree:  64%|   | 6429/10000 [02:31<01:41, 35.36it/s]Training CobwebTree:  64%|   | 6434/10000 [02:31<01:34, 37.78it/s]Training CobwebTree:  64%|   | 6438/10000 [02:31<01:36, 36.82it/s]Training CobwebTree:  64%|   | 6442/10000 [02:31<01:40, 35.43it/s]Training CobwebTree:  64%|   | 6447/10000 [02:31<01:33, 38.10it/s]Training CobwebTree:  65%|   | 6451/10000 [02:31<01:33, 38.09it/s]Training CobwebTree:  65%|   | 6455/10000 [02:31<01:35, 37.08it/s]Training CobwebTree:  65%|   | 6459/10000 [02:32<01:38, 35.84it/s]Training CobwebTree:  65%|   | 6463/10000 [02:32<01:37, 36.13it/s]Training CobwebTree:  65%|   | 6467/10000 [02:32<01:36, 36.77it/s]Training CobwebTree:  65%|   | 6471/10000 [02:32<01:39, 35.62it/s]Training CobwebTree:  65%|   | 6476/10000 [02:32<01:33, 37.80it/s]Training CobwebTree:  65%|   | 6480/10000 [02:32<01:33, 37.45it/s]Training CobwebTree:  65%|   | 6484/10000 [02:32<01:32, 37.88it/s]Training CobwebTree:  65%|   | 6488/10000 [02:32<01:31, 38.42it/s]Training CobwebTree:  65%|   | 6492/10000 [02:32<01:35, 36.62it/s]Training CobwebTree:  65%|   | 6497/10000 [02:33<01:30, 38.62it/s]Training CobwebTree:  65%|   | 6502/10000 [02:33<01:27, 39.77it/s]Training CobwebTree:  65%|   | 6506/10000 [02:33<01:29, 39.14it/s]Training CobwebTree:  65%|   | 6511/10000 [02:33<01:27, 39.66it/s]Training CobwebTree:  65%|   | 6515/10000 [02:33<01:30, 38.38it/s]Training CobwebTree:  65%|   | 6519/10000 [02:33<01:32, 37.50it/s]Training CobwebTree:  65%|   | 6523/10000 [02:33<01:32, 37.45it/s]Training CobwebTree:  65%|   | 6527/10000 [02:33<01:32, 37.39it/s]Training CobwebTree:  65%|   | 6532/10000 [02:33<01:28, 39.13it/s]Training CobwebTree:  65%|   | 6536/10000 [02:34<01:28, 38.93it/s]Training CobwebTree:  65%|   | 6540/10000 [02:34<01:31, 37.81it/s]Training CobwebTree:  65%|   | 6544/10000 [02:34<01:32, 37.47it/s]Training CobwebTree:  65%|   | 6548/10000 [02:34<01:33, 37.06it/s]Training CobwebTree:  66%|   | 6552/10000 [02:34<01:37, 35.44it/s]Training CobwebTree:  66%|   | 6556/10000 [02:34<01:35, 35.92it/s]Training CobwebTree:  66%|   | 6560/10000 [02:34<01:38, 34.78it/s]Training CobwebTree:  66%|   | 6565/10000 [02:34<01:35, 36.03it/s]Training CobwebTree:  66%|   | 6569/10000 [02:34<01:34, 36.44it/s]Training CobwebTree:  66%|   | 6574/10000 [02:35<01:31, 37.30it/s]Training CobwebTree:  66%|   | 6578/10000 [02:35<01:31, 37.23it/s]Training CobwebTree:  66%|   | 6583/10000 [02:35<01:28, 38.70it/s]Training CobwebTree:  66%|   | 6588/10000 [02:35<01:24, 40.29it/s]Training CobwebTree:  66%|   | 6593/10000 [02:35<01:26, 39.49it/s]Training CobwebTree:  66%|   | 6597/10000 [02:35<01:28, 38.63it/s]Training CobwebTree:  66%|   | 6601/10000 [02:35<01:28, 38.56it/s]Training CobwebTree:  66%|   | 6605/10000 [02:35<01:32, 36.82it/s]Training CobwebTree:  66%|   | 6609/10000 [02:36<01:35, 35.54it/s]Training CobwebTree:  66%|   | 6613/10000 [02:36<01:37, 34.60it/s]Training CobwebTree:  66%|   | 6618/10000 [02:36<01:31, 36.88it/s]Training CobwebTree:  66%|   | 6622/10000 [02:36<01:30, 37.25it/s]Training CobwebTree:  66%|   | 6626/10000 [02:36<01:29, 37.68it/s]Training CobwebTree:  66%|   | 6630/10000 [02:36<01:31, 37.00it/s]Training CobwebTree:  66%|   | 6634/10000 [02:36<01:30, 37.04it/s]Training CobwebTree:  66%|   | 6638/10000 [02:36<01:33, 36.04it/s]Training CobwebTree:  66%|   | 6642/10000 [02:36<01:32, 36.20it/s]Training CobwebTree:  66%|   | 6646/10000 [02:37<01:34, 35.48it/s]Training CobwebTree:  66%|   | 6650/10000 [02:37<01:33, 35.88it/s]Training CobwebTree:  67%|   | 6654/10000 [02:37<01:33, 35.78it/s]Training CobwebTree:  67%|   | 6659/10000 [02:37<01:27, 37.98it/s]Training CobwebTree:  67%|   | 6663/10000 [02:37<01:32, 35.90it/s]Training CobwebTree:  67%|   | 6667/10000 [02:37<01:32, 36.15it/s]Training CobwebTree:  67%|   | 6671/10000 [02:37<01:30, 36.90it/s]Training CobwebTree:  67%|   | 6676/10000 [02:37<01:28, 37.49it/s]Training CobwebTree:  67%|   | 6680/10000 [02:37<01:27, 37.85it/s]Training CobwebTree:  67%|   | 6684/10000 [02:38<01:28, 37.53it/s]Training CobwebTree:  67%|   | 6689/10000 [02:38<01:24, 39.23it/s]Training CobwebTree:  67%|   | 6693/10000 [02:38<01:31, 35.97it/s]Training CobwebTree:  67%|   | 6697/10000 [02:38<01:31, 36.18it/s]Training CobwebTree:  67%|   | 6701/10000 [02:38<01:32, 35.66it/s]Training CobwebTree:  67%|   | 6705/10000 [02:38<01:32, 35.68it/s]Training CobwebTree:  67%|   | 6709/10000 [02:38<01:31, 35.85it/s]Training CobwebTree:  67%|   | 6714/10000 [02:38<01:24, 38.79it/s]Training CobwebTree:  67%|   | 6718/10000 [02:38<01:26, 37.85it/s]Training CobwebTree:  67%|   | 6722/10000 [02:39<01:30, 36.18it/s]Training CobwebTree:  67%|   | 6726/10000 [02:39<01:32, 35.36it/s]Training CobwebTree:  67%|   | 6730/10000 [02:39<01:33, 35.03it/s]Training CobwebTree:  67%|   | 6735/10000 [02:39<01:28, 36.76it/s]Training CobwebTree:  67%|   | 6739/10000 [02:39<01:27, 37.06it/s]Training CobwebTree:  67%|   | 6743/10000 [02:39<01:27, 37.40it/s]Training CobwebTree:  67%|   | 6747/10000 [02:39<01:25, 37.99it/s]Training CobwebTree:  68%|   | 6751/10000 [02:39<01:29, 36.22it/s]Training CobwebTree:  68%|   | 6755/10000 [02:39<01:28, 36.64it/s]Training CobwebTree:  68%|   | 6760/10000 [02:40<01:24, 38.55it/s]Training CobwebTree:  68%|   | 6764/10000 [02:40<01:23, 38.92it/s]Training CobwebTree:  68%|   | 6768/10000 [02:40<01:26, 37.25it/s]Training CobwebTree:  68%|   | 6772/10000 [02:40<01:30, 35.53it/s]Training CobwebTree:  68%|   | 6776/10000 [02:40<01:34, 34.26it/s]Training CobwebTree:  68%|   | 6780/10000 [02:40<01:31, 35.18it/s]Training CobwebTree:  68%|   | 6784/10000 [02:40<01:29, 35.80it/s]Training CobwebTree:  68%|   | 6788/10000 [02:40<01:27, 36.78it/s]Training CobwebTree:  68%|   | 6792/10000 [02:40<01:29, 35.82it/s]Training CobwebTree:  68%|   | 6796/10000 [02:41<01:26, 36.93it/s]Training CobwebTree:  68%|   | 6800/10000 [02:41<01:28, 36.08it/s]Training CobwebTree:  68%|   | 6804/10000 [02:41<01:26, 37.12it/s]Training CobwebTree:  68%|   | 6808/10000 [02:41<01:27, 36.33it/s]Training CobwebTree:  68%|   | 6813/10000 [02:41<01:23, 38.09it/s]Training CobwebTree:  68%|   | 6817/10000 [02:41<01:25, 37.28it/s]Training CobwebTree:  68%|   | 6821/10000 [02:41<01:26, 36.79it/s]Training CobwebTree:  68%|   | 6825/10000 [02:41<01:26, 36.67it/s]Training CobwebTree:  68%|   | 6829/10000 [02:41<01:27, 36.36it/s]Training CobwebTree:  68%|   | 6833/10000 [02:42<01:26, 36.55it/s]Training CobwebTree:  68%|   | 6837/10000 [02:42<01:24, 37.48it/s]Training CobwebTree:  68%|   | 6841/10000 [02:42<01:23, 37.80it/s]Training CobwebTree:  68%|   | 6845/10000 [02:42<01:24, 37.12it/s]Training CobwebTree:  68%|   | 6850/10000 [02:42<01:18, 40.24it/s]Training CobwebTree:  69%|   | 6855/10000 [02:42<01:19, 39.74it/s]Training CobwebTree:  69%|   | 6859/10000 [02:42<01:19, 39.42it/s]Training CobwebTree:  69%|   | 6863/10000 [02:42<01:19, 39.41it/s]Training CobwebTree:  69%|   | 6867/10000 [02:42<01:21, 38.63it/s]Training CobwebTree:  69%|   | 6872/10000 [02:43<01:18, 39.83it/s]Training CobwebTree:  69%|   | 6876/10000 [02:43<01:22, 37.95it/s]Training CobwebTree:  69%|   | 6880/10000 [02:43<01:24, 36.85it/s]Training CobwebTree:  69%|   | 6884/10000 [02:43<01:22, 37.64it/s]Training CobwebTree:  69%|   | 6888/10000 [02:43<01:25, 36.34it/s]Training CobwebTree:  69%|   | 6892/10000 [02:43<01:29, 34.58it/s]Training CobwebTree:  69%|   | 6897/10000 [02:43<01:24, 36.93it/s]Training CobwebTree:  69%|   | 6902/10000 [02:43<01:20, 38.37it/s]Training CobwebTree:  69%|   | 6906/10000 [02:44<01:22, 37.64it/s]Training CobwebTree:  69%|   | 6910/10000 [02:44<01:25, 36.32it/s]Training CobwebTree:  69%|   | 6915/10000 [02:44<01:23, 36.82it/s]Training CobwebTree:  69%|   | 6919/10000 [02:44<01:27, 35.39it/s]Training CobwebTree:  69%|   | 6923/10000 [02:44<01:24, 36.51it/s]Training CobwebTree:  69%|   | 6927/10000 [02:44<01:22, 37.41it/s]Training CobwebTree:  69%|   | 6931/10000 [02:44<01:20, 38.09it/s]Training CobwebTree:  69%|   | 6936/10000 [02:44<01:15, 40.46it/s]Training CobwebTree:  69%|   | 6941/10000 [02:44<01:16, 40.05it/s]Training CobwebTree:  69%|   | 6946/10000 [02:45<01:15, 40.34it/s]Training CobwebTree:  70%|   | 6951/10000 [02:45<01:13, 41.56it/s]Training CobwebTree:  70%|   | 6956/10000 [02:45<01:15, 40.21it/s]Training CobwebTree:  70%|   | 6961/10000 [02:45<01:19, 38.37it/s]Training CobwebTree:  70%|   | 6965/10000 [02:45<01:19, 38.03it/s]Training CobwebTree:  70%|   | 6970/10000 [02:45<01:18, 38.54it/s]Training CobwebTree:  70%|   | 6974/10000 [02:45<01:20, 37.49it/s]Training CobwebTree:  70%|   | 6979/10000 [02:45<01:19, 38.20it/s]Training CobwebTree:  70%|   | 6983/10000 [02:46<01:24, 35.84it/s]Training CobwebTree:  70%|   | 6987/10000 [02:46<01:24, 35.74it/s]Training CobwebTree:  70%|   | 6991/10000 [02:46<01:22, 36.31it/s]Training CobwebTree:  70%|   | 6995/10000 [02:46<01:21, 36.85it/s]Training CobwebTree:  70%|   | 6999/10000 [02:46<01:23, 35.80it/s]Training CobwebTree:  70%|   | 7004/10000 [02:46<01:20, 37.39it/s]Training CobwebTree:  70%|   | 7008/10000 [02:46<01:19, 37.86it/s]Training CobwebTree:  70%|   | 7012/10000 [02:46<01:18, 38.00it/s]Training CobwebTree:  70%|   | 7016/10000 [02:46<01:18, 38.13it/s]Training CobwebTree:  70%|   | 7020/10000 [02:47<01:19, 37.30it/s]Training CobwebTree:  70%|   | 7024/10000 [02:47<01:20, 36.84it/s]Training CobwebTree:  70%|   | 7028/10000 [02:47<01:20, 36.91it/s]Training CobwebTree:  70%|   | 7033/10000 [02:47<01:16, 38.67it/s]Training CobwebTree:  70%|   | 7037/10000 [02:47<01:21, 36.14it/s]Training CobwebTree:  70%|   | 7041/10000 [02:47<01:22, 35.73it/s]Training CobwebTree:  70%|   | 7045/10000 [02:47<01:22, 35.68it/s]Training CobwebTree:  70%|   | 7049/10000 [02:47<01:24, 34.82it/s]Training CobwebTree:  71%|   | 7054/10000 [02:47<01:20, 36.81it/s]Training CobwebTree:  71%|   | 7058/10000 [02:48<01:21, 36.11it/s]Training CobwebTree:  71%|   | 7062/10000 [02:48<01:19, 36.93it/s]Training CobwebTree:  71%|   | 7066/10000 [02:48<01:21, 35.79it/s]Training CobwebTree:  71%|   | 7071/10000 [02:48<01:18, 37.27it/s]Training CobwebTree:  71%|   | 7075/10000 [02:48<01:18, 37.20it/s]Training CobwebTree:  71%|   | 7080/10000 [02:48<01:16, 38.14it/s]Training CobwebTree:  71%|   | 7084/10000 [02:48<01:17, 37.50it/s]Training CobwebTree:  71%|   | 7088/10000 [02:48<01:18, 37.08it/s]Training CobwebTree:  71%|   | 7093/10000 [02:49<01:14, 39.22it/s]Training CobwebTree:  71%|   | 7097/10000 [02:49<01:15, 38.65it/s]Training CobwebTree:  71%|   | 7101/10000 [02:49<01:18, 36.93it/s]Training CobwebTree:  71%|   | 7105/10000 [02:49<01:17, 37.37it/s]Training CobwebTree:  71%|   | 7109/10000 [02:49<01:21, 35.29it/s]Training CobwebTree:  71%|   | 7113/10000 [02:49<01:19, 36.47it/s]Training CobwebTree:  71%|   | 7117/10000 [02:49<01:17, 37.22it/s]Training CobwebTree:  71%|   | 7121/10000 [02:49<01:21, 35.54it/s]Training CobwebTree:  71%|  | 7126/10000 [02:49<01:19, 36.26it/s]Training CobwebTree:  71%|  | 7130/10000 [02:50<01:18, 36.71it/s]Training CobwebTree:  71%|  | 7135/10000 [02:50<01:14, 38.61it/s]Training CobwebTree:  71%|  | 7139/10000 [02:50<01:17, 36.94it/s]Training CobwebTree:  71%|  | 7144/10000 [02:50<01:14, 38.56it/s]Training CobwebTree:  71%|  | 7149/10000 [02:50<01:12, 39.59it/s]Training CobwebTree:  72%|  | 7153/10000 [02:51<02:29, 19.01it/s]Training CobwebTree:  72%|  | 7157/10000 [02:51<02:08, 22.10it/s]Training CobwebTree:  72%|  | 7161/10000 [02:51<01:53, 25.06it/s]Training CobwebTree:  72%|  | 7165/10000 [02:51<01:42, 27.63it/s]Training CobwebTree:  72%|  | 7169/10000 [02:51<01:36, 29.32it/s]Training CobwebTree:  72%|  | 7173/10000 [02:51<01:36, 29.26it/s]Training CobwebTree:  72%|  | 7177/10000 [02:51<01:29, 31.59it/s]Training CobwebTree:  72%|  | 7181/10000 [02:51<01:26, 32.64it/s]Training CobwebTree:  72%|  | 7185/10000 [02:51<01:25, 33.09it/s]Training CobwebTree:  72%|  | 7189/10000 [02:52<01:21, 34.69it/s]Training CobwebTree:  72%|  | 7193/10000 [02:52<01:19, 35.50it/s]Training CobwebTree:  72%|  | 7198/10000 [02:52<01:16, 36.77it/s]Training CobwebTree:  72%|  | 7203/10000 [02:52<01:12, 38.48it/s]Training CobwebTree:  72%|  | 7207/10000 [02:52<01:14, 37.35it/s]Training CobwebTree:  72%|  | 7211/10000 [02:52<01:16, 36.32it/s]Training CobwebTree:  72%|  | 7215/10000 [02:52<01:16, 36.20it/s]Training CobwebTree:  72%|  | 7219/10000 [02:52<01:17, 35.94it/s]Training CobwebTree:  72%|  | 7223/10000 [02:52<01:15, 36.62it/s]Training CobwebTree:  72%|  | 7227/10000 [02:53<01:17, 35.72it/s]Training CobwebTree:  72%|  | 7232/10000 [02:53<01:14, 37.38it/s]Training CobwebTree:  72%|  | 7236/10000 [02:53<01:17, 35.80it/s]Training CobwebTree:  72%|  | 7240/10000 [02:53<01:16, 36.30it/s]Training CobwebTree:  72%|  | 7244/10000 [02:53<01:14, 36.86it/s]Training CobwebTree:  72%|  | 7248/10000 [02:53<01:13, 37.20it/s]Training CobwebTree:  73%|  | 7252/10000 [02:53<01:14, 36.93it/s]Training CobwebTree:  73%|  | 7256/10000 [02:53<01:13, 37.46it/s]Training CobwebTree:  73%|  | 7260/10000 [02:53<01:14, 36.83it/s]Training CobwebTree:  73%|  | 7264/10000 [02:54<01:13, 37.20it/s]Training CobwebTree:  73%|  | 7268/10000 [02:54<01:15, 36.23it/s]Training CobwebTree:  73%|  | 7272/10000 [02:54<01:13, 36.97it/s]Training CobwebTree:  73%|  | 7276/10000 [02:54<01:15, 36.09it/s]Training CobwebTree:  73%|  | 7280/10000 [02:54<01:15, 35.95it/s]Training CobwebTree:  73%|  | 7284/10000 [02:54<01:15, 35.98it/s]Training CobwebTree:  73%|  | 7288/10000 [02:54<01:18, 34.49it/s]Training CobwebTree:  73%|  | 7293/10000 [02:54<01:14, 36.22it/s]Training CobwebTree:  73%|  | 7297/10000 [02:54<01:20, 33.70it/s]Training CobwebTree:  73%|  | 7302/10000 [02:55<01:15, 35.81it/s]Training CobwebTree:  73%|  | 7306/10000 [02:55<01:14, 36.40it/s]Training CobwebTree:  73%|  | 7310/10000 [02:55<01:16, 35.17it/s]Training CobwebTree:  73%|  | 7315/10000 [02:55<01:11, 37.60it/s]Training CobwebTree:  73%|  | 7319/10000 [02:55<01:12, 37.21it/s]Training CobwebTree:  73%|  | 7323/10000 [02:55<01:10, 37.74it/s]Training CobwebTree:  73%|  | 7327/10000 [02:55<01:11, 37.58it/s]Training CobwebTree:  73%|  | 7331/10000 [02:55<01:12, 37.00it/s]Training CobwebTree:  73%|  | 7335/10000 [02:56<01:11, 37.43it/s]Training CobwebTree:  73%|  | 7340/10000 [02:56<01:09, 38.16it/s]Training CobwebTree:  73%|  | 7344/10000 [02:56<01:10, 37.78it/s]Training CobwebTree:  73%|  | 7348/10000 [02:56<01:10, 37.45it/s]Training CobwebTree:  74%|  | 7352/10000 [02:56<01:09, 37.98it/s]Training CobwebTree:  74%|  | 7356/10000 [02:56<01:09, 37.97it/s]Training CobwebTree:  74%|  | 7360/10000 [02:56<01:12, 36.60it/s]Training CobwebTree:  74%|  | 7364/10000 [02:56<01:10, 37.22it/s]Training CobwebTree:  74%|  | 7368/10000 [02:56<01:10, 37.33it/s]Training CobwebTree:  74%|  | 7372/10000 [02:56<01:11, 36.68it/s]Training CobwebTree:  74%|  | 7376/10000 [02:57<01:11, 36.82it/s]Training CobwebTree:  74%|  | 7380/10000 [02:57<01:09, 37.65it/s]Training CobwebTree:  74%|  | 7384/10000 [02:57<01:08, 38.20it/s]Training CobwebTree:  74%|  | 7388/10000 [02:57<01:08, 38.22it/s]Training CobwebTree:  74%|  | 7392/10000 [02:57<01:08, 38.11it/s]Training CobwebTree:  74%|  | 7396/10000 [02:57<01:09, 37.62it/s]Training CobwebTree:  74%|  | 7400/10000 [02:57<01:11, 36.18it/s]Training CobwebTree:  74%|  | 7404/10000 [02:57<01:10, 36.70it/s]Training CobwebTree:  74%|  | 7408/10000 [02:57<01:08, 37.63it/s]Training CobwebTree:  74%|  | 7412/10000 [02:58<01:09, 37.38it/s]Training CobwebTree:  74%|  | 7416/10000 [02:58<01:13, 34.97it/s]Training CobwebTree:  74%|  | 7420/10000 [02:58<01:13, 35.07it/s]Training CobwebTree:  74%|  | 7424/10000 [02:58<01:11, 35.90it/s]Training CobwebTree:  74%|  | 7428/10000 [02:58<01:14, 34.43it/s]Training CobwebTree:  74%|  | 7432/10000 [02:58<01:13, 35.18it/s]Training CobwebTree:  74%|  | 7436/10000 [02:58<01:10, 36.41it/s]Training CobwebTree:  74%|  | 7440/10000 [02:58<01:11, 35.69it/s]Training CobwebTree:  74%|  | 7445/10000 [02:58<01:08, 37.43it/s]Training CobwebTree:  74%|  | 7449/10000 [02:59<01:07, 37.88it/s]Training CobwebTree:  75%|  | 7453/10000 [02:59<01:09, 36.56it/s]Training CobwebTree:  75%|  | 7457/10000 [02:59<01:09, 36.35it/s]Training CobwebTree:  75%|  | 7461/10000 [02:59<01:11, 35.32it/s]Training CobwebTree:  75%|  | 7465/10000 [02:59<01:12, 34.91it/s]Training CobwebTree:  75%|  | 7469/10000 [02:59<01:10, 36.01it/s]Training CobwebTree:  75%|  | 7474/10000 [02:59<01:07, 37.46it/s]Training CobwebTree:  75%|  | 7478/10000 [02:59<01:06, 38.04it/s]Training CobwebTree:  75%|  | 7482/10000 [03:00<01:11, 35.26it/s]Training CobwebTree:  75%|  | 7486/10000 [03:00<01:08, 36.46it/s]Training CobwebTree:  75%|  | 7490/10000 [03:00<01:07, 37.27it/s]Training CobwebTree:  75%|  | 7494/10000 [03:00<01:09, 36.23it/s]Training CobwebTree:  75%|  | 7498/10000 [03:00<01:09, 35.90it/s]Training CobwebTree:  75%|  | 7502/10000 [03:00<01:07, 37.01it/s]Training CobwebTree:  75%|  | 7506/10000 [03:00<01:11, 34.98it/s]Training CobwebTree:  75%|  | 7511/10000 [03:00<01:06, 37.28it/s]Training CobwebTree:  75%|  | 7516/10000 [03:00<01:05, 38.08it/s]Training CobwebTree:  75%|  | 7520/10000 [03:01<01:08, 36.25it/s]Training CobwebTree:  75%|  | 7525/10000 [03:01<01:09, 35.75it/s]Training CobwebTree:  75%|  | 7529/10000 [03:01<01:11, 34.67it/s]Training CobwebTree:  75%|  | 7533/10000 [03:01<01:11, 34.33it/s]Training CobwebTree:  75%|  | 7537/10000 [03:01<01:09, 35.31it/s]Training CobwebTree:  75%|  | 7541/10000 [03:01<01:10, 34.98it/s]Training CobwebTree:  75%|  | 7545/10000 [03:01<01:09, 35.40it/s]Training CobwebTree:  75%|  | 7549/10000 [03:01<01:07, 36.25it/s]Training CobwebTree:  76%|  | 7553/10000 [03:01<01:09, 35.06it/s]Training CobwebTree:  76%|  | 7558/10000 [03:02<01:06, 36.46it/s]Training CobwebTree:  76%|  | 7562/10000 [03:02<01:05, 37.27it/s]Training CobwebTree:  76%|  | 7567/10000 [03:02<01:04, 37.80it/s]Training CobwebTree:  76%|  | 7571/10000 [03:02<01:07, 35.80it/s]Training CobwebTree:  76%|  | 7576/10000 [03:02<01:05, 37.09it/s]Training CobwebTree:  76%|  | 7580/10000 [03:02<01:04, 37.35it/s]Training CobwebTree:  76%|  | 7584/10000 [03:02<01:05, 37.06it/s]Training CobwebTree:  76%|  | 7588/10000 [03:02<01:04, 37.15it/s]Training CobwebTree:  76%|  | 7592/10000 [03:03<01:06, 36.04it/s]Training CobwebTree:  76%|  | 7596/10000 [03:03<01:06, 36.34it/s]Training CobwebTree:  76%|  | 7600/10000 [03:03<01:04, 37.27it/s]Training CobwebTree:  76%|  | 7604/10000 [03:03<01:04, 37.24it/s]Training CobwebTree:  76%|  | 7608/10000 [03:03<01:04, 37.09it/s]Training CobwebTree:  76%|  | 7612/10000 [03:03<01:07, 35.36it/s]Training CobwebTree:  76%|  | 7616/10000 [03:03<01:08, 35.02it/s]Training CobwebTree:  76%|  | 7620/10000 [03:03<01:08, 34.55it/s]Training CobwebTree:  76%|  | 7624/10000 [03:03<01:08, 34.75it/s]Training CobwebTree:  76%|  | 7628/10000 [03:04<01:07, 35.32it/s]Training CobwebTree:  76%|  | 7632/10000 [03:04<01:08, 34.52it/s]Training CobwebTree:  76%|  | 7636/10000 [03:04<01:06, 35.68it/s]Training CobwebTree:  76%|  | 7640/10000 [03:04<01:05, 36.20it/s]Training CobwebTree:  76%|  | 7644/10000 [03:04<01:04, 36.59it/s]Training CobwebTree:  76%|  | 7648/10000 [03:04<01:04, 36.36it/s]Training CobwebTree:  77%|  | 7652/10000 [03:04<01:08, 34.27it/s]Training CobwebTree:  77%|  | 7656/10000 [03:04<01:10, 33.49it/s]Training CobwebTree:  77%|  | 7660/10000 [03:04<01:08, 34.17it/s]Training CobwebTree:  77%|  | 7664/10000 [03:05<01:10, 33.26it/s]Training CobwebTree:  77%|  | 7668/10000 [03:05<01:09, 33.77it/s]Training CobwebTree:  77%|  | 7672/10000 [03:05<01:10, 33.00it/s]Training CobwebTree:  77%|  | 7676/10000 [03:05<01:08, 33.70it/s]Training CobwebTree:  77%|  | 7680/10000 [03:05<01:05, 35.29it/s]Training CobwebTree:  77%|  | 7684/10000 [03:05<01:04, 35.90it/s]Training CobwebTree:  77%|  | 7688/10000 [03:05<01:03, 36.63it/s]Training CobwebTree:  77%|  | 7692/10000 [03:05<01:03, 36.17it/s]Training CobwebTree:  77%|  | 7697/10000 [03:06<01:01, 37.68it/s]Training CobwebTree:  77%|  | 7701/10000 [03:06<01:01, 37.66it/s]Training CobwebTree:  77%|  | 7705/10000 [03:06<01:02, 37.02it/s]Training CobwebTree:  77%|  | 7709/10000 [03:06<01:01, 37.26it/s]Training CobwebTree:  77%|  | 7713/10000 [03:06<01:03, 35.94it/s]Training CobwebTree:  77%|  | 7717/10000 [03:06<01:01, 37.01it/s]Training CobwebTree:  77%|  | 7721/10000 [03:06<01:01, 37.34it/s]Training CobwebTree:  77%|  | 7725/10000 [03:06<01:02, 36.21it/s]Training CobwebTree:  77%|  | 7729/10000 [03:06<01:00, 37.25it/s]Training CobwebTree:  77%|  | 7733/10000 [03:06<00:59, 37.79it/s]Training CobwebTree:  77%|  | 7737/10000 [03:07<01:00, 37.25it/s]Training CobwebTree:  77%|  | 7741/10000 [03:07<01:01, 36.61it/s]Training CobwebTree:  77%|  | 7745/10000 [03:07<01:02, 35.87it/s]Training CobwebTree:  77%|  | 7749/10000 [03:07<01:07, 33.35it/s]Training CobwebTree:  78%|  | 7753/10000 [03:07<01:05, 34.17it/s]Training CobwebTree:  78%|  | 7757/10000 [03:07<01:06, 33.86it/s]Training CobwebTree:  78%|  | 7761/10000 [03:07<01:03, 35.00it/s]Training CobwebTree:  78%|  | 7765/10000 [03:07<01:04, 34.53it/s]Training CobwebTree:  78%|  | 7769/10000 [03:08<01:04, 34.82it/s]Training CobwebTree:  78%|  | 7773/10000 [03:08<01:02, 35.41it/s]Training CobwebTree:  78%|  | 7777/10000 [03:08<01:01, 36.32it/s]Training CobwebTree:  78%|  | 7782/10000 [03:08<00:58, 37.96it/s]Training CobwebTree:  78%|  | 7786/10000 [03:08<00:58, 37.79it/s]Training CobwebTree:  78%|  | 7790/10000 [03:08<00:58, 37.68it/s]Training CobwebTree:  78%|  | 7794/10000 [03:08<00:58, 37.66it/s]Training CobwebTree:  78%|  | 7798/10000 [03:08<00:59, 36.94it/s]Training CobwebTree:  78%|  | 7802/10000 [03:08<01:03, 34.85it/s]Training CobwebTree:  78%|  | 7807/10000 [03:09<01:00, 36.32it/s]Training CobwebTree:  78%|  | 7811/10000 [03:09<01:00, 36.47it/s]Training CobwebTree:  78%|  | 7815/10000 [03:09<01:01, 35.42it/s]Training CobwebTree:  78%|  | 7819/10000 [03:09<01:00, 36.28it/s]Training CobwebTree:  78%|  | 7823/10000 [03:09<00:58, 37.16it/s]Training CobwebTree:  78%|  | 7827/10000 [03:09<00:59, 36.68it/s]Training CobwebTree:  78%|  | 7831/10000 [03:09<01:00, 35.97it/s]Training CobwebTree:  78%|  | 7835/10000 [03:09<01:01, 35.17it/s]Training CobwebTree:  78%|  | 7839/10000 [03:09<01:02, 34.53it/s]Training CobwebTree:  78%|  | 7843/10000 [03:10<01:01, 34.82it/s]Training CobwebTree:  78%|  | 7847/10000 [03:10<01:02, 34.45it/s]Training CobwebTree:  79%|  | 7851/10000 [03:10<01:02, 34.55it/s]Training CobwebTree:  79%|  | 7855/10000 [03:10<01:02, 34.41it/s]Training CobwebTree:  79%|  | 7859/10000 [03:10<01:01, 34.61it/s]Training CobwebTree:  79%|  | 7863/10000 [03:10<01:04, 33.06it/s]Training CobwebTree:  79%|  | 7867/10000 [03:10<01:04, 32.93it/s]Training CobwebTree:  79%|  | 7871/10000 [03:10<01:02, 34.03it/s]Training CobwebTree:  79%|  | 7875/10000 [03:11<01:01, 34.36it/s]Training CobwebTree:  79%|  | 7879/10000 [03:11<01:03, 33.62it/s]Training CobwebTree:  79%|  | 7883/10000 [03:11<01:00, 34.72it/s]Training CobwebTree:  79%|  | 7887/10000 [03:11<01:01, 34.54it/s]Training CobwebTree:  79%|  | 7891/10000 [03:11<01:02, 33.62it/s]Training CobwebTree:  79%|  | 7895/10000 [03:11<01:03, 33.07it/s]Training CobwebTree:  79%|  | 7899/10000 [03:11<01:03, 33.33it/s]Training CobwebTree:  79%|  | 7903/10000 [03:11<01:03, 33.22it/s]Training CobwebTree:  79%|  | 7907/10000 [03:11<01:00, 34.72it/s]Training CobwebTree:  79%|  | 7911/10000 [03:12<00:59, 35.03it/s]Training CobwebTree:  79%|  | 7915/10000 [03:12<00:57, 36.21it/s]Training CobwebTree:  79%|  | 7919/10000 [03:12<00:57, 36.18it/s]Training CobwebTree:  79%|  | 7923/10000 [03:12<00:56, 36.86it/s]Training CobwebTree:  79%|  | 7927/10000 [03:12<00:59, 34.78it/s]Training CobwebTree:  79%|  | 7931/10000 [03:12<00:59, 34.56it/s]Training CobwebTree:  79%|  | 7935/10000 [03:12<00:58, 35.23it/s]Training CobwebTree:  79%|  | 7939/10000 [03:12<01:01, 33.64it/s]Training CobwebTree:  79%|  | 7943/10000 [03:12<01:02, 33.02it/s]Training CobwebTree:  79%|  | 7947/10000 [03:13<01:02, 32.77it/s]Training CobwebTree:  80%|  | 7951/10000 [03:13<00:59, 34.28it/s]Training CobwebTree:  80%|  | 7955/10000 [03:13<00:58, 35.13it/s]Training CobwebTree:  80%|  | 7959/10000 [03:13<00:58, 34.93it/s]Training CobwebTree:  80%|  | 7963/10000 [03:13<00:58, 34.82it/s]Training CobwebTree:  80%|  | 7967/10000 [03:13<00:56, 36.13it/s]Training CobwebTree:  80%|  | 7971/10000 [03:13<00:55, 36.64it/s]Training CobwebTree:  80%|  | 7976/10000 [03:13<00:53, 38.15it/s]Training CobwebTree:  80%|  | 7980/10000 [03:13<00:54, 37.33it/s]Training CobwebTree:  80%|  | 7984/10000 [03:14<00:56, 35.92it/s]Training CobwebTree:  80%|  | 7989/10000 [03:14<00:54, 37.17it/s]Training CobwebTree:  80%|  | 7993/10000 [03:14<00:54, 36.57it/s]Training CobwebTree:  80%|  | 7997/10000 [03:14<00:55, 36.11it/s]Training CobwebTree:  80%|  | 8001/10000 [03:14<00:54, 36.70it/s]Training CobwebTree:  80%|  | 8005/10000 [03:14<00:53, 37.13it/s]Training CobwebTree:  80%|  | 8009/10000 [03:14<00:55, 35.94it/s]Training CobwebTree:  80%|  | 8013/10000 [03:14<00:53, 37.04it/s]Training CobwebTree:  80%|  | 8017/10000 [03:15<00:56, 35.09it/s]Training CobwebTree:  80%|  | 8021/10000 [03:15<00:56, 34.74it/s]Training CobwebTree:  80%|  | 8025/10000 [03:15<00:56, 35.04it/s]Training CobwebTree:  80%|  | 8029/10000 [03:15<00:56, 34.78it/s]Training CobwebTree:  80%|  | 8033/10000 [03:15<00:59, 32.94it/s]Training CobwebTree:  80%|  | 8037/10000 [03:15<00:59, 33.20it/s]Training CobwebTree:  80%|  | 8041/10000 [03:15<00:56, 34.64it/s]Training CobwebTree:  80%|  | 8045/10000 [03:15<00:56, 34.82it/s]Training CobwebTree:  80%|  | 8049/10000 [03:15<00:55, 34.93it/s]Training CobwebTree:  81%|  | 8053/10000 [03:16<00:55, 34.96it/s]Training CobwebTree:  81%|  | 8058/10000 [03:16<00:52, 37.17it/s]Training CobwebTree:  81%|  | 8062/10000 [03:16<00:51, 37.64it/s]Training CobwebTree:  81%|  | 8066/10000 [03:16<00:50, 38.22it/s]Training CobwebTree:  81%|  | 8070/10000 [03:16<00:53, 36.32it/s]Training CobwebTree:  81%|  | 8074/10000 [03:16<00:51, 37.06it/s]Training CobwebTree:  81%|  | 8078/10000 [03:16<00:53, 35.70it/s]Training CobwebTree:  81%|  | 8082/10000 [03:16<00:54, 35.06it/s]Training CobwebTree:  81%|  | 8086/10000 [03:16<00:52, 36.14it/s]Training CobwebTree:  81%|  | 8090/10000 [03:17<00:54, 35.02it/s]Training CobwebTree:  81%|  | 8095/10000 [03:17<00:50, 37.72it/s]Training CobwebTree:  81%|  | 8099/10000 [03:17<00:49, 38.11it/s]Training CobwebTree:  81%|  | 8103/10000 [03:17<00:50, 37.29it/s]Training CobwebTree:  81%|  | 8107/10000 [03:17<00:51, 36.42it/s]Training CobwebTree:  81%|  | 8111/10000 [03:17<00:54, 34.51it/s]Training CobwebTree:  81%|  | 8115/10000 [03:17<00:53, 35.07it/s]Training CobwebTree:  81%|  | 8119/10000 [03:17<00:55, 33.74it/s]Training CobwebTree:  81%|  | 8124/10000 [03:18<00:53, 35.36it/s]Training CobwebTree:  81%| | 8128/10000 [03:18<00:52, 35.55it/s]Training CobwebTree:  81%| | 8132/10000 [03:18<00:52, 35.87it/s]Training CobwebTree:  81%| | 8136/10000 [03:18<00:52, 35.50it/s]Training CobwebTree:  81%| | 8140/10000 [03:18<00:51, 36.31it/s]Training CobwebTree:  81%| | 8144/10000 [03:18<00:50, 36.56it/s]Training CobwebTree:  81%| | 8148/10000 [03:18<00:49, 37.14it/s]Training CobwebTree:  82%| | 8153/10000 [03:18<00:49, 37.33it/s]Training CobwebTree:  82%| | 8157/10000 [03:18<00:49, 36.87it/s]Training CobwebTree:  82%| | 8161/10000 [03:19<00:50, 36.64it/s]Training CobwebTree:  82%| | 8165/10000 [03:19<00:50, 36.39it/s]Training CobwebTree:  82%| | 8170/10000 [03:19<00:49, 37.14it/s]Training CobwebTree:  82%| | 8174/10000 [03:19<00:50, 35.83it/s]Training CobwebTree:  82%| | 8178/10000 [03:19<00:50, 36.41it/s]Training CobwebTree:  82%| | 8182/10000 [03:19<00:50, 36.28it/s]Training CobwebTree:  82%| | 8187/10000 [03:19<00:50, 36.18it/s]Training CobwebTree:  82%| | 8191/10000 [03:19<00:49, 36.71it/s]Training CobwebTree:  82%| | 8196/10000 [03:19<00:47, 38.17it/s]Training CobwebTree:  82%| | 8200/10000 [03:20<00:48, 36.84it/s]Training CobwebTree:  82%| | 8205/10000 [03:20<00:46, 38.53it/s]Training CobwebTree:  82%| | 8209/10000 [03:20<00:46, 38.72it/s]Training CobwebTree:  82%| | 8213/10000 [03:20<00:46, 38.09it/s]Training CobwebTree:  82%| | 8217/10000 [03:20<00:48, 36.58it/s]Training CobwebTree:  82%| | 8221/10000 [03:20<00:49, 36.16it/s]Training CobwebTree:  82%| | 8226/10000 [03:20<00:46, 37.84it/s]Training CobwebTree:  82%| | 8231/10000 [03:20<00:45, 38.47it/s]Training CobwebTree:  82%| | 8235/10000 [03:21<00:45, 38.55it/s]Training CobwebTree:  82%| | 8239/10000 [03:21<00:45, 38.93it/s]Training CobwebTree:  82%| | 8243/10000 [03:21<00:45, 38.24it/s]Training CobwebTree:  82%| | 8247/10000 [03:21<00:46, 37.60it/s]Training CobwebTree:  83%| | 8251/10000 [03:21<00:47, 36.89it/s]Training CobwebTree:  83%| | 8255/10000 [03:21<00:47, 36.81it/s]Training CobwebTree:  83%| | 8259/10000 [03:21<00:47, 36.40it/s]Training CobwebTree:  83%| | 8263/10000 [03:21<00:49, 34.93it/s]Training CobwebTree:  83%| | 8267/10000 [03:21<00:48, 35.85it/s]Training CobwebTree:  83%| | 8271/10000 [03:22<00:49, 35.20it/s]Training CobwebTree:  83%| | 8276/10000 [03:22<00:47, 36.42it/s]Training CobwebTree:  83%| | 8280/10000 [03:22<00:47, 36.24it/s]Training CobwebTree:  83%| | 8285/10000 [03:22<00:47, 36.00it/s]Training CobwebTree:  83%| | 8289/10000 [03:22<00:47, 36.24it/s]Training CobwebTree:  83%| | 8293/10000 [03:22<00:48, 35.14it/s]Training CobwebTree:  83%| | 8297/10000 [03:22<00:48, 35.35it/s]Training CobwebTree:  83%| | 8301/10000 [03:22<00:47, 35.56it/s]Training CobwebTree:  83%| | 8305/10000 [03:22<00:50, 33.70it/s]Training CobwebTree:  83%| | 8310/10000 [03:23<00:46, 36.64it/s]Training CobwebTree:  83%| | 8315/10000 [03:23<00:45, 37.24it/s]Training CobwebTree:  83%| | 8319/10000 [03:23<00:44, 37.70it/s]Training CobwebTree:  83%| | 8323/10000 [03:23<00:44, 37.76it/s]Training CobwebTree:  83%| | 8327/10000 [03:23<00:45, 36.88it/s]Training CobwebTree:  83%| | 8331/10000 [03:23<00:45, 37.07it/s]Training CobwebTree:  83%| | 8335/10000 [03:23<00:45, 36.24it/s]Training CobwebTree:  83%| | 8339/10000 [03:23<00:45, 36.45it/s]Training CobwebTree:  83%| | 8343/10000 [03:23<00:45, 36.04it/s]Training CobwebTree:  83%| | 8347/10000 [03:24<00:46, 35.21it/s]Training CobwebTree:  84%| | 8351/10000 [03:24<00:48, 34.06it/s]Training CobwebTree:  84%| | 8355/10000 [03:24<00:47, 34.79it/s]Training CobwebTree:  84%| | 8360/10000 [03:24<00:43, 37.60it/s]Training CobwebTree:  84%| | 8364/10000 [03:24<00:44, 36.78it/s]Training CobwebTree:  84%| | 8368/10000 [03:24<00:44, 37.04it/s]Training CobwebTree:  84%| | 8372/10000 [03:24<00:44, 36.95it/s]Training CobwebTree:  84%| | 8376/10000 [03:24<00:44, 36.35it/s]Training CobwebTree:  84%| | 8380/10000 [03:25<00:44, 36.74it/s]Training CobwebTree:  84%| | 8384/10000 [03:25<00:45, 35.14it/s]Training CobwebTree:  84%| | 8388/10000 [03:25<00:46, 34.67it/s]Training CobwebTree:  84%| | 8392/10000 [03:25<00:46, 34.47it/s]Training CobwebTree:  84%| | 8396/10000 [03:25<00:46, 34.32it/s]Training CobwebTree:  84%| | 8401/10000 [03:25<00:44, 35.98it/s]Training CobwebTree:  84%| | 8405/10000 [03:25<00:43, 36.51it/s]Training CobwebTree:  84%| | 8409/10000 [03:25<00:43, 36.33it/s]Training CobwebTree:  84%| | 8413/10000 [03:25<00:44, 35.51it/s]Training CobwebTree:  84%| | 8417/10000 [03:26<00:44, 35.39it/s]Training CobwebTree:  84%| | 8421/10000 [03:26<00:45, 34.90it/s]Training CobwebTree:  84%| | 8425/10000 [03:26<00:45, 34.30it/s]Training CobwebTree:  84%| | 8429/10000 [03:26<00:45, 34.45it/s]Training CobwebTree:  84%| | 8433/10000 [03:26<00:45, 34.09it/s]Training CobwebTree:  84%| | 8438/10000 [03:26<00:42, 36.93it/s]Training CobwebTree:  84%| | 8442/10000 [03:26<00:42, 36.97it/s]Training CobwebTree:  84%| | 8446/10000 [03:26<00:43, 35.65it/s]Training CobwebTree:  84%| | 8450/10000 [03:27<00:44, 34.96it/s]Training CobwebTree:  85%| | 8454/10000 [03:27<00:43, 35.29it/s]Training CobwebTree:  85%| | 8458/10000 [03:27<00:42, 36.13it/s]Training CobwebTree:  85%| | 8462/10000 [03:27<00:41, 37.19it/s]Training CobwebTree:  85%| | 8466/10000 [03:27<00:42, 36.22it/s]Training CobwebTree:  85%| | 8470/10000 [03:27<00:42, 36.40it/s]Training CobwebTree:  85%| | 8474/10000 [03:27<00:43, 35.02it/s]Training CobwebTree:  85%| | 8478/10000 [03:27<00:42, 35.82it/s]Training CobwebTree:  85%| | 8482/10000 [03:27<00:41, 36.50it/s]Training CobwebTree:  85%| | 8486/10000 [03:27<00:41, 36.49it/s]Training CobwebTree:  85%| | 8490/10000 [03:28<00:41, 36.72it/s]Training CobwebTree:  85%| | 8494/10000 [03:28<00:41, 36.63it/s]Training CobwebTree:  85%| | 8499/10000 [03:28<00:39, 38.45it/s]Training CobwebTree:  85%| | 8503/10000 [03:28<00:39, 38.09it/s]Training CobwebTree:  85%| | 8507/10000 [03:28<00:39, 37.89it/s]Training CobwebTree:  85%| | 8511/10000 [03:28<00:39, 37.37it/s]Training CobwebTree:  85%| | 8515/10000 [03:28<00:40, 36.79it/s]Training CobwebTree:  85%| | 8520/10000 [03:28<00:39, 37.61it/s]Training CobwebTree:  85%| | 8524/10000 [03:29<00:39, 37.54it/s]Training CobwebTree:  85%| | 8528/10000 [03:29<00:40, 36.06it/s]Training CobwebTree:  85%| | 8533/10000 [03:29<00:37, 38.99it/s]Training CobwebTree:  85%| | 8537/10000 [03:29<00:38, 38.21it/s]Training CobwebTree:  85%| | 8541/10000 [03:29<00:40, 36.22it/s]Training CobwebTree:  85%| | 8546/10000 [03:29<00:38, 37.51it/s]Training CobwebTree:  86%| | 8550/10000 [03:29<00:38, 38.03it/s]Training CobwebTree:  86%| | 8554/10000 [03:29<00:38, 37.76it/s]Training CobwebTree:  86%| | 8558/10000 [03:29<00:37, 38.10it/s]Training CobwebTree:  86%| | 8563/10000 [03:30<00:38, 37.50it/s]Training CobwebTree:  86%| | 8567/10000 [03:30<00:38, 36.89it/s]Training CobwebTree:  86%| | 8571/10000 [03:30<00:39, 35.89it/s]Training CobwebTree:  86%| | 8576/10000 [03:30<00:37, 38.09it/s]Training CobwebTree:  86%| | 8580/10000 [03:30<00:36, 38.60it/s]Training CobwebTree:  86%| | 8585/10000 [03:30<00:35, 39.63it/s]Training CobwebTree:  86%| | 8589/10000 [03:30<00:36, 38.59it/s]Training CobwebTree:  86%| | 8593/10000 [03:30<00:37, 37.51it/s]Training CobwebTree:  86%| | 8597/10000 [03:30<00:37, 37.49it/s]Training CobwebTree:  86%| | 8602/10000 [03:31<00:35, 38.85it/s]Training CobwebTree:  86%| | 8607/10000 [03:31<00:35, 39.52it/s]Training CobwebTree:  86%| | 8611/10000 [03:31<00:36, 37.55it/s]Training CobwebTree:  86%| | 8615/10000 [03:31<00:37, 36.66it/s]Training CobwebTree:  86%| | 8619/10000 [03:31<00:38, 36.15it/s]Training CobwebTree:  86%| | 8623/10000 [03:31<00:38, 35.66it/s]Training CobwebTree:  86%| | 8627/10000 [03:31<00:40, 34.19it/s]Training CobwebTree:  86%| | 8632/10000 [03:31<00:38, 35.97it/s]Training CobwebTree:  86%| | 8637/10000 [03:32<00:36, 37.33it/s]Training CobwebTree:  86%| | 8642/10000 [03:32<00:35, 38.33it/s]Training CobwebTree:  86%| | 8646/10000 [03:32<00:35, 38.40it/s]Training CobwebTree:  86%| | 8650/10000 [03:32<00:36, 36.82it/s]Training CobwebTree:  87%| | 8654/10000 [03:32<00:36, 36.77it/s]Training CobwebTree:  87%| | 8658/10000 [03:32<00:36, 36.43it/s]Training CobwebTree:  87%| | 8662/10000 [03:32<00:35, 37.32it/s]Training CobwebTree:  87%| | 8666/10000 [03:32<00:36, 36.31it/s]Training CobwebTree:  87%| | 8671/10000 [03:32<00:35, 37.41it/s]Training CobwebTree:  87%| | 8675/10000 [03:33<00:35, 37.33it/s]Training CobwebTree:  87%| | 8679/10000 [03:33<00:35, 37.32it/s]Training CobwebTree:  87%| | 8683/10000 [03:33<00:34, 37.69it/s]Training CobwebTree:  87%| | 8687/10000 [03:33<00:35, 36.60it/s]Training CobwebTree:  87%| | 8691/10000 [03:33<00:35, 37.30it/s]Training CobwebTree:  87%| | 8695/10000 [03:33<00:38, 34.23it/s]Training CobwebTree:  87%| | 8699/10000 [03:33<00:39, 33.16it/s]Training CobwebTree:  87%| | 8703/10000 [03:33<00:39, 33.21it/s]Training CobwebTree:  87%| | 8707/10000 [03:33<00:37, 34.08it/s]Training CobwebTree:  87%| | 8711/10000 [03:34<00:37, 34.09it/s]Training CobwebTree:  87%| | 8715/10000 [03:34<00:38, 32.99it/s]Training CobwebTree:  87%| | 8719/10000 [03:34<00:36, 34.75it/s]Training CobwebTree:  87%| | 8723/10000 [03:34<00:36, 34.65it/s]Training CobwebTree:  87%| | 8727/10000 [03:34<00:36, 35.15it/s]Training CobwebTree:  87%| | 8731/10000 [03:34<00:35, 35.92it/s]Training CobwebTree:  87%| | 8735/10000 [03:34<00:34, 36.25it/s]Training CobwebTree:  87%| | 8739/10000 [03:34<00:34, 36.79it/s]Training CobwebTree:  87%| | 8743/10000 [03:34<00:33, 37.14it/s]Training CobwebTree:  87%| | 8747/10000 [03:35<00:34, 36.63it/s]Training CobwebTree:  88%| | 8751/10000 [03:35<00:33, 37.51it/s]Training CobwebTree:  88%| | 8755/10000 [03:35<00:34, 36.56it/s]Training CobwebTree:  88%| | 8759/10000 [03:35<00:34, 36.06it/s]Training CobwebTree:  88%| | 8764/10000 [03:35<00:33, 36.93it/s]Training CobwebTree:  88%| | 8768/10000 [03:35<00:32, 37.59it/s]Training CobwebTree:  88%| | 8772/10000 [03:35<00:34, 36.01it/s]Training CobwebTree:  88%| | 8776/10000 [03:35<00:33, 36.05it/s]Training CobwebTree:  88%| | 8780/10000 [03:36<00:34, 35.38it/s]Training CobwebTree:  88%| | 8785/10000 [03:36<00:33, 36.13it/s]Training CobwebTree:  88%| | 8789/10000 [03:36<00:33, 35.80it/s]Training CobwebTree:  88%| | 8793/10000 [03:36<00:33, 35.70it/s]Training CobwebTree:  88%| | 8797/10000 [03:36<00:33, 35.85it/s]Training CobwebTree:  88%| | 8802/10000 [03:36<00:32, 36.80it/s]Training CobwebTree:  88%| | 8806/10000 [03:36<00:31, 37.53it/s]Training CobwebTree:  88%| | 8811/10000 [03:36<00:30, 38.56it/s]Training CobwebTree:  88%| | 8815/10000 [03:36<00:30, 38.37it/s]Training CobwebTree:  88%| | 8819/10000 [03:37<00:31, 37.04it/s]Training CobwebTree:  88%| | 8823/10000 [03:37<00:31, 37.02it/s]Training CobwebTree:  88%| | 8827/10000 [03:37<00:31, 37.25it/s]Training CobwebTree:  88%| | 8831/10000 [03:37<00:31, 36.73it/s]Training CobwebTree:  88%| | 8835/10000 [03:37<00:32, 35.31it/s]Training CobwebTree:  88%| | 8839/10000 [03:37<00:33, 34.99it/s]Training CobwebTree:  88%| | 8843/10000 [03:37<00:33, 34.52it/s]Training CobwebTree:  88%| | 8847/10000 [03:37<00:33, 34.50it/s]Training CobwebTree:  89%| | 8851/10000 [03:37<00:33, 34.33it/s]Training CobwebTree:  89%| | 8855/10000 [03:38<00:32, 34.77it/s]Training CobwebTree:  89%| | 8860/10000 [03:38<00:31, 36.01it/s]Training CobwebTree:  89%| | 8864/10000 [03:38<00:31, 36.37it/s]Training CobwebTree:  89%| | 8868/10000 [03:38<00:31, 35.63it/s]Training CobwebTree:  89%| | 8872/10000 [03:38<00:32, 34.98it/s]Training CobwebTree:  89%| | 8876/10000 [03:38<00:33, 33.12it/s]Training CobwebTree:  89%| | 8880/10000 [03:38<00:33, 33.21it/s]Training CobwebTree:  89%| | 8884/10000 [03:38<00:34, 32.56it/s]Training CobwebTree:  89%| | 8888/10000 [03:39<00:32, 33.74it/s]Training CobwebTree:  89%| | 8892/10000 [03:39<00:32, 34.07it/s]Training CobwebTree:  89%| | 8896/10000 [03:39<00:32, 34.26it/s]Training CobwebTree:  89%| | 8900/10000 [03:39<00:31, 35.21it/s]Training CobwebTree:  89%| | 8904/10000 [03:39<00:31, 34.71it/s]Training CobwebTree:  89%| | 8908/10000 [03:39<00:31, 34.82it/s]Training CobwebTree:  89%| | 8912/10000 [03:39<00:30, 35.56it/s]Training CobwebTree:  89%| | 8916/10000 [03:39<00:31, 34.37it/s]Training CobwebTree:  89%| | 8921/10000 [03:39<00:30, 35.19it/s]Training CobwebTree:  89%| | 8925/10000 [03:40<00:30, 35.13it/s]Training CobwebTree:  89%| | 8930/10000 [03:40<00:29, 36.37it/s]Training CobwebTree:  89%| | 8934/10000 [03:40<00:29, 36.31it/s]Training CobwebTree:  89%| | 8938/10000 [03:40<00:28, 37.13it/s]Training CobwebTree:  89%| | 8942/10000 [03:40<00:29, 35.98it/s]Training CobwebTree:  89%| | 8946/10000 [03:40<00:28, 36.74it/s]Training CobwebTree:  90%| | 8950/10000 [03:40<00:28, 36.32it/s]Training CobwebTree:  90%| | 8954/10000 [03:40<00:29, 36.03it/s]Training CobwebTree:  90%| | 8958/10000 [03:40<00:28, 36.03it/s]Training CobwebTree:  90%| | 8962/10000 [03:41<00:28, 36.21it/s]Training CobwebTree:  90%| | 8966/10000 [03:41<00:28, 36.54it/s]Training CobwebTree:  90%| | 8970/10000 [03:41<00:28, 35.63it/s]Training CobwebTree:  90%| | 8974/10000 [03:41<00:28, 35.56it/s]Training CobwebTree:  90%| | 8978/10000 [03:41<00:27, 36.54it/s]Training CobwebTree:  90%| | 8982/10000 [03:41<00:27, 36.51it/s]Training CobwebTree:  90%| | 8986/10000 [03:41<00:27, 36.62it/s]Training CobwebTree:  90%| | 8990/10000 [03:41<00:28, 35.16it/s]Training CobwebTree:  90%| | 8994/10000 [03:42<00:28, 35.56it/s]Training CobwebTree:  90%| | 8998/10000 [03:42<00:28, 35.38it/s]Training CobwebTree:  90%| | 9002/10000 [03:42<00:28, 35.14it/s]Training CobwebTree:  90%| | 9006/10000 [03:42<00:27, 35.54it/s]Training CobwebTree:  90%| | 9011/10000 [03:42<00:26, 37.63it/s]Training CobwebTree:  90%| | 9015/10000 [03:42<00:26, 37.36it/s]Training CobwebTree:  90%| | 9019/10000 [03:42<00:28, 34.22it/s]Training CobwebTree:  90%| | 9023/10000 [03:42<00:29, 33.68it/s]Training CobwebTree:  90%| | 9027/10000 [03:42<00:28, 34.26it/s]Training CobwebTree:  90%| | 9031/10000 [03:43<00:27, 35.68it/s]Training CobwebTree:  90%| | 9035/10000 [03:43<00:27, 35.32it/s]Training CobwebTree:  90%| | 9039/10000 [03:43<00:28, 33.87it/s]Training CobwebTree:  90%| | 9043/10000 [03:43<00:27, 34.76it/s]Training CobwebTree:  90%| | 9047/10000 [03:43<00:26, 35.62it/s]Training CobwebTree:  91%| | 9052/10000 [03:43<00:25, 36.71it/s]Training CobwebTree:  91%| | 9056/10000 [03:43<00:25, 36.97it/s]Training CobwebTree:  91%| | 9060/10000 [03:43<00:25, 37.18it/s]Training CobwebTree:  91%| | 9064/10000 [03:43<00:25, 37.41it/s]Training CobwebTree:  91%| | 9069/10000 [03:44<00:24, 38.52it/s]Training CobwebTree:  91%| | 9073/10000 [03:44<00:25, 35.84it/s]Training CobwebTree:  91%| | 9077/10000 [03:44<00:26, 35.23it/s]Training CobwebTree:  91%| | 9081/10000 [03:44<00:26, 35.03it/s]Training CobwebTree:  91%| | 9085/10000 [03:44<00:25, 35.58it/s]Training CobwebTree:  91%| | 9089/10000 [03:44<00:25, 35.86it/s]Training CobwebTree:  91%| | 9093/10000 [03:44<00:26, 34.66it/s]Training CobwebTree:  91%| | 9097/10000 [03:44<00:25, 35.29it/s]Training CobwebTree:  91%| | 9101/10000 [03:44<00:24, 36.55it/s]Training CobwebTree:  91%| | 9105/10000 [03:45<00:23, 37.42it/s]Training CobwebTree:  91%| | 9109/10000 [03:45<00:24, 35.86it/s]Training CobwebTree:  91%| | 9113/10000 [03:45<00:24, 36.07it/s]Training CobwebTree:  91%| | 9117/10000 [03:45<00:24, 36.33it/s]Training CobwebTree:  91%| | 9121/10000 [03:45<00:23, 37.15it/s]Training CobwebTree:  91%|| 9125/10000 [03:45<00:23, 37.53it/s]Training CobwebTree:  91%|| 9129/10000 [03:45<00:24, 36.21it/s]Training CobwebTree:  91%|| 9133/10000 [03:45<00:23, 36.31it/s]Training CobwebTree:  91%|| 9138/10000 [03:45<00:23, 37.48it/s]Training CobwebTree:  91%|| 9142/10000 [03:46<00:22, 37.79it/s]Training CobwebTree:  91%|| 9146/10000 [03:46<00:22, 37.65it/s]Training CobwebTree:  92%|| 9150/10000 [03:46<00:23, 36.90it/s]Training CobwebTree:  92%|| 9154/10000 [03:46<00:22, 37.21it/s]Training CobwebTree:  92%|| 9158/10000 [03:46<00:24, 34.94it/s]Training CobwebTree:  92%|| 9162/10000 [03:46<00:24, 34.39it/s]Training CobwebTree:  92%|| 9166/10000 [03:46<00:24, 33.82it/s]Training CobwebTree:  92%|| 9170/10000 [03:46<00:24, 33.71it/s]Training CobwebTree:  92%|| 9174/10000 [03:47<00:25, 32.85it/s]Training CobwebTree:  92%|| 9178/10000 [03:47<00:23, 34.37it/s]Training CobwebTree:  92%|| 9182/10000 [03:47<00:23, 34.33it/s]Training CobwebTree:  92%|| 9186/10000 [03:47<00:26, 30.54it/s]Training CobwebTree:  92%|| 9190/10000 [03:47<00:25, 31.97it/s]Training CobwebTree:  92%|| 9194/10000 [03:47<00:23, 33.74it/s]Training CobwebTree:  92%|| 9198/10000 [03:47<00:23, 34.48it/s]Training CobwebTree:  92%|| 9202/10000 [03:47<00:22, 35.35it/s]Training CobwebTree:  92%|| 9206/10000 [03:47<00:21, 36.44it/s]Training CobwebTree:  92%|| 9210/10000 [03:48<00:22, 35.25it/s]Training CobwebTree:  92%|| 9214/10000 [03:48<00:22, 35.58it/s]Training CobwebTree:  92%|| 9218/10000 [03:48<00:22, 35.39it/s]Training CobwebTree:  92%|| 9222/10000 [03:48<00:22, 35.01it/s]Training CobwebTree:  92%|| 9226/10000 [03:48<00:21, 35.65it/s]Training CobwebTree:  92%|| 9230/10000 [03:48<00:21, 35.65it/s]Training CobwebTree:  92%|| 9234/10000 [03:48<00:20, 36.81it/s]Training CobwebTree:  92%|| 9238/10000 [03:48<00:20, 37.11it/s]Training CobwebTree:  92%|| 9242/10000 [03:48<00:20, 36.11it/s]Training CobwebTree:  92%|| 9246/10000 [03:49<00:20, 37.01it/s]Training CobwebTree:  92%|| 9250/10000 [03:49<00:21, 34.65it/s]Training CobwebTree:  93%|| 9255/10000 [03:49<00:20, 36.14it/s]Training CobwebTree:  93%|| 9259/10000 [03:49<00:20, 36.41it/s]Training CobwebTree:  93%|| 9263/10000 [03:49<00:20, 36.81it/s]Training CobwebTree:  93%|| 9267/10000 [03:49<00:19, 37.00it/s]Training CobwebTree:  93%|| 9271/10000 [03:49<00:20, 35.95it/s]Training CobwebTree:  93%|| 9275/10000 [03:49<00:20, 35.33it/s]Training CobwebTree:  93%|| 9279/10000 [03:49<00:19, 36.42it/s]Training CobwebTree:  93%|| 9283/10000 [03:50<00:19, 36.81it/s]Training CobwebTree:  93%|| 9287/10000 [03:50<00:20, 35.07it/s]Training CobwebTree:  93%|| 9292/10000 [03:50<00:19, 36.82it/s]Training CobwebTree:  93%|| 9296/10000 [03:50<00:19, 36.31it/s]Training CobwebTree:  93%|| 9300/10000 [03:50<00:19, 35.23it/s]Training CobwebTree:  93%|| 9304/10000 [03:50<00:19, 35.44it/s]Training CobwebTree:  93%|| 9308/10000 [03:50<00:19, 35.21it/s]Training CobwebTree:  93%|| 9313/10000 [03:50<00:18, 36.83it/s]Training CobwebTree:  93%|| 9317/10000 [03:51<00:18, 36.19it/s]Training CobwebTree:  93%|| 9322/10000 [03:51<00:18, 37.15it/s]Training CobwebTree:  93%|| 9326/10000 [03:51<00:18, 36.61it/s]Training CobwebTree:  93%|| 9330/10000 [03:51<00:18, 36.19it/s]Training CobwebTree:  93%|| 9334/10000 [03:51<00:18, 36.50it/s]Training CobwebTree:  93%|| 9338/10000 [03:51<00:19, 34.63it/s]Training CobwebTree:  93%|| 9342/10000 [03:51<00:18, 35.41it/s]Training CobwebTree:  93%|| 9346/10000 [03:51<00:18, 34.54it/s]Training CobwebTree:  94%|| 9350/10000 [03:51<00:19, 33.89it/s]Training CobwebTree:  94%|| 9354/10000 [03:52<00:18, 34.63it/s]Training CobwebTree:  94%|| 9358/10000 [03:52<00:18, 34.95it/s]Training CobwebTree:  94%|| 9363/10000 [03:52<00:17, 36.66it/s]Training CobwebTree:  94%|| 9367/10000 [03:52<00:17, 36.68it/s]Training CobwebTree:  94%|| 9371/10000 [03:52<00:18, 34.32it/s]Training CobwebTree:  94%|| 9375/10000 [03:52<00:18, 34.24it/s]Training CobwebTree:  94%|| 9380/10000 [03:52<00:17, 35.11it/s]Training CobwebTree:  94%|| 9384/10000 [03:52<00:17, 35.07it/s]Training CobwebTree:  94%|| 9388/10000 [03:53<00:17, 35.36it/s]Training CobwebTree:  94%|| 9392/10000 [03:53<00:16, 36.08it/s]Training CobwebTree:  94%|| 9396/10000 [03:53<00:16, 36.10it/s]Training CobwebTree:  94%|| 9401/10000 [03:53<00:16, 36.80it/s]Training CobwebTree:  94%|| 9405/10000 [03:53<00:16, 36.54it/s]Training CobwebTree:  94%|| 9410/10000 [03:53<00:15, 37.53it/s]Training CobwebTree:  94%|| 9414/10000 [03:53<00:15, 37.98it/s]Training CobwebTree:  94%|| 9418/10000 [03:53<00:15, 37.03it/s]Training CobwebTree:  94%|| 9422/10000 [03:53<00:15, 36.67it/s]Training CobwebTree:  94%|| 9426/10000 [03:54<00:15, 36.08it/s]Training CobwebTree:  94%|| 9430/10000 [03:54<00:15, 35.71it/s]Training CobwebTree:  94%|| 9434/10000 [03:54<00:15, 36.38it/s]Training CobwebTree:  94%|| 9438/10000 [03:54<00:15, 35.38it/s]Training CobwebTree:  94%|| 9442/10000 [03:54<00:15, 35.84it/s]Training CobwebTree:  94%|| 9446/10000 [03:54<00:15, 35.47it/s]Training CobwebTree:  94%|| 9450/10000 [03:54<00:15, 35.68it/s]Training CobwebTree:  95%|| 9454/10000 [03:54<00:15, 35.30it/s]Training CobwebTree:  95%|| 9458/10000 [03:55<00:16, 33.53it/s]Training CobwebTree:  95%|| 9462/10000 [03:55<00:15, 33.74it/s]Training CobwebTree:  95%|| 9466/10000 [03:55<00:15, 34.12it/s]Training CobwebTree:  95%|| 9470/10000 [03:55<00:15, 34.14it/s]Training CobwebTree:  95%|| 9474/10000 [03:55<00:15, 35.00it/s]Training CobwebTree:  95%|| 9478/10000 [03:55<00:14, 34.83it/s]Training CobwebTree:  95%|| 9482/10000 [03:55<00:14, 34.65it/s]Training CobwebTree:  95%|| 9486/10000 [03:55<00:15, 33.28it/s]Training CobwebTree:  95%|| 9490/10000 [03:55<00:14, 34.73it/s]Training CobwebTree:  95%|| 9494/10000 [03:56<00:15, 32.82it/s]Training CobwebTree:  95%|| 9498/10000 [03:56<00:14, 34.09it/s]Training CobwebTree:  95%|| 9502/10000 [03:56<00:14, 34.12it/s]Training CobwebTree:  95%|| 9506/10000 [03:56<00:14, 33.70it/s]Training CobwebTree:  95%|| 9510/10000 [03:56<00:14, 34.14it/s]Training CobwebTree:  95%|| 9514/10000 [03:56<00:14, 33.19it/s]Training CobwebTree:  95%|| 9518/10000 [03:56<00:14, 33.58it/s]Training CobwebTree:  95%|| 9522/10000 [03:56<00:13, 34.44it/s]Training CobwebTree:  95%|| 9526/10000 [03:57<00:13, 34.49it/s]Training CobwebTree:  95%|| 9530/10000 [03:57<00:13, 35.84it/s]Training CobwebTree:  95%|| 9534/10000 [03:57<00:12, 36.32it/s]Training CobwebTree:  95%|| 9538/10000 [03:57<00:12, 36.88it/s]Training CobwebTree:  95%|| 9542/10000 [03:57<00:12, 35.51it/s]Training CobwebTree:  95%|| 9546/10000 [03:57<00:13, 34.10it/s]Training CobwebTree:  96%|| 9550/10000 [03:57<00:12, 34.63it/s]Training CobwebTree:  96%|| 9554/10000 [03:57<00:12, 34.95it/s]Training CobwebTree:  96%|| 9558/10000 [03:57<00:12, 35.55it/s]Training CobwebTree:  96%|| 9562/10000 [03:58<00:12, 35.83it/s]Training CobwebTree:  96%|| 9566/10000 [03:58<00:12, 35.34it/s]Training CobwebTree:  96%|| 9571/10000 [03:58<00:11, 37.20it/s]Training CobwebTree:  96%|| 9575/10000 [03:58<00:11, 35.91it/s]Training CobwebTree:  96%|| 9579/10000 [03:58<00:11, 35.85it/s]Training CobwebTree:  96%|| 9583/10000 [03:58<00:11, 35.63it/s]Training CobwebTree:  96%|| 9587/10000 [03:58<00:11, 34.72it/s]Training CobwebTree:  96%|| 9591/10000 [03:58<00:11, 35.10it/s]Training CobwebTree:  96%|| 9595/10000 [03:58<00:11, 35.71it/s]Training CobwebTree:  96%|| 9599/10000 [03:59<00:11, 36.31it/s]Training CobwebTree:  96%|| 9603/10000 [03:59<00:10, 36.33it/s]Training CobwebTree:  96%|| 9607/10000 [03:59<00:10, 36.65it/s]Training CobwebTree:  96%|| 9611/10000 [03:59<00:10, 36.26it/s]Training CobwebTree:  96%|| 9615/10000 [03:59<00:11, 33.26it/s]Training CobwebTree:  96%|| 9619/10000 [03:59<00:11, 32.43it/s]Training CobwebTree:  96%|| 9623/10000 [03:59<00:11, 33.06it/s]Training CobwebTree:  96%|| 9627/10000 [03:59<00:11, 33.70it/s]Training CobwebTree:  96%|| 9631/10000 [04:00<00:11, 32.61it/s]Training CobwebTree:  96%|| 9635/10000 [04:00<00:10, 33.88it/s]Training CobwebTree:  96%|| 9639/10000 [04:00<00:10, 32.92it/s]Training CobwebTree:  96%|| 9644/10000 [04:00<00:10, 34.66it/s]Training CobwebTree:  96%|| 9648/10000 [04:00<00:09, 35.29it/s]Training CobwebTree:  97%|| 9652/10000 [04:00<00:10, 34.67it/s]Training CobwebTree:  97%|| 9656/10000 [04:00<00:09, 34.73it/s]Training CobwebTree:  97%|| 9660/10000 [04:00<00:09, 35.16it/s]Training CobwebTree:  97%|| 9664/10000 [04:00<00:09, 35.97it/s]Training CobwebTree:  97%|| 9668/10000 [04:01<00:09, 36.61it/s]Training CobwebTree:  97%|| 9672/10000 [04:01<00:08, 36.70it/s]Training CobwebTree:  97%|| 9676/10000 [04:01<00:08, 37.13it/s]Training CobwebTree:  97%|| 9680/10000 [04:01<00:08, 35.87it/s]Training CobwebTree:  97%|| 9684/10000 [04:01<00:08, 36.51it/s]Training CobwebTree:  97%|| 9688/10000 [04:01<00:09, 34.45it/s]Training CobwebTree:  97%|| 9692/10000 [04:01<00:08, 34.54it/s]Training CobwebTree:  97%|| 9696/10000 [04:01<00:08, 34.72it/s]Training CobwebTree:  97%|| 9700/10000 [04:01<00:08, 34.84it/s]Training CobwebTree:  97%|| 9704/10000 [04:02<00:08, 34.77it/s]Training CobwebTree:  97%|| 9708/10000 [04:02<00:08, 34.67it/s]Training CobwebTree:  97%|| 9712/10000 [04:02<00:08, 35.02it/s]Training CobwebTree:  97%|| 9716/10000 [04:02<00:08, 33.59it/s]Training CobwebTree:  97%|| 9720/10000 [04:02<00:08, 34.81it/s]Training CobwebTree:  97%|| 9724/10000 [04:02<00:07, 34.63it/s]Training CobwebTree:  97%|| 9728/10000 [04:02<00:07, 34.85it/s]Training CobwebTree:  97%|| 9732/10000 [04:02<00:07, 34.60it/s]Training CobwebTree:  97%|| 9736/10000 [04:02<00:07, 35.58it/s]Training CobwebTree:  97%|| 9740/10000 [04:03<00:07, 35.57it/s]Training CobwebTree:  97%|| 9744/10000 [04:03<00:07, 34.08it/s]Training CobwebTree:  97%|| 9748/10000 [04:03<00:07, 34.60it/s]Training CobwebTree:  98%|| 9752/10000 [04:03<00:06, 35.75it/s]Training CobwebTree:  98%|| 9756/10000 [04:03<00:07, 34.57it/s]Training CobwebTree:  98%|| 9760/10000 [04:03<00:06, 34.38it/s]Training CobwebTree:  98%|| 9764/10000 [04:03<00:06, 33.88it/s]Training CobwebTree:  98%|| 9768/10000 [04:03<00:07, 32.59it/s]Training CobwebTree:  98%|| 9772/10000 [04:04<00:07, 32.13it/s]Training CobwebTree:  98%|| 9776/10000 [04:04<00:06, 33.89it/s]Training CobwebTree:  98%|| 9780/10000 [04:04<00:06, 33.43it/s]Training CobwebTree:  98%|| 9784/10000 [04:04<00:06, 33.27it/s]Training CobwebTree:  98%|| 9788/10000 [04:04<00:06, 34.61it/s]Training CobwebTree:  98%|| 9793/10000 [04:04<00:05, 36.36it/s]Training CobwebTree:  98%|| 9797/10000 [04:04<00:05, 35.73it/s]Training CobwebTree:  98%|| 9801/10000 [04:04<00:05, 36.29it/s]Training CobwebTree:  98%|| 9805/10000 [04:04<00:05, 36.08it/s]Training CobwebTree:  98%|| 9809/10000 [04:05<00:05, 35.28it/s]Training CobwebTree:  98%|| 9813/10000 [04:05<00:05, 34.69it/s]Training CobwebTree:  98%|| 9817/10000 [04:05<00:05, 34.59it/s]Training CobwebTree:  98%|| 9821/10000 [04:05<00:05, 35.07it/s]Training CobwebTree:  98%|| 9825/10000 [04:05<00:04, 36.06it/s]Training CobwebTree:  98%|| 9829/10000 [04:05<00:04, 36.25it/s]Training CobwebTree:  98%|| 9833/10000 [04:05<00:04, 36.47it/s]Training CobwebTree:  98%|| 9837/10000 [04:05<00:04, 37.17it/s]Training CobwebTree:  98%|| 9841/10000 [04:05<00:04, 34.98it/s]Training CobwebTree:  98%|| 9845/10000 [04:06<00:04, 34.17it/s]Training CobwebTree:  98%|| 9849/10000 [04:06<00:04, 34.85it/s]Training CobwebTree:  99%|| 9853/10000 [04:06<00:04, 34.78it/s]Training CobwebTree:  99%|| 9858/10000 [04:06<00:03, 36.60it/s]Training CobwebTree:  99%|| 9863/10000 [04:06<00:03, 34.61it/s]Training CobwebTree:  99%|| 9867/10000 [04:06<00:03, 34.93it/s]Training CobwebTree:  99%|| 9871/10000 [04:06<00:03, 34.73it/s]Training CobwebTree:  99%|| 9876/10000 [04:06<00:03, 35.46it/s]Training CobwebTree:  99%|| 9880/10000 [04:07<00:03, 34.76it/s]Training CobwebTree:  99%|| 9884/10000 [04:07<00:03, 35.07it/s]Training CobwebTree:  99%|| 9888/10000 [04:07<00:03, 34.89it/s]Training CobwebTree:  99%|| 9892/10000 [04:07<00:02, 36.19it/s]Training CobwebTree:  99%|| 9896/10000 [04:07<00:02, 37.02it/s]Training CobwebTree:  99%|| 9900/10000 [04:07<00:02, 36.37it/s]Training CobwebTree:  99%|| 9904/10000 [04:07<00:02, 34.48it/s]Training CobwebTree:  99%|| 9908/10000 [04:07<00:02, 34.14it/s]Training CobwebTree:  99%|| 9912/10000 [04:08<00:02, 33.56it/s]Training CobwebTree:  99%|| 9916/10000 [04:08<00:02, 33.59it/s]Training CobwebTree:  99%|| 9920/10000 [04:08<00:02, 33.11it/s]Training CobwebTree:  99%|| 9924/10000 [04:08<00:02, 33.31it/s]Training CobwebTree:  99%|| 9928/10000 [04:08<00:02, 34.47it/s]Training CobwebTree:  99%|| 9932/10000 [04:08<00:01, 34.48it/s]Training CobwebTree:  99%|| 9936/10000 [04:08<00:01, 34.96it/s]Training CobwebTree:  99%|| 9940/10000 [04:08<00:01, 33.39it/s]Training CobwebTree:  99%|| 9944/10000 [04:08<00:01, 33.99it/s]Training CobwebTree:  99%|| 9948/10000 [04:09<00:01, 34.60it/s]Training CobwebTree: 100%|| 9952/10000 [04:09<00:01, 34.00it/s]Training CobwebTree: 100%|| 9956/10000 [04:09<00:01, 33.77it/s]Training CobwebTree: 100%|| 9960/10000 [04:09<00:01, 33.78it/s]Training CobwebTree: 100%|| 9964/10000 [04:09<00:01, 33.66it/s]Training CobwebTree: 100%|| 9969/10000 [04:09<00:00, 35.60it/s]Training CobwebTree: 100%|| 9973/10000 [04:09<00:00, 34.87it/s]Training CobwebTree: 100%|| 9977/10000 [04:09<00:00, 34.14it/s]Training CobwebTree: 100%|| 9981/10000 [04:10<00:00, 34.73it/s]Training CobwebTree: 100%|| 9985/10000 [04:10<00:00, 34.30it/s]Training CobwebTree: 100%|| 9989/10000 [04:10<00:00, 35.46it/s]Training CobwebTree: 100%|| 9993/10000 [04:10<00:00, 35.58it/s]Training CobwebTree: 100%|| 9997/10000 [04:10<00:00, 35.89it/s]Training CobwebTree: 100%|| 10000/10000 [04:10<00:00, 39.91it/s]
2025-12-21 12:24:17,108 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:24:20,649 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (-70568 virtual)
2025-12-21 12:24:22,211 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (-99218 virtual)
2025-12-21 12:24:23,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,520 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,561 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,766 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,868 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,918 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:23,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:23,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,049 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,052 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,130 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,534 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,760 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,840 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,863 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:24,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:24,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:25,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:25,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:25,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:25,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:25,085 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:25,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:25,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:25,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:25,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:25,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:25,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:28,349 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-21 12:24:28,777 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 86452 virtual documents
2025-12-21 12:24:30,346 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:24:33,014 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5038 virtual)
2025-12-21 12:24:33,017 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (9939 virtual)
2025-12-21 12:24:33,018 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15346 virtual)
2025-12-21 12:24:33,020 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19775 virtual)
2025-12-21 12:24:33,022 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (24513 virtual)
2025-12-21 12:24:33,024 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30004 virtual)
2025-12-21 12:24:33,026 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35561 virtual)
2025-12-21 12:24:33,029 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (40599 virtual)
2025-12-21 12:24:33,031 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45821 virtual)
2025-12-21 12:24:33,033 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51004 virtual)
2025-12-21 12:24:33,035 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (56880 virtual)
2025-12-21 12:24:33,037 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62578 virtual)
2025-12-21 12:24:33,040 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (68501 virtual)
2025-12-21 12:24:33,042 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (74285 virtual)
2025-12-21 12:24:33,044 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (78940 virtual)
2025-12-21 12:24:33,046 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (84436 virtual)
2025-12-21 12:24:33,048 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (90137 virtual)
2025-12-21 12:24:33,051 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (95470 virtual)
2025-12-21 12:24:33,053 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (100900 virtual)
2025-12-21 12:24:33,055 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (106659 virtual)
2025-12-21 12:24:33,057 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (111500 virtual)
2025-12-21 12:24:33,059 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (116869 virtual)
2025-12-21 12:24:33,062 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (122293 virtual)
2025-12-21 12:24:33,064 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (127898 virtual)
2025-12-21 12:24:33,066 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (132828 virtual)
2025-12-21 12:24:33,068 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (138085 virtual)
2025-12-21 12:24:33,070 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (143343 virtual)
2025-12-21 12:24:33,072 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (148098 virtual)
2025-12-21 12:24:33,074 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (152948 virtual)
2025-12-21 12:24:33,088 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (157644 virtual)
2025-12-21 12:24:33,100 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (162784 virtual)
2025-12-21 12:24:33,103 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (167970 virtual)
2025-12-21 12:24:33,105 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (173334 virtual)
2025-12-21 12:24:33,107 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (179046 virtual)
2025-12-21 12:24:33,129 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (184503 virtual)
2025-12-21 12:24:33,229 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (190482 virtual)
2025-12-21 12:24:33,249 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (195734 virtual)
2025-12-21 12:24:33,251 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (201130 virtual)
2025-12-21 12:24:33,285 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (206629 virtual)
2025-12-21 12:24:33,287 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (212054 virtual)
2025-12-21 12:24:33,289 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (217483 virtual)
2025-12-21 12:24:33,300 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (222543 virtual)
2025-12-21 12:24:33,308 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (227517 virtual)
2025-12-21 12:24:33,320 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (232518 virtual)
2025-12-21 12:24:33,340 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (237974 virtual)
2025-12-21 12:24:33,353 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (243778 virtual)
2025-12-21 12:24:33,409 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (249684 virtual)
2025-12-21 12:24:33,411 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (255577 virtual)
2025-12-21 12:24:33,416 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (261521 virtual)
2025-12-21 12:24:33,432 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (267029 virtual)
2025-12-21 12:24:33,469 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (272359 virtual)
2025-12-21 12:24:33,485 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (278302 virtual)
2025-12-21 12:24:33,581 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (284138 virtual)
2025-12-21 12:24:33,601 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (289694 virtual)
2025-12-21 12:24:33,657 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (295460 virtual)
2025-12-21 12:24:33,659 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (301061 virtual)
2025-12-21 12:24:33,738 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (306311 virtual)
2025-12-21 12:24:33,805 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (312194 virtual)
2025-12-21 12:24:33,808 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (318140 virtual)
2025-12-21 12:24:33,814 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (323661 virtual)
2025-12-21 12:24:33,914 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (328872 virtual)
2025-12-21 12:24:33,968 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (334616 virtual)
2025-12-21 12:24:34,031 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (340760 virtual)
2025-12-21 12:24:34,034 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (347024 virtual)
2025-12-21 12:24:34,036 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (353005 virtual)
2025-12-21 12:24:34,135 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (358895 virtual)
2025-12-21 12:24:34,142 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (364790 virtual)
2025-12-21 12:24:34,201 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (370572 virtual)
2025-12-21 12:24:34,263 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (376025 virtual)
2025-12-21 12:24:34,265 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (381884 virtual)
2025-12-21 12:24:34,268 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (387336 virtual)
2025-12-21 12:24:34,355 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (393046 virtual)
2025-12-21 12:24:34,357 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (398587 virtual)
2025-12-21 12:24:34,360 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404515 virtual)
2025-12-21 12:24:34,372 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (409840 virtual)
2025-12-21 12:24:34,397 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (415807 virtual)
2025-12-21 12:24:34,433 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (422232 virtual)
2025-12-21 12:24:34,473 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427794 virtual)
2025-12-21 12:24:34,505 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (433810 virtual)
2025-12-21 12:24:34,613 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (439773 virtual)
2025-12-21 12:24:34,615 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (445625 virtual)
2025-12-21 12:24:34,624 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (451425 virtual)
2025-12-21 12:24:34,649 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (457024 virtual)
2025-12-21 12:24:34,693 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (462670 virtual)
2025-12-21 12:24:34,725 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (468463 virtual)
2025-12-21 12:24:34,833 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (474212 virtual)
2025-12-21 12:24:34,835 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (479446 virtual)
2025-12-21 12:24:34,837 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (484627 virtual)
2025-12-21 12:24:34,941 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (490059 virtual)
2025-12-21 12:24:34,946 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (495865 virtual)
2025-12-21 12:24:35,030 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (501609 virtual)
2025-12-21 12:24:35,033 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (507454 virtual)
2025-12-21 12:24:35,035 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (512888 virtual)
2025-12-21 12:24:35,136 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (518689 virtual)
2025-12-21 12:24:35,142 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (524392 virtual)
2025-12-21 12:24:35,145 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (530209 virtual)
2025-12-21 12:24:35,218 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (536088 virtual)
2025-12-21 12:24:35,221 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (542046 virtual)
2025-12-21 12:24:35,290 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (548028 virtual)
2025-12-21 12:24:35,301 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (553932 virtual)
2025-12-21 12:24:35,379 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (560021 virtual)
2025-12-21 12:24:35,393 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (566327 virtual)
2025-12-21 12:24:35,470 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (572119 virtual)
2025-12-21 12:24:35,485 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (577590 virtual)
2025-12-21 12:24:35,551 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (583031 virtual)
2025-12-21 12:24:35,565 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (588967 virtual)
2025-12-21 12:24:35,633 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (594321 virtual)
2025-12-21 12:24:35,636 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (599788 virtual)
2025-12-21 12:24:35,642 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (605199 virtual)
2025-12-21 12:24:35,715 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (611045 virtual)
2025-12-21 12:24:35,721 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (616902 virtual)
2025-12-21 12:24:35,749 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (622705 virtual)
2025-12-21 12:24:35,781 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (628573 virtual)
2025-12-21 12:24:35,863 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (634432 virtual)
2025-12-21 12:24:35,865 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (640090 virtual)
2025-12-21 12:24:35,868 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (645815 virtual)
2025-12-21 12:24:35,983 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (651571 virtual)
2025-12-21 12:24:35,985 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (657387 virtual)
2025-12-21 12:24:36,054 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (663236 virtual)
2025-12-21 12:24:36,057 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (669362 virtual)
2025-12-21 12:24:36,073 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (675610 virtual)
2025-12-21 12:24:36,170 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (681771 virtual)
2025-12-21 12:24:36,227 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (687925 virtual)
2025-12-21 12:24:36,230 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (694382 virtual)
2025-12-21 12:24:36,298 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (700400 virtual)
2025-12-21 12:24:36,301 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (706047 virtual)
2025-12-21 12:24:36,306 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (712150 virtual)
2025-12-21 12:24:36,411 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (718089 virtual)
2025-12-21 12:24:36,414 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (723908 virtual)
2025-12-21 12:24:36,499 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (729792 virtual)
2025-12-21 12:24:36,513 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (735737 virtual)
2025-12-21 12:24:36,518 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (740988 virtual)
2025-12-21 12:24:36,617 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (746714 virtual)
2025-12-21 12:24:36,620 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (752115 virtual)
2025-12-21 12:24:36,622 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (757747 virtual)
2025-12-21 12:24:36,715 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (763660 virtual)
2025-12-21 12:24:36,717 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (769419 virtual)
2025-12-21 12:24:36,786 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (774931 virtual)
2025-12-21 12:24:36,788 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (780589 virtual)
2025-12-21 12:24:36,791 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (786081 virtual)
2025-12-21 12:24:36,894 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (791777 virtual)
2025-12-21 12:24:36,897 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (798138 virtual)
2025-12-21 12:24:36,899 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (803809 virtual)
2025-12-21 12:24:36,983 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (809775 virtual)
2025-12-21 12:24:36,990 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (815761 virtual)
2025-12-21 12:24:37,054 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (821960 virtual)
2025-12-21 12:24:37,057 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (827580 virtual)
2025-12-21 12:24:37,123 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (833163 virtual)
2025-12-21 12:24:37,125 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (838905 virtual)
2025-12-21 12:24:37,209 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (844537 virtual)
2025-12-21 12:24:37,261 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (850171 virtual)
2025-12-21 12:24:37,303 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (855698 virtual)
2025-12-21 12:24:37,317 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (861596 virtual)
2025-12-21 12:24:37,322 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (867137 virtual)
2025-12-21 12:24:37,401 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (873138 virtual)
2025-12-21 12:24:37,443 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (878914 virtual)
2025-12-21 12:24:37,444 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (880327 virtual)
2025-12-21 12:24:37,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,465 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,705 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,711 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,715 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,781 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:37,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:37,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,250 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,690 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,773 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,798 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,838 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,909 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,953 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,989 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:38,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:38,993 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,049 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,063 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,095 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,154 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:24:39,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:39,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:24:41,794 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-21 12:24:42,048 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 880332 virtual documents
2025-12-21 12:24:42,815 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.4876942899622222, 'coherence_npmi': 0.06923623921296287, 'topic_diversity': 0.9, 'inter_topic_similarity': 0.7047315835952759}
2025-12-21 12:24:42,815 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6215883718103745, 'coherence_npmi': 0.12170004896715993, 'topic_diversity': 0.662, 'inter_topic_similarity': 0.6785516142845154}
2025-12-21 12:24:42,815 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.5857835677710994, 'coherence_npmi': 0.0868922970423318, 'topic_diversity': 0.6125, 'inter_topic_similarity': 0.5951550006866455}
