2025-12-23 23:34:39,473 INFO __main__: Starting benchmark for dataset=ml_arxiv
2025-12-23 23:34:44,152 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-23 23:34:44,680 INFO gensim.corpora.dictionary: built Dictionary<25044 unique tokens: ['achievable', 'admissible', 'allowable', 'allowed', 'approach']...> from 10000 documents (total 970327 corpus positions)
2025-12-23 23:34:44,682 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<25044 unique tokens: ['achievable', 'admissible', 'allowable', 'allowed', 'approach']...> from 10000 documents (total 970327 corpus positions)", 'datetime': '2025-12-23T23:34:44.680261', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-200-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-23 23:34:45,236 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-23 23:34:45,236 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-23 23:34:47,261 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10000 docs
2025-12-23 23:38:22,806 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:38:28,563 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (-70568 virtual)
2025-12-23 23:38:31,182 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (-99218 virtual)
2025-12-23 23:38:32,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,520 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,520 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,520 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,521 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,521 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,521 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,526 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,526 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,685 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,856 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,041 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,070 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,780 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,145 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,434 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,466 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,530 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,630 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,860 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,914 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:34,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:34,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,180 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,476 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,868 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,915 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,944 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,842 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,956 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:35,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:35,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,002 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,206 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,210 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,695 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:36,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:36,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:38,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:38,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:38,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:38,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:38,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:38,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:38,532 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:38,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:41,793 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:38:44,741 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 86452 virtual documents
2025-12-23 23:38:51,673 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:38:58,240 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5038 virtual)
2025-12-23 23:38:58,244 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (9939 virtual)
2025-12-23 23:38:58,246 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15346 virtual)
2025-12-23 23:38:58,248 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19775 virtual)
2025-12-23 23:38:58,250 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (24513 virtual)
2025-12-23 23:38:58,252 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30004 virtual)
2025-12-23 23:38:58,255 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35561 virtual)
2025-12-23 23:38:58,257 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (40599 virtual)
2025-12-23 23:38:58,259 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45821 virtual)
2025-12-23 23:38:58,261 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51004 virtual)
2025-12-23 23:38:58,263 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (56880 virtual)
2025-12-23 23:38:58,265 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62578 virtual)
2025-12-23 23:38:58,267 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (68501 virtual)
2025-12-23 23:38:58,269 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (74285 virtual)
2025-12-23 23:38:58,272 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (78940 virtual)
2025-12-23 23:38:58,274 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (84436 virtual)
2025-12-23 23:38:58,276 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (90137 virtual)
2025-12-23 23:38:58,278 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (95470 virtual)
2025-12-23 23:38:58,280 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (100900 virtual)
2025-12-23 23:38:58,282 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (106659 virtual)
2025-12-23 23:38:58,284 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (111500 virtual)
2025-12-23 23:38:58,286 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (116869 virtual)
2025-12-23 23:38:58,288 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (122293 virtual)
2025-12-23 23:38:58,317 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (127898 virtual)
2025-12-23 23:38:58,319 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (132828 virtual)
2025-12-23 23:38:58,321 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (138085 virtual)
2025-12-23 23:38:58,336 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (143343 virtual)
2025-12-23 23:38:58,338 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (148098 virtual)
2025-12-23 23:38:58,357 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (152948 virtual)
2025-12-23 23:38:58,365 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (157644 virtual)
2025-12-23 23:38:58,377 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (162784 virtual)
2025-12-23 23:38:58,385 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (167970 virtual)
2025-12-23 23:38:58,397 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (173334 virtual)
2025-12-23 23:38:58,405 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (179046 virtual)
2025-12-23 23:38:58,417 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (184503 virtual)
2025-12-23 23:38:58,443 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (190482 virtual)
2025-12-23 23:38:58,466 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (195734 virtual)
2025-12-23 23:38:58,485 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (201130 virtual)
2025-12-23 23:38:58,517 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (206629 virtual)
2025-12-23 23:38:58,578 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (212054 virtual)
2025-12-23 23:38:58,593 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (217483 virtual)
2025-12-23 23:38:58,649 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (222543 virtual)
2025-12-23 23:38:58,679 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (227517 virtual)
2025-12-23 23:38:58,693 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (232518 virtual)
2025-12-23 23:38:58,765 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (237974 virtual)
2025-12-23 23:38:58,797 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (243778 virtual)
2025-12-23 23:38:58,854 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (249684 virtual)
2025-12-23 23:38:58,869 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (255577 virtual)
2025-12-23 23:38:58,946 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (261521 virtual)
2025-12-23 23:38:58,961 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (267029 virtual)
2025-12-23 23:38:59,033 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (272359 virtual)
2025-12-23 23:38:59,101 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (278302 virtual)
2025-12-23 23:38:59,104 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (284138 virtual)
2025-12-23 23:38:59,110 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (289694 virtual)
2025-12-23 23:38:59,182 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (295460 virtual)
2025-12-23 23:38:59,197 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (301061 virtual)
2025-12-23 23:38:59,289 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (306311 virtual)
2025-12-23 23:38:59,292 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (312194 virtual)
2025-12-23 23:38:59,378 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (318140 virtual)
2025-12-23 23:38:59,393 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (323661 virtual)
2025-12-23 23:38:59,462 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (328872 virtual)
2025-12-23 23:38:59,509 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (334616 virtual)
2025-12-23 23:38:59,512 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (340760 virtual)
2025-12-23 23:38:59,614 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (347024 virtual)
2025-12-23 23:38:59,629 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (353005 virtual)
2025-12-23 23:38:59,713 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (358895 virtual)
2025-12-23 23:38:59,716 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (364790 virtual)
2025-12-23 23:38:59,722 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (370572 virtual)
2025-12-23 23:38:59,817 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (376025 virtual)
2025-12-23 23:38:59,890 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (381884 virtual)
2025-12-23 23:38:59,893 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (387336 virtual)
2025-12-23 23:38:59,895 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (393046 virtual)
2025-12-23 23:38:59,989 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (398587 virtual)
2025-12-23 23:38:59,992 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404515 virtual)
2025-12-23 23:39:00,090 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (409840 virtual)
2025-12-23 23:39:00,093 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (415807 virtual)
2025-12-23 23:39:00,170 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (422232 virtual)
2025-12-23 23:39:00,185 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427794 virtual)
2025-12-23 23:39:00,274 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (433810 virtual)
2025-12-23 23:39:00,277 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (439773 virtual)
2025-12-23 23:39:00,334 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (445625 virtual)
2025-12-23 23:39:00,338 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (451425 virtual)
2025-12-23 23:39:00,434 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (457024 virtual)
2025-12-23 23:39:00,437 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (462670 virtual)
2025-12-23 23:39:00,530 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (468463 virtual)
2025-12-23 23:39:00,534 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (474212 virtual)
2025-12-23 23:39:00,590 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (479446 virtual)
2025-12-23 23:39:00,593 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (484627 virtual)
2025-12-23 23:39:00,657 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (490059 virtual)
2025-12-23 23:39:00,660 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (495865 virtual)
2025-12-23 23:39:00,734 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (501609 virtual)
2025-12-23 23:39:00,737 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (507454 virtual)
2025-12-23 23:39:00,798 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (512888 virtual)
2025-12-23 23:39:00,802 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (518689 virtual)
2025-12-23 23:39:00,858 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (524392 virtual)
2025-12-23 23:39:00,873 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (530209 virtual)
2025-12-23 23:39:00,950 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (536088 virtual)
2025-12-23 23:39:00,954 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (542046 virtual)
2025-12-23 23:39:01,017 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (548028 virtual)
2025-12-23 23:39:01,070 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (553932 virtual)
2025-12-23 23:39:01,127 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (560021 virtual)
2025-12-23 23:39:01,141 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (566327 virtual)
2025-12-23 23:39:01,230 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (572119 virtual)
2025-12-23 23:39:01,233 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (577590 virtual)
2025-12-23 23:39:01,236 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (583031 virtual)
2025-12-23 23:39:01,305 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (588967 virtual)
2025-12-23 23:39:01,366 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (594321 virtual)
2025-12-23 23:39:01,381 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (599788 virtual)
2025-12-23 23:39:01,466 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (605199 virtual)
2025-12-23 23:39:01,481 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (611045 virtual)
2025-12-23 23:39:01,578 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (616902 virtual)
2025-12-23 23:39:01,581 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (622705 virtual)
2025-12-23 23:39:01,662 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (628573 virtual)
2025-12-23 23:39:01,677 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (634432 virtual)
2025-12-23 23:39:01,746 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (640090 virtual)
2025-12-23 23:39:01,749 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (645815 virtual)
2025-12-23 23:39:01,834 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (651571 virtual)
2025-12-23 23:39:01,849 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (657387 virtual)
2025-12-23 23:39:01,941 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (663236 virtual)
2025-12-23 23:39:01,945 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (669362 virtual)
2025-12-23 23:39:02,026 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (675610 virtual)
2025-12-23 23:39:02,041 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (681771 virtual)
2025-12-23 23:39:02,085 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (687925 virtual)
2025-12-23 23:39:02,162 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (694382 virtual)
2025-12-23 23:39:02,177 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (700400 virtual)
2025-12-23 23:39:02,274 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (706047 virtual)
2025-12-23 23:39:02,289 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (712150 virtual)
2025-12-23 23:39:02,361 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (718089 virtual)
2025-12-23 23:39:02,422 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (723908 virtual)
2025-12-23 23:39:02,437 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (729792 virtual)
2025-12-23 23:39:02,511 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (735737 virtual)
2025-12-23 23:39:02,514 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (740988 virtual)
2025-12-23 23:39:02,570 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (746714 virtual)
2025-12-23 23:39:02,574 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (752115 virtual)
2025-12-23 23:39:02,630 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (757747 virtual)
2025-12-23 23:39:02,634 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (763660 virtual)
2025-12-23 23:39:02,702 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (769419 virtual)
2025-12-23 23:39:02,705 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (774931 virtual)
2025-12-23 23:39:02,766 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (780589 virtual)
2025-12-23 23:39:02,769 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (786081 virtual)
2025-12-23 23:39:02,833 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (791777 virtual)
2025-12-23 23:39:02,836 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (798138 virtual)
2025-12-23 23:39:02,898 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (803809 virtual)
2025-12-23 23:39:02,902 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (809775 virtual)
2025-12-23 23:39:02,965 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (815761 virtual)
2025-12-23 23:39:02,968 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (821960 virtual)
2025-12-23 23:39:03,037 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (827580 virtual)
2025-12-23 23:39:03,094 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (833163 virtual)
2025-12-23 23:39:03,098 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (838905 virtual)
2025-12-23 23:39:03,156 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (844537 virtual)
2025-12-23 23:39:03,159 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (850171 virtual)
2025-12-23 23:39:03,206 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (855698 virtual)
2025-12-23 23:39:03,221 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (861596 virtual)
2025-12-23 23:39:03,310 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (867137 virtual)
2025-12-23 23:39:03,313 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (873138 virtual)
2025-12-23 23:39:03,394 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (878914 virtual)
2025-12-23 23:39:03,396 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (880327 virtual)
2025-12-23 23:39:03,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,428 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,428 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,932 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,046 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,938 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,096 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:04,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:04,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,314 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,442 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,683 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,852 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:05,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:05,918 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,146 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,226 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,274 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,514 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,670 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,697 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,718 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,726 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:06,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:06,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:07,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,765 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,039 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,277 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:11,792 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:39:13,461 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 880332 virtual documents
2025-12-23 23:39:14,881 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10000 docs
2025-12-23 23:42:03,275 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:42:09,584 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (-70568 virtual)
2025-12-23 23:42:10,604 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (-99218 virtual)
2025-12-23 23:42:11,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,253 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,253 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,253 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,253 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,261 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,262 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,262 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,262 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,263 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,263 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,265 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,265 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,265 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,267 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,267 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,273 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,310 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,334 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,338 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,358 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,381 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,391 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,412 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,686 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,714 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:11,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:11,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,029 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,138 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,855 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:12,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:12,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:13,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:13,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:13,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:13,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:13,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:13,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:13,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:13,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:13,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:13,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,408 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:42:17,634 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 86452 virtual documents
2025-12-23 23:42:18,163 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:42:24,317 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5038 virtual)
2025-12-23 23:42:24,320 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (9939 virtual)
2025-12-23 23:42:24,322 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15346 virtual)
2025-12-23 23:42:24,324 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19775 virtual)
2025-12-23 23:42:24,326 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (24513 virtual)
2025-12-23 23:42:24,328 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30004 virtual)
2025-12-23 23:42:24,330 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35561 virtual)
2025-12-23 23:42:24,332 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (40599 virtual)
2025-12-23 23:42:24,334 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45821 virtual)
2025-12-23 23:42:24,336 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51004 virtual)
2025-12-23 23:42:24,338 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (56880 virtual)
2025-12-23 23:42:24,340 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62578 virtual)
2025-12-23 23:42:24,342 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (68501 virtual)
2025-12-23 23:42:24,344 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (74285 virtual)
2025-12-23 23:42:24,346 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (78940 virtual)
2025-12-23 23:42:24,348 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (84436 virtual)
2025-12-23 23:42:24,350 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (90137 virtual)
2025-12-23 23:42:24,352 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (95470 virtual)
2025-12-23 23:42:24,355 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (100900 virtual)
2025-12-23 23:42:24,357 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (106659 virtual)
2025-12-23 23:42:24,359 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (111500 virtual)
2025-12-23 23:42:24,361 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (116869 virtual)
2025-12-23 23:42:24,363 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (122293 virtual)
2025-12-23 23:42:24,365 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (127898 virtual)
2025-12-23 23:42:24,366 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (132828 virtual)
2025-12-23 23:42:24,384 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (138085 virtual)
2025-12-23 23:42:24,386 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (143343 virtual)
2025-12-23 23:42:24,388 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (148098 virtual)
2025-12-23 23:42:24,390 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (152948 virtual)
2025-12-23 23:42:24,392 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (157644 virtual)
2025-12-23 23:42:24,394 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (162784 virtual)
2025-12-23 23:42:24,396 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (167970 virtual)
2025-12-23 23:42:24,398 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (173334 virtual)
2025-12-23 23:42:24,400 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (179046 virtual)
2025-12-23 23:42:24,401 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (184503 virtual)
2025-12-23 23:42:24,416 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (190482 virtual)
2025-12-23 23:42:24,419 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (195734 virtual)
2025-12-23 23:42:24,420 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (201130 virtual)
2025-12-23 23:42:24,422 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (206629 virtual)
2025-12-23 23:42:24,424 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (212054 virtual)
2025-12-23 23:42:24,426 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (217483 virtual)
2025-12-23 23:42:24,427 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (222543 virtual)
2025-12-23 23:42:24,444 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (227517 virtual)
2025-12-23 23:42:24,446 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (232518 virtual)
2025-12-23 23:42:24,448 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (237974 virtual)
2025-12-23 23:42:24,451 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (243778 virtual)
2025-12-23 23:42:24,453 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (249684 virtual)
2025-12-23 23:42:24,455 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (255577 virtual)
2025-12-23 23:42:24,457 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (261521 virtual)
2025-12-23 23:42:24,458 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (267029 virtual)
2025-12-23 23:42:24,460 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (272359 virtual)
2025-12-23 23:42:24,649 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (278302 virtual)
2025-12-23 23:42:24,685 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (284138 virtual)
2025-12-23 23:42:24,701 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (289694 virtual)
2025-12-23 23:42:24,798 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (295460 virtual)
2025-12-23 23:42:24,813 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (301061 virtual)
2025-12-23 23:42:24,874 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (306311 virtual)
2025-12-23 23:42:24,877 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (312194 virtual)
2025-12-23 23:42:24,938 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (318140 virtual)
2025-12-23 23:42:24,953 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (323661 virtual)
2025-12-23 23:42:24,958 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (328872 virtual)
2025-12-23 23:42:25,069 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (334616 virtual)
2025-12-23 23:42:25,121 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (340760 virtual)
2025-12-23 23:42:25,124 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (347024 virtual)
2025-12-23 23:42:25,206 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (353005 virtual)
2025-12-23 23:42:25,209 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (358895 virtual)
2025-12-23 23:42:25,225 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (364790 virtual)
2025-12-23 23:42:25,326 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (370572 virtual)
2025-12-23 23:42:25,341 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (376025 virtual)
2025-12-23 23:42:25,346 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (381884 virtual)
2025-12-23 23:42:25,470 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (387336 virtual)
2025-12-23 23:42:25,473 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (393046 virtual)
2025-12-23 23:42:25,475 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (398587 virtual)
2025-12-23 23:42:25,569 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404515 virtual)
2025-12-23 23:42:25,621 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (409840 virtual)
2025-12-23 23:42:25,682 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (415807 virtual)
2025-12-23 23:42:25,685 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (422232 virtual)
2025-12-23 23:42:25,754 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427794 virtual)
2025-12-23 23:42:25,805 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (433810 virtual)
2025-12-23 23:42:25,862 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (439773 virtual)
2025-12-23 23:42:25,877 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (445625 virtual)
2025-12-23 23:42:25,969 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (451425 virtual)
2025-12-23 23:42:26,014 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (457024 virtual)
2025-12-23 23:42:26,017 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (462670 virtual)
2025-12-23 23:42:26,020 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (468463 virtual)
2025-12-23 23:42:26,142 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (474212 virtual)
2025-12-23 23:42:26,145 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (479446 virtual)
2025-12-23 23:42:26,147 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (484627 virtual)
2025-12-23 23:42:26,218 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (490059 virtual)
2025-12-23 23:42:26,221 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (495865 virtual)
2025-12-23 23:42:26,224 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (501609 virtual)
2025-12-23 23:42:26,302 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (507454 virtual)
2025-12-23 23:42:26,317 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (512888 virtual)
2025-12-23 23:42:26,322 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (518689 virtual)
2025-12-23 23:42:26,390 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (524392 virtual)
2025-12-23 23:42:26,405 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (530209 virtual)
2025-12-23 23:42:26,410 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (536088 virtual)
2025-12-23 23:42:26,478 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (542046 virtual)
2025-12-23 23:42:26,493 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (548028 virtual)
2025-12-23 23:42:26,549 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (553932 virtual)
2025-12-23 23:42:26,585 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (560021 virtual)
2025-12-23 23:42:26,637 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (566327 virtual)
2025-12-23 23:42:26,669 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (572119 virtual)
2025-12-23 23:42:26,684 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (577590 virtual)
2025-12-23 23:42:26,701 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (583031 virtual)
2025-12-23 23:42:26,764 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (588967 virtual)
2025-12-23 23:42:26,767 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (594321 virtual)
2025-12-23 23:42:26,769 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (599788 virtual)
2025-12-23 23:42:26,837 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (605199 virtual)
2025-12-23 23:42:26,857 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (611045 virtual)
2025-12-23 23:42:26,859 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (616902 virtual)
2025-12-23 23:42:26,923 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (622705 virtual)
2025-12-23 23:42:26,925 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (628573 virtual)
2025-12-23 23:42:26,941 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (634432 virtual)
2025-12-23 23:42:27,010 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (640090 virtual)
2025-12-23 23:42:27,025 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (645815 virtual)
2025-12-23 23:42:27,082 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (651571 virtual)
2025-12-23 23:42:27,085 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (657387 virtual)
2025-12-23 23:42:27,101 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (663236 virtual)
2025-12-23 23:42:27,149 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (669362 virtual)
2025-12-23 23:42:27,186 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (675610 virtual)
2025-12-23 23:42:27,201 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (681771 virtual)
2025-12-23 23:42:27,250 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (687925 virtual)
2025-12-23 23:42:27,265 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (694382 virtual)
2025-12-23 23:42:27,318 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (700400 virtual)
2025-12-23 23:42:27,333 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (706047 virtual)
2025-12-23 23:42:27,394 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (712150 virtual)
2025-12-23 23:42:27,397 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (718089 virtual)
2025-12-23 23:42:27,399 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (723908 virtual)
2025-12-23 23:42:27,470 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (729792 virtual)
2025-12-23 23:42:27,476 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (735737 virtual)
2025-12-23 23:42:27,484 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (740988 virtual)
2025-12-23 23:42:27,512 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (746714 virtual)
2025-12-23 23:42:27,515 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (752115 virtual)
2025-12-23 23:42:27,586 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (757747 virtual)
2025-12-23 23:42:27,601 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (763660 virtual)
2025-12-23 23:42:27,606 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (769419 virtual)
2025-12-23 23:42:27,690 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (774931 virtual)
2025-12-23 23:42:27,705 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (780589 virtual)
2025-12-23 23:42:27,710 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (786081 virtual)
2025-12-23 23:42:27,774 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (791777 virtual)
2025-12-23 23:42:27,776 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (798138 virtual)
2025-12-23 23:42:27,778 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (803809 virtual)
2025-12-23 23:42:27,846 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (809775 virtual)
2025-12-23 23:42:27,849 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (815761 virtual)
2025-12-23 23:42:27,853 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (821960 virtual)
2025-12-23 23:42:27,926 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (827580 virtual)
2025-12-23 23:42:27,929 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (833163 virtual)
2025-12-23 23:42:27,930 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (838905 virtual)
2025-12-23 23:42:27,983 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (844537 virtual)
2025-12-23 23:42:27,997 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (850171 virtual)
2025-12-23 23:42:28,002 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (855698 virtual)
2025-12-23 23:42:28,075 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (861596 virtual)
2025-12-23 23:42:28,077 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (867137 virtual)
2025-12-23 23:42:28,093 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (873138 virtual)
2025-12-23 23:42:28,190 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (878914 virtual)
2025-12-23 23:42:28,203 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (880327 virtual)
2025-12-23 23:42:28,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,242 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,242 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,244 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,244 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,514 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,681 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:28,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:28,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,042 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,122 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,172 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,316 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,701 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,762 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,770 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,772 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,779 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:29,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:29,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,039 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,162 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,273 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,432 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:30,654 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,654 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:30,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:32,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:32,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:32,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:32,689 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:32,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:32,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:32,708 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:32,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:35,536 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:42:35,715 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 880332 virtual documents
2025-12-23 23:42:36,114 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10000 docs
Training CobwebTree:   0%|          | 0/10000 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 17/10000 [00:00<01:00, 165.66it/s]Training CobwebTree:   0%|          | 34/10000 [00:00<01:24, 117.84it/s]Training CobwebTree:   0%|          | 47/10000 [00:00<01:48, 92.11it/s] Training CobwebTree:   1%|          | 57/10000 [00:00<01:58, 84.15it/s]Training CobwebTree:   1%|          | 66/10000 [00:00<01:59, 83.00it/s]Training CobwebTree:   1%|          | 75/10000 [00:00<02:05, 78.91it/s]Training CobwebTree:   1%|          | 84/10000 [00:00<02:12, 74.97it/s]Training CobwebTree:   1%|          | 93/10000 [00:01<02:06, 78.07it/s]Training CobwebTree:   1%|          | 101/10000 [00:01<02:08, 77.28it/s]Training CobwebTree:   1%|          | 109/10000 [00:01<02:17, 72.14it/s]Training CobwebTree:   1%|          | 117/10000 [00:01<02:20, 70.57it/s]Training CobwebTree:   1%|         | 125/10000 [00:01<02:25, 67.79it/s]Training CobwebTree:   1%|         | 132/10000 [00:01<02:26, 67.59it/s]Training CobwebTree:   1%|         | 140/10000 [00:01<02:25, 67.88it/s]Training CobwebTree:   1%|         | 147/10000 [00:01<02:27, 66.99it/s]Training CobwebTree:   2%|         | 154/10000 [00:02<02:31, 64.99it/s]Training CobwebTree:   2%|         | 161/10000 [00:02<02:33, 64.12it/s]Training CobwebTree:   2%|         | 168/10000 [00:02<02:37, 62.33it/s]Training CobwebTree:   2%|         | 175/10000 [00:02<02:39, 61.49it/s]Training CobwebTree:   2%|         | 182/10000 [00:02<02:44, 59.63it/s]Training CobwebTree:   2%|         | 190/10000 [00:02<02:35, 63.03it/s]Training CobwebTree:   2%|         | 197/10000 [00:02<02:36, 62.76it/s]Training CobwebTree:   2%|         | 204/10000 [00:02<02:39, 61.30it/s]Training CobwebTree:   2%|         | 211/10000 [00:02<02:43, 59.94it/s]Training CobwebTree:   2%|         | 218/10000 [00:03<02:48, 58.01it/s]Training CobwebTree:   2%|         | 224/10000 [00:03<02:47, 58.44it/s]Training CobwebTree:   2%|         | 231/10000 [00:03<02:44, 59.39it/s]Training CobwebTree:   2%|         | 238/10000 [00:03<02:39, 61.29it/s]Training CobwebTree:   2%|         | 245/10000 [00:03<02:42, 60.06it/s]Training CobwebTree:   3%|         | 252/10000 [00:03<02:44, 59.25it/s]Training CobwebTree:   3%|         | 258/10000 [00:03<02:48, 57.76it/s]Training CobwebTree:   3%|         | 265/10000 [00:03<02:40, 60.58it/s]Training CobwebTree:   3%|         | 272/10000 [00:03<02:44, 59.19it/s]Training CobwebTree:   3%|         | 278/10000 [00:04<02:47, 57.94it/s]Training CobwebTree:   3%|         | 284/10000 [00:04<02:48, 57.83it/s]Training CobwebTree:   3%|         | 290/10000 [00:04<02:52, 56.40it/s]Training CobwebTree:   3%|         | 296/10000 [00:04<02:53, 55.90it/s]Training CobwebTree:   3%|         | 302/10000 [00:04<02:56, 54.90it/s]Training CobwebTree:   3%|         | 308/10000 [00:04<02:53, 55.86it/s]Training CobwebTree:   3%|         | 315/10000 [00:04<02:46, 58.04it/s]Training CobwebTree:   3%|         | 321/10000 [00:04<02:53, 55.94it/s]Training CobwebTree:   3%|         | 328/10000 [00:04<02:46, 58.08it/s]Training CobwebTree:   3%|         | 334/10000 [00:05<02:49, 56.98it/s]Training CobwebTree:   3%|         | 340/10000 [00:05<02:52, 56.00it/s]Training CobwebTree:   3%|         | 347/10000 [00:05<02:44, 58.76it/s]Training CobwebTree:   4%|         | 353/10000 [00:05<02:45, 58.39it/s]Training CobwebTree:   4%|         | 359/10000 [00:05<02:45, 58.38it/s]Training CobwebTree:   4%|         | 365/10000 [00:05<02:49, 56.71it/s]Training CobwebTree:   4%|         | 371/10000 [00:05<02:55, 54.89it/s]Training CobwebTree:   4%|         | 377/10000 [00:05<03:02, 52.62it/s]Training CobwebTree:   4%|         | 383/10000 [00:05<03:00, 53.31it/s]Training CobwebTree:   4%|         | 389/10000 [00:06<03:05, 51.95it/s]Training CobwebTree:   4%|         | 395/10000 [00:06<03:11, 50.05it/s]Training CobwebTree:   4%|         | 401/10000 [00:06<03:09, 50.56it/s]Training CobwebTree:   4%|         | 407/10000 [00:06<03:07, 51.07it/s]Training CobwebTree:   4%|         | 413/10000 [00:06<03:01, 52.74it/s]Training CobwebTree:   4%|         | 419/10000 [00:06<03:04, 51.87it/s]Training CobwebTree:   4%|         | 425/10000 [00:06<03:13, 49.41it/s]Training CobwebTree:   4%|         | 431/10000 [00:06<03:07, 50.94it/s]Training CobwebTree:   4%|         | 437/10000 [00:07<03:11, 49.86it/s]Training CobwebTree:   4%|         | 443/10000 [00:07<03:11, 49.98it/s]Training CobwebTree:   4%|         | 449/10000 [00:07<03:11, 49.77it/s]Training CobwebTree:   5%|         | 455/10000 [00:07<03:11, 49.89it/s]Training CobwebTree:   5%|         | 461/10000 [00:07<03:07, 50.90it/s]Training CobwebTree:   5%|         | 467/10000 [00:07<03:00, 52.78it/s]Training CobwebTree:   5%|         | 473/10000 [00:07<03:07, 50.90it/s]Training CobwebTree:   5%|         | 479/10000 [00:07<03:04, 51.72it/s]Training CobwebTree:   5%|         | 485/10000 [00:08<03:05, 51.41it/s]Training CobwebTree:   5%|         | 491/10000 [00:08<03:01, 52.30it/s]Training CobwebTree:   5%|         | 497/10000 [00:08<03:03, 51.75it/s]Training CobwebTree:   5%|         | 503/10000 [00:08<03:11, 49.67it/s]Training CobwebTree:   5%|         | 509/10000 [00:08<03:06, 50.91it/s]Training CobwebTree:   5%|         | 515/10000 [00:08<03:00, 52.51it/s]Training CobwebTree:   5%|         | 521/10000 [00:08<02:55, 53.98it/s]Training CobwebTree:   5%|         | 527/10000 [00:08<03:05, 51.20it/s]Training CobwebTree:   5%|         | 533/10000 [00:08<03:08, 50.28it/s]Training CobwebTree:   5%|         | 539/10000 [00:09<03:08, 50.24it/s]Training CobwebTree:   5%|         | 545/10000 [00:09<03:03, 51.49it/s]Training CobwebTree:   6%|         | 551/10000 [00:09<03:09, 49.94it/s]Training CobwebTree:   6%|         | 557/10000 [00:09<03:06, 50.66it/s]Training CobwebTree:   6%|         | 563/10000 [00:09<03:08, 50.18it/s]Training CobwebTree:   6%|         | 569/10000 [00:09<03:04, 51.15it/s]Training CobwebTree:   6%|         | 575/10000 [00:09<03:07, 50.24it/s]Training CobwebTree:   6%|         | 581/10000 [00:09<03:06, 50.47it/s]Training CobwebTree:   6%|         | 587/10000 [00:10<03:05, 50.70it/s]Training CobwebTree:   6%|         | 593/10000 [00:10<03:00, 52.16it/s]Training CobwebTree:   6%|         | 599/10000 [00:10<03:05, 50.71it/s]Training CobwebTree:   6%|         | 605/10000 [00:10<02:59, 52.48it/s]Training CobwebTree:   6%|         | 611/10000 [00:10<03:06, 50.41it/s]Training CobwebTree:   6%|         | 617/10000 [00:10<03:15, 48.01it/s]Training CobwebTree:   6%|         | 622/10000 [00:10<03:27, 45.19it/s]Training CobwebTree:   6%|         | 627/10000 [00:10<03:24, 45.81it/s]Training CobwebTree:   6%|         | 632/10000 [00:10<03:29, 44.66it/s]Training CobwebTree:   6%|         | 637/10000 [00:11<03:27, 45.07it/s]Training CobwebTree:   6%|         | 643/10000 [00:11<03:15, 47.80it/s]Training CobwebTree:   6%|         | 649/10000 [00:11<03:14, 48.04it/s]Training CobwebTree:   7%|         | 655/10000 [00:11<03:11, 48.82it/s]Training CobwebTree:   7%|         | 661/10000 [00:11<03:08, 49.62it/s]Training CobwebTree:   7%|         | 666/10000 [00:11<03:08, 49.61it/s]Training CobwebTree:   7%|         | 671/10000 [00:11<03:13, 48.31it/s]Training CobwebTree:   7%|         | 677/10000 [00:11<03:04, 50.56it/s]Training CobwebTree:   7%|         | 683/10000 [00:11<03:10, 48.89it/s]Training CobwebTree:   7%|         | 688/10000 [00:12<03:12, 48.37it/s]Training CobwebTree:   7%|         | 693/10000 [00:12<03:11, 48.64it/s]Training CobwebTree:   7%|         | 698/10000 [00:12<03:10, 48.75it/s]Training CobwebTree:   7%|         | 703/10000 [00:12<03:12, 48.36it/s]Training CobwebTree:   7%|         | 709/10000 [00:12<03:04, 50.25it/s]Training CobwebTree:   7%|         | 715/10000 [00:12<03:09, 49.00it/s]Training CobwebTree:   7%|         | 720/10000 [00:12<03:14, 47.77it/s]Training CobwebTree:   7%|         | 726/10000 [00:12<03:04, 50.31it/s]Training CobwebTree:   7%|         | 732/10000 [00:12<03:07, 49.49it/s]Training CobwebTree:   7%|         | 738/10000 [00:13<03:07, 49.29it/s]Training CobwebTree:   7%|         | 743/10000 [00:13<03:08, 49.19it/s]Training CobwebTree:   7%|         | 748/10000 [00:13<03:14, 47.48it/s]Training CobwebTree:   8%|         | 753/10000 [00:13<03:28, 44.40it/s]Training CobwebTree:   8%|         | 758/10000 [00:13<03:25, 44.92it/s]Training CobwebTree:   8%|         | 763/10000 [00:13<03:27, 44.43it/s]Training CobwebTree:   8%|         | 769/10000 [00:13<03:18, 46.55it/s]Training CobwebTree:   8%|         | 774/10000 [00:13<03:20, 46.09it/s]Training CobwebTree:   8%|         | 779/10000 [00:14<03:19, 46.19it/s]Training CobwebTree:   8%|         | 784/10000 [00:14<03:22, 45.46it/s]Training CobwebTree:   8%|         | 789/10000 [00:14<03:29, 44.03it/s]Training CobwebTree:   8%|         | 794/10000 [00:14<03:21, 45.64it/s]Training CobwebTree:   8%|         | 800/10000 [00:14<03:15, 46.96it/s]Training CobwebTree:   8%|         | 805/10000 [00:14<03:19, 46.16it/s]Training CobwebTree:   8%|         | 811/10000 [00:14<03:12, 47.65it/s]Training CobwebTree:   8%|         | 816/10000 [00:14<03:10, 48.22it/s]Training CobwebTree:   8%|         | 821/10000 [00:14<03:09, 48.51it/s]Training CobwebTree:   8%|         | 826/10000 [00:15<03:19, 46.00it/s]Training CobwebTree:   8%|         | 831/10000 [00:15<03:21, 45.50it/s]Training CobwebTree:   8%|         | 836/10000 [00:15<03:16, 46.73it/s]Training CobwebTree:   8%|         | 842/10000 [00:15<03:07, 48.75it/s]Training CobwebTree:   8%|         | 847/10000 [00:15<03:14, 47.09it/s]Training CobwebTree:   9%|         | 852/10000 [00:15<03:12, 47.40it/s]Training CobwebTree:   9%|         | 857/10000 [00:15<03:19, 45.75it/s]Training CobwebTree:   9%|         | 862/10000 [00:15<03:34, 42.59it/s]Training CobwebTree:   9%|         | 867/10000 [00:15<03:35, 42.42it/s]Training CobwebTree:   9%|         | 873/10000 [00:16<03:27, 43.98it/s]Training CobwebTree:   9%|         | 878/10000 [00:16<03:34, 42.57it/s]Training CobwebTree:   9%|         | 883/10000 [00:16<03:31, 43.02it/s]Training CobwebTree:   9%|         | 888/10000 [00:16<03:24, 44.49it/s]Training CobwebTree:   9%|         | 893/10000 [00:16<03:21, 45.24it/s]Training CobwebTree:   9%|         | 899/10000 [00:16<03:09, 47.99it/s]Training CobwebTree:   9%|         | 904/10000 [00:16<03:11, 47.52it/s]Training CobwebTree:   9%|         | 909/10000 [00:16<03:12, 47.32it/s]Training CobwebTree:   9%|         | 915/10000 [00:16<03:07, 48.49it/s]Training CobwebTree:   9%|         | 920/10000 [00:17<03:12, 47.20it/s]Training CobwebTree:   9%|         | 925/10000 [00:17<03:15, 46.39it/s]Training CobwebTree:   9%|         | 930/10000 [00:17<03:13, 46.99it/s]Training CobwebTree:   9%|         | 935/10000 [00:17<03:10, 47.71it/s]Training CobwebTree:   9%|         | 940/10000 [00:17<03:18, 45.73it/s]Training CobwebTree:   9%|         | 945/10000 [00:17<03:19, 45.43it/s]Training CobwebTree:  10%|         | 950/10000 [00:17<03:27, 43.66it/s]Training CobwebTree:  10%|         | 956/10000 [00:17<03:14, 46.53it/s]Training CobwebTree:  10%|         | 961/10000 [00:17<03:21, 44.92it/s]Training CobwebTree:  10%|         | 966/10000 [00:18<03:17, 45.85it/s]Training CobwebTree:  10%|         | 971/10000 [00:18<03:21, 44.86it/s]Training CobwebTree:  10%|         | 976/10000 [00:18<03:20, 44.90it/s]Training CobwebTree:  10%|         | 981/10000 [00:18<03:24, 44.08it/s]Training CobwebTree:  10%|         | 987/10000 [00:18<03:16, 45.95it/s]Training CobwebTree:  10%|         | 992/10000 [00:18<03:17, 45.51it/s]Training CobwebTree:  10%|         | 997/10000 [00:18<03:13, 46.56it/s]Training CobwebTree:  10%|         | 1002/10000 [00:18<03:10, 47.22it/s]Training CobwebTree:  10%|         | 1007/10000 [00:18<03:14, 46.34it/s]Training CobwebTree:  10%|         | 1012/10000 [00:19<03:17, 45.57it/s]Training CobwebTree:  10%|         | 1017/10000 [00:19<03:15, 45.90it/s]Training CobwebTree:  10%|         | 1022/10000 [00:19<03:17, 45.41it/s]Training CobwebTree:  10%|         | 1027/10000 [00:19<03:21, 44.59it/s]Training CobwebTree:  10%|         | 1032/10000 [00:19<03:19, 44.98it/s]Training CobwebTree:  10%|         | 1037/10000 [00:19<03:16, 45.63it/s]Training CobwebTree:  10%|         | 1042/10000 [00:19<03:15, 45.90it/s]Training CobwebTree:  10%|         | 1047/10000 [00:19<03:16, 45.66it/s]Training CobwebTree:  11%|         | 1052/10000 [00:19<03:21, 44.37it/s]Training CobwebTree:  11%|         | 1057/10000 [00:20<03:15, 45.84it/s]Training CobwebTree:  11%|         | 1062/10000 [00:20<03:17, 45.29it/s]Training CobwebTree:  11%|         | 1067/10000 [00:20<03:14, 45.88it/s]Training CobwebTree:  11%|         | 1072/10000 [00:20<03:15, 45.74it/s]Training CobwebTree:  11%|         | 1077/10000 [00:20<03:18, 44.98it/s]Training CobwebTree:  11%|         | 1082/10000 [00:20<03:27, 43.00it/s]Training CobwebTree:  11%|         | 1087/10000 [00:20<03:23, 43.80it/s]Training CobwebTree:  11%|         | 1092/10000 [00:20<03:22, 43.89it/s]Training CobwebTree:  11%|         | 1097/10000 [00:20<03:19, 44.73it/s]Training CobwebTree:  11%|         | 1102/10000 [00:21<03:20, 44.49it/s]Training CobwebTree:  11%|         | 1107/10000 [00:21<03:18, 44.89it/s]Training CobwebTree:  11%|         | 1112/10000 [00:21<03:23, 43.62it/s]Training CobwebTree:  11%|         | 1117/10000 [00:21<03:22, 43.93it/s]Training CobwebTree:  11%|         | 1122/10000 [00:21<03:14, 45.54it/s]Training CobwebTree:  11%|        | 1127/10000 [00:21<03:09, 46.70it/s]Training CobwebTree:  11%|        | 1132/10000 [00:21<03:13, 45.75it/s]Training CobwebTree:  11%|        | 1137/10000 [00:21<03:20, 44.25it/s]Training CobwebTree:  11%|        | 1142/10000 [00:21<03:15, 45.39it/s]Training CobwebTree:  11%|        | 1147/10000 [00:22<03:10, 46.50it/s]Training CobwebTree:  12%|        | 1152/10000 [00:22<03:10, 46.54it/s]Training CobwebTree:  12%|        | 1158/10000 [00:22<03:04, 47.81it/s]Training CobwebTree:  12%|        | 1163/10000 [00:22<03:07, 47.16it/s]Training CobwebTree:  12%|        | 1169/10000 [00:22<02:58, 49.38it/s]Training CobwebTree:  12%|        | 1175/10000 [00:22<03:03, 48.13it/s]Training CobwebTree:  12%|        | 1180/10000 [00:22<03:05, 47.44it/s]Training CobwebTree:  12%|        | 1185/10000 [00:22<03:07, 47.07it/s]Training CobwebTree:  12%|        | 1190/10000 [00:22<03:09, 46.50it/s]Training CobwebTree:  12%|        | 1195/10000 [00:23<03:09, 46.57it/s]Training CobwebTree:  12%|        | 1200/10000 [00:23<03:14, 45.18it/s]Training CobwebTree:  12%|        | 1205/10000 [00:23<03:17, 44.61it/s]Training CobwebTree:  12%|        | 1210/10000 [00:23<03:11, 45.94it/s]Training CobwebTree:  12%|        | 1215/10000 [00:23<03:24, 43.06it/s]Training CobwebTree:  12%|        | 1220/10000 [00:23<03:22, 43.28it/s]Training CobwebTree:  12%|        | 1225/10000 [00:23<03:25, 42.78it/s]Training CobwebTree:  12%|        | 1230/10000 [00:23<03:39, 39.97it/s]Training CobwebTree:  12%|        | 1235/10000 [00:24<03:33, 41.08it/s]Training CobwebTree:  12%|        | 1240/10000 [00:24<03:33, 41.09it/s]Training CobwebTree:  12%|        | 1245/10000 [00:24<03:28, 41.92it/s]Training CobwebTree:  12%|        | 1250/10000 [00:24<03:23, 42.94it/s]Training CobwebTree:  13%|        | 1255/10000 [00:24<03:21, 43.41it/s]Training CobwebTree:  13%|        | 1260/10000 [00:24<03:26, 42.38it/s]Training CobwebTree:  13%|        | 1265/10000 [00:24<03:17, 44.15it/s]Training CobwebTree:  13%|        | 1270/10000 [00:24<03:23, 42.96it/s]Training CobwebTree:  13%|        | 1275/10000 [00:24<03:28, 41.84it/s]Training CobwebTree:  13%|        | 1280/10000 [00:25<03:30, 41.40it/s]Training CobwebTree:  13%|        | 1285/10000 [00:25<03:26, 42.30it/s]Training CobwebTree:  13%|        | 1290/10000 [00:25<03:34, 40.53it/s]Training CobwebTree:  13%|        | 1295/10000 [00:25<03:25, 42.42it/s]Training CobwebTree:  13%|        | 1300/10000 [00:25<03:21, 43.08it/s]Training CobwebTree:  13%|        | 1305/10000 [00:25<03:21, 43.21it/s]Training CobwebTree:  13%|        | 1310/10000 [00:25<03:15, 44.38it/s]Training CobwebTree:  13%|        | 1315/10000 [00:25<03:11, 45.30it/s]Training CobwebTree:  13%|        | 1320/10000 [00:26<03:21, 43.05it/s]Training CobwebTree:  13%|        | 1325/10000 [00:26<03:27, 41.86it/s]Training CobwebTree:  13%|        | 1330/10000 [00:26<03:24, 42.50it/s]Training CobwebTree:  13%|        | 1335/10000 [00:26<03:29, 41.45it/s]Training CobwebTree:  13%|        | 1340/10000 [00:26<03:24, 42.25it/s]Training CobwebTree:  13%|        | 1345/10000 [00:26<03:27, 41.76it/s]Training CobwebTree:  14%|        | 1350/10000 [00:26<03:21, 43.02it/s]Training CobwebTree:  14%|        | 1355/10000 [00:26<03:23, 42.58it/s]Training CobwebTree:  14%|        | 1360/10000 [00:26<03:23, 42.39it/s]Training CobwebTree:  14%|        | 1365/10000 [00:27<03:22, 42.54it/s]Training CobwebTree:  14%|        | 1370/10000 [00:27<03:21, 42.80it/s]Training CobwebTree:  14%|        | 1375/10000 [00:27<03:28, 41.38it/s]Training CobwebTree:  14%|        | 1380/10000 [00:27<03:28, 41.44it/s]Training CobwebTree:  14%|        | 1385/10000 [00:27<03:22, 42.63it/s]Training CobwebTree:  14%|        | 1390/10000 [00:27<03:27, 41.54it/s]Training CobwebTree:  14%|        | 1395/10000 [00:27<03:31, 40.70it/s]Training CobwebTree:  14%|        | 1400/10000 [00:27<03:35, 39.98it/s]Training CobwebTree:  14%|        | 1405/10000 [00:28<03:36, 39.73it/s]Training CobwebTree:  14%|        | 1410/10000 [00:28<03:31, 40.58it/s]Training CobwebTree:  14%|        | 1415/10000 [00:28<03:31, 40.65it/s]Training CobwebTree:  14%|        | 1420/10000 [00:28<03:33, 40.15it/s]Training CobwebTree:  14%|        | 1425/10000 [00:28<03:31, 40.55it/s]Training CobwebTree:  14%|        | 1430/10000 [00:28<03:32, 40.25it/s]Training CobwebTree:  14%|        | 1435/10000 [00:28<03:22, 42.28it/s]Training CobwebTree:  14%|        | 1440/10000 [00:28<03:17, 43.36it/s]Training CobwebTree:  14%|        | 1445/10000 [00:29<03:22, 42.24it/s]Training CobwebTree:  14%|        | 1450/10000 [00:29<03:20, 42.56it/s]Training CobwebTree:  15%|        | 1455/10000 [00:29<03:25, 41.58it/s]Training CobwebTree:  15%|        | 1460/10000 [00:29<03:25, 41.55it/s]Training CobwebTree:  15%|        | 1465/10000 [00:29<03:22, 42.07it/s]Training CobwebTree:  15%|        | 1470/10000 [00:29<03:30, 40.61it/s]Training CobwebTree:  15%|        | 1475/10000 [00:29<03:25, 41.40it/s]Training CobwebTree:  15%|        | 1481/10000 [00:29<03:08, 45.30it/s]Training CobwebTree:  15%|        | 1486/10000 [00:29<03:06, 45.68it/s]Training CobwebTree:  15%|        | 1491/10000 [00:30<03:08, 45.15it/s]Training CobwebTree:  15%|        | 1496/10000 [00:30<03:08, 45.14it/s]Training CobwebTree:  15%|        | 1501/10000 [00:30<03:17, 42.95it/s]Training CobwebTree:  15%|        | 1506/10000 [00:30<03:26, 41.20it/s]Training CobwebTree:  15%|        | 1511/10000 [00:30<03:27, 40.82it/s]Training CobwebTree:  15%|        | 1516/10000 [00:30<03:24, 41.57it/s]Training CobwebTree:  15%|        | 1521/10000 [00:30<03:21, 41.98it/s]Training CobwebTree:  15%|        | 1526/10000 [00:30<03:25, 41.16it/s]Training CobwebTree:  15%|        | 1531/10000 [00:31<03:34, 39.57it/s]Training CobwebTree:  15%|        | 1535/10000 [00:31<03:36, 39.02it/s]Training CobwebTree:  15%|        | 1540/10000 [00:31<03:26, 40.93it/s]Training CobwebTree:  15%|        | 1545/10000 [00:31<03:21, 41.90it/s]Training CobwebTree:  16%|        | 1550/10000 [00:31<03:15, 43.13it/s]Training CobwebTree:  16%|        | 1555/10000 [00:31<03:20, 42.08it/s]Training CobwebTree:  16%|        | 1560/10000 [00:31<03:27, 40.76it/s]Training CobwebTree:  16%|        | 1565/10000 [00:31<03:24, 41.27it/s]Training CobwebTree:  16%|        | 1570/10000 [00:32<03:29, 40.31it/s]Training CobwebTree:  16%|        | 1575/10000 [00:32<03:18, 42.36it/s]Training CobwebTree:  16%|        | 1580/10000 [00:32<03:19, 42.10it/s]Training CobwebTree:  16%|        | 1585/10000 [00:32<03:24, 41.21it/s]Training CobwebTree:  16%|        | 1590/10000 [00:32<03:15, 43.11it/s]Training CobwebTree:  16%|        | 1595/10000 [00:32<03:14, 43.21it/s]Training CobwebTree:  16%|        | 1600/10000 [00:32<03:18, 42.37it/s]Training CobwebTree:  16%|        | 1605/10000 [00:32<03:25, 40.86it/s]Training CobwebTree:  16%|        | 1610/10000 [00:33<03:32, 39.47it/s]Training CobwebTree:  16%|        | 1614/10000 [00:33<03:35, 38.84it/s]Training CobwebTree:  16%|        | 1618/10000 [00:33<03:34, 39.03it/s]Training CobwebTree:  16%|        | 1623/10000 [00:33<03:30, 39.86it/s]Training CobwebTree:  16%|        | 1628/10000 [00:33<03:20, 41.85it/s]Training CobwebTree:  16%|        | 1633/10000 [00:33<03:25, 40.78it/s]Training CobwebTree:  16%|        | 1638/10000 [00:33<03:21, 41.58it/s]Training CobwebTree:  16%|        | 1643/10000 [00:33<03:21, 41.47it/s]Training CobwebTree:  16%|        | 1648/10000 [00:33<03:12, 43.45it/s]Training CobwebTree:  17%|        | 1653/10000 [00:34<03:08, 44.23it/s]Training CobwebTree:  17%|        | 1658/10000 [00:34<03:09, 44.13it/s]Training CobwebTree:  17%|        | 1663/10000 [00:34<03:13, 42.98it/s]Training CobwebTree:  17%|        | 1668/10000 [00:34<03:19, 41.84it/s]Training CobwebTree:  17%|        | 1673/10000 [00:34<03:30, 39.62it/s]Training CobwebTree:  17%|        | 1678/10000 [00:34<03:26, 40.24it/s]Training CobwebTree:  17%|        | 1683/10000 [00:34<03:25, 40.41it/s]Training CobwebTree:  17%|        | 1688/10000 [00:34<03:20, 41.55it/s]Training CobwebTree:  17%|        | 1693/10000 [00:35<03:24, 40.70it/s]Training CobwebTree:  17%|        | 1698/10000 [00:35<03:19, 41.66it/s]Training CobwebTree:  17%|        | 1703/10000 [00:35<03:20, 41.31it/s]Training CobwebTree:  17%|        | 1708/10000 [00:35<03:29, 39.58it/s]Training CobwebTree:  17%|        | 1712/10000 [00:35<03:31, 39.26it/s]Training CobwebTree:  17%|        | 1716/10000 [00:35<03:32, 39.00it/s]Training CobwebTree:  17%|        | 1720/10000 [00:35<03:35, 38.44it/s]Training CobwebTree:  17%|        | 1725/10000 [00:35<03:26, 40.00it/s]Training CobwebTree:  17%|        | 1730/10000 [00:35<03:25, 40.28it/s]Training CobwebTree:  17%|        | 1735/10000 [00:36<03:21, 40.98it/s]Training CobwebTree:  17%|        | 1740/10000 [00:36<03:29, 39.35it/s]Training CobwebTree:  17%|        | 1745/10000 [00:36<03:26, 39.93it/s]Training CobwebTree:  18%|        | 1750/10000 [00:36<03:16, 41.90it/s]Training CobwebTree:  18%|        | 1755/10000 [00:36<03:15, 42.17it/s]Training CobwebTree:  18%|        | 1760/10000 [00:36<03:15, 42.24it/s]Training CobwebTree:  18%|        | 1765/10000 [00:36<03:12, 42.68it/s]Training CobwebTree:  18%|        | 1770/10000 [00:36<03:11, 43.02it/s]Training CobwebTree:  18%|        | 1775/10000 [00:37<03:15, 42.10it/s]Training CobwebTree:  18%|        | 1780/10000 [00:37<03:16, 41.83it/s]Training CobwebTree:  18%|        | 1785/10000 [00:37<03:20, 41.03it/s]Training CobwebTree:  18%|        | 1790/10000 [00:37<03:23, 40.44it/s]Training CobwebTree:  18%|        | 1795/10000 [00:37<03:19, 41.17it/s]Training CobwebTree:  18%|        | 1800/10000 [00:37<03:22, 40.58it/s]Training CobwebTree:  18%|        | 1805/10000 [00:37<03:19, 41.14it/s]Training CobwebTree:  18%|        | 1810/10000 [00:37<03:26, 39.63it/s]Training CobwebTree:  18%|        | 1814/10000 [00:37<03:26, 39.69it/s]Training CobwebTree:  18%|        | 1819/10000 [00:38<03:24, 40.10it/s]Training CobwebTree:  18%|        | 1824/10000 [00:38<03:22, 40.30it/s]Training CobwebTree:  18%|        | 1829/10000 [00:38<03:13, 42.13it/s]Training CobwebTree:  18%|        | 1834/10000 [00:38<03:07, 43.62it/s]Training CobwebTree:  18%|        | 1839/10000 [00:38<03:02, 44.61it/s]Training CobwebTree:  18%|        | 1844/10000 [00:38<03:06, 43.66it/s]Training CobwebTree:  18%|        | 1849/10000 [00:38<03:07, 43.56it/s]Training CobwebTree:  19%|        | 1854/10000 [00:38<03:01, 44.95it/s]Training CobwebTree:  19%|        | 1859/10000 [00:39<03:05, 43.81it/s]Training CobwebTree:  19%|        | 1864/10000 [00:39<03:07, 43.50it/s]Training CobwebTree:  19%|        | 1869/10000 [00:39<03:14, 41.82it/s]Training CobwebTree:  19%|        | 1874/10000 [00:39<03:15, 41.65it/s]Training CobwebTree:  19%|        | 1879/10000 [00:39<03:11, 42.51it/s]Training CobwebTree:  19%|        | 1884/10000 [00:39<03:07, 43.26it/s]Training CobwebTree:  19%|        | 1889/10000 [00:39<03:03, 44.24it/s]Training CobwebTree:  19%|        | 1894/10000 [00:39<03:05, 43.81it/s]Training CobwebTree:  19%|        | 1899/10000 [00:39<03:06, 43.42it/s]Training CobwebTree:  19%|        | 1904/10000 [00:40<03:17, 41.05it/s]Training CobwebTree:  19%|        | 1909/10000 [00:40<03:21, 40.20it/s]Training CobwebTree:  19%|        | 1914/10000 [00:40<03:26, 39.21it/s]Training CobwebTree:  19%|        | 1918/10000 [00:40<03:31, 38.21it/s]Training CobwebTree:  19%|        | 1923/10000 [00:40<03:24, 39.40it/s]Training CobwebTree:  19%|        | 1928/10000 [00:40<03:17, 40.78it/s]Training CobwebTree:  19%|        | 1933/10000 [00:40<03:23, 39.63it/s]Training CobwebTree:  19%|        | 1938/10000 [00:40<03:19, 40.32it/s]Training CobwebTree:  19%|        | 1943/10000 [00:41<03:20, 40.23it/s]Training CobwebTree:  19%|        | 1948/10000 [00:41<03:25, 39.18it/s]Training CobwebTree:  20%|        | 1953/10000 [00:41<03:20, 40.16it/s]Training CobwebTree:  20%|        | 1958/10000 [00:41<03:23, 39.51it/s]Training CobwebTree:  20%|        | 1963/10000 [00:41<03:15, 41.03it/s]Training CobwebTree:  20%|        | 1968/10000 [00:41<03:10, 42.11it/s]Training CobwebTree:  20%|        | 1973/10000 [00:41<03:20, 40.06it/s]Training CobwebTree:  20%|        | 1978/10000 [00:41<03:19, 40.19it/s]Training CobwebTree:  20%|        | 1983/10000 [00:42<03:17, 40.56it/s]Training CobwebTree:  20%|        | 1988/10000 [00:42<03:13, 41.34it/s]Training CobwebTree:  20%|        | 1993/10000 [00:42<03:11, 41.84it/s]Training CobwebTree:  20%|        | 1998/10000 [00:42<03:06, 42.86it/s]Training CobwebTree:  20%|        | 2003/10000 [00:42<03:03, 43.58it/s]Training CobwebTree:  20%|        | 2008/10000 [00:42<03:09, 42.28it/s]Training CobwebTree:  20%|        | 2013/10000 [00:42<03:11, 41.76it/s]Training CobwebTree:  20%|        | 2018/10000 [00:42<03:17, 40.43it/s]Training CobwebTree:  20%|        | 2023/10000 [00:42<03:10, 41.81it/s]Training CobwebTree:  20%|        | 2028/10000 [00:43<03:15, 40.84it/s]Training CobwebTree:  20%|        | 2034/10000 [00:43<03:03, 43.45it/s]Training CobwebTree:  20%|        | 2039/10000 [00:43<03:08, 42.15it/s]Training CobwebTree:  20%|        | 2044/10000 [00:43<03:08, 42.14it/s]Training CobwebTree:  20%|        | 2049/10000 [00:43<03:06, 42.53it/s]Training CobwebTree:  21%|        | 2054/10000 [00:43<03:05, 42.78it/s]Training CobwebTree:  21%|        | 2059/10000 [00:43<03:04, 43.08it/s]Training CobwebTree:  21%|        | 2064/10000 [00:43<03:07, 42.43it/s]Training CobwebTree:  21%|        | 2069/10000 [00:44<03:06, 42.48it/s]Training CobwebTree:  21%|        | 2074/10000 [00:44<03:07, 42.22it/s]Training CobwebTree:  21%|        | 2079/10000 [00:44<03:07, 42.18it/s]Training CobwebTree:  21%|        | 2084/10000 [00:44<03:09, 41.78it/s]Training CobwebTree:  21%|        | 2089/10000 [00:44<03:08, 42.02it/s]Training CobwebTree:  21%|        | 2094/10000 [00:44<03:06, 42.44it/s]Training CobwebTree:  21%|        | 2099/10000 [00:44<03:10, 41.56it/s]Training CobwebTree:  21%|        | 2104/10000 [00:44<03:05, 42.50it/s]Training CobwebTree:  21%|        | 2109/10000 [00:45<03:06, 42.20it/s]Training CobwebTree:  21%|        | 2114/10000 [00:45<03:11, 41.14it/s]Training CobwebTree:  21%|        | 2119/10000 [00:45<03:06, 42.36it/s]Training CobwebTree:  21%|        | 2124/10000 [00:45<03:03, 42.86it/s]Training CobwebTree:  21%|       | 2129/10000 [00:45<03:02, 43.11it/s]Training CobwebTree:  21%|       | 2134/10000 [00:45<03:01, 43.44it/s]Training CobwebTree:  21%|       | 2139/10000 [00:45<03:13, 40.61it/s]Training CobwebTree:  21%|       | 2144/10000 [00:45<03:15, 40.23it/s]Training CobwebTree:  21%|       | 2149/10000 [00:46<03:22, 38.76it/s]Training CobwebTree:  22%|       | 2153/10000 [00:46<03:21, 38.89it/s]Training CobwebTree:  22%|       | 2158/10000 [00:46<03:11, 41.03it/s]Training CobwebTree:  22%|       | 2163/10000 [00:46<03:09, 41.26it/s]Training CobwebTree:  22%|       | 2168/10000 [00:46<03:12, 40.78it/s]Training CobwebTree:  22%|       | 2173/10000 [00:46<03:17, 39.53it/s]Training CobwebTree:  22%|       | 2178/10000 [00:46<03:11, 40.79it/s]Training CobwebTree:  22%|       | 2183/10000 [00:46<03:11, 40.72it/s]Training CobwebTree:  22%|       | 2188/10000 [00:46<03:17, 39.52it/s]Training CobwebTree:  22%|       | 2192/10000 [00:47<03:19, 39.04it/s]Training CobwebTree:  22%|       | 2197/10000 [00:47<03:16, 39.78it/s]Training CobwebTree:  22%|       | 2202/10000 [00:47<03:08, 41.46it/s]Training CobwebTree:  22%|       | 2207/10000 [00:47<03:09, 41.09it/s]Training CobwebTree:  22%|       | 2212/10000 [00:47<03:10, 40.83it/s]Training CobwebTree:  22%|       | 2217/10000 [00:47<03:11, 40.72it/s]Training CobwebTree:  22%|       | 2222/10000 [00:47<03:08, 41.26it/s]Training CobwebTree:  22%|       | 2227/10000 [00:47<03:01, 42.90it/s]Training CobwebTree:  22%|       | 2232/10000 [00:48<03:02, 42.54it/s]Training CobwebTree:  22%|       | 2237/10000 [00:48<03:03, 42.27it/s]Training CobwebTree:  22%|       | 2242/10000 [00:48<02:59, 43.19it/s]Training CobwebTree:  22%|       | 2247/10000 [00:48<03:12, 40.27it/s]Training CobwebTree:  23%|       | 2252/10000 [00:48<03:01, 42.58it/s]Training CobwebTree:  23%|       | 2257/10000 [00:48<03:08, 41.17it/s]Training CobwebTree:  23%|       | 2262/10000 [00:48<03:08, 41.06it/s]Training CobwebTree:  23%|       | 2267/10000 [00:48<03:08, 41.00it/s]Training CobwebTree:  23%|       | 2272/10000 [00:48<03:06, 41.34it/s]Training CobwebTree:  23%|       | 2277/10000 [00:49<03:09, 40.85it/s]Training CobwebTree:  23%|       | 2282/10000 [00:49<03:08, 40.92it/s]Training CobwebTree:  23%|       | 2287/10000 [00:49<03:13, 39.92it/s]Training CobwebTree:  23%|       | 2292/10000 [00:49<03:05, 41.46it/s]Training CobwebTree:  23%|       | 2297/10000 [00:49<03:09, 40.57it/s]Training CobwebTree:  23%|       | 2302/10000 [00:49<03:11, 40.11it/s]Training CobwebTree:  23%|       | 2307/10000 [00:49<03:06, 41.14it/s]Training CobwebTree:  23%|       | 2312/10000 [00:49<02:59, 42.79it/s]Training CobwebTree:  23%|       | 2317/10000 [00:50<03:07, 41.04it/s]Training CobwebTree:  23%|       | 2322/10000 [00:50<03:03, 41.87it/s]Training CobwebTree:  23%|       | 2327/10000 [00:50<03:03, 41.81it/s]Training CobwebTree:  23%|       | 2332/10000 [00:50<03:05, 41.32it/s]Training CobwebTree:  23%|       | 2337/10000 [00:50<03:03, 41.84it/s]Training CobwebTree:  23%|       | 2342/10000 [00:50<03:04, 41.45it/s]Training CobwebTree:  23%|       | 2347/10000 [00:50<02:57, 43.20it/s]Training CobwebTree:  24%|       | 2353/10000 [00:50<02:43, 46.89it/s]Training CobwebTree:  24%|       | 2358/10000 [00:51<02:44, 46.37it/s]Training CobwebTree:  24%|       | 2363/10000 [00:51<02:55, 43.52it/s]Training CobwebTree:  24%|       | 2368/10000 [00:51<02:57, 42.95it/s]Training CobwebTree:  24%|       | 2373/10000 [00:51<03:07, 40.67it/s]Training CobwebTree:  24%|       | 2378/10000 [00:51<03:07, 40.70it/s]Training CobwebTree:  24%|       | 2383/10000 [00:51<03:10, 39.90it/s]Training CobwebTree:  24%|       | 2388/10000 [00:51<03:13, 39.36it/s]Training CobwebTree:  24%|       | 2393/10000 [00:51<03:09, 40.14it/s]Training CobwebTree:  24%|       | 2398/10000 [00:52<03:02, 41.65it/s]Training CobwebTree:  24%|       | 2403/10000 [00:52<03:03, 41.48it/s]Training CobwebTree:  24%|       | 2408/10000 [00:52<03:01, 41.86it/s]Training CobwebTree:  24%|       | 2413/10000 [00:52<03:07, 40.45it/s]Training CobwebTree:  24%|       | 2418/10000 [00:52<03:08, 40.29it/s]Training CobwebTree:  24%|       | 2423/10000 [00:52<03:05, 40.78it/s]Training CobwebTree:  24%|       | 2428/10000 [00:52<03:11, 39.62it/s]Training CobwebTree:  24%|       | 2432/10000 [00:52<03:13, 39.18it/s]Training CobwebTree:  24%|       | 2436/10000 [00:52<03:12, 39.36it/s]Training CobwebTree:  24%|       | 2441/10000 [00:53<03:06, 40.57it/s]Training CobwebTree:  24%|       | 2446/10000 [00:53<03:09, 39.80it/s]Training CobwebTree:  24%|       | 2450/10000 [00:53<03:15, 38.56it/s]Training CobwebTree:  25%|       | 2455/10000 [00:53<03:07, 40.20it/s]Training CobwebTree:  25%|       | 2460/10000 [00:53<03:07, 40.27it/s]Training CobwebTree:  25%|       | 2465/10000 [00:53<03:08, 40.01it/s]Training CobwebTree:  25%|       | 2470/10000 [00:53<03:07, 40.25it/s]Training CobwebTree:  25%|       | 2475/10000 [00:53<03:08, 39.98it/s]Training CobwebTree:  25%|       | 2479/10000 [00:54<03:12, 39.09it/s]Training CobwebTree:  25%|       | 2483/10000 [00:54<03:13, 38.83it/s]Training CobwebTree:  25%|       | 2487/10000 [00:54<03:13, 38.74it/s]Training CobwebTree:  25%|       | 2491/10000 [00:54<03:19, 37.66it/s]Training CobwebTree:  25%|       | 2496/10000 [00:54<03:12, 38.98it/s]Training CobwebTree:  25%|       | 2501/10000 [00:54<03:04, 40.71it/s]Training CobwebTree:  25%|       | 2506/10000 [00:54<03:13, 38.68it/s]Training CobwebTree:  25%|       | 2510/10000 [00:54<03:26, 36.36it/s]Training CobwebTree:  25%|       | 2515/10000 [00:54<03:10, 39.36it/s]Training CobwebTree:  25%|       | 2519/10000 [00:55<03:12, 38.83it/s]Training CobwebTree:  25%|       | 2523/10000 [00:55<03:14, 38.54it/s]Training CobwebTree:  25%|       | 2527/10000 [00:55<03:13, 38.68it/s]Training CobwebTree:  25%|       | 2531/10000 [00:55<03:13, 38.69it/s]Training CobwebTree:  25%|       | 2536/10000 [00:55<03:10, 39.12it/s]Training CobwebTree:  25%|       | 2540/10000 [00:55<03:10, 39.10it/s]Training CobwebTree:  25%|       | 2544/10000 [00:55<03:14, 38.28it/s]Training CobwebTree:  25%|       | 2548/10000 [00:55<03:13, 38.48it/s]Training CobwebTree:  26%|       | 2553/10000 [00:55<03:09, 39.31it/s]Training CobwebTree:  26%|       | 2558/10000 [00:56<03:13, 38.56it/s]Training CobwebTree:  26%|       | 2562/10000 [00:56<03:12, 38.69it/s]Training CobwebTree:  26%|       | 2567/10000 [00:56<03:03, 40.51it/s]Training CobwebTree:  26%|       | 2572/10000 [00:56<03:10, 38.90it/s]Training CobwebTree:  26%|       | 2576/10000 [00:56<03:12, 38.60it/s]Training CobwebTree:  26%|       | 2580/10000 [00:56<03:16, 37.68it/s]Training CobwebTree:  26%|       | 2584/10000 [00:56<03:16, 37.77it/s]Training CobwebTree:  26%|       | 2589/10000 [00:56<03:04, 40.10it/s]Training CobwebTree:  26%|       | 2594/10000 [00:57<02:59, 41.24it/s]Training CobwebTree:  26%|       | 2599/10000 [00:57<03:02, 40.66it/s]Training CobwebTree:  26%|       | 2604/10000 [00:57<03:02, 40.43it/s]Training CobwebTree:  26%|       | 2609/10000 [00:57<03:03, 40.31it/s]Training CobwebTree:  26%|       | 2614/10000 [00:57<03:01, 40.77it/s]Training CobwebTree:  26%|       | 2619/10000 [00:57<03:04, 40.09it/s]Training CobwebTree:  26%|       | 2624/10000 [00:57<03:02, 40.51it/s]Training CobwebTree:  26%|       | 2629/10000 [00:57<03:02, 40.32it/s]Training CobwebTree:  26%|       | 2634/10000 [00:58<03:03, 40.04it/s]Training CobwebTree:  26%|       | 2639/10000 [00:58<03:07, 39.31it/s]Training CobwebTree:  26%|       | 2644/10000 [00:58<03:05, 39.63it/s]Training CobwebTree:  26%|       | 2648/10000 [00:58<03:07, 39.29it/s]Training CobwebTree:  27%|       | 2653/10000 [00:58<03:01, 40.44it/s]Training CobwebTree:  27%|       | 2658/10000 [00:58<03:14, 37.78it/s]Training CobwebTree:  27%|       | 2663/10000 [00:58<03:06, 39.24it/s]Training CobwebTree:  27%|       | 2667/10000 [00:58<03:10, 38.56it/s]Training CobwebTree:  27%|       | 2672/10000 [00:58<03:05, 39.44it/s]Training CobwebTree:  27%|       | 2676/10000 [00:59<03:07, 39.05it/s]Training CobwebTree:  27%|       | 2681/10000 [00:59<03:04, 39.70it/s]Training CobwebTree:  27%|       | 2686/10000 [00:59<03:02, 39.99it/s]Training CobwebTree:  27%|       | 2691/10000 [00:59<03:01, 40.21it/s]Training CobwebTree:  27%|       | 2696/10000 [00:59<03:04, 39.60it/s]Training CobwebTree:  27%|       | 2700/10000 [00:59<03:07, 39.03it/s]Training CobwebTree:  27%|       | 2705/10000 [00:59<03:03, 39.70it/s]Training CobwebTree:  27%|       | 2709/10000 [00:59<03:09, 38.40it/s]Training CobwebTree:  27%|       | 2713/10000 [01:00<03:10, 38.24it/s]Training CobwebTree:  27%|       | 2717/10000 [01:00<03:13, 37.72it/s]Training CobwebTree:  27%|       | 2721/10000 [01:00<03:12, 37.77it/s]Training CobwebTree:  27%|       | 2725/10000 [01:00<03:11, 37.90it/s]Training CobwebTree:  27%|       | 2730/10000 [01:00<02:58, 40.66it/s]Training CobwebTree:  27%|       | 2735/10000 [01:00<02:58, 40.79it/s]Training CobwebTree:  27%|       | 2740/10000 [01:00<03:00, 40.26it/s]Training CobwebTree:  27%|       | 2745/10000 [01:00<03:04, 39.24it/s]Training CobwebTree:  27%|       | 2749/10000 [01:00<03:08, 38.53it/s]Training CobwebTree:  28%|       | 2753/10000 [01:01<03:07, 38.61it/s]Training CobwebTree:  28%|       | 2758/10000 [01:01<02:58, 40.59it/s]Training CobwebTree:  28%|       | 2763/10000 [01:01<03:03, 39.50it/s]Training CobwebTree:  28%|       | 2767/10000 [01:01<03:08, 38.35it/s]Training CobwebTree:  28%|       | 2772/10000 [01:01<03:01, 39.81it/s]Training CobwebTree:  28%|       | 2776/10000 [01:01<03:03, 39.39it/s]Training CobwebTree:  28%|       | 2781/10000 [01:01<03:01, 39.74it/s]Training CobwebTree:  28%|       | 2786/10000 [01:01<02:59, 40.27it/s]Training CobwebTree:  28%|       | 2791/10000 [01:02<03:06, 38.71it/s]Training CobwebTree:  28%|       | 2795/10000 [01:02<03:09, 38.11it/s]Training CobwebTree:  28%|       | 2800/10000 [01:02<03:01, 39.72it/s]Training CobwebTree:  28%|       | 2804/10000 [01:02<03:12, 37.38it/s]Training CobwebTree:  28%|       | 2809/10000 [01:02<03:02, 39.33it/s]Training CobwebTree:  28%|       | 2813/10000 [01:02<03:05, 38.84it/s]Training CobwebTree:  28%|       | 2818/10000 [01:02<02:59, 40.03it/s]Training CobwebTree:  28%|       | 2823/10000 [01:02<02:58, 40.19it/s]Training CobwebTree:  28%|       | 2828/10000 [01:02<03:09, 37.77it/s]Training CobwebTree:  28%|       | 2833/10000 [01:03<03:05, 38.66it/s]Training CobwebTree:  28%|       | 2837/10000 [01:03<03:15, 36.55it/s]Training CobwebTree:  28%|       | 2842/10000 [01:03<03:03, 38.92it/s]Training CobwebTree:  28%|       | 2846/10000 [01:03<03:06, 38.45it/s]Training CobwebTree:  28%|       | 2850/10000 [01:03<03:07, 38.17it/s]Training CobwebTree:  29%|       | 2855/10000 [01:03<03:04, 38.67it/s]Training CobwebTree:  29%|       | 2859/10000 [01:03<03:12, 37.04it/s]Training CobwebTree:  29%|       | 2863/10000 [01:03<03:18, 36.00it/s]Training CobwebTree:  29%|       | 2868/10000 [01:04<03:13, 36.79it/s]Training CobwebTree:  29%|       | 2873/10000 [01:04<03:06, 38.19it/s]Training CobwebTree:  29%|       | 2878/10000 [01:04<02:58, 39.81it/s]Training CobwebTree:  29%|       | 2882/10000 [01:04<03:00, 39.46it/s]Training CobwebTree:  29%|       | 2886/10000 [01:04<03:04, 38.65it/s]Training CobwebTree:  29%|       | 2890/10000 [01:04<03:08, 37.64it/s]Training CobwebTree:  29%|       | 2895/10000 [01:04<03:04, 38.57it/s]Training CobwebTree:  29%|       | 2899/10000 [01:04<03:03, 38.71it/s]Training CobwebTree:  29%|       | 2903/10000 [01:04<03:14, 36.51it/s]Training CobwebTree:  29%|       | 2907/10000 [01:05<03:10, 37.25it/s]Training CobwebTree:  29%|       | 2912/10000 [01:05<03:06, 38.05it/s]Training CobwebTree:  29%|       | 2917/10000 [01:05<03:01, 39.11it/s]Training CobwebTree:  29%|       | 2922/10000 [01:05<02:56, 40.05it/s]Training CobwebTree:  29%|       | 2927/10000 [01:05<02:58, 39.73it/s]Training CobwebTree:  29%|       | 2932/10000 [01:05<02:56, 40.09it/s]Training CobwebTree:  29%|       | 2937/10000 [01:05<03:02, 38.62it/s]Training CobwebTree:  29%|       | 2941/10000 [01:05<03:04, 38.33it/s]Training CobwebTree:  29%|       | 2946/10000 [01:06<02:59, 39.32it/s]Training CobwebTree:  30%|       | 2950/10000 [01:06<03:02, 38.67it/s]Training CobwebTree:  30%|       | 2954/10000 [01:06<03:09, 37.17it/s]Training CobwebTree:  30%|       | 2959/10000 [01:06<02:59, 39.31it/s]Training CobwebTree:  30%|       | 2964/10000 [01:06<02:58, 39.45it/s]Training CobwebTree:  30%|       | 2969/10000 [01:06<02:53, 40.48it/s]Training CobwebTree:  30%|       | 2974/10000 [01:06<02:55, 40.14it/s]Training CobwebTree:  30%|       | 2979/10000 [01:06<03:01, 38.67it/s]Training CobwebTree:  30%|       | 2983/10000 [01:06<03:02, 38.50it/s]Training CobwebTree:  30%|       | 2987/10000 [01:07<03:01, 38.62it/s]Training CobwebTree:  30%|       | 2991/10000 [01:07<03:05, 37.79it/s]Training CobwebTree:  30%|       | 2995/10000 [01:07<03:02, 38.37it/s]Training CobwebTree:  30%|       | 2999/10000 [01:07<03:02, 38.28it/s]Training CobwebTree:  30%|       | 3004/10000 [01:07<03:00, 38.84it/s]Training CobwebTree:  30%|       | 3009/10000 [01:07<02:54, 39.96it/s]Training CobwebTree:  30%|       | 3013/10000 [01:07<02:57, 39.36it/s]Training CobwebTree:  30%|       | 3017/10000 [01:07<03:03, 38.00it/s]Training CobwebTree:  30%|       | 3022/10000 [01:07<02:58, 39.20it/s]Training CobwebTree:  30%|       | 3026/10000 [01:08<03:01, 38.33it/s]Training CobwebTree:  30%|       | 3030/10000 [01:08<02:59, 38.76it/s]Training CobwebTree:  30%|       | 3035/10000 [01:08<02:58, 38.97it/s]Training CobwebTree:  30%|       | 3039/10000 [01:08<02:57, 39.22it/s]Training CobwebTree:  30%|       | 3044/10000 [01:08<02:54, 39.93it/s]Training CobwebTree:  30%|       | 3048/10000 [01:08<03:00, 38.50it/s]Training CobwebTree:  31%|       | 3053/10000 [01:08<02:51, 40.62it/s]Training CobwebTree:  31%|       | 3058/10000 [01:08<02:55, 39.60it/s]Training CobwebTree:  31%|       | 3062/10000 [01:09<02:59, 38.73it/s]Training CobwebTree:  31%|       | 3066/10000 [01:09<02:57, 39.01it/s]Training CobwebTree:  31%|       | 3070/10000 [01:09<02:57, 39.15it/s]Training CobwebTree:  31%|       | 3074/10000 [01:09<02:58, 38.77it/s]Training CobwebTree:  31%|       | 3078/10000 [01:09<03:00, 38.42it/s]Training CobwebTree:  31%|       | 3082/10000 [01:09<03:01, 38.17it/s]Training CobwebTree:  31%|       | 3086/10000 [01:09<03:04, 37.56it/s]Training CobwebTree:  31%|       | 3091/10000 [01:09<02:57, 38.88it/s]Training CobwebTree:  31%|       | 3096/10000 [01:09<02:54, 39.60it/s]Training CobwebTree:  31%|       | 3100/10000 [01:09<02:59, 38.41it/s]Training CobwebTree:  31%|       | 3105/10000 [01:10<02:57, 38.88it/s]Training CobwebTree:  31%|       | 3110/10000 [01:10<02:52, 39.91it/s]Training CobwebTree:  31%|       | 3114/10000 [01:10<02:53, 39.64it/s]Training CobwebTree:  31%|       | 3119/10000 [01:10<02:51, 40.01it/s]Training CobwebTree:  31%|       | 3123/10000 [01:10<03:03, 37.56it/s]Training CobwebTree:  31%|      | 3127/10000 [01:10<03:02, 37.57it/s]Training CobwebTree:  31%|      | 3131/10000 [01:10<03:05, 37.12it/s]Training CobwebTree:  31%|      | 3135/10000 [01:10<03:01, 37.75it/s]Training CobwebTree:  31%|      | 3140/10000 [01:11<02:55, 39.18it/s]Training CobwebTree:  31%|      | 3145/10000 [01:11<02:57, 38.57it/s]Training CobwebTree:  32%|      | 3150/10000 [01:11<02:55, 39.06it/s]Training CobwebTree:  32%|      | 3155/10000 [01:11<02:49, 40.45it/s]Training CobwebTree:  32%|      | 3160/10000 [01:11<02:43, 41.95it/s]Training CobwebTree:  32%|      | 3165/10000 [01:11<02:47, 40.88it/s]Training CobwebTree:  32%|      | 3170/10000 [01:11<02:50, 40.04it/s]Training CobwebTree:  32%|      | 3175/10000 [01:11<02:49, 40.19it/s]Training CobwebTree:  32%|      | 3180/10000 [01:12<02:46, 41.00it/s]Training CobwebTree:  32%|      | 3185/10000 [01:12<02:47, 40.77it/s]Training CobwebTree:  32%|      | 3190/10000 [01:12<02:50, 40.00it/s]Training CobwebTree:  32%|      | 3195/10000 [01:12<02:50, 39.85it/s]Training CobwebTree:  32%|      | 3199/10000 [01:12<02:54, 39.03it/s]Training CobwebTree:  32%|      | 3203/10000 [01:12<02:56, 38.47it/s]Training CobwebTree:  32%|      | 3207/10000 [01:12<03:04, 36.78it/s]Training CobwebTree:  32%|      | 3211/10000 [01:12<03:06, 36.33it/s]Training CobwebTree:  32%|      | 3216/10000 [01:12<02:59, 37.86it/s]Training CobwebTree:  32%|      | 3220/10000 [01:13<02:58, 37.89it/s]Training CobwebTree:  32%|      | 3224/10000 [01:13<02:58, 37.95it/s]Training CobwebTree:  32%|      | 3229/10000 [01:13<02:51, 39.46it/s]Training CobwebTree:  32%|      | 3233/10000 [01:13<02:55, 38.53it/s]Training CobwebTree:  32%|      | 3237/10000 [01:13<02:57, 38.09it/s]Training CobwebTree:  32%|      | 3241/10000 [01:13<02:58, 37.81it/s]Training CobwebTree:  32%|      | 3245/10000 [01:13<03:00, 37.46it/s]Training CobwebTree:  32%|      | 3249/10000 [01:13<02:58, 37.83it/s]Training CobwebTree:  33%|      | 3253/10000 [01:13<02:58, 37.76it/s]Training CobwebTree:  33%|      | 3257/10000 [01:14<03:04, 36.48it/s]Training CobwebTree:  33%|      | 3261/10000 [01:14<03:02, 36.90it/s]Training CobwebTree:  33%|      | 3265/10000 [01:14<02:59, 37.53it/s]Training CobwebTree:  33%|      | 3270/10000 [01:14<02:53, 38.76it/s]Training CobwebTree:  33%|      | 3274/10000 [01:14<02:59, 37.54it/s]Training CobwebTree:  33%|      | 3278/10000 [01:14<02:56, 38.00it/s]Training CobwebTree:  33%|      | 3283/10000 [01:14<02:56, 38.09it/s]Training CobwebTree:  33%|      | 3287/10000 [01:14<02:59, 37.43it/s]Training CobwebTree:  33%|      | 3292/10000 [01:14<02:51, 39.15it/s]Training CobwebTree:  33%|      | 3297/10000 [01:15<02:49, 39.48it/s]Training CobwebTree:  33%|      | 3301/10000 [01:15<02:54, 38.42it/s]Training CobwebTree:  33%|      | 3305/10000 [01:15<02:53, 38.54it/s]Training CobwebTree:  33%|      | 3310/10000 [01:15<02:46, 40.07it/s]Training CobwebTree:  33%|      | 3315/10000 [01:15<02:50, 39.11it/s]Training CobwebTree:  33%|      | 3320/10000 [01:15<02:47, 39.85it/s]Training CobwebTree:  33%|      | 3325/10000 [01:15<02:48, 39.73it/s]Training CobwebTree:  33%|      | 3329/10000 [01:15<02:51, 38.85it/s]Training CobwebTree:  33%|      | 3333/10000 [01:16<02:56, 37.77it/s]Training CobwebTree:  33%|      | 3338/10000 [01:16<02:49, 39.22it/s]Training CobwebTree:  33%|      | 3343/10000 [01:16<02:48, 39.45it/s]Training CobwebTree:  33%|      | 3347/10000 [01:16<02:51, 38.76it/s]Training CobwebTree:  34%|      | 3351/10000 [01:16<02:58, 37.21it/s]Training CobwebTree:  34%|      | 3355/10000 [01:16<02:57, 37.40it/s]Training CobwebTree:  34%|      | 3360/10000 [01:16<02:52, 38.56it/s]Training CobwebTree:  34%|      | 3364/10000 [01:16<02:52, 38.46it/s]Training CobwebTree:  34%|      | 3369/10000 [01:16<02:52, 38.54it/s]Training CobwebTree:  34%|      | 3373/10000 [01:17<02:53, 38.13it/s]Training CobwebTree:  34%|      | 3377/10000 [01:17<03:04, 35.84it/s]Training CobwebTree:  34%|      | 3382/10000 [01:17<02:58, 37.05it/s]Training CobwebTree:  34%|      | 3387/10000 [01:17<02:46, 39.77it/s]Training CobwebTree:  34%|      | 3392/10000 [01:17<02:44, 40.10it/s]Training CobwebTree:  34%|      | 3397/10000 [01:17<02:42, 40.52it/s]Training CobwebTree:  34%|      | 3402/10000 [01:17<02:49, 38.99it/s]Training CobwebTree:  34%|      | 3406/10000 [01:17<02:52, 38.26it/s]Training CobwebTree:  34%|      | 3410/10000 [01:18<02:51, 38.45it/s]Training CobwebTree:  34%|      | 3414/10000 [01:18<02:56, 37.21it/s]Training CobwebTree:  34%|      | 3419/10000 [01:18<02:45, 39.71it/s]Training CobwebTree:  34%|      | 3423/10000 [01:18<02:47, 39.36it/s]Training CobwebTree:  34%|      | 3427/10000 [01:18<02:56, 37.31it/s]Training CobwebTree:  34%|      | 3432/10000 [01:18<02:51, 38.38it/s]Training CobwebTree:  34%|      | 3436/10000 [01:18<02:49, 38.75it/s]Training CobwebTree:  34%|      | 3440/10000 [01:18<02:49, 38.69it/s]Training CobwebTree:  34%|      | 3445/10000 [01:18<02:47, 39.09it/s]Training CobwebTree:  34%|      | 3450/10000 [01:19<02:42, 40.28it/s]Training CobwebTree:  35%|      | 3455/10000 [01:19<02:42, 40.36it/s]Training CobwebTree:  35%|      | 3460/10000 [01:19<02:41, 40.48it/s]Training CobwebTree:  35%|      | 3465/10000 [01:19<02:45, 39.56it/s]Training CobwebTree:  35%|      | 3469/10000 [01:19<02:51, 37.98it/s]Training CobwebTree:  35%|      | 3473/10000 [01:19<02:56, 37.05it/s]Training CobwebTree:  35%|      | 3478/10000 [01:19<02:51, 38.00it/s]Training CobwebTree:  35%|      | 3482/10000 [01:19<02:52, 37.82it/s]Training CobwebTree:  35%|      | 3486/10000 [01:19<02:50, 38.23it/s]Training CobwebTree:  35%|      | 3490/10000 [01:20<02:51, 37.87it/s]Training CobwebTree:  35%|      | 3494/10000 [01:20<02:52, 37.77it/s]Training CobwebTree:  35%|      | 3498/10000 [01:20<02:53, 37.41it/s]Training CobwebTree:  35%|      | 3503/10000 [01:20<02:43, 39.70it/s]Training CobwebTree:  35%|      | 3507/10000 [01:20<02:49, 38.35it/s]Training CobwebTree:  35%|      | 3511/10000 [01:20<02:51, 37.73it/s]Training CobwebTree:  35%|      | 3515/10000 [01:20<02:54, 37.10it/s]Training CobwebTree:  35%|      | 3519/10000 [01:20<03:00, 35.87it/s]Training CobwebTree:  35%|      | 3523/10000 [01:20<02:59, 36.04it/s]Training CobwebTree:  35%|      | 3527/10000 [01:21<03:00, 35.84it/s]Training CobwebTree:  35%|      | 3531/10000 [01:21<02:58, 36.18it/s]Training CobwebTree:  35%|      | 3535/10000 [01:21<02:55, 36.89it/s]Training CobwebTree:  35%|      | 3540/10000 [01:21<02:48, 38.42it/s]Training CobwebTree:  35%|      | 3544/10000 [01:21<02:46, 38.74it/s]Training CobwebTree:  35%|      | 3548/10000 [01:21<02:51, 37.70it/s]Training CobwebTree:  36%|      | 3552/10000 [01:21<02:49, 38.13it/s]Training CobwebTree:  36%|      | 3556/10000 [01:21<02:50, 37.77it/s]Training CobwebTree:  36%|      | 3560/10000 [01:21<02:48, 38.25it/s]Training CobwebTree:  36%|      | 3564/10000 [01:22<02:46, 38.70it/s]Training CobwebTree:  36%|      | 3568/10000 [01:22<02:49, 37.99it/s]Training CobwebTree:  36%|      | 3572/10000 [01:22<02:48, 38.18it/s]Training CobwebTree:  36%|      | 3576/10000 [01:22<02:50, 37.78it/s]Training CobwebTree:  36%|      | 3580/10000 [01:22<02:53, 37.10it/s]Training CobwebTree:  36%|      | 3584/10000 [01:22<03:00, 35.52it/s]Training CobwebTree:  36%|      | 3589/10000 [01:22<02:54, 36.72it/s]Training CobwebTree:  36%|      | 3594/10000 [01:22<02:51, 37.32it/s]Training CobwebTree:  36%|      | 3599/10000 [01:22<02:47, 38.23it/s]Training CobwebTree:  36%|      | 3603/10000 [01:23<02:47, 38.17it/s]Training CobwebTree:  36%|      | 3608/10000 [01:23<02:42, 39.33it/s]Training CobwebTree:  36%|      | 3612/10000 [01:23<02:45, 38.48it/s]Training CobwebTree:  36%|      | 3616/10000 [01:23<02:45, 38.53it/s]Training CobwebTree:  36%|      | 3620/10000 [01:23<02:54, 36.50it/s]Training CobwebTree:  36%|      | 3624/10000 [01:23<02:57, 35.99it/s]Training CobwebTree:  36%|      | 3628/10000 [01:23<02:54, 36.47it/s]Training CobwebTree:  36%|      | 3632/10000 [01:23<02:51, 37.05it/s]Training CobwebTree:  36%|      | 3636/10000 [01:23<02:48, 37.77it/s]Training CobwebTree:  36%|      | 3640/10000 [01:24<02:49, 37.42it/s]Training CobwebTree:  36%|      | 3645/10000 [01:24<02:47, 37.89it/s]Training CobwebTree:  36%|      | 3649/10000 [01:24<02:48, 37.65it/s]Training CobwebTree:  37%|      | 3653/10000 [01:24<02:54, 36.39it/s]Training CobwebTree:  37%|      | 3658/10000 [01:24<02:47, 37.80it/s]Training CobwebTree:  37%|      | 3662/10000 [01:24<02:46, 38.02it/s]Training CobwebTree:  37%|      | 3666/10000 [01:24<02:52, 36.80it/s]Training CobwebTree:  37%|      | 3671/10000 [01:24<02:46, 38.09it/s]Training CobwebTree:  37%|      | 3675/10000 [01:25<02:53, 36.37it/s]Training CobwebTree:  37%|      | 3680/10000 [01:25<02:52, 36.69it/s]Training CobwebTree:  37%|      | 3685/10000 [01:25<02:46, 37.85it/s]Training CobwebTree:  37%|      | 3689/10000 [01:25<02:50, 36.94it/s]Training CobwebTree:  37%|      | 3693/10000 [01:25<02:50, 37.05it/s]Training CobwebTree:  37%|      | 3697/10000 [01:25<02:47, 37.56it/s]Training CobwebTree:  37%|      | 3701/10000 [01:25<02:44, 38.19it/s]Training CobwebTree:  37%|      | 3706/10000 [01:25<02:37, 40.08it/s]Training CobwebTree:  37%|      | 3711/10000 [01:25<02:36, 40.12it/s]Training CobwebTree:  37%|      | 3716/10000 [01:26<02:37, 39.89it/s]Training CobwebTree:  37%|      | 3720/10000 [01:26<02:51, 36.59it/s]Training CobwebTree:  37%|      | 3724/10000 [01:26<02:47, 37.40it/s]Training CobwebTree:  37%|      | 3728/10000 [01:26<02:45, 37.99it/s]Training CobwebTree:  37%|      | 3732/10000 [01:26<02:47, 37.31it/s]Training CobwebTree:  37%|      | 3736/10000 [01:26<02:47, 37.45it/s]Training CobwebTree:  37%|      | 3740/10000 [01:26<02:48, 37.10it/s]Training CobwebTree:  37%|      | 3744/10000 [01:26<02:48, 37.05it/s]Training CobwebTree:  37%|      | 3748/10000 [01:26<02:48, 37.19it/s]Training CobwebTree:  38%|      | 3752/10000 [01:27<02:50, 36.65it/s]Training CobwebTree:  38%|      | 3757/10000 [01:27<02:48, 37.04it/s]Training CobwebTree:  38%|      | 3761/10000 [01:27<02:49, 36.77it/s]Training CobwebTree:  38%|      | 3766/10000 [01:27<02:42, 38.33it/s]Training CobwebTree:  38%|      | 3770/10000 [01:27<02:46, 37.35it/s]Training CobwebTree:  38%|      | 3774/10000 [01:27<02:43, 38.06it/s]Training CobwebTree:  38%|      | 3778/10000 [01:27<02:45, 37.62it/s]Training CobwebTree:  38%|      | 3783/10000 [01:27<02:36, 39.85it/s]Training CobwebTree:  38%|      | 3788/10000 [01:27<02:33, 40.52it/s]Training CobwebTree:  38%|      | 3793/10000 [01:28<02:35, 39.88it/s]Training CobwebTree:  38%|      | 3797/10000 [01:28<02:37, 39.28it/s]Training CobwebTree:  38%|      | 3802/10000 [01:28<02:35, 39.76it/s]Training CobwebTree:  38%|      | 3806/10000 [01:28<02:40, 38.58it/s]Training CobwebTree:  38%|      | 3810/10000 [01:28<02:42, 38.00it/s]Training CobwebTree:  38%|      | 3814/10000 [01:28<02:49, 36.58it/s]Training CobwebTree:  38%|      | 3818/10000 [01:28<02:50, 36.32it/s]Training CobwebTree:  38%|      | 3823/10000 [01:28<02:47, 36.97it/s]Training CobwebTree:  38%|      | 3827/10000 [01:29<02:48, 36.74it/s]Training CobwebTree:  38%|      | 3831/10000 [01:29<02:49, 36.43it/s]Training CobwebTree:  38%|      | 3835/10000 [01:29<02:47, 36.76it/s]Training CobwebTree:  38%|      | 3839/10000 [01:29<02:47, 36.82it/s]Training CobwebTree:  38%|      | 3843/10000 [01:29<02:45, 37.13it/s]Training CobwebTree:  38%|      | 3847/10000 [01:29<02:47, 36.77it/s]Training CobwebTree:  39%|      | 3851/10000 [01:29<02:43, 37.51it/s]Training CobwebTree:  39%|      | 3855/10000 [01:29<02:41, 37.96it/s]Training CobwebTree:  39%|      | 3859/10000 [01:29<02:47, 36.65it/s]Training CobwebTree:  39%|      | 3863/10000 [01:30<02:53, 35.31it/s]Training CobwebTree:  39%|      | 3868/10000 [01:30<02:45, 36.97it/s]Training CobwebTree:  39%|      | 3872/10000 [01:30<02:43, 37.56it/s]Training CobwebTree:  39%|      | 3877/10000 [01:30<02:37, 38.93it/s]Training CobwebTree:  39%|      | 3881/10000 [01:30<02:48, 36.24it/s]Training CobwebTree:  39%|      | 3885/10000 [01:30<02:48, 36.39it/s]Training CobwebTree:  39%|      | 3890/10000 [01:30<02:39, 38.41it/s]Training CobwebTree:  39%|      | 3894/10000 [01:30<02:39, 38.38it/s]Training CobwebTree:  39%|      | 3898/10000 [01:30<02:47, 36.37it/s]Training CobwebTree:  39%|      | 3902/10000 [01:31<02:51, 35.65it/s]Training CobwebTree:  39%|      | 3906/10000 [01:31<02:53, 35.19it/s]Training CobwebTree:  39%|      | 3910/10000 [01:31<02:51, 35.53it/s]Training CobwebTree:  39%|      | 3915/10000 [01:31<02:43, 37.18it/s]Training CobwebTree:  39%|      | 3920/10000 [01:31<02:36, 38.86it/s]Training CobwebTree:  39%|      | 3925/10000 [01:31<02:35, 39.18it/s]Training CobwebTree:  39%|      | 3929/10000 [01:31<02:35, 39.08it/s]Training CobwebTree:  39%|      | 3933/10000 [01:31<02:39, 37.93it/s]Training CobwebTree:  39%|      | 3938/10000 [01:32<02:37, 38.53it/s]Training CobwebTree:  39%|      | 3943/10000 [01:32<02:34, 39.24it/s]Training CobwebTree:  39%|      | 3947/10000 [01:32<02:39, 38.02it/s]Training CobwebTree:  40%|      | 3951/10000 [01:32<02:43, 36.94it/s]Training CobwebTree:  40%|      | 3955/10000 [01:32<02:47, 36.13it/s]Training CobwebTree:  40%|      | 3959/10000 [01:32<02:43, 36.99it/s]Training CobwebTree:  40%|      | 3963/10000 [01:32<02:46, 36.17it/s]Training CobwebTree:  40%|      | 3967/10000 [01:32<02:47, 36.01it/s]Training CobwebTree:  40%|      | 3971/10000 [01:32<02:42, 37.06it/s]Training CobwebTree:  40%|      | 3975/10000 [01:33<02:44, 36.62it/s]Training CobwebTree:  40%|      | 3979/10000 [01:33<02:46, 36.13it/s]Training CobwebTree:  40%|      | 3983/10000 [01:33<02:44, 36.47it/s]Training CobwebTree:  40%|      | 3987/10000 [01:33<02:45, 36.25it/s]Training CobwebTree:  40%|      | 3991/10000 [01:33<02:46, 36.20it/s]Training CobwebTree:  40%|      | 3995/10000 [01:33<02:45, 36.35it/s]Training CobwebTree:  40%|      | 3999/10000 [01:33<02:47, 35.81it/s]Training CobwebTree:  40%|      | 4003/10000 [01:33<02:49, 35.35it/s]Training CobwebTree:  40%|      | 4007/10000 [01:33<02:45, 36.29it/s]Training CobwebTree:  40%|      | 4011/10000 [01:34<02:51, 34.89it/s]Training CobwebTree:  40%|      | 4015/10000 [01:34<02:49, 35.26it/s]Training CobwebTree:  40%|      | 4020/10000 [01:34<02:41, 36.99it/s]Training CobwebTree:  40%|      | 4024/10000 [01:34<02:39, 37.39it/s]Training CobwebTree:  40%|      | 4028/10000 [01:34<02:40, 37.25it/s]Training CobwebTree:  40%|      | 4032/10000 [01:34<02:40, 37.13it/s]Training CobwebTree:  40%|      | 4036/10000 [01:34<02:40, 37.10it/s]Training CobwebTree:  40%|      | 4040/10000 [01:34<02:41, 36.84it/s]Training CobwebTree:  40%|      | 4044/10000 [01:34<02:42, 36.64it/s]Training CobwebTree:  40%|      | 4048/10000 [01:35<02:39, 37.32it/s]Training CobwebTree:  41%|      | 4053/10000 [01:35<02:36, 38.07it/s]Training CobwebTree:  41%|      | 4057/10000 [01:35<02:36, 38.00it/s]Training CobwebTree:  41%|      | 4061/10000 [01:35<02:37, 37.66it/s]Training CobwebTree:  41%|      | 4065/10000 [01:35<02:36, 37.91it/s]Training CobwebTree:  41%|      | 4069/10000 [01:35<02:43, 36.19it/s]Training CobwebTree:  41%|      | 4073/10000 [01:35<02:40, 36.83it/s]Training CobwebTree:  41%|      | 4077/10000 [01:35<02:49, 34.88it/s]Training CobwebTree:  41%|      | 4081/10000 [01:35<02:48, 35.19it/s]Training CobwebTree:  41%|      | 4085/10000 [01:36<02:43, 36.27it/s]Training CobwebTree:  41%|      | 4089/10000 [01:36<02:46, 35.41it/s]Training CobwebTree:  41%|      | 4093/10000 [01:36<02:50, 34.73it/s]Training CobwebTree:  41%|      | 4097/10000 [01:36<02:44, 35.98it/s]Training CobwebTree:  41%|      | 4101/10000 [01:36<02:41, 36.51it/s]Training CobwebTree:  41%|      | 4106/10000 [01:36<02:38, 37.20it/s]Training CobwebTree:  41%|      | 4110/10000 [01:36<02:39, 36.89it/s]Training CobwebTree:  41%|      | 4115/10000 [01:36<02:34, 38.10it/s]Training CobwebTree:  41%|      | 4119/10000 [01:36<02:35, 37.89it/s]Training CobwebTree:  41%|      | 4123/10000 [01:37<02:40, 36.72it/s]Training CobwebTree:  41%|     | 4127/10000 [01:37<02:44, 35.61it/s]Training CobwebTree:  41%|     | 4131/10000 [01:37<02:48, 34.83it/s]Training CobwebTree:  41%|     | 4136/10000 [01:37<02:43, 35.96it/s]Training CobwebTree:  41%|     | 4141/10000 [01:37<02:35, 37.59it/s]Training CobwebTree:  41%|     | 4145/10000 [01:37<02:33, 38.02it/s]Training CobwebTree:  42%|     | 4150/10000 [01:37<02:31, 38.53it/s]Training CobwebTree:  42%|     | 4154/10000 [01:37<02:34, 37.88it/s]Training CobwebTree:  42%|     | 4158/10000 [01:38<02:38, 36.86it/s]Training CobwebTree:  42%|     | 4163/10000 [01:38<02:29, 38.98it/s]Training CobwebTree:  42%|     | 4167/10000 [01:38<02:38, 36.88it/s]Training CobwebTree:  42%|     | 4171/10000 [01:38<02:41, 36.05it/s]Training CobwebTree:  42%|     | 4175/10000 [01:38<02:38, 36.75it/s]Training CobwebTree:  42%|     | 4179/10000 [01:38<02:36, 37.19it/s]Training CobwebTree:  42%|     | 4183/10000 [01:38<02:34, 37.60it/s]Training CobwebTree:  42%|     | 4187/10000 [01:38<02:32, 38.17it/s]Training CobwebTree:  42%|     | 4191/10000 [01:38<02:35, 37.24it/s]Training CobwebTree:  42%|     | 4195/10000 [01:39<02:38, 36.72it/s]Training CobwebTree:  42%|     | 4199/10000 [01:39<02:40, 36.04it/s]Training CobwebTree:  42%|     | 4203/10000 [01:39<02:42, 35.74it/s]Training CobwebTree:  42%|     | 4208/10000 [01:39<02:26, 39.52it/s]Training CobwebTree:  42%|     | 4212/10000 [01:39<02:33, 37.66it/s]Training CobwebTree:  42%|     | 4216/10000 [01:39<02:39, 36.37it/s]Training CobwebTree:  42%|     | 4221/10000 [01:39<02:32, 37.79it/s]Training CobwebTree:  42%|     | 4225/10000 [01:39<02:32, 37.98it/s]Training CobwebTree:  42%|     | 4229/10000 [01:39<02:35, 37.22it/s]Training CobwebTree:  42%|     | 4233/10000 [01:40<02:34, 37.26it/s]Training CobwebTree:  42%|     | 4237/10000 [01:40<02:33, 37.63it/s]Training CobwebTree:  42%|     | 4242/10000 [01:40<02:27, 39.13it/s]Training CobwebTree:  42%|     | 4246/10000 [01:40<02:33, 37.56it/s]Training CobwebTree:  42%|     | 4250/10000 [01:40<02:32, 37.58it/s]Training CobwebTree:  43%|     | 4254/10000 [01:40<02:38, 36.19it/s]Training CobwebTree:  43%|     | 4258/10000 [01:40<02:43, 35.18it/s]Training CobwebTree:  43%|     | 4262/10000 [01:40<02:46, 34.36it/s]Training CobwebTree:  43%|     | 4266/10000 [01:40<02:48, 34.08it/s]Training CobwebTree:  43%|     | 4271/10000 [01:41<02:38, 36.07it/s]Training CobwebTree:  43%|     | 4275/10000 [01:41<02:36, 36.59it/s]Training CobwebTree:  43%|     | 4279/10000 [01:41<02:34, 36.92it/s]Training CobwebTree:  43%|     | 4283/10000 [01:41<02:37, 36.21it/s]Training CobwebTree:  43%|     | 4287/10000 [01:41<02:35, 36.67it/s]Training CobwebTree:  43%|     | 4291/10000 [01:41<02:34, 36.96it/s]Training CobwebTree:  43%|     | 4295/10000 [01:41<02:37, 36.11it/s]Training CobwebTree:  43%|     | 4300/10000 [01:41<02:31, 37.62it/s]Training CobwebTree:  43%|     | 4304/10000 [01:41<02:31, 37.56it/s]Training CobwebTree:  43%|     | 4308/10000 [01:42<02:29, 38.02it/s]Training CobwebTree:  43%|     | 4312/10000 [01:42<02:32, 37.34it/s]Training CobwebTree:  43%|     | 4316/10000 [01:42<02:32, 37.36it/s]Training CobwebTree:  43%|     | 4321/10000 [01:42<02:27, 38.51it/s]Training CobwebTree:  43%|     | 4325/10000 [01:42<02:27, 38.44it/s]Training CobwebTree:  43%|     | 4329/10000 [01:42<02:36, 36.31it/s]Training CobwebTree:  43%|     | 4334/10000 [01:42<02:34, 36.78it/s]Training CobwebTree:  43%|     | 4338/10000 [01:42<02:38, 35.71it/s]Training CobwebTree:  43%|     | 4342/10000 [01:42<02:35, 36.32it/s]Training CobwebTree:  43%|     | 4347/10000 [01:43<02:29, 37.75it/s]Training CobwebTree:  44%|     | 4351/10000 [01:43<02:30, 37.60it/s]Training CobwebTree:  44%|     | 4356/10000 [01:43<02:23, 39.20it/s]Training CobwebTree:  44%|     | 4360/10000 [01:43<02:24, 39.01it/s]Training CobwebTree:  44%|     | 4364/10000 [01:43<02:25, 38.82it/s]Training CobwebTree:  44%|     | 4368/10000 [01:43<02:25, 38.75it/s]Training CobwebTree:  44%|     | 4372/10000 [01:43<02:33, 36.56it/s]Training CobwebTree:  44%|     | 4376/10000 [01:43<02:36, 35.97it/s]Training CobwebTree:  44%|     | 4380/10000 [01:44<02:39, 35.25it/s]Training CobwebTree:  44%|     | 4384/10000 [01:44<02:33, 36.49it/s]Training CobwebTree:  44%|     | 4388/10000 [01:44<02:37, 35.71it/s]Training CobwebTree:  44%|     | 4392/10000 [01:44<02:37, 35.60it/s]Training CobwebTree:  44%|     | 4396/10000 [01:44<02:36, 35.76it/s]Training CobwebTree:  44%|     | 4400/10000 [01:44<02:37, 35.49it/s]Training CobwebTree:  44%|     | 4405/10000 [01:44<02:31, 36.88it/s]Training CobwebTree:  44%|     | 4409/10000 [01:44<02:35, 36.06it/s]Training CobwebTree:  44%|     | 4413/10000 [01:44<02:31, 37.00it/s]Training CobwebTree:  44%|     | 4417/10000 [01:45<02:30, 37.14it/s]Training CobwebTree:  44%|     | 4421/10000 [01:45<02:31, 36.70it/s]Training CobwebTree:  44%|     | 4425/10000 [01:45<02:36, 35.74it/s]Training CobwebTree:  44%|     | 4429/10000 [01:45<02:36, 35.53it/s]Training CobwebTree:  44%|     | 4433/10000 [01:45<02:38, 35.02it/s]Training CobwebTree:  44%|     | 4437/10000 [01:45<02:36, 35.61it/s]Training CobwebTree:  44%|     | 4441/10000 [01:45<02:42, 34.31it/s]Training CobwebTree:  44%|     | 4446/10000 [01:45<02:29, 37.15it/s]Training CobwebTree:  45%|     | 4451/10000 [01:45<02:26, 37.99it/s]Training CobwebTree:  45%|     | 4455/10000 [01:46<02:31, 36.49it/s]Training CobwebTree:  45%|     | 4459/10000 [01:46<02:31, 36.65it/s]Training CobwebTree:  45%|     | 4464/10000 [01:46<02:24, 38.25it/s]Training CobwebTree:  45%|     | 4468/10000 [01:46<02:31, 36.61it/s]Training CobwebTree:  45%|     | 4473/10000 [01:46<02:29, 36.96it/s]Training CobwebTree:  45%|     | 4478/10000 [01:46<02:23, 38.54it/s]Training CobwebTree:  45%|     | 4482/10000 [01:46<02:25, 37.83it/s]Training CobwebTree:  45%|     | 4486/10000 [01:46<02:32, 36.24it/s]Training CobwebTree:  45%|     | 4490/10000 [01:47<02:38, 34.87it/s]Training CobwebTree:  45%|     | 4494/10000 [01:47<02:33, 35.98it/s]Training CobwebTree:  45%|     | 4498/10000 [01:47<02:33, 35.95it/s]Training CobwebTree:  45%|     | 4502/10000 [01:47<02:32, 35.98it/s]Training CobwebTree:  45%|     | 4507/10000 [01:47<02:24, 38.04it/s]Training CobwebTree:  45%|     | 4511/10000 [01:47<02:28, 37.03it/s]Training CobwebTree:  45%|     | 4515/10000 [01:47<02:28, 36.94it/s]Training CobwebTree:  45%|     | 4519/10000 [01:47<02:29, 36.69it/s]Training CobwebTree:  45%|     | 4523/10000 [01:47<02:30, 36.49it/s]Training CobwebTree:  45%|     | 4528/10000 [01:48<02:23, 38.11it/s]Training CobwebTree:  45%|     | 4533/10000 [01:48<02:19, 39.17it/s]Training CobwebTree:  45%|     | 4537/10000 [01:48<02:29, 36.64it/s]Training CobwebTree:  45%|     | 4542/10000 [01:48<02:23, 38.01it/s]Training CobwebTree:  45%|     | 4546/10000 [01:48<02:26, 37.20it/s]Training CobwebTree:  46%|     | 4550/10000 [01:48<02:25, 37.40it/s]Training CobwebTree:  46%|     | 4554/10000 [01:48<02:25, 37.47it/s]Training CobwebTree:  46%|     | 4558/10000 [01:48<02:30, 36.15it/s]Training CobwebTree:  46%|     | 4562/10000 [01:48<02:27, 36.74it/s]Training CobwebTree:  46%|     | 4566/10000 [01:49<02:29, 36.37it/s]Training CobwebTree:  46%|     | 4570/10000 [01:49<02:33, 35.39it/s]Training CobwebTree:  46%|     | 4574/10000 [01:49<02:30, 36.17it/s]Training CobwebTree:  46%|     | 4578/10000 [01:49<02:31, 35.89it/s]Training CobwebTree:  46%|     | 4582/10000 [01:49<02:34, 35.06it/s]Training CobwebTree:  46%|     | 4587/10000 [01:49<02:28, 36.57it/s]Training CobwebTree:  46%|     | 4591/10000 [01:49<02:33, 35.13it/s]Training CobwebTree:  46%|     | 4595/10000 [01:49<02:28, 36.36it/s]Training CobwebTree:  46%|     | 4599/10000 [01:49<02:27, 36.61it/s]Training CobwebTree:  46%|     | 4603/10000 [01:50<02:29, 36.06it/s]Training CobwebTree:  46%|     | 4607/10000 [01:50<02:29, 36.08it/s]Training CobwebTree:  46%|     | 4611/10000 [01:50<02:27, 36.60it/s]Training CobwebTree:  46%|     | 4616/10000 [01:50<02:25, 37.09it/s]Training CobwebTree:  46%|     | 4620/10000 [01:50<02:25, 36.92it/s]Training CobwebTree:  46%|     | 4625/10000 [01:50<02:19, 38.40it/s]Training CobwebTree:  46%|     | 4629/10000 [01:50<02:20, 38.26it/s]Training CobwebTree:  46%|     | 4634/10000 [01:50<02:16, 39.25it/s]Training CobwebTree:  46%|     | 4638/10000 [01:51<02:19, 38.53it/s]Training CobwebTree:  46%|     | 4642/10000 [01:51<02:25, 36.87it/s]Training CobwebTree:  46%|     | 4646/10000 [01:51<02:27, 36.30it/s]Training CobwebTree:  46%|     | 4650/10000 [01:51<02:27, 36.25it/s]Training CobwebTree:  47%|     | 4654/10000 [01:51<02:24, 37.02it/s]Training CobwebTree:  47%|     | 4658/10000 [01:51<02:22, 37.58it/s]Training CobwebTree:  47%|     | 4662/10000 [01:51<02:21, 37.68it/s]Training CobwebTree:  47%|     | 4666/10000 [01:51<02:28, 35.90it/s]Training CobwebTree:  47%|     | 4670/10000 [01:51<02:35, 34.17it/s]Training CobwebTree:  47%|     | 4674/10000 [01:52<02:38, 33.54it/s]Training CobwebTree:  47%|     | 4678/10000 [01:52<02:37, 33.82it/s]Training CobwebTree:  47%|     | 4683/10000 [01:52<02:30, 35.42it/s]Training CobwebTree:  47%|     | 4688/10000 [01:52<02:22, 37.31it/s]Training CobwebTree:  47%|     | 4693/10000 [01:52<02:17, 38.55it/s]Training CobwebTree:  47%|     | 4697/10000 [01:52<02:18, 38.33it/s]Training CobwebTree:  47%|     | 4701/10000 [01:52<02:20, 37.74it/s]Training CobwebTree:  47%|     | 4706/10000 [01:52<02:20, 37.60it/s]Training CobwebTree:  47%|     | 4710/10000 [01:52<02:19, 37.83it/s]Training CobwebTree:  47%|     | 4714/10000 [01:53<02:24, 36.51it/s]Training CobwebTree:  47%|     | 4718/10000 [01:53<02:24, 36.62it/s]Training CobwebTree:  47%|     | 4723/10000 [01:53<02:16, 38.79it/s]Training CobwebTree:  47%|     | 4727/10000 [01:53<02:19, 37.85it/s]Training CobwebTree:  47%|     | 4731/10000 [01:53<02:20, 37.49it/s]Training CobwebTree:  47%|     | 4735/10000 [01:53<02:21, 37.23it/s]Training CobwebTree:  47%|     | 4740/10000 [01:53<02:16, 38.44it/s]Training CobwebTree:  47%|     | 4744/10000 [01:53<02:17, 38.26it/s]Training CobwebTree:  47%|     | 4748/10000 [01:54<02:20, 37.33it/s]Training CobwebTree:  48%|     | 4752/10000 [01:54<02:28, 35.25it/s]Training CobwebTree:  48%|     | 4757/10000 [01:54<02:23, 36.56it/s]Training CobwebTree:  48%|     | 4761/10000 [01:54<02:22, 36.68it/s]Training CobwebTree:  48%|     | 4765/10000 [01:54<02:24, 36.33it/s]Training CobwebTree:  48%|     | 4769/10000 [01:54<02:27, 35.44it/s]Training CobwebTree:  48%|     | 4773/10000 [01:54<02:23, 36.33it/s]Training CobwebTree:  48%|     | 4777/10000 [01:54<02:24, 36.24it/s]Training CobwebTree:  48%|     | 4781/10000 [01:54<02:21, 36.86it/s]Training CobwebTree:  48%|     | 4785/10000 [01:55<02:20, 36.99it/s]Training CobwebTree:  48%|     | 4789/10000 [01:55<02:33, 33.94it/s]Training CobwebTree:  48%|     | 4793/10000 [01:55<02:37, 33.05it/s]Training CobwebTree:  48%|     | 4797/10000 [01:55<02:33, 33.85it/s]Training CobwebTree:  48%|     | 4802/10000 [01:55<02:25, 35.71it/s]Training CobwebTree:  48%|     | 4806/10000 [01:55<02:27, 35.22it/s]Training CobwebTree:  48%|     | 4810/10000 [01:55<02:26, 35.34it/s]Training CobwebTree:  48%|     | 4814/10000 [01:55<02:23, 36.24it/s]Training CobwebTree:  48%|     | 4819/10000 [01:55<02:14, 38.52it/s]Training CobwebTree:  48%|     | 4823/10000 [01:56<02:14, 38.40it/s]Training CobwebTree:  48%|     | 4827/10000 [01:56<02:14, 38.45it/s]Training CobwebTree:  48%|     | 4831/10000 [01:56<02:13, 38.81it/s]Training CobwebTree:  48%|     | 4835/10000 [01:56<02:15, 38.05it/s]Training CobwebTree:  48%|     | 4839/10000 [01:56<02:16, 37.85it/s]Training CobwebTree:  48%|     | 4843/10000 [01:56<02:22, 36.32it/s]Training CobwebTree:  48%|     | 4847/10000 [01:56<02:21, 36.43it/s]Training CobwebTree:  49%|     | 4851/10000 [01:56<02:24, 35.72it/s]Training CobwebTree:  49%|     | 4855/10000 [01:56<02:28, 34.74it/s]Training CobwebTree:  49%|     | 4859/10000 [01:57<02:31, 34.02it/s]Training CobwebTree:  49%|     | 4863/10000 [01:57<02:34, 33.31it/s]Training CobwebTree:  49%|     | 4868/10000 [01:57<02:25, 35.21it/s]Training CobwebTree:  49%|     | 4872/10000 [01:57<02:23, 35.76it/s]Training CobwebTree:  49%|     | 4876/10000 [01:57<02:19, 36.84it/s]Training CobwebTree:  49%|     | 4881/10000 [01:57<02:09, 39.39it/s]Training CobwebTree:  49%|     | 4885/10000 [01:57<02:10, 39.31it/s]Training CobwebTree:  49%|     | 4890/10000 [01:57<02:08, 39.62it/s]Training CobwebTree:  49%|     | 4894/10000 [01:58<02:11, 38.79it/s]Training CobwebTree:  49%|     | 4898/10000 [01:58<02:14, 37.88it/s]Training CobwebTree:  49%|     | 4902/10000 [01:58<02:14, 37.92it/s]Training CobwebTree:  49%|     | 4907/10000 [01:58<02:11, 38.83it/s]Training CobwebTree:  49%|     | 4911/10000 [01:58<02:12, 38.36it/s]Training CobwebTree:  49%|     | 4916/10000 [01:58<02:13, 38.09it/s]Training CobwebTree:  49%|     | 4920/10000 [01:58<02:12, 38.22it/s]Training CobwebTree:  49%|     | 4925/10000 [01:58<02:08, 39.39it/s]Training CobwebTree:  49%|     | 4929/10000 [01:58<02:08, 39.48it/s]Training CobwebTree:  49%|     | 4933/10000 [01:59<02:09, 38.99it/s]Training CobwebTree:  49%|     | 4937/10000 [01:59<02:11, 38.39it/s]Training CobwebTree:  49%|     | 4941/10000 [01:59<02:13, 37.80it/s]Training CobwebTree:  49%|     | 4945/10000 [01:59<02:20, 35.96it/s]Training CobwebTree:  49%|     | 4949/10000 [01:59<02:17, 36.83it/s]Training CobwebTree:  50%|     | 4953/10000 [01:59<02:16, 37.05it/s]Training CobwebTree:  50%|     | 4957/10000 [01:59<02:23, 35.22it/s]Training CobwebTree:  50%|     | 4961/10000 [01:59<02:24, 34.79it/s]Training CobwebTree:  50%|     | 4965/10000 [01:59<02:25, 34.53it/s]Training CobwebTree:  50%|     | 4969/10000 [02:00<02:29, 33.56it/s]Training CobwebTree:  50%|     | 4973/10000 [02:00<02:28, 33.89it/s]Training CobwebTree:  50%|     | 4978/10000 [02:00<02:19, 36.07it/s]Training CobwebTree:  50%|     | 4982/10000 [02:00<02:19, 35.95it/s]Training CobwebTree:  50%|     | 4986/10000 [02:00<02:18, 36.26it/s]Training CobwebTree:  50%|     | 4990/10000 [02:00<02:20, 35.55it/s]Training CobwebTree:  50%|     | 4994/10000 [02:00<02:18, 36.03it/s]Training CobwebTree:  50%|     | 4998/10000 [02:00<02:19, 35.80it/s]Training CobwebTree:  50%|     | 5002/10000 [02:00<02:24, 34.63it/s]Training CobwebTree:  50%|     | 5006/10000 [02:01<02:25, 34.28it/s]Training CobwebTree:  50%|     | 5010/10000 [02:01<02:24, 34.48it/s]Training CobwebTree:  50%|     | 5014/10000 [02:01<02:20, 35.57it/s]Training CobwebTree:  50%|     | 5018/10000 [02:01<02:16, 36.56it/s]Training CobwebTree:  50%|     | 5022/10000 [02:01<02:15, 36.66it/s]Training CobwebTree:  50%|     | 5027/10000 [02:01<02:09, 38.44it/s]Training CobwebTree:  50%|     | 5031/10000 [02:01<02:17, 36.13it/s]Training CobwebTree:  50%|     | 5035/10000 [02:01<02:18, 35.79it/s]Training CobwebTree:  50%|     | 5039/10000 [02:02<02:20, 35.26it/s]Training CobwebTree:  50%|     | 5043/10000 [02:02<02:27, 33.67it/s]Training CobwebTree:  50%|     | 5048/10000 [02:02<02:17, 36.10it/s]Training CobwebTree:  51%|     | 5052/10000 [02:02<02:16, 36.37it/s]Training CobwebTree:  51%|     | 5056/10000 [02:02<02:21, 34.90it/s]Training CobwebTree:  51%|     | 5061/10000 [02:02<02:15, 36.38it/s]Training CobwebTree:  51%|     | 5066/10000 [02:02<02:11, 37.64it/s]Training CobwebTree:  51%|     | 5070/10000 [02:02<02:14, 36.77it/s]Training CobwebTree:  51%|     | 5074/10000 [02:02<02:16, 35.97it/s]Training CobwebTree:  51%|     | 5078/10000 [02:03<02:17, 35.84it/s]Training CobwebTree:  51%|     | 5082/10000 [02:03<02:13, 36.75it/s]Training CobwebTree:  51%|     | 5086/10000 [02:03<02:13, 36.70it/s]Training CobwebTree:  51%|     | 5090/10000 [02:03<02:11, 37.22it/s]Training CobwebTree:  51%|     | 5094/10000 [02:03<02:10, 37.47it/s]Training CobwebTree:  51%|     | 5098/10000 [02:03<02:08, 38.01it/s]Training CobwebTree:  51%|     | 5102/10000 [02:03<02:15, 36.06it/s]Training CobwebTree:  51%|     | 5106/10000 [02:03<02:16, 35.91it/s]Training CobwebTree:  51%|     | 5110/10000 [02:03<02:16, 35.86it/s]Training CobwebTree:  51%|     | 5114/10000 [02:04<02:15, 36.10it/s]Training CobwebTree:  51%|     | 5118/10000 [02:04<02:13, 36.55it/s]Training CobwebTree:  51%|     | 5123/10000 [02:04<02:08, 37.84it/s]Training CobwebTree:  51%|    | 5127/10000 [02:04<02:10, 37.38it/s]Training CobwebTree:  51%|    | 5131/10000 [02:04<02:12, 36.85it/s]Training CobwebTree:  51%|    | 5135/10000 [02:04<02:17, 35.36it/s]Training CobwebTree:  51%|    | 5139/10000 [02:04<02:17, 35.25it/s]Training CobwebTree:  51%|    | 5143/10000 [02:04<02:24, 33.57it/s]Training CobwebTree:  51%|    | 5148/10000 [02:05<02:15, 35.70it/s]Training CobwebTree:  52%|    | 5152/10000 [02:05<02:21, 34.22it/s]Training CobwebTree:  52%|    | 5156/10000 [02:05<02:23, 33.68it/s]Training CobwebTree:  52%|    | 5160/10000 [02:05<02:27, 32.85it/s]Training CobwebTree:  52%|    | 5164/10000 [02:05<02:21, 34.16it/s]Training CobwebTree:  52%|    | 5168/10000 [02:05<02:18, 34.81it/s]Training CobwebTree:  52%|    | 5172/10000 [02:05<02:16, 35.44it/s]Training CobwebTree:  52%|    | 5176/10000 [02:05<02:17, 35.14it/s]Training CobwebTree:  52%|    | 5180/10000 [02:05<02:12, 36.31it/s]Training CobwebTree:  52%|    | 5184/10000 [02:06<02:16, 35.18it/s]Training CobwebTree:  52%|    | 5188/10000 [02:06<02:18, 34.85it/s]Training CobwebTree:  52%|    | 5193/10000 [02:06<02:09, 37.18it/s]Training CobwebTree:  52%|    | 5197/10000 [02:06<02:13, 36.08it/s]Training CobwebTree:  52%|    | 5201/10000 [02:06<02:12, 36.09it/s]Training CobwebTree:  52%|    | 5205/10000 [02:06<02:10, 36.61it/s]Training CobwebTree:  52%|    | 5209/10000 [02:06<02:14, 35.72it/s]Training CobwebTree:  52%|    | 5213/10000 [02:06<02:18, 34.62it/s]Training CobwebTree:  52%|    | 5217/10000 [02:06<02:17, 34.67it/s]Training CobwebTree:  52%|    | 5222/10000 [02:07<02:09, 36.88it/s]Training CobwebTree:  52%|    | 5226/10000 [02:07<02:09, 36.88it/s]Training CobwebTree:  52%|    | 5230/10000 [02:07<02:10, 36.54it/s]Training CobwebTree:  52%|    | 5234/10000 [02:07<02:14, 35.36it/s]Training CobwebTree:  52%|    | 5239/10000 [02:07<02:08, 37.07it/s]Training CobwebTree:  52%|    | 5243/10000 [02:07<02:10, 36.36it/s]Training CobwebTree:  52%|    | 5247/10000 [02:07<02:12, 35.91it/s]Training CobwebTree:  53%|    | 5252/10000 [02:07<02:04, 38.00it/s]Training CobwebTree:  53%|    | 5257/10000 [02:08<02:01, 39.08it/s]Training CobwebTree:  53%|    | 5262/10000 [02:08<01:56, 40.82it/s]Training CobwebTree:  53%|    | 5267/10000 [02:08<02:02, 38.74it/s]Training CobwebTree:  53%|    | 5271/10000 [02:08<02:02, 38.72it/s]Training CobwebTree:  53%|    | 5275/10000 [02:08<02:04, 37.95it/s]Training CobwebTree:  53%|    | 5279/10000 [02:08<02:06, 37.44it/s]Training CobwebTree:  53%|    | 5283/10000 [02:08<02:08, 36.66it/s]Training CobwebTree:  53%|    | 5287/10000 [02:08<02:08, 36.78it/s]Training CobwebTree:  53%|    | 5292/10000 [02:08<02:04, 37.96it/s]Training CobwebTree:  53%|    | 5296/10000 [02:09<02:02, 38.50it/s]Training CobwebTree:  53%|    | 5300/10000 [02:09<02:06, 37.26it/s]Training CobwebTree:  53%|    | 5304/10000 [02:09<02:07, 36.79it/s]Training CobwebTree:  53%|    | 5309/10000 [02:09<02:06, 37.22it/s]Training CobwebTree:  53%|    | 5313/10000 [02:09<02:06, 37.09it/s]Training CobwebTree:  53%|    | 5317/10000 [02:09<02:08, 36.53it/s]Training CobwebTree:  53%|    | 5321/10000 [02:09<02:09, 36.23it/s]Training CobwebTree:  53%|    | 5325/10000 [02:09<02:08, 36.31it/s]Training CobwebTree:  53%|    | 5329/10000 [02:09<02:12, 35.34it/s]Training CobwebTree:  53%|    | 5334/10000 [02:10<02:06, 36.88it/s]Training CobwebTree:  53%|    | 5338/10000 [02:10<02:05, 37.25it/s]Training CobwebTree:  53%|    | 5342/10000 [02:10<02:10, 35.79it/s]Training CobwebTree:  53%|    | 5346/10000 [02:10<02:09, 35.90it/s]Training CobwebTree:  54%|    | 5350/10000 [02:10<02:10, 35.68it/s]Training CobwebTree:  54%|    | 5354/10000 [02:10<02:14, 34.57it/s]Training CobwebTree:  54%|    | 5358/10000 [02:10<02:11, 35.43it/s]Training CobwebTree:  54%|    | 5362/10000 [02:10<02:14, 34.38it/s]Training CobwebTree:  54%|    | 5366/10000 [02:11<02:15, 34.16it/s]Training CobwebTree:  54%|    | 5370/10000 [02:11<02:13, 34.60it/s]Training CobwebTree:  54%|    | 5374/10000 [02:11<02:12, 35.02it/s]Training CobwebTree:  54%|    | 5379/10000 [02:11<02:03, 37.44it/s]Training CobwebTree:  54%|    | 5384/10000 [02:11<02:02, 37.61it/s]Training CobwebTree:  54%|    | 5388/10000 [02:11<02:01, 37.89it/s]Training CobwebTree:  54%|    | 5392/10000 [02:11<02:04, 37.03it/s]Training CobwebTree:  54%|    | 5396/10000 [02:11<02:06, 36.29it/s]Training CobwebTree:  54%|    | 5400/10000 [02:11<02:06, 36.47it/s]Training CobwebTree:  54%|    | 5404/10000 [02:12<02:07, 36.15it/s]Training CobwebTree:  54%|    | 5408/10000 [02:12<02:05, 36.54it/s]Training CobwebTree:  54%|    | 5412/10000 [02:12<02:02, 37.42it/s]Training CobwebTree:  54%|    | 5416/10000 [02:12<02:08, 35.74it/s]Training CobwebTree:  54%|    | 5420/10000 [02:12<02:11, 34.81it/s]Training CobwebTree:  54%|    | 5424/10000 [02:12<02:13, 34.40it/s]Training CobwebTree:  54%|    | 5428/10000 [02:12<02:08, 35.56it/s]Training CobwebTree:  54%|    | 5432/10000 [02:12<02:12, 34.60it/s]Training CobwebTree:  54%|    | 5436/10000 [02:12<02:13, 34.11it/s]Training CobwebTree:  54%|    | 5440/10000 [02:13<02:11, 34.62it/s]Training CobwebTree:  54%|    | 5444/10000 [02:13<02:09, 35.21it/s]Training CobwebTree:  54%|    | 5448/10000 [02:13<02:08, 35.53it/s]Training CobwebTree:  55%|    | 5452/10000 [02:13<02:06, 35.89it/s]Training CobwebTree:  55%|    | 5456/10000 [02:13<02:04, 36.40it/s]Training CobwebTree:  55%|    | 5460/10000 [02:13<02:01, 37.32it/s]Training CobwebTree:  55%|    | 5464/10000 [02:13<02:02, 36.96it/s]Training CobwebTree:  55%|    | 5468/10000 [02:13<02:04, 36.42it/s]Training CobwebTree:  55%|    | 5472/10000 [02:13<02:03, 36.74it/s]Training CobwebTree:  55%|    | 5476/10000 [02:14<02:01, 37.35it/s]Training CobwebTree:  55%|    | 5480/10000 [02:14<02:03, 36.75it/s]Training CobwebTree:  55%|    | 5484/10000 [02:14<02:03, 36.52it/s]Training CobwebTree:  55%|    | 5488/10000 [02:14<02:04, 36.17it/s]Training CobwebTree:  55%|    | 5492/10000 [02:14<02:06, 35.63it/s]Training CobwebTree:  55%|    | 5496/10000 [02:14<02:07, 35.32it/s]Training CobwebTree:  55%|    | 5500/10000 [02:14<02:08, 35.02it/s]Training CobwebTree:  55%|    | 5504/10000 [02:14<02:07, 35.37it/s]Training CobwebTree:  55%|    | 5508/10000 [02:14<02:08, 34.95it/s]Training CobwebTree:  55%|    | 5512/10000 [02:15<02:07, 35.19it/s]Training CobwebTree:  55%|    | 5516/10000 [02:15<02:07, 35.16it/s]Training CobwebTree:  55%|    | 5520/10000 [02:15<02:07, 35.01it/s]Training CobwebTree:  55%|    | 5524/10000 [02:15<02:05, 35.54it/s]Training CobwebTree:  55%|    | 5528/10000 [02:15<02:04, 35.89it/s]Training CobwebTree:  55%|    | 5532/10000 [02:15<02:02, 36.59it/s]Training CobwebTree:  55%|    | 5536/10000 [02:15<01:58, 37.54it/s]Training CobwebTree:  55%|    | 5540/10000 [02:15<01:57, 38.06it/s]Training CobwebTree:  55%|    | 5545/10000 [02:15<01:55, 38.60it/s]Training CobwebTree:  55%|    | 5549/10000 [02:16<01:55, 38.46it/s]Training CobwebTree:  56%|    | 5553/10000 [02:16<02:00, 36.87it/s]Training CobwebTree:  56%|    | 5557/10000 [02:16<02:04, 35.77it/s]Training CobwebTree:  56%|    | 5562/10000 [02:16<02:01, 36.61it/s]Training CobwebTree:  56%|    | 5566/10000 [02:16<02:00, 36.68it/s]Training CobwebTree:  56%|    | 5570/10000 [02:16<02:01, 36.34it/s]Training CobwebTree:  56%|    | 5574/10000 [02:16<02:01, 36.47it/s]Training CobwebTree:  56%|    | 5578/10000 [02:16<01:59, 37.07it/s]Training CobwebTree:  56%|    | 5582/10000 [02:16<01:58, 37.18it/s]Training CobwebTree:  56%|    | 5586/10000 [02:17<01:58, 37.31it/s]Training CobwebTree:  56%|    | 5590/10000 [02:17<01:59, 36.79it/s]Training CobwebTree:  56%|    | 5594/10000 [02:17<02:03, 35.62it/s]Training CobwebTree:  56%|    | 5598/10000 [02:17<02:04, 35.42it/s]Training CobwebTree:  56%|    | 5602/10000 [02:17<02:06, 34.70it/s]Training CobwebTree:  56%|    | 5606/10000 [02:17<02:10, 33.66it/s]Training CobwebTree:  56%|    | 5610/10000 [02:17<02:10, 33.55it/s]Training CobwebTree:  56%|    | 5615/10000 [02:17<02:00, 36.49it/s]Training CobwebTree:  56%|    | 5619/10000 [02:18<02:05, 34.90it/s]Training CobwebTree:  56%|    | 5623/10000 [02:18<02:03, 35.55it/s]Training CobwebTree:  56%|    | 5627/10000 [02:18<02:03, 35.34it/s]Training CobwebTree:  56%|    | 5631/10000 [02:18<02:06, 34.46it/s]Training CobwebTree:  56%|    | 5635/10000 [02:18<02:06, 34.54it/s]Training CobwebTree:  56%|    | 5639/10000 [02:18<02:02, 35.52it/s]Training CobwebTree:  56%|    | 5643/10000 [02:18<02:06, 34.54it/s]Training CobwebTree:  56%|    | 5647/10000 [02:18<02:04, 34.99it/s]Training CobwebTree:  57%|    | 5651/10000 [02:18<02:04, 35.04it/s]Training CobwebTree:  57%|    | 5655/10000 [02:19<02:04, 34.78it/s]Training CobwebTree:  57%|    | 5660/10000 [02:19<01:58, 36.65it/s]Training CobwebTree:  57%|    | 5664/10000 [02:19<01:58, 36.50it/s]Training CobwebTree:  57%|    | 5668/10000 [02:19<02:04, 34.87it/s]Training CobwebTree:  57%|    | 5672/10000 [02:19<02:03, 34.99it/s]Training CobwebTree:  57%|    | 5676/10000 [02:19<02:01, 35.60it/s]Training CobwebTree:  57%|    | 5681/10000 [02:19<01:57, 36.88it/s]Training CobwebTree:  57%|    | 5685/10000 [02:19<01:56, 36.98it/s]Training CobwebTree:  57%|    | 5689/10000 [02:20<02:00, 35.83it/s]Training CobwebTree:  57%|    | 5693/10000 [02:20<02:00, 35.85it/s]Training CobwebTree:  57%|    | 5697/10000 [02:20<01:59, 35.86it/s]Training CobwebTree:  57%|    | 5701/10000 [02:20<01:58, 36.31it/s]Training CobwebTree:  57%|    | 5705/10000 [02:20<02:00, 35.60it/s]Training CobwebTree:  57%|    | 5710/10000 [02:20<01:53, 37.84it/s]Training CobwebTree:  57%|    | 5714/10000 [02:20<01:57, 36.57it/s]Training CobwebTree:  57%|    | 5718/10000 [02:20<01:57, 36.45it/s]Training CobwebTree:  57%|    | 5722/10000 [02:20<02:02, 35.00it/s]Training CobwebTree:  57%|    | 5726/10000 [02:21<02:00, 35.41it/s]Training CobwebTree:  57%|    | 5730/10000 [02:21<02:05, 33.95it/s]Training CobwebTree:  57%|    | 5735/10000 [02:21<01:57, 36.17it/s]Training CobwebTree:  57%|    | 5739/10000 [02:21<02:00, 35.45it/s]Training CobwebTree:  57%|    | 5743/10000 [02:21<01:59, 35.63it/s]Training CobwebTree:  57%|    | 5747/10000 [02:21<01:59, 35.62it/s]Training CobwebTree:  58%|    | 5751/10000 [02:21<01:59, 35.45it/s]Training CobwebTree:  58%|    | 5755/10000 [02:21<02:00, 35.18it/s]Training CobwebTree:  58%|    | 5759/10000 [02:21<01:58, 35.66it/s]Training CobwebTree:  58%|    | 5763/10000 [02:22<02:03, 34.37it/s]Training CobwebTree:  58%|    | 5767/10000 [02:22<02:02, 34.59it/s]Training CobwebTree:  58%|    | 5771/10000 [02:22<02:02, 34.66it/s]Training CobwebTree:  58%|    | 5775/10000 [02:22<01:58, 35.54it/s]Training CobwebTree:  58%|    | 5780/10000 [02:22<01:53, 37.28it/s]Training CobwebTree:  58%|    | 5784/10000 [02:22<01:55, 36.66it/s]Training CobwebTree:  58%|    | 5788/10000 [02:22<01:52, 37.37it/s]Training CobwebTree:  58%|    | 5792/10000 [02:22<01:53, 36.94it/s]Training CobwebTree:  58%|    | 5796/10000 [02:22<01:52, 37.39it/s]Training CobwebTree:  58%|    | 5801/10000 [02:23<01:47, 39.04it/s]Training CobwebTree:  58%|    | 5805/10000 [02:23<01:49, 38.35it/s]Training CobwebTree:  58%|    | 5809/10000 [02:23<01:49, 38.12it/s]Training CobwebTree:  58%|    | 5813/10000 [02:23<01:50, 38.01it/s]Training CobwebTree:  58%|    | 5817/10000 [02:23<01:54, 36.60it/s]Training CobwebTree:  58%|    | 5821/10000 [02:23<01:54, 36.47it/s]Training CobwebTree:  58%|    | 5825/10000 [02:23<01:56, 35.95it/s]Training CobwebTree:  58%|    | 5829/10000 [02:23<01:56, 35.73it/s]Training CobwebTree:  58%|    | 5833/10000 [02:24<02:03, 33.77it/s]Training CobwebTree:  58%|    | 5838/10000 [02:24<01:52, 37.11it/s]Training CobwebTree:  58%|    | 5842/10000 [02:24<01:52, 37.03it/s]Training CobwebTree:  58%|    | 5846/10000 [02:24<01:51, 37.22it/s]Training CobwebTree:  58%|    | 5850/10000 [02:24<01:50, 37.53it/s]Training CobwebTree:  59%|    | 5854/10000 [02:24<01:49, 37.80it/s]Training CobwebTree:  59%|    | 5858/10000 [02:24<01:48, 38.14it/s]Training CobwebTree:  59%|    | 5863/10000 [02:24<01:47, 38.43it/s]Training CobwebTree:  59%|    | 5867/10000 [02:24<01:49, 37.76it/s]Training CobwebTree:  59%|    | 5871/10000 [02:25<01:52, 36.74it/s]Training CobwebTree:  59%|    | 5875/10000 [02:25<01:54, 36.11it/s]Training CobwebTree:  59%|    | 5879/10000 [02:25<01:54, 35.87it/s]Training CobwebTree:  59%|    | 5883/10000 [02:25<02:00, 34.12it/s]Training CobwebTree:  59%|    | 5887/10000 [02:25<02:01, 33.79it/s]Training CobwebTree:  59%|    | 5892/10000 [02:25<01:56, 35.32it/s]Training CobwebTree:  59%|    | 5897/10000 [02:25<01:52, 36.36it/s]Training CobwebTree:  59%|    | 5901/10000 [02:25<01:53, 36.21it/s]Training CobwebTree:  59%|    | 5906/10000 [02:25<01:49, 37.40it/s]Training CobwebTree:  59%|    | 5910/10000 [02:26<01:51, 36.53it/s]Training CobwebTree:  59%|    | 5914/10000 [02:26<01:57, 34.88it/s]Training CobwebTree:  59%|    | 5919/10000 [02:26<01:53, 35.92it/s]Training CobwebTree:  59%|    | 5923/10000 [02:26<01:57, 34.66it/s]Training CobwebTree:  59%|    | 5927/10000 [02:26<01:56, 34.87it/s]Training CobwebTree:  59%|    | 5932/10000 [02:26<01:48, 37.66it/s]Training CobwebTree:  59%|    | 5937/10000 [02:26<01:48, 37.29it/s]Training CobwebTree:  59%|    | 5941/10000 [02:26<01:53, 35.82it/s]Training CobwebTree:  59%|    | 5945/10000 [02:27<01:51, 36.49it/s]Training CobwebTree:  59%|    | 5949/10000 [02:27<01:49, 36.88it/s]Training CobwebTree:  60%|    | 5953/10000 [02:27<01:49, 36.96it/s]Training CobwebTree:  60%|    | 5957/10000 [02:27<01:48, 37.26it/s]Training CobwebTree:  60%|    | 5961/10000 [02:27<01:50, 36.41it/s]Training CobwebTree:  60%|    | 5965/10000 [02:27<01:52, 35.89it/s]Training CobwebTree:  60%|    | 5969/10000 [02:27<01:53, 35.52it/s]Training CobwebTree:  60%|    | 5973/10000 [02:27<01:53, 35.42it/s]Training CobwebTree:  60%|    | 5977/10000 [02:27<01:55, 34.97it/s]Training CobwebTree:  60%|    | 5981/10000 [02:28<01:52, 35.79it/s]Training CobwebTree:  60%|    | 5985/10000 [02:28<01:56, 34.49it/s]Training CobwebTree:  60%|    | 5989/10000 [02:28<01:58, 33.75it/s]Training CobwebTree:  60%|    | 5993/10000 [02:28<01:54, 35.00it/s]Training CobwebTree:  60%|    | 5997/10000 [02:28<01:50, 36.32it/s]Training CobwebTree:  60%|    | 6001/10000 [02:28<01:50, 36.26it/s]Training CobwebTree:  60%|    | 6005/10000 [02:28<01:57, 33.92it/s]Training CobwebTree:  60%|    | 6009/10000 [02:28<01:53, 35.11it/s]Training CobwebTree:  60%|    | 6013/10000 [02:28<01:54, 34.97it/s]Training CobwebTree:  60%|    | 6017/10000 [02:29<01:52, 35.45it/s]Training CobwebTree:  60%|    | 6021/10000 [02:29<01:48, 36.51it/s]Training CobwebTree:  60%|    | 6025/10000 [02:29<01:46, 37.35it/s]Training CobwebTree:  60%|    | 6029/10000 [02:29<01:45, 37.59it/s]Training CobwebTree:  60%|    | 6033/10000 [02:29<01:53, 35.03it/s]Training CobwebTree:  60%|    | 6037/10000 [02:29<01:52, 35.12it/s]Training CobwebTree:  60%|    | 6041/10000 [02:29<01:53, 34.93it/s]Training CobwebTree:  60%|    | 6045/10000 [02:29<01:55, 34.17it/s]Training CobwebTree:  60%|    | 6049/10000 [02:29<01:51, 35.57it/s]Training CobwebTree:  61%|    | 6054/10000 [02:30<01:46, 37.06it/s]Training CobwebTree:  61%|    | 6058/10000 [02:30<01:46, 36.85it/s]Training CobwebTree:  61%|    | 6062/10000 [02:30<01:45, 37.19it/s]Training CobwebTree:  61%|    | 6066/10000 [02:30<01:45, 37.25it/s]Training CobwebTree:  61%|    | 6070/10000 [02:30<01:43, 37.82it/s]Training CobwebTree:  61%|    | 6074/10000 [02:30<01:47, 36.45it/s]Training CobwebTree:  61%|    | 6078/10000 [02:30<01:46, 36.93it/s]Training CobwebTree:  61%|    | 6082/10000 [02:30<01:51, 35.17it/s]Training CobwebTree:  61%|    | 6086/10000 [02:31<01:52, 34.81it/s]Training CobwebTree:  61%|    | 6090/10000 [02:31<01:49, 35.73it/s]Training CobwebTree:  61%|    | 6094/10000 [02:31<01:49, 35.65it/s]Training CobwebTree:  61%|    | 6098/10000 [02:31<01:48, 35.98it/s]Training CobwebTree:  61%|    | 6103/10000 [02:31<01:45, 36.88it/s]Training CobwebTree:  61%|    | 6107/10000 [02:31<01:47, 36.29it/s]Training CobwebTree:  61%|    | 6111/10000 [02:31<01:46, 36.59it/s]Training CobwebTree:  61%|    | 6115/10000 [02:31<01:48, 35.89it/s]Training CobwebTree:  61%|    | 6119/10000 [02:31<01:52, 34.45it/s]Training CobwebTree:  61%|    | 6123/10000 [02:32<01:51, 34.92it/s]Training CobwebTree:  61%|   | 6127/10000 [02:32<01:54, 33.68it/s]Training CobwebTree:  61%|   | 6131/10000 [02:32<01:51, 34.55it/s]Training CobwebTree:  61%|   | 6136/10000 [02:32<01:45, 36.46it/s]Training CobwebTree:  61%|   | 6140/10000 [02:32<01:50, 34.98it/s]Training CobwebTree:  61%|   | 6144/10000 [02:32<01:50, 34.87it/s]Training CobwebTree:  61%|   | 6148/10000 [02:32<01:54, 33.78it/s]Training CobwebTree:  62%|   | 6152/10000 [02:32<01:54, 33.67it/s]Training CobwebTree:  62%|   | 6156/10000 [02:33<01:50, 34.64it/s]Training CobwebTree:  62%|   | 6161/10000 [02:33<01:43, 36.93it/s]Training CobwebTree:  62%|   | 6165/10000 [02:33<01:46, 36.07it/s]Training CobwebTree:  62%|   | 6169/10000 [02:33<01:48, 35.18it/s]Training CobwebTree:  62%|   | 6173/10000 [02:33<01:45, 36.31it/s]Training CobwebTree:  62%|   | 6177/10000 [02:33<01:49, 34.93it/s]Training CobwebTree:  62%|   | 6181/10000 [02:33<01:48, 35.32it/s]Training CobwebTree:  62%|   | 6185/10000 [02:33<01:49, 34.70it/s]Training CobwebTree:  62%|   | 6189/10000 [02:33<01:47, 35.59it/s]Training CobwebTree:  62%|   | 6193/10000 [02:34<01:48, 34.94it/s]Training CobwebTree:  62%|   | 6197/10000 [02:34<01:49, 34.83it/s]Training CobwebTree:  62%|   | 6201/10000 [02:34<01:51, 33.97it/s]Training CobwebTree:  62%|   | 6205/10000 [02:34<01:52, 33.78it/s]Training CobwebTree:  62%|   | 6209/10000 [02:34<01:51, 33.96it/s]Training CobwebTree:  62%|   | 6213/10000 [02:34<01:51, 34.06it/s]Training CobwebTree:  62%|   | 6218/10000 [02:34<01:43, 36.52it/s]Training CobwebTree:  62%|   | 6222/10000 [02:34<01:42, 36.78it/s]Training CobwebTree:  62%|   | 6226/10000 [02:35<03:38, 17.24it/s]Training CobwebTree:  62%|   | 6230/10000 [02:35<03:04, 20.38it/s]Training CobwebTree:  62%|   | 6234/10000 [02:35<02:40, 23.41it/s]Training CobwebTree:  62%|   | 6238/10000 [02:35<02:21, 26.65it/s]Training CobwebTree:  62%|   | 6243/10000 [02:35<02:03, 30.37it/s]Training CobwebTree:  62%|   | 6247/10000 [02:35<01:57, 31.84it/s]Training CobwebTree:  63%|   | 6251/10000 [02:36<01:57, 31.99it/s]Training CobwebTree:  63%|   | 6256/10000 [02:36<01:48, 34.52it/s]Training CobwebTree:  63%|   | 6261/10000 [02:36<01:42, 36.52it/s]Training CobwebTree:  63%|   | 6265/10000 [02:36<01:43, 36.00it/s]Training CobwebTree:  63%|   | 6269/10000 [02:36<01:47, 34.65it/s]Training CobwebTree:  63%|   | 6273/10000 [02:36<01:45, 35.29it/s]Training CobwebTree:  63%|   | 6277/10000 [02:36<01:46, 34.88it/s]Training CobwebTree:  63%|   | 6281/10000 [02:36<01:43, 36.07it/s]Training CobwebTree:  63%|   | 6285/10000 [02:37<01:47, 34.54it/s]Training CobwebTree:  63%|   | 6289/10000 [02:37<01:47, 34.57it/s]Training CobwebTree:  63%|   | 6293/10000 [02:37<01:48, 34.28it/s]Training CobwebTree:  63%|   | 6297/10000 [02:37<01:46, 34.76it/s]Training CobwebTree:  63%|   | 6301/10000 [02:37<01:49, 33.82it/s]Training CobwebTree:  63%|   | 6305/10000 [02:37<01:49, 33.88it/s]Training CobwebTree:  63%|   | 6309/10000 [02:37<01:52, 32.83it/s]Training CobwebTree:  63%|   | 6313/10000 [02:37<01:48, 33.94it/s]Training CobwebTree:  63%|   | 6317/10000 [02:37<01:48, 33.88it/s]Training CobwebTree:  63%|   | 6321/10000 [02:38<01:48, 34.03it/s]Training CobwebTree:  63%|   | 6325/10000 [02:38<01:45, 34.75it/s]Training CobwebTree:  63%|   | 6329/10000 [02:38<01:41, 36.09it/s]Training CobwebTree:  63%|   | 6333/10000 [02:38<01:38, 37.06it/s]Training CobwebTree:  63%|   | 6338/10000 [02:38<01:36, 37.84it/s]Training CobwebTree:  63%|   | 6342/10000 [02:38<01:43, 35.50it/s]Training CobwebTree:  63%|   | 6346/10000 [02:38<01:45, 34.70it/s]Training CobwebTree:  64%|   | 6350/10000 [02:38<01:44, 34.96it/s]Training CobwebTree:  64%|   | 6354/10000 [02:39<01:49, 33.23it/s]Training CobwebTree:  64%|   | 6358/10000 [02:39<01:49, 33.35it/s]Training CobwebTree:  64%|   | 6362/10000 [02:39<01:48, 33.56it/s]Training CobwebTree:  64%|   | 6366/10000 [02:39<01:43, 35.21it/s]Training CobwebTree:  64%|   | 6370/10000 [02:39<01:45, 34.50it/s]Training CobwebTree:  64%|   | 6374/10000 [02:39<01:44, 34.80it/s]Training CobwebTree:  64%|   | 6379/10000 [02:39<01:39, 36.26it/s]Training CobwebTree:  64%|   | 6383/10000 [02:39<01:41, 35.77it/s]Training CobwebTree:  64%|   | 6387/10000 [02:39<01:41, 35.51it/s]Training CobwebTree:  64%|   | 6391/10000 [02:40<01:41, 35.66it/s]Training CobwebTree:  64%|   | 6395/10000 [02:40<01:39, 36.29it/s]Training CobwebTree:  64%|   | 6399/10000 [02:40<01:41, 35.52it/s]Training CobwebTree:  64%|   | 6403/10000 [02:40<01:40, 35.90it/s]Training CobwebTree:  64%|   | 6407/10000 [02:40<01:39, 35.99it/s]Training CobwebTree:  64%|   | 6411/10000 [02:40<01:42, 34.98it/s]Training CobwebTree:  64%|   | 6415/10000 [02:40<01:39, 36.04it/s]Training CobwebTree:  64%|   | 6420/10000 [02:40<01:34, 38.00it/s]Training CobwebTree:  64%|   | 6424/10000 [02:40<01:38, 36.35it/s]Training CobwebTree:  64%|   | 6428/10000 [02:41<01:41, 35.18it/s]Training CobwebTree:  64%|   | 6433/10000 [02:41<01:33, 38.01it/s]Training CobwebTree:  64%|   | 6437/10000 [02:41<01:38, 36.00it/s]Training CobwebTree:  64%|   | 6441/10000 [02:41<01:42, 34.75it/s]Training CobwebTree:  64%|   | 6446/10000 [02:41<01:37, 36.42it/s]Training CobwebTree:  64%|   | 6450/10000 [02:41<01:37, 36.26it/s]Training CobwebTree:  65%|   | 6454/10000 [02:41<01:42, 34.67it/s]Training CobwebTree:  65%|   | 6458/10000 [02:41<01:44, 33.83it/s]Training CobwebTree:  65%|   | 6462/10000 [02:42<01:44, 33.85it/s]Training CobwebTree:  65%|   | 6466/10000 [02:42<01:46, 33.07it/s]Training CobwebTree:  65%|   | 6470/10000 [02:42<01:46, 33.23it/s]Training CobwebTree:  65%|   | 6474/10000 [02:42<01:43, 34.06it/s]Training CobwebTree:  65%|   | 6478/10000 [02:42<01:43, 33.87it/s]Training CobwebTree:  65%|   | 6482/10000 [02:42<01:39, 35.39it/s]Training CobwebTree:  65%|   | 6486/10000 [02:42<01:39, 35.33it/s]Training CobwebTree:  65%|   | 6490/10000 [02:42<01:36, 36.48it/s]Training CobwebTree:  65%|   | 6494/10000 [02:42<01:36, 36.26it/s]Training CobwebTree:  65%|   | 6498/10000 [02:43<01:34, 36.95it/s]Training CobwebTree:  65%|   | 6503/10000 [02:43<01:30, 38.73it/s]Training CobwebTree:  65%|   | 6507/10000 [02:43<01:29, 39.01it/s]Training CobwebTree:  65%|   | 6512/10000 [02:43<01:27, 39.67it/s]Training CobwebTree:  65%|   | 6516/10000 [02:43<01:28, 39.19it/s]Training CobwebTree:  65%|   | 6520/10000 [02:43<01:30, 38.28it/s]Training CobwebTree:  65%|   | 6524/10000 [02:43<01:36, 36.16it/s]Training CobwebTree:  65%|   | 6528/10000 [02:43<01:35, 36.18it/s]Training CobwebTree:  65%|   | 6533/10000 [02:43<01:30, 38.35it/s]Training CobwebTree:  65%|   | 6537/10000 [02:44<01:33, 37.07it/s]Training CobwebTree:  65%|   | 6541/10000 [02:44<01:36, 35.99it/s]Training CobwebTree:  65%|   | 6545/10000 [02:44<01:35, 36.24it/s]Training CobwebTree:  65%|   | 6549/10000 [02:44<01:34, 36.43it/s]Training CobwebTree:  66%|   | 6553/10000 [02:44<01:39, 34.52it/s]Training CobwebTree:  66%|   | 6557/10000 [02:44<01:39, 34.77it/s]Training CobwebTree:  66%|   | 6561/10000 [02:44<01:39, 34.57it/s]Training CobwebTree:  66%|   | 6565/10000 [02:44<01:36, 35.63it/s]Training CobwebTree:  66%|   | 6569/10000 [02:45<01:37, 35.24it/s]Training CobwebTree:  66%|   | 6574/10000 [02:45<01:33, 36.64it/s]Training CobwebTree:  66%|   | 6578/10000 [02:45<01:35, 35.97it/s]Training CobwebTree:  66%|   | 6583/10000 [02:45<01:30, 37.59it/s]Training CobwebTree:  66%|   | 6587/10000 [02:45<01:30, 37.87it/s]Training CobwebTree:  66%|   | 6591/10000 [02:45<01:33, 36.46it/s]Training CobwebTree:  66%|   | 6595/10000 [02:45<01:31, 37.25it/s]Training CobwebTree:  66%|   | 6599/10000 [02:45<01:32, 36.97it/s]Training CobwebTree:  66%|   | 6603/10000 [02:45<01:31, 37.31it/s]Training CobwebTree:  66%|   | 6607/10000 [02:46<01:35, 35.46it/s]Training CobwebTree:  66%|   | 6611/10000 [02:46<01:35, 35.36it/s]Training CobwebTree:  66%|   | 6615/10000 [02:46<01:34, 35.84it/s]Training CobwebTree:  66%|   | 6620/10000 [02:46<01:29, 37.72it/s]Training CobwebTree:  66%|   | 6624/10000 [02:46<01:30, 37.32it/s]Training CobwebTree:  66%|   | 6628/10000 [02:46<01:29, 37.66it/s]Training CobwebTree:  66%|   | 6632/10000 [02:46<01:34, 35.55it/s]Training CobwebTree:  66%|   | 6636/10000 [02:46<01:37, 34.57it/s]Training CobwebTree:  66%|   | 6640/10000 [02:46<01:33, 35.94it/s]Training CobwebTree:  66%|   | 6644/10000 [02:47<01:33, 36.05it/s]Training CobwebTree:  66%|   | 6648/10000 [02:47<01:33, 36.02it/s]Training CobwebTree:  67%|   | 6652/10000 [02:47<01:34, 35.57it/s]Training CobwebTree:  67%|   | 6656/10000 [02:47<01:31, 36.36it/s]Training CobwebTree:  67%|   | 6661/10000 [02:47<01:30, 37.09it/s]Training CobwebTree:  67%|   | 6665/10000 [02:47<01:35, 35.00it/s]Training CobwebTree:  67%|   | 6670/10000 [02:47<01:32, 35.83it/s]Training CobwebTree:  67%|   | 6674/10000 [02:47<01:30, 36.87it/s]Training CobwebTree:  67%|   | 6678/10000 [02:48<01:36, 34.28it/s]Training CobwebTree:  67%|   | 6683/10000 [02:48<01:31, 36.15it/s]Training CobwebTree:  67%|   | 6687/10000 [02:48<01:31, 36.37it/s]Training CobwebTree:  67%|   | 6691/10000 [02:48<01:31, 36.05it/s]Training CobwebTree:  67%|   | 6695/10000 [02:48<01:30, 36.38it/s]Training CobwebTree:  67%|   | 6699/10000 [02:48<01:31, 35.99it/s]Training CobwebTree:  67%|   | 6703/10000 [02:48<01:31, 36.23it/s]Training CobwebTree:  67%|   | 6707/10000 [02:48<01:29, 36.84it/s]Training CobwebTree:  67%|   | 6711/10000 [02:48<01:28, 37.11it/s]Training CobwebTree:  67%|   | 6715/10000 [02:49<01:29, 36.66it/s]Training CobwebTree:  67%|   | 6719/10000 [02:49<01:30, 36.11it/s]Training CobwebTree:  67%|   | 6723/10000 [02:49<01:31, 35.74it/s]Training CobwebTree:  67%|   | 6727/10000 [02:49<01:31, 35.85it/s]Training CobwebTree:  67%|   | 6731/10000 [02:49<01:35, 34.32it/s]Training CobwebTree:  67%|   | 6735/10000 [02:49<01:33, 34.99it/s]Training CobwebTree:  67%|   | 6739/10000 [02:49<01:34, 34.44it/s]Training CobwebTree:  67%|   | 6743/10000 [02:49<01:30, 35.93it/s]Training CobwebTree:  67%|   | 6747/10000 [02:49<01:29, 36.49it/s]Training CobwebTree:  68%|   | 6751/10000 [02:50<01:34, 34.49it/s]Training CobwebTree:  68%|   | 6755/10000 [02:50<01:31, 35.59it/s]Training CobwebTree:  68%|   | 6759/10000 [02:50<01:31, 35.26it/s]Training CobwebTree:  68%|   | 6763/10000 [02:50<01:31, 35.30it/s]Training CobwebTree:  68%|   | 6767/10000 [02:50<01:37, 33.03it/s]Training CobwebTree:  68%|   | 6771/10000 [02:50<01:38, 32.76it/s]Training CobwebTree:  68%|   | 6775/10000 [02:50<01:33, 34.58it/s]Training CobwebTree:  68%|   | 6779/10000 [02:50<01:33, 34.37it/s]Training CobwebTree:  68%|   | 6784/10000 [02:50<01:29, 35.99it/s]Training CobwebTree:  68%|   | 6788/10000 [02:51<01:30, 35.42it/s]Training CobwebTree:  68%|   | 6793/10000 [02:51<01:28, 36.32it/s]Training CobwebTree:  68%|   | 6798/10000 [02:51<01:24, 37.75it/s]Training CobwebTree:  68%|   | 6802/10000 [02:51<01:26, 37.17it/s]Training CobwebTree:  68%|   | 6806/10000 [02:51<01:29, 35.81it/s]Training CobwebTree:  68%|   | 6810/10000 [02:51<01:28, 36.10it/s]Training CobwebTree:  68%|   | 6814/10000 [02:51<01:26, 36.73it/s]Training CobwebTree:  68%|   | 6818/10000 [02:51<01:24, 37.49it/s]Training CobwebTree:  68%|   | 6822/10000 [02:52<01:25, 37.13it/s]Training CobwebTree:  68%|   | 6826/10000 [02:52<01:26, 36.84it/s]Training CobwebTree:  68%|   | 6830/10000 [02:52<01:29, 35.27it/s]Training CobwebTree:  68%|   | 6834/10000 [02:52<01:28, 35.82it/s]Training CobwebTree:  68%|   | 6838/10000 [02:52<01:26, 36.38it/s]Training CobwebTree:  68%|   | 6842/10000 [02:52<01:30, 35.04it/s]Training CobwebTree:  68%|   | 6846/10000 [02:52<01:32, 34.19it/s]Training CobwebTree:  68%|   | 6850/10000 [02:52<01:29, 35.31it/s]Training CobwebTree:  69%|   | 6854/10000 [02:52<01:28, 35.63it/s]Training CobwebTree:  69%|   | 6858/10000 [02:53<01:31, 34.34it/s]Training CobwebTree:  69%|   | 6862/10000 [02:53<01:28, 35.52it/s]Training CobwebTree:  69%|   | 6866/10000 [02:53<01:30, 34.75it/s]Training CobwebTree:  69%|   | 6870/10000 [02:53<01:30, 34.72it/s]Training CobwebTree:  69%|   | 6874/10000 [02:53<01:26, 35.96it/s]Training CobwebTree:  69%|   | 6878/10000 [02:53<01:24, 36.92it/s]Training CobwebTree:  69%|   | 6882/10000 [02:53<01:25, 36.50it/s]Training CobwebTree:  69%|   | 6886/10000 [02:53<01:27, 35.61it/s]Training CobwebTree:  69%|   | 6890/10000 [02:53<01:28, 35.20it/s]Training CobwebTree:  69%|   | 6894/10000 [02:54<01:29, 34.54it/s]Training CobwebTree:  69%|   | 6898/10000 [02:54<01:26, 35.83it/s]Training CobwebTree:  69%|   | 6902/10000 [02:54<01:25, 36.19it/s]Training CobwebTree:  69%|   | 6906/10000 [02:54<01:23, 37.13it/s]Training CobwebTree:  69%|   | 6910/10000 [02:54<01:25, 36.29it/s]Training CobwebTree:  69%|   | 6914/10000 [02:54<01:26, 35.71it/s]Training CobwebTree:  69%|   | 6918/10000 [02:54<01:27, 35.17it/s]Training CobwebTree:  69%|   | 6922/10000 [02:54<01:27, 35.19it/s]Training CobwebTree:  69%|   | 6926/10000 [02:54<01:25, 36.16it/s]Training CobwebTree:  69%|   | 6930/10000 [02:55<01:25, 35.96it/s]Training CobwebTree:  69%|   | 6934/10000 [02:55<01:25, 35.95it/s]Training CobwebTree:  69%|   | 6938/10000 [02:55<01:24, 36.35it/s]Training CobwebTree:  69%|   | 6943/10000 [02:55<01:21, 37.50it/s]Training CobwebTree:  69%|   | 6947/10000 [02:55<01:22, 36.96it/s]Training CobwebTree:  70%|   | 6952/10000 [02:55<01:19, 38.20it/s]Training CobwebTree:  70%|   | 6956/10000 [02:55<01:25, 35.74it/s]Training CobwebTree:  70%|   | 6960/10000 [02:55<01:26, 35.26it/s]Training CobwebTree:  70%|   | 6964/10000 [02:56<01:27, 34.77it/s]Training CobwebTree:  70%|   | 6968/10000 [02:56<01:25, 35.41it/s]Training CobwebTree:  70%|   | 6972/10000 [02:56<01:25, 35.34it/s]Training CobwebTree:  70%|   | 6976/10000 [02:56<01:27, 34.61it/s]Training CobwebTree:  70%|   | 6980/10000 [02:56<01:26, 34.96it/s]Training CobwebTree:  70%|   | 6984/10000 [02:56<01:29, 33.63it/s]Training CobwebTree:  70%|   | 6988/10000 [02:56<01:30, 33.25it/s]Training CobwebTree:  70%|   | 6992/10000 [02:56<01:29, 33.71it/s]Training CobwebTree:  70%|   | 6996/10000 [02:56<01:29, 33.39it/s]Training CobwebTree:  70%|   | 7000/10000 [02:57<01:26, 34.70it/s]Training CobwebTree:  70%|   | 7004/10000 [02:57<01:26, 34.48it/s]Training CobwebTree:  70%|   | 7008/10000 [02:57<01:26, 34.51it/s]Training CobwebTree:  70%|   | 7012/10000 [02:57<01:30, 33.17it/s]Training CobwebTree:  70%|   | 7016/10000 [02:57<01:27, 34.05it/s]Training CobwebTree:  70%|   | 7020/10000 [02:57<01:24, 35.09it/s]Training CobwebTree:  70%|   | 7024/10000 [02:57<01:28, 33.51it/s]Training CobwebTree:  70%|   | 7028/10000 [02:57<01:25, 34.70it/s]Training CobwebTree:  70%|   | 7033/10000 [02:57<01:19, 37.27it/s]Training CobwebTree:  70%|   | 7037/10000 [02:58<01:20, 36.62it/s]Training CobwebTree:  70%|   | 7041/10000 [02:58<01:22, 35.95it/s]Training CobwebTree:  70%|   | 7045/10000 [02:58<01:24, 34.78it/s]Training CobwebTree:  70%|   | 7049/10000 [02:58<01:26, 34.06it/s]Training CobwebTree:  71%|   | 7053/10000 [02:58<01:26, 33.96it/s]Training CobwebTree:  71%|   | 7057/10000 [02:58<01:26, 34.14it/s]Training CobwebTree:  71%|   | 7061/10000 [02:58<01:26, 33.81it/s]Training CobwebTree:  71%|   | 7065/10000 [02:58<01:26, 33.85it/s]Training CobwebTree:  71%|   | 7070/10000 [02:59<01:20, 36.61it/s]Training CobwebTree:  71%|   | 7074/10000 [02:59<01:25, 34.39it/s]Training CobwebTree:  71%|   | 7078/10000 [02:59<01:23, 35.05it/s]Training CobwebTree:  71%|   | 7082/10000 [02:59<01:23, 34.93it/s]Training CobwebTree:  71%|   | 7086/10000 [02:59<01:20, 36.17it/s]Training CobwebTree:  71%|   | 7090/10000 [02:59<01:21, 35.79it/s]Training CobwebTree:  71%|   | 7094/10000 [02:59<01:22, 35.03it/s]Training CobwebTree:  71%|   | 7098/10000 [02:59<01:25, 33.89it/s]Training CobwebTree:  71%|   | 7102/10000 [02:59<01:24, 34.21it/s]Training CobwebTree:  71%|   | 7106/10000 [03:00<01:23, 34.47it/s]Training CobwebTree:  71%|   | 7110/10000 [03:00<01:23, 34.81it/s]Training CobwebTree:  71%|   | 7114/10000 [03:00<01:21, 35.42it/s]Training CobwebTree:  71%|   | 7118/10000 [03:00<01:24, 34.25it/s]Training CobwebTree:  71%|   | 7122/10000 [03:00<01:25, 33.66it/s]Training CobwebTree:  71%|  | 7126/10000 [03:00<01:23, 34.60it/s]Training CobwebTree:  71%|  | 7130/10000 [03:00<01:23, 34.46it/s]Training CobwebTree:  71%|  | 7134/10000 [03:00<01:25, 33.45it/s]Training CobwebTree:  71%|  | 7138/10000 [03:01<01:25, 33.31it/s]Training CobwebTree:  71%|  | 7142/10000 [03:01<01:23, 34.37it/s]Training CobwebTree:  71%|  | 7146/10000 [03:01<01:20, 35.30it/s]Training CobwebTree:  72%|  | 7150/10000 [03:01<01:19, 35.81it/s]Training CobwebTree:  72%|  | 7154/10000 [03:01<01:21, 35.02it/s]Training CobwebTree:  72%|  | 7158/10000 [03:01<01:21, 34.87it/s]Training CobwebTree:  72%|  | 7162/10000 [03:01<01:20, 35.25it/s]Training CobwebTree:  72%|  | 7166/10000 [03:01<01:18, 36.11it/s]Training CobwebTree:  72%|  | 7170/10000 [03:01<01:21, 34.88it/s]Training CobwebTree:  72%|  | 7174/10000 [03:02<01:23, 33.81it/s]Training CobwebTree:  72%|  | 7178/10000 [03:02<01:24, 33.38it/s]Training CobwebTree:  72%|  | 7182/10000 [03:02<01:26, 32.70it/s]Training CobwebTree:  72%|  | 7186/10000 [03:02<01:23, 33.84it/s]Training CobwebTree:  72%|  | 7190/10000 [03:02<01:24, 33.37it/s]Training CobwebTree:  72%|  | 7194/10000 [03:02<01:24, 33.28it/s]Training CobwebTree:  72%|  | 7198/10000 [03:02<01:22, 34.11it/s]Training CobwebTree:  72%|  | 7202/10000 [03:02<01:18, 35.56it/s]Training CobwebTree:  72%|  | 7206/10000 [03:02<01:18, 35.38it/s]Training CobwebTree:  72%|  | 7210/10000 [03:03<01:18, 35.60it/s]Training CobwebTree:  72%|  | 7214/10000 [03:03<01:17, 36.08it/s]Training CobwebTree:  72%|  | 7218/10000 [03:03<01:17, 35.89it/s]Training CobwebTree:  72%|  | 7222/10000 [03:03<01:15, 36.82it/s]Training CobwebTree:  72%|  | 7226/10000 [03:03<01:16, 36.28it/s]Training CobwebTree:  72%|  | 7230/10000 [03:03<01:16, 36.04it/s]Training CobwebTree:  72%|  | 7234/10000 [03:03<01:15, 36.51it/s]Training CobwebTree:  72%|  | 7238/10000 [03:03<01:17, 35.63it/s]Training CobwebTree:  72%|  | 7242/10000 [03:04<01:18, 35.21it/s]Training CobwebTree:  72%|  | 7246/10000 [03:04<01:18, 34.86it/s]Training CobwebTree:  72%|  | 7250/10000 [03:04<01:18, 34.90it/s]Training CobwebTree:  73%|  | 7254/10000 [03:04<01:19, 34.46it/s]Training CobwebTree:  73%|  | 7258/10000 [03:04<01:20, 34.26it/s]Training CobwebTree:  73%|  | 7262/10000 [03:04<01:18, 35.05it/s]Training CobwebTree:  73%|  | 7266/10000 [03:04<01:17, 35.19it/s]Training CobwebTree:  73%|  | 7270/10000 [03:04<01:19, 34.40it/s]Training CobwebTree:  73%|  | 7274/10000 [03:04<01:17, 35.28it/s]Training CobwebTree:  73%|  | 7278/10000 [03:05<01:15, 35.82it/s]Training CobwebTree:  73%|  | 7282/10000 [03:05<01:14, 36.37it/s]Training CobwebTree:  73%|  | 7286/10000 [03:05<01:19, 34.06it/s]Training CobwebTree:  73%|  | 7290/10000 [03:05<01:21, 33.33it/s]Training CobwebTree:  73%|  | 7294/10000 [03:05<01:17, 34.75it/s]Training CobwebTree:  73%|  | 7298/10000 [03:05<01:18, 34.45it/s]Training CobwebTree:  73%|  | 7302/10000 [03:05<01:15, 35.61it/s]Training CobwebTree:  73%|  | 7306/10000 [03:05<01:18, 34.40it/s]Training CobwebTree:  73%|  | 7310/10000 [03:05<01:19, 33.91it/s]Training CobwebTree:  73%|  | 7314/10000 [03:06<01:15, 35.36it/s]Training CobwebTree:  73%|  | 7318/10000 [03:06<01:16, 34.98it/s]Training CobwebTree:  73%|  | 7322/10000 [03:06<01:17, 34.59it/s]Training CobwebTree:  73%|  | 7326/10000 [03:06<01:17, 34.56it/s]Training CobwebTree:  73%|  | 7330/10000 [03:06<01:18, 34.20it/s]Training CobwebTree:  73%|  | 7334/10000 [03:06<01:17, 34.30it/s]Training CobwebTree:  73%|  | 7338/10000 [03:06<01:17, 34.55it/s]Training CobwebTree:  73%|  | 7342/10000 [03:06<01:16, 34.78it/s]Training CobwebTree:  73%|  | 7346/10000 [03:07<01:17, 34.32it/s]Training CobwebTree:  74%|  | 7350/10000 [03:07<01:14, 35.68it/s]Training CobwebTree:  74%|  | 7354/10000 [03:07<01:12, 36.33it/s]Training CobwebTree:  74%|  | 7358/10000 [03:07<01:14, 35.35it/s]Training CobwebTree:  74%|  | 7362/10000 [03:07<01:13, 35.74it/s]Training CobwebTree:  74%|  | 7366/10000 [03:07<01:15, 34.89it/s]Training CobwebTree:  74%|  | 7371/10000 [03:07<01:14, 35.53it/s]Training CobwebTree:  74%|  | 7375/10000 [03:07<01:13, 35.93it/s]Training CobwebTree:  74%|  | 7379/10000 [03:07<01:11, 36.79it/s]Training CobwebTree:  74%|  | 7383/10000 [03:08<01:11, 36.83it/s]Training CobwebTree:  74%|  | 7387/10000 [03:08<01:13, 35.45it/s]Training CobwebTree:  74%|  | 7391/10000 [03:08<01:16, 34.19it/s]Training CobwebTree:  74%|  | 7395/10000 [03:08<01:13, 35.42it/s]Training CobwebTree:  74%|  | 7399/10000 [03:08<01:14, 34.89it/s]Training CobwebTree:  74%|  | 7403/10000 [03:08<01:12, 35.65it/s]Training CobwebTree:  74%|  | 7407/10000 [03:08<01:13, 35.45it/s]Training CobwebTree:  74%|  | 7412/10000 [03:08<01:09, 37.39it/s]Training CobwebTree:  74%|  | 7416/10000 [03:08<01:10, 36.47it/s]Training CobwebTree:  74%|  | 7420/10000 [03:09<01:09, 37.07it/s]Training CobwebTree:  74%|  | 7424/10000 [03:09<01:09, 37.21it/s]Training CobwebTree:  74%|  | 7428/10000 [03:09<01:13, 35.19it/s]Training CobwebTree:  74%|  | 7432/10000 [03:09<01:12, 35.44it/s]Training CobwebTree:  74%|  | 7436/10000 [03:09<01:12, 35.46it/s]Training CobwebTree:  74%|  | 7440/10000 [03:09<01:16, 33.61it/s]Training CobwebTree:  74%|  | 7444/10000 [03:09<01:13, 34.93it/s]Training CobwebTree:  74%|  | 7448/10000 [03:09<01:10, 35.97it/s]Training CobwebTree:  75%|  | 7452/10000 [03:09<01:13, 34.73it/s]Training CobwebTree:  75%|  | 7456/10000 [03:10<01:14, 34.10it/s]Training CobwebTree:  75%|  | 7460/10000 [03:10<01:14, 33.98it/s]Training CobwebTree:  75%|  | 7464/10000 [03:10<01:14, 34.04it/s]Training CobwebTree:  75%|  | 7469/10000 [03:10<01:10, 35.65it/s]Training CobwebTree:  75%|  | 7473/10000 [03:10<01:11, 35.20it/s]Training CobwebTree:  75%|  | 7478/10000 [03:10<01:07, 37.35it/s]Training CobwebTree:  75%|  | 7482/10000 [03:10<01:10, 35.50it/s]Training CobwebTree:  75%|  | 7486/10000 [03:10<01:11, 35.22it/s]Training CobwebTree:  75%|  | 7490/10000 [03:11<01:09, 36.00it/s]Training CobwebTree:  75%|  | 7494/10000 [03:11<01:11, 34.84it/s]Training CobwebTree:  75%|  | 7498/10000 [03:11<01:14, 33.67it/s]Training CobwebTree:  75%|  | 7502/10000 [03:11<01:13, 33.95it/s]Training CobwebTree:  75%|  | 7506/10000 [03:11<01:11, 35.02it/s]Training CobwebTree:  75%|  | 7510/10000 [03:11<01:10, 35.50it/s]Training CobwebTree:  75%|  | 7514/10000 [03:11<01:09, 36.00it/s]Training CobwebTree:  75%|  | 7518/10000 [03:11<01:08, 36.40it/s]Training CobwebTree:  75%|  | 7522/10000 [03:11<01:06, 37.01it/s]Training CobwebTree:  75%|  | 7527/10000 [03:12<01:02, 39.75it/s]Training CobwebTree:  75%|  | 7531/10000 [03:12<01:06, 36.98it/s]Training CobwebTree:  75%|  | 7535/10000 [03:12<01:10, 34.93it/s]Training CobwebTree:  75%|  | 7539/10000 [03:12<01:09, 35.50it/s]Training CobwebTree:  75%|  | 7543/10000 [03:12<01:08, 35.80it/s]Training CobwebTree:  75%|  | 7547/10000 [03:12<01:08, 36.00it/s]Training CobwebTree:  76%|  | 7551/10000 [03:12<01:12, 33.67it/s]Training CobwebTree:  76%|  | 7555/10000 [03:12<01:14, 33.03it/s]Training CobwebTree:  76%|  | 7559/10000 [03:13<01:13, 33.32it/s]Training CobwebTree:  76%|  | 7563/10000 [03:13<01:13, 33.25it/s]Training CobwebTree:  76%|  | 7567/10000 [03:13<01:10, 34.59it/s]Training CobwebTree:  76%|  | 7571/10000 [03:13<01:11, 34.13it/s]Training CobwebTree:  76%|  | 7575/10000 [03:13<01:09, 34.82it/s]Training CobwebTree:  76%|  | 7579/10000 [03:13<01:08, 35.47it/s]Training CobwebTree:  76%|  | 7583/10000 [03:13<01:06, 36.46it/s]Training CobwebTree:  76%|  | 7587/10000 [03:13<01:08, 35.32it/s]Training CobwebTree:  76%|  | 7591/10000 [03:13<01:09, 34.82it/s]Training CobwebTree:  76%|  | 7595/10000 [03:14<01:11, 33.83it/s]Training CobwebTree:  76%|  | 7599/10000 [03:14<01:11, 33.53it/s]Training CobwebTree:  76%|  | 7603/10000 [03:14<01:08, 34.77it/s]Training CobwebTree:  76%|  | 7607/10000 [03:14<01:06, 35.72it/s]Training CobwebTree:  76%|  | 7611/10000 [03:14<01:12, 32.79it/s]Training CobwebTree:  76%|  | 7615/10000 [03:14<01:11, 33.41it/s]Training CobwebTree:  76%|  | 7619/10000 [03:14<01:09, 34.33it/s]Training CobwebTree:  76%|  | 7623/10000 [03:14<01:07, 35.07it/s]Training CobwebTree:  76%|  | 7627/10000 [03:14<01:09, 34.14it/s]Training CobwebTree:  76%|  | 7631/10000 [03:15<01:10, 33.64it/s]Training CobwebTree:  76%|  | 7636/10000 [03:15<01:06, 35.50it/s]Training CobwebTree:  76%|  | 7640/10000 [03:15<01:05, 35.97it/s]Training CobwebTree:  76%|  | 7644/10000 [03:15<01:04, 36.49it/s]Training CobwebTree:  76%|  | 7648/10000 [03:15<01:04, 36.31it/s]Training CobwebTree:  77%|  | 7652/10000 [03:15<01:06, 35.37it/s]Training CobwebTree:  77%|  | 7656/10000 [03:15<01:06, 35.28it/s]Training CobwebTree:  77%|  | 7660/10000 [03:15<01:08, 33.96it/s]Training CobwebTree:  77%|  | 7664/10000 [03:16<01:08, 34.26it/s]Training CobwebTree:  77%|  | 7668/10000 [03:16<01:08, 34.10it/s]Training CobwebTree:  77%|  | 7672/10000 [03:16<01:09, 33.62it/s]Training CobwebTree:  77%|  | 7676/10000 [03:16<01:11, 32.51it/s]Training CobwebTree:  77%|  | 7680/10000 [03:16<01:10, 32.84it/s]Training CobwebTree:  77%|  | 7685/10000 [03:16<01:06, 34.62it/s]Training CobwebTree:  77%|  | 7689/10000 [03:16<01:04, 35.80it/s]Training CobwebTree:  77%|  | 7693/10000 [03:16<01:05, 35.39it/s]Training CobwebTree:  77%|  | 7697/10000 [03:16<01:04, 35.57it/s]Training CobwebTree:  77%|  | 7702/10000 [03:17<01:00, 37.90it/s]Training CobwebTree:  77%|  | 7706/10000 [03:17<01:00, 37.81it/s]Training CobwebTree:  77%|  | 7710/10000 [03:17<01:02, 36.76it/s]Training CobwebTree:  77%|  | 7714/10000 [03:17<01:06, 34.52it/s]Training CobwebTree:  77%|  | 7718/10000 [03:17<01:06, 34.44it/s]Training CobwebTree:  77%|  | 7722/10000 [03:17<01:04, 35.33it/s]Training CobwebTree:  77%|  | 7726/10000 [03:17<01:06, 34.26it/s]Training CobwebTree:  77%|  | 7730/10000 [03:17<01:06, 33.98it/s]Training CobwebTree:  77%|  | 7735/10000 [03:18<01:02, 36.27it/s]Training CobwebTree:  77%|  | 7740/10000 [03:18<01:00, 37.17it/s]Training CobwebTree:  77%|  | 7744/10000 [03:18<01:00, 37.27it/s]Training CobwebTree:  77%|  | 7748/10000 [03:18<01:01, 36.68it/s]Training CobwebTree:  78%|  | 7752/10000 [03:18<01:03, 35.64it/s]Training CobwebTree:  78%|  | 7756/10000 [03:18<01:06, 33.95it/s]Training CobwebTree:  78%|  | 7760/10000 [03:18<01:05, 34.24it/s]Training CobwebTree:  78%|  | 7764/10000 [03:18<01:03, 35.33it/s]Training CobwebTree:  78%|  | 7768/10000 [03:18<01:04, 34.55it/s]Training CobwebTree:  78%|  | 7773/10000 [03:19<01:03, 35.30it/s]Training CobwebTree:  78%|  | 7777/10000 [03:19<01:06, 33.46it/s]Training CobwebTree:  78%|  | 7781/10000 [03:19<01:03, 34.99it/s]Training CobwebTree:  78%|  | 7785/10000 [03:19<01:01, 36.22it/s]Training CobwebTree:  78%|  | 7789/10000 [03:19<01:02, 35.65it/s]Training CobwebTree:  78%|  | 7793/10000 [03:19<01:01, 36.15it/s]Training CobwebTree:  78%|  | 7797/10000 [03:19<00:59, 36.73it/s]Training CobwebTree:  78%|  | 7801/10000 [03:19<01:00, 36.35it/s]Training CobwebTree:  78%|  | 7805/10000 [03:20<01:00, 36.37it/s]Training CobwebTree:  78%|  | 7809/10000 [03:20<00:59, 36.54it/s]Training CobwebTree:  78%|  | 7813/10000 [03:20<00:59, 37.03it/s]Training CobwebTree:  78%|  | 7817/10000 [03:20<01:01, 35.57it/s]Training CobwebTree:  78%|  | 7821/10000 [03:20<01:00, 35.81it/s]Training CobwebTree:  78%|  | 7825/10000 [03:20<01:02, 34.52it/s]Training CobwebTree:  78%|  | 7829/10000 [03:20<01:01, 35.38it/s]Training CobwebTree:  78%|  | 7833/10000 [03:20<01:01, 35.00it/s]Training CobwebTree:  78%|  | 7837/10000 [03:20<01:01, 35.05it/s]Training CobwebTree:  78%|  | 7841/10000 [03:21<01:01, 35.17it/s]Training CobwebTree:  78%|  | 7845/10000 [03:21<01:00, 35.53it/s]Training CobwebTree:  78%|  | 7849/10000 [03:21<01:00, 35.29it/s]Training CobwebTree:  79%|  | 7853/10000 [03:21<01:01, 34.92it/s]Training CobwebTree:  79%|  | 7857/10000 [03:21<01:01, 35.11it/s]Training CobwebTree:  79%|  | 7861/10000 [03:21<01:02, 34.41it/s]Training CobwebTree:  79%|  | 7865/10000 [03:21<01:02, 34.34it/s]Training CobwebTree:  79%|  | 7869/10000 [03:21<01:00, 35.02it/s]Training CobwebTree:  79%|  | 7873/10000 [03:21<01:02, 34.00it/s]Training CobwebTree:  79%|  | 7877/10000 [03:22<01:00, 35.36it/s]Training CobwebTree:  79%|  | 7881/10000 [03:22<01:01, 34.36it/s]Training CobwebTree:  79%|  | 7885/10000 [03:22<01:01, 34.50it/s]Training CobwebTree:  79%|  | 7889/10000 [03:22<01:00, 34.86it/s]Training CobwebTree:  79%|  | 7893/10000 [03:22<00:59, 35.39it/s]Training CobwebTree:  79%|  | 7897/10000 [03:22<01:01, 34.31it/s]Training CobwebTree:  79%|  | 7901/10000 [03:22<01:01, 33.89it/s]Training CobwebTree:  79%|  | 7905/10000 [03:22<00:59, 34.97it/s]Training CobwebTree:  79%|  | 7909/10000 [03:22<00:58, 35.89it/s]Training CobwebTree:  79%|  | 7913/10000 [03:23<01:00, 34.28it/s]Training CobwebTree:  79%|  | 7917/10000 [03:23<00:58, 35.63it/s]Training CobwebTree:  79%|  | 7921/10000 [03:23<01:00, 34.15it/s]Training CobwebTree:  79%|  | 7925/10000 [03:23<01:02, 33.45it/s]Training CobwebTree:  79%|  | 7929/10000 [03:23<01:05, 31.57it/s]Training CobwebTree:  79%|  | 7933/10000 [03:23<01:04, 32.18it/s]Training CobwebTree:  79%|  | 7937/10000 [03:23<01:01, 33.43it/s]Training CobwebTree:  79%|  | 7941/10000 [03:23<01:01, 33.50it/s]Training CobwebTree:  79%|  | 7945/10000 [03:24<01:02, 32.69it/s]Training CobwebTree:  79%|  | 7949/10000 [03:24<01:00, 33.98it/s]Training CobwebTree:  80%|  | 7953/10000 [03:24<01:00, 33.98it/s]Training CobwebTree:  80%|  | 7957/10000 [03:24<01:02, 32.75it/s]Training CobwebTree:  80%|  | 7961/10000 [03:24<01:00, 33.73it/s]Training CobwebTree:  80%|  | 7965/10000 [03:24<01:00, 33.77it/s]Training CobwebTree:  80%|  | 7969/10000 [03:24<00:59, 34.37it/s]Training CobwebTree:  80%|  | 7974/10000 [03:24<00:56, 35.57it/s]Training CobwebTree:  80%|  | 7978/10000 [03:25<00:57, 35.18it/s]Training CobwebTree:  80%|  | 7982/10000 [03:25<00:58, 34.27it/s]Training CobwebTree:  80%|  | 7986/10000 [03:25<00:57, 35.05it/s]Training CobwebTree:  80%|  | 7990/10000 [03:25<00:58, 34.31it/s]Training CobwebTree:  80%|  | 7994/10000 [03:25<00:58, 34.53it/s]Training CobwebTree:  80%|  | 7998/10000 [03:25<00:58, 34.45it/s]Training CobwebTree:  80%|  | 8002/10000 [03:25<00:58, 34.01it/s]Training CobwebTree:  80%|  | 8006/10000 [03:25<00:58, 33.98it/s]Training CobwebTree:  80%|  | 8010/10000 [03:26<01:03, 31.24it/s]Training CobwebTree:  80%|  | 8014/10000 [03:26<01:02, 31.72it/s]Training CobwebTree:  80%|  | 8018/10000 [03:26<01:01, 32.02it/s]Training CobwebTree:  80%|  | 8022/10000 [03:26<01:01, 32.08it/s]Training CobwebTree:  80%|  | 8026/10000 [03:26<00:58, 33.64it/s]Training CobwebTree:  80%|  | 8030/10000 [03:26<00:58, 33.90it/s]Training CobwebTree:  80%|  | 8034/10000 [03:26<00:55, 35.22it/s]Training CobwebTree:  80%|  | 8038/10000 [03:26<00:56, 35.03it/s]Training CobwebTree:  80%|  | 8043/10000 [03:26<00:54, 35.81it/s]Training CobwebTree:  80%|  | 8047/10000 [03:27<00:55, 34.98it/s]Training CobwebTree:  81%|  | 8051/10000 [03:27<00:56, 34.52it/s]Training CobwebTree:  81%|  | 8055/10000 [03:27<00:54, 35.63it/s]Training CobwebTree:  81%|  | 8059/10000 [03:27<00:54, 35.67it/s]Training CobwebTree:  81%|  | 8064/10000 [03:27<00:51, 37.27it/s]Training CobwebTree:  81%|  | 8068/10000 [03:27<00:54, 35.48it/s]Training CobwebTree:  81%|  | 8072/10000 [03:27<00:55, 34.75it/s]Training CobwebTree:  81%|  | 8076/10000 [03:27<00:55, 34.36it/s]Training CobwebTree:  81%|  | 8080/10000 [03:28<00:56, 34.08it/s]Training CobwebTree:  81%|  | 8084/10000 [03:28<00:56, 34.06it/s]Training CobwebTree:  81%|  | 8088/10000 [03:28<00:54, 35.40it/s]Training CobwebTree:  81%|  | 8092/10000 [03:28<00:53, 35.70it/s]Training CobwebTree:  81%|  | 8097/10000 [03:28<00:51, 37.25it/s]Training CobwebTree:  81%|  | 8101/10000 [03:28<00:50, 37.30it/s]Training CobwebTree:  81%|  | 8105/10000 [03:28<00:50, 37.62it/s]Training CobwebTree:  81%|  | 8110/10000 [03:28<00:48, 39.06it/s]Training CobwebTree:  81%|  | 8114/10000 [03:28<00:50, 37.17it/s]Training CobwebTree:  81%|  | 8118/10000 [03:29<00:52, 35.63it/s]Training CobwebTree:  81%|  | 8122/10000 [03:29<00:53, 35.42it/s]Training CobwebTree:  81%| | 8126/10000 [03:29<00:52, 35.38it/s]Training CobwebTree:  81%| | 8130/10000 [03:29<00:51, 36.08it/s]Training CobwebTree:  81%| | 8134/10000 [03:29<00:54, 33.94it/s]Training CobwebTree:  81%| | 8138/10000 [03:29<00:53, 34.71it/s]Training CobwebTree:  81%| | 8142/10000 [03:29<00:54, 33.90it/s]Training CobwebTree:  81%| | 8146/10000 [03:29<00:54, 33.92it/s]Training CobwebTree:  82%| | 8150/10000 [03:29<00:55, 33.30it/s]Training CobwebTree:  82%| | 8154/10000 [03:30<00:55, 33.34it/s]Training CobwebTree:  82%| | 8158/10000 [03:30<00:54, 33.85it/s]Training CobwebTree:  82%| | 8162/10000 [03:30<00:56, 32.46it/s]Training CobwebTree:  82%| | 8166/10000 [03:30<00:56, 32.28it/s]Training CobwebTree:  82%| | 8170/10000 [03:30<00:54, 33.69it/s]Training CobwebTree:  82%| | 8174/10000 [03:30<00:55, 32.86it/s]Training CobwebTree:  82%| | 8178/10000 [03:30<00:54, 33.36it/s]Training CobwebTree:  82%| | 8182/10000 [03:30<00:54, 33.26it/s]Training CobwebTree:  82%| | 8186/10000 [03:31<00:52, 34.42it/s]Training CobwebTree:  82%| | 8190/10000 [03:31<00:53, 33.77it/s]Training CobwebTree:  82%| | 8194/10000 [03:31<00:53, 34.01it/s]Training CobwebTree:  82%| | 8198/10000 [03:31<00:54, 32.92it/s]Training CobwebTree:  82%| | 8202/10000 [03:31<00:55, 32.40it/s]Training CobwebTree:  82%| | 8206/10000 [03:31<00:53, 33.52it/s]Training CobwebTree:  82%| | 8210/10000 [03:31<00:53, 33.56it/s]Training CobwebTree:  82%| | 8214/10000 [03:31<00:55, 32.39it/s]Training CobwebTree:  82%| | 8218/10000 [03:32<00:54, 32.83it/s]Training CobwebTree:  82%| | 8222/10000 [03:32<00:53, 33.46it/s]Training CobwebTree:  82%| | 8226/10000 [03:32<00:52, 33.84it/s]Training CobwebTree:  82%| | 8230/10000 [03:32<00:52, 33.55it/s]Training CobwebTree:  82%| | 8234/10000 [03:32<00:52, 33.72it/s]Training CobwebTree:  82%| | 8238/10000 [03:32<00:51, 34.09it/s]Training CobwebTree:  82%| | 8242/10000 [03:32<00:50, 34.60it/s]Training CobwebTree:  82%| | 8246/10000 [03:32<00:50, 34.86it/s]Training CobwebTree:  82%| | 8250/10000 [03:32<00:49, 35.68it/s]Training CobwebTree:  83%| | 8254/10000 [03:33<00:51, 34.01it/s]Training CobwebTree:  83%| | 8258/10000 [03:33<00:50, 34.52it/s]Training CobwebTree:  83%| | 8262/10000 [03:33<00:51, 34.03it/s]Training CobwebTree:  83%| | 8266/10000 [03:33<00:50, 34.05it/s]Training CobwebTree:  83%| | 8270/10000 [03:33<00:51, 33.29it/s]Training CobwebTree:  83%| | 8275/10000 [03:33<00:47, 36.13it/s]Training CobwebTree:  83%| | 8279/10000 [03:33<00:48, 35.54it/s]Training CobwebTree:  83%| | 8283/10000 [03:33<00:48, 35.48it/s]Training CobwebTree:  83%| | 8287/10000 [03:34<00:47, 35.79it/s]Training CobwebTree:  83%| | 8291/10000 [03:34<00:47, 35.85it/s]Training CobwebTree:  83%| | 8295/10000 [03:34<00:49, 34.21it/s]Training CobwebTree:  83%| | 8299/10000 [03:34<00:47, 35.72it/s]Training CobwebTree:  83%| | 8303/10000 [03:34<00:48, 35.00it/s]Training CobwebTree:  83%| | 8307/10000 [03:34<00:49, 34.22it/s]Training CobwebTree:  83%| | 8312/10000 [03:34<00:45, 36.83it/s]Training CobwebTree:  83%| | 8316/10000 [03:34<00:44, 37.48it/s]Training CobwebTree:  83%| | 8320/10000 [03:34<00:47, 35.46it/s]Training CobwebTree:  83%| | 8324/10000 [03:35<00:47, 35.05it/s]Training CobwebTree:  83%| | 8328/10000 [03:35<00:47, 35.39it/s]Training CobwebTree:  83%| | 8332/10000 [03:35<00:48, 34.73it/s]Training CobwebTree:  83%| | 8336/10000 [03:35<00:47, 35.18it/s]Training CobwebTree:  83%| | 8340/10000 [03:35<00:47, 34.84it/s]Training CobwebTree:  83%| | 8344/10000 [03:35<00:49, 33.41it/s]Training CobwebTree:  83%| | 8348/10000 [03:35<00:49, 33.62it/s]Training CobwebTree:  84%| | 8352/10000 [03:35<00:50, 32.42it/s]Training CobwebTree:  84%| | 8356/10000 [03:36<00:50, 32.85it/s]Training CobwebTree:  84%| | 8360/10000 [03:36<00:47, 34.40it/s]Training CobwebTree:  84%| | 8364/10000 [03:36<00:48, 33.72it/s]Training CobwebTree:  84%| | 8368/10000 [03:36<00:46, 34.78it/s]Training CobwebTree:  84%| | 8372/10000 [03:36<00:46, 34.81it/s]Training CobwebTree:  84%| | 8376/10000 [03:36<00:45, 35.49it/s]Training CobwebTree:  84%| | 8380/10000 [03:36<00:45, 35.38it/s]Training CobwebTree:  84%| | 8384/10000 [03:36<00:47, 33.86it/s]Training CobwebTree:  84%| | 8388/10000 [03:36<00:47, 33.96it/s]Training CobwebTree:  84%| | 8392/10000 [03:37<00:46, 34.70it/s]Training CobwebTree:  84%| | 8396/10000 [03:37<00:45, 35.22it/s]Training CobwebTree:  84%| | 8400/10000 [03:37<00:44, 35.81it/s]Training CobwebTree:  84%| | 8404/10000 [03:37<00:45, 35.23it/s]Training CobwebTree:  84%| | 8408/10000 [03:37<00:45, 35.07it/s]Training CobwebTree:  84%| | 8412/10000 [03:37<00:44, 35.36it/s]Training CobwebTree:  84%| | 8416/10000 [03:37<00:45, 34.59it/s]Training CobwebTree:  84%| | 8420/10000 [03:37<00:45, 34.86it/s]Training CobwebTree:  84%| | 8424/10000 [03:37<00:44, 35.47it/s]Training CobwebTree:  84%| | 8428/10000 [03:38<00:44, 35.49it/s]Training CobwebTree:  84%| | 8432/10000 [03:38<00:45, 34.67it/s]Training CobwebTree:  84%| | 8437/10000 [03:38<00:42, 36.50it/s]Training CobwebTree:  84%| | 8441/10000 [03:38<00:44, 34.90it/s]Training CobwebTree:  84%| | 8445/10000 [03:38<00:44, 34.98it/s]Training CobwebTree:  84%| | 8449/10000 [03:38<00:44, 34.88it/s]Training CobwebTree:  85%| | 8453/10000 [03:38<00:43, 35.89it/s]Training CobwebTree:  85%| | 8458/10000 [03:38<00:41, 37.20it/s]Training CobwebTree:  85%| | 8462/10000 [03:38<00:41, 37.09it/s]Training CobwebTree:  85%| | 8466/10000 [03:39<00:41, 36.61it/s]Training CobwebTree:  85%| | 8470/10000 [03:39<00:44, 34.38it/s]Training CobwebTree:  85%| | 8474/10000 [03:39<00:44, 34.29it/s]Training CobwebTree:  85%| | 8478/10000 [03:39<00:45, 33.80it/s]Training CobwebTree:  85%| | 8482/10000 [03:39<00:43, 35.30it/s]Training CobwebTree:  85%| | 8486/10000 [03:39<00:42, 35.36it/s]Training CobwebTree:  85%| | 8490/10000 [03:39<00:44, 33.89it/s]Training CobwebTree:  85%| | 8494/10000 [03:39<00:45, 32.82it/s]Training CobwebTree:  85%| | 8498/10000 [03:40<00:45, 32.78it/s]Training CobwebTree:  85%| | 8502/10000 [03:40<00:43, 34.59it/s]Training CobwebTree:  85%| | 8506/10000 [03:40<00:41, 35.84it/s]Training CobwebTree:  85%| | 8510/10000 [03:40<00:41, 35.79it/s]Training CobwebTree:  85%| | 8514/10000 [03:40<00:43, 34.07it/s]Training CobwebTree:  85%| | 8518/10000 [03:40<00:42, 34.90it/s]Training CobwebTree:  85%| | 8522/10000 [03:40<00:42, 34.75it/s]Training CobwebTree:  85%| | 8526/10000 [03:40<00:43, 34.00it/s]Training CobwebTree:  85%| | 8531/10000 [03:41<00:41, 35.49it/s]Training CobwebTree:  85%| | 8535/10000 [03:41<00:43, 34.07it/s]Training CobwebTree:  85%| | 8539/10000 [03:41<00:41, 34.99it/s]Training CobwebTree:  85%| | 8543/10000 [03:41<00:41, 34.82it/s]Training CobwebTree:  85%| | 8547/10000 [03:41<00:41, 35.16it/s]Training CobwebTree:  86%| | 8551/10000 [03:41<00:40, 35.48it/s]Training CobwebTree:  86%| | 8556/10000 [03:41<00:37, 38.18it/s]Training CobwebTree:  86%| | 8560/10000 [03:41<00:37, 37.92it/s]Training CobwebTree:  86%| | 8564/10000 [03:41<00:38, 36.83it/s]Training CobwebTree:  86%| | 8568/10000 [03:42<00:40, 35.51it/s]Training CobwebTree:  86%| | 8572/10000 [03:42<00:41, 34.68it/s]Training CobwebTree:  86%| | 8576/10000 [03:42<00:39, 35.95it/s]Training CobwebTree:  86%| | 8580/10000 [03:42<00:39, 35.71it/s]Training CobwebTree:  86%| | 8585/10000 [03:42<00:38, 37.10it/s]Training CobwebTree:  86%| | 8589/10000 [03:42<00:38, 36.63it/s]Training CobwebTree:  86%| | 8593/10000 [03:42<00:38, 36.47it/s]Training CobwebTree:  86%| | 8597/10000 [03:42<00:39, 35.84it/s]Training CobwebTree:  86%| | 8601/10000 [03:42<00:38, 36.51it/s]Training CobwebTree:  86%| | 8605/10000 [03:43<00:37, 37.17it/s]Training CobwebTree:  86%| | 8609/10000 [03:43<00:37, 37.07it/s]Training CobwebTree:  86%| | 8613/10000 [03:43<00:38, 36.09it/s]Training CobwebTree:  86%| | 8617/10000 [03:43<00:41, 33.06it/s]Training CobwebTree:  86%| | 8621/10000 [03:43<00:41, 33.15it/s]Training CobwebTree:  86%| | 8625/10000 [03:43<00:41, 33.19it/s]Training CobwebTree:  86%| | 8629/10000 [03:43<00:40, 34.09it/s]Training CobwebTree:  86%| | 8633/10000 [03:43<00:41, 32.78it/s]Training CobwebTree:  86%| | 8637/10000 [03:44<00:40, 33.52it/s]Training CobwebTree:  86%| | 8641/10000 [03:44<00:39, 34.29it/s]Training CobwebTree:  86%| | 8645/10000 [03:44<00:40, 33.84it/s]Training CobwebTree:  86%| | 8649/10000 [03:44<00:40, 33.60it/s]Training CobwebTree:  87%| | 8653/10000 [03:44<00:40, 33.43it/s]Training CobwebTree:  87%| | 8657/10000 [03:44<00:38, 34.77it/s]Training CobwebTree:  87%| | 8661/10000 [03:44<00:38, 35.10it/s]Training CobwebTree:  87%| | 8665/10000 [03:44<00:37, 35.67it/s]Training CobwebTree:  87%| | 8669/10000 [03:44<00:37, 35.24it/s]Training CobwebTree:  87%| | 8673/10000 [03:45<00:38, 34.66it/s]Training CobwebTree:  87%| | 8677/10000 [03:45<00:37, 35.53it/s]Training CobwebTree:  87%| | 8681/10000 [03:45<00:37, 34.91it/s]Training CobwebTree:  87%| | 8685/10000 [03:45<00:37, 34.67it/s]Training CobwebTree:  87%| | 8689/10000 [03:45<00:38, 34.17it/s]Training CobwebTree:  87%| | 8693/10000 [03:45<00:38, 33.52it/s]Training CobwebTree:  87%| | 8697/10000 [03:45<00:38, 33.91it/s]Training CobwebTree:  87%| | 8701/10000 [03:45<00:39, 33.29it/s]Training CobwebTree:  87%| | 8705/10000 [03:45<00:38, 33.28it/s]Training CobwebTree:  87%| | 8709/10000 [03:46<00:38, 33.49it/s]Training CobwebTree:  87%| | 8713/10000 [03:46<00:38, 33.73it/s]Training CobwebTree:  87%| | 8717/10000 [03:46<00:38, 33.16it/s]Training CobwebTree:  87%| | 8721/10000 [03:46<00:38, 33.35it/s]Training CobwebTree:  87%| | 8725/10000 [03:46<00:38, 32.75it/s]Training CobwebTree:  87%| | 8729/10000 [03:46<00:38, 32.90it/s]Training CobwebTree:  87%| | 8734/10000 [03:46<00:37, 34.04it/s]Training CobwebTree:  87%| | 8738/10000 [03:46<00:36, 34.17it/s]Training CobwebTree:  87%| | 8742/10000 [03:47<00:37, 33.94it/s]Training CobwebTree:  87%| | 8746/10000 [03:47<00:39, 31.95it/s]Training CobwebTree:  88%| | 8750/10000 [03:47<00:37, 33.00it/s]Training CobwebTree:  88%| | 8754/10000 [03:47<00:37, 33.64it/s]Training CobwebTree:  88%| | 8758/10000 [03:47<00:37, 33.20it/s]Training CobwebTree:  88%| | 8762/10000 [03:47<00:36, 34.02it/s]Training CobwebTree:  88%| | 8766/10000 [03:47<00:36, 33.87it/s]Training CobwebTree:  88%| | 8770/10000 [03:47<00:36, 33.42it/s]Training CobwebTree:  88%| | 8774/10000 [03:48<00:35, 34.61it/s]Training CobwebTree:  88%| | 8778/10000 [03:48<00:36, 33.82it/s]Training CobwebTree:  88%| | 8782/10000 [03:48<00:35, 34.78it/s]Training CobwebTree:  88%| | 8786/10000 [03:48<00:35, 33.78it/s]Training CobwebTree:  88%| | 8790/10000 [03:48<00:35, 33.93it/s]Training CobwebTree:  88%| | 8794/10000 [03:48<00:34, 34.59it/s]Training CobwebTree:  88%| | 8798/10000 [03:48<00:35, 34.14it/s]Training CobwebTree:  88%| | 8802/10000 [03:48<00:33, 35.25it/s]Training CobwebTree:  88%| | 8807/10000 [03:48<00:33, 35.80it/s]Training CobwebTree:  88%| | 8811/10000 [03:49<00:32, 36.83it/s]Training CobwebTree:  88%| | 8815/10000 [03:49<00:31, 37.22it/s]Training CobwebTree:  88%| | 8819/10000 [03:49<00:33, 35.05it/s]Training CobwebTree:  88%| | 8823/10000 [03:49<00:32, 35.97it/s]Training CobwebTree:  88%| | 8827/10000 [03:49<00:33, 35.30it/s]Training CobwebTree:  88%| | 8831/10000 [03:49<00:32, 36.23it/s]Training CobwebTree:  88%| | 8835/10000 [03:49<00:32, 36.19it/s]Training CobwebTree:  88%| | 8839/10000 [03:49<00:32, 36.09it/s]Training CobwebTree:  88%| | 8843/10000 [03:50<00:33, 34.43it/s]Training CobwebTree:  88%| | 8847/10000 [03:50<00:34, 33.66it/s]Training CobwebTree:  89%| | 8851/10000 [03:50<00:32, 34.88it/s]Training CobwebTree:  89%| | 8855/10000 [03:50<00:32, 35.24it/s]Training CobwebTree:  89%| | 8860/10000 [03:50<00:30, 36.95it/s]Training CobwebTree:  89%| | 8864/10000 [03:50<00:32, 35.40it/s]Training CobwebTree:  89%| | 8868/10000 [03:50<00:31, 35.79it/s]Training CobwebTree:  89%| | 8872/10000 [03:50<00:31, 35.86it/s]Training CobwebTree:  89%| | 8876/10000 [03:50<00:32, 35.03it/s]Training CobwebTree:  89%| | 8880/10000 [03:51<00:31, 35.30it/s]Training CobwebTree:  89%| | 8884/10000 [03:51<00:31, 35.63it/s]Training CobwebTree:  89%| | 8888/10000 [03:51<00:31, 35.14it/s]Training CobwebTree:  89%| | 8892/10000 [03:51<00:31, 35.60it/s]Training CobwebTree:  89%| | 8896/10000 [03:51<00:31, 34.98it/s]Training CobwebTree:  89%| | 8900/10000 [03:51<00:31, 34.60it/s]Training CobwebTree:  89%| | 8904/10000 [03:51<00:33, 32.96it/s]Training CobwebTree:  89%| | 8908/10000 [03:51<00:31, 34.34it/s]Training CobwebTree:  89%| | 8912/10000 [03:51<00:32, 33.82it/s]Training CobwebTree:  89%| | 8916/10000 [03:52<00:33, 32.55it/s]Training CobwebTree:  89%| | 8920/10000 [03:52<00:33, 32.37it/s]Training CobwebTree:  89%| | 8924/10000 [03:52<00:32, 32.89it/s]Training CobwebTree:  89%| | 8928/10000 [03:52<00:32, 32.66it/s]Training CobwebTree:  89%| | 8932/10000 [03:52<00:31, 33.85it/s]Training CobwebTree:  89%| | 8936/10000 [03:52<00:30, 34.53it/s]Training CobwebTree:  89%| | 8940/10000 [03:52<00:31, 33.56it/s]Training CobwebTree:  89%| | 8944/10000 [03:52<00:30, 34.69it/s]Training CobwebTree:  89%| | 8949/10000 [03:53<00:29, 35.19it/s]Training CobwebTree:  90%| | 8953/10000 [03:53<00:30, 34.50it/s]Training CobwebTree:  90%| | 8957/10000 [03:53<00:30, 34.73it/s]Training CobwebTree:  90%| | 8961/10000 [03:53<00:29, 35.42it/s]Training CobwebTree:  90%| | 8966/10000 [03:53<00:27, 37.20it/s]Training CobwebTree:  90%| | 8970/10000 [03:53<00:27, 37.81it/s]Training CobwebTree:  90%| | 8974/10000 [03:53<00:28, 36.09it/s]Training CobwebTree:  90%| | 8978/10000 [03:53<00:28, 35.85it/s]Training CobwebTree:  90%| | 8982/10000 [03:53<00:28, 35.77it/s]Training CobwebTree:  90%| | 8986/10000 [03:54<00:29, 34.04it/s]Training CobwebTree:  90%| | 8991/10000 [03:54<00:28, 35.42it/s]Training CobwebTree:  90%| | 8995/10000 [03:54<00:29, 33.95it/s]Training CobwebTree:  90%| | 8999/10000 [03:54<00:28, 34.71it/s]Training CobwebTree:  90%| | 9003/10000 [03:54<00:29, 33.77it/s]Training CobwebTree:  90%| | 9007/10000 [03:54<00:29, 33.91it/s]Training CobwebTree:  90%| | 9012/10000 [03:54<00:26, 36.77it/s]Training CobwebTree:  90%| | 9016/10000 [03:54<00:28, 34.63it/s]Training CobwebTree:  90%| | 9020/10000 [03:55<00:28, 34.55it/s]Training CobwebTree:  90%| | 9024/10000 [03:55<00:29, 33.51it/s]Training CobwebTree:  90%| | 9028/10000 [03:55<00:28, 34.69it/s]Training CobwebTree:  90%| | 9032/10000 [03:55<00:28, 33.77it/s]Training CobwebTree:  90%| | 9036/10000 [03:55<00:28, 33.88it/s]Training CobwebTree:  90%| | 9040/10000 [03:55<00:29, 32.73it/s]Training CobwebTree:  90%| | 9045/10000 [03:55<00:28, 33.88it/s]Training CobwebTree:  90%| | 9049/10000 [03:55<00:27, 34.82it/s]Training CobwebTree:  91%| | 9053/10000 [03:56<00:26, 35.93it/s]Training CobwebTree:  91%| | 9057/10000 [03:56<00:25, 36.27it/s]Training CobwebTree:  91%| | 9061/10000 [03:56<00:25, 37.02it/s]Training CobwebTree:  91%| | 9065/10000 [03:56<00:26, 35.23it/s]Training CobwebTree:  91%| | 9069/10000 [03:56<00:27, 34.42it/s]Training CobwebTree:  91%| | 9073/10000 [03:56<00:27, 33.85it/s]Training CobwebTree:  91%| | 9077/10000 [03:56<00:27, 33.59it/s]Training CobwebTree:  91%| | 9081/10000 [03:56<00:27, 33.14it/s]Training CobwebTree:  91%| | 9085/10000 [03:56<00:27, 33.20it/s]Training CobwebTree:  91%| | 9089/10000 [03:57<00:27, 32.75it/s]Training CobwebTree:  91%| | 9093/10000 [03:57<00:27, 32.85it/s]Training CobwebTree:  91%| | 9097/10000 [03:57<00:26, 34.34it/s]Training CobwebTree:  91%| | 9101/10000 [03:57<00:26, 34.23it/s]Training CobwebTree:  91%| | 9105/10000 [03:57<00:25, 34.77it/s]Training CobwebTree:  91%| | 9109/10000 [03:57<00:25, 35.39it/s]Training CobwebTree:  91%| | 9113/10000 [03:57<00:25, 34.23it/s]Training CobwebTree:  91%| | 9117/10000 [03:57<00:25, 34.25it/s]Training CobwebTree:  91%| | 9122/10000 [03:58<00:24, 35.66it/s]Training CobwebTree:  91%|| 9127/10000 [03:58<00:23, 37.58it/s]Training CobwebTree:  91%|| 9131/10000 [03:58<00:23, 36.45it/s]Training CobwebTree:  91%|| 9135/10000 [03:58<00:24, 35.45it/s]Training CobwebTree:  91%|| 9139/10000 [03:58<00:23, 36.24it/s]Training CobwebTree:  91%|| 9143/10000 [03:58<00:23, 36.16it/s]Training CobwebTree:  91%|| 9147/10000 [03:58<00:24, 35.11it/s]Training CobwebTree:  92%|| 9151/10000 [03:58<00:24, 34.33it/s]Training CobwebTree:  92%|| 9155/10000 [03:59<00:25, 33.24it/s]Training CobwebTree:  92%|| 9159/10000 [03:59<00:24, 34.11it/s]Training CobwebTree:  92%|| 9163/10000 [03:59<00:25, 33.08it/s]Training CobwebTree:  92%|| 9167/10000 [03:59<00:25, 33.09it/s]Training CobwebTree:  92%|| 9171/10000 [03:59<00:24, 33.23it/s]Training CobwebTree:  92%|| 9175/10000 [03:59<00:24, 33.78it/s]Training CobwebTree:  92%|| 9179/10000 [03:59<00:24, 33.46it/s]Training CobwebTree:  92%|| 9183/10000 [03:59<00:25, 32.20it/s]Training CobwebTree:  92%|| 9187/10000 [04:00<00:26, 30.55it/s]Training CobwebTree:  92%|| 9191/10000 [04:00<00:25, 31.54it/s]Training CobwebTree:  92%|| 9195/10000 [04:00<00:25, 31.55it/s]Training CobwebTree:  92%|| 9199/10000 [04:00<00:24, 32.56it/s]Training CobwebTree:  92%|| 9203/10000 [04:00<00:23, 33.99it/s]Training CobwebTree:  92%|| 9207/10000 [04:00<00:23, 33.06it/s]Training CobwebTree:  92%|| 9211/10000 [04:00<00:23, 33.27it/s]Training CobwebTree:  92%|| 9215/10000 [04:00<00:23, 32.71it/s]Training CobwebTree:  92%|| 9219/10000 [04:00<00:23, 33.81it/s]Training CobwebTree:  92%|| 9223/10000 [04:01<00:23, 33.67it/s]Training CobwebTree:  92%|| 9227/10000 [04:01<00:23, 33.52it/s]Training CobwebTree:  92%|| 9231/10000 [04:01<00:23, 32.28it/s]Training CobwebTree:  92%|| 9235/10000 [04:01<00:22, 33.50it/s]Training CobwebTree:  92%|| 9239/10000 [04:01<00:22, 33.30it/s]Training CobwebTree:  92%|| 9243/10000 [04:01<00:22, 32.92it/s]Training CobwebTree:  92%|| 9247/10000 [04:01<00:22, 34.05it/s]Training CobwebTree:  93%|| 9251/10000 [04:01<00:21, 34.16it/s]Training CobwebTree:  93%|| 9255/10000 [04:02<00:22, 33.85it/s]Training CobwebTree:  93%|| 9259/10000 [04:02<00:21, 34.97it/s]Training CobwebTree:  93%|| 9263/10000 [04:02<00:21, 35.06it/s]Training CobwebTree:  93%|| 9267/10000 [04:02<00:20, 35.16it/s]Training CobwebTree:  93%|| 9271/10000 [04:02<00:22, 33.03it/s]Training CobwebTree:  93%|| 9275/10000 [04:02<00:21, 33.12it/s]Training CobwebTree:  93%|| 9279/10000 [04:02<00:22, 32.00it/s]Training CobwebTree:  93%|| 9283/10000 [04:02<00:21, 33.44it/s]Training CobwebTree:  93%|| 9287/10000 [04:02<00:22, 32.37it/s]Training CobwebTree:  93%|| 9291/10000 [04:03<00:20, 33.81it/s]Training CobwebTree:  93%|| 9295/10000 [04:03<00:20, 34.76it/s]Training CobwebTree:  93%|| 9299/10000 [04:03<00:21, 33.16it/s]Training CobwebTree:  93%|| 9303/10000 [04:03<00:21, 32.65it/s]Training CobwebTree:  93%|| 9307/10000 [04:03<00:21, 32.73it/s]Training CobwebTree:  93%|| 9311/10000 [04:03<00:21, 31.74it/s]Training CobwebTree:  93%|| 9315/10000 [04:03<00:20, 33.01it/s]Training CobwebTree:  93%|| 9319/10000 [04:03<00:20, 33.34it/s]Training CobwebTree:  93%|| 9323/10000 [04:04<00:19, 34.37it/s]Training CobwebTree:  93%|| 9327/10000 [04:04<00:19, 34.76it/s]Training CobwebTree:  93%|| 9331/10000 [04:04<00:19, 33.65it/s]Training CobwebTree:  93%|| 9335/10000 [04:04<00:20, 32.50it/s]Training CobwebTree:  93%|| 9339/10000 [04:04<00:20, 32.07it/s]Training CobwebTree:  93%|| 9343/10000 [04:04<00:21, 30.54it/s]Training CobwebTree:  93%|| 9347/10000 [04:04<00:20, 31.35it/s]Training CobwebTree:  94%|| 9351/10000 [04:04<00:19, 32.97it/s]Training CobwebTree:  94%|| 9355/10000 [04:05<00:18, 34.22it/s]Training CobwebTree:  94%|| 9359/10000 [04:05<00:18, 34.96it/s]Training CobwebTree:  94%|| 9363/10000 [04:05<00:18, 35.02it/s]Training CobwebTree:  94%|| 9367/10000 [04:05<00:17, 35.31it/s]Training CobwebTree:  94%|| 9371/10000 [04:05<00:18, 34.23it/s]Training CobwebTree:  94%|| 9375/10000 [04:05<00:17, 34.86it/s]Training CobwebTree:  94%|| 9379/10000 [04:05<00:17, 36.23it/s]Training CobwebTree:  94%|| 9383/10000 [04:05<00:17, 36.04it/s]Training CobwebTree:  94%|| 9387/10000 [04:05<00:17, 34.97it/s]Training CobwebTree:  94%|| 9391/10000 [04:06<00:17, 35.61it/s]Training CobwebTree:  94%|| 9395/10000 [04:06<00:16, 36.28it/s]Training CobwebTree:  94%|| 9399/10000 [04:06<00:16, 35.37it/s]Training CobwebTree:  94%|| 9403/10000 [04:06<00:16, 35.70it/s]Training CobwebTree:  94%|| 9407/10000 [04:06<00:16, 36.61it/s]Training CobwebTree:  94%|| 9411/10000 [04:06<00:16, 36.56it/s]Training CobwebTree:  94%|| 9415/10000 [04:06<00:16, 35.67it/s]Training CobwebTree:  94%|| 9419/10000 [04:06<00:16, 35.03it/s]Training CobwebTree:  94%|| 9423/10000 [04:06<00:16, 35.72it/s]Training CobwebTree:  94%|| 9427/10000 [04:07<00:15, 36.51it/s]Training CobwebTree:  94%|| 9431/10000 [04:07<00:15, 36.22it/s]Training CobwebTree:  94%|| 9435/10000 [04:07<00:15, 37.01it/s]Training CobwebTree:  94%|| 9439/10000 [04:07<00:14, 37.66it/s]Training CobwebTree:  94%|| 9443/10000 [04:07<00:15, 35.42it/s]Training CobwebTree:  94%|| 9447/10000 [04:07<00:15, 36.50it/s]Training CobwebTree:  95%|| 9451/10000 [04:07<00:15, 35.43it/s]Training CobwebTree:  95%|| 9455/10000 [04:07<00:16, 34.05it/s]Training CobwebTree:  95%|| 9459/10000 [04:07<00:15, 34.53it/s]Training CobwebTree:  95%|| 9463/10000 [04:08<00:15, 33.94it/s]Training CobwebTree:  95%|| 9467/10000 [04:08<00:15, 34.22it/s]Training CobwebTree:  95%|| 9471/10000 [04:08<00:15, 34.71it/s]Training CobwebTree:  95%|| 9475/10000 [04:08<00:15, 34.45it/s]Training CobwebTree:  95%|| 9479/10000 [04:08<00:15, 34.35it/s]Training CobwebTree:  95%|| 9483/10000 [04:08<00:15, 32.81it/s]Training CobwebTree:  95%|| 9487/10000 [04:08<00:16, 31.80it/s]Training CobwebTree:  95%|| 9491/10000 [04:08<00:15, 32.12it/s]Training CobwebTree:  95%|| 9495/10000 [04:09<00:15, 32.36it/s]Training CobwebTree:  95%|| 9499/10000 [04:09<00:15, 33.30it/s]Training CobwebTree:  95%|| 9503/10000 [04:09<00:14, 34.31it/s]Training CobwebTree:  95%|| 9507/10000 [04:09<00:14, 33.88it/s]Training CobwebTree:  95%|| 9511/10000 [04:09<00:14, 34.17it/s]Training CobwebTree:  95%|| 9515/10000 [04:09<00:14, 34.10it/s]Training CobwebTree:  95%|| 9519/10000 [04:09<00:14, 32.84it/s]Training CobwebTree:  95%|| 9523/10000 [04:09<00:14, 33.89it/s]Training CobwebTree:  95%|| 9527/10000 [04:09<00:13, 34.76it/s]Training CobwebTree:  95%|| 9531/10000 [04:10<00:13, 34.61it/s]Training CobwebTree:  95%|| 9535/10000 [04:10<00:13, 33.71it/s]Training CobwebTree:  95%|| 9539/10000 [04:10<00:13, 34.09it/s]Training CobwebTree:  95%|| 9543/10000 [04:10<00:13, 34.31it/s]Training CobwebTree:  95%|| 9547/10000 [04:10<00:13, 33.33it/s]Training CobwebTree:  96%|| 9551/10000 [04:10<00:13, 32.68it/s]Training CobwebTree:  96%|| 9555/10000 [04:10<00:14, 31.32it/s]Training CobwebTree:  96%|| 9559/10000 [04:10<00:14, 30.84it/s]Training CobwebTree:  96%|| 9563/10000 [04:11<00:13, 31.83it/s]Training CobwebTree:  96%|| 9567/10000 [04:11<00:13, 32.18it/s]Training CobwebTree:  96%|| 9571/10000 [04:11<00:12, 33.74it/s]Training CobwebTree:  96%|| 9575/10000 [04:11<00:13, 32.65it/s]Training CobwebTree:  96%|| 9579/10000 [04:11<00:12, 33.26it/s]Training CobwebTree:  96%|| 9583/10000 [04:11<00:12, 33.08it/s]Training CobwebTree:  96%|| 9587/10000 [04:11<00:12, 33.45it/s]Training CobwebTree:  96%|| 9591/10000 [04:11<00:11, 34.49it/s]Training CobwebTree:  96%|| 9595/10000 [04:12<00:11, 34.73it/s]Training CobwebTree:  96%|| 9599/10000 [04:12<00:11, 35.08it/s]Training CobwebTree:  96%|| 9603/10000 [04:12<00:11, 34.73it/s]Training CobwebTree:  96%|| 9607/10000 [04:12<00:11, 34.85it/s]Training CobwebTree:  96%|| 9611/10000 [04:12<00:11, 35.35it/s]Training CobwebTree:  96%|| 9615/10000 [04:12<00:10, 35.99it/s]Training CobwebTree:  96%|| 9619/10000 [04:12<00:10, 35.41it/s]Training CobwebTree:  96%|| 9623/10000 [04:12<00:10, 35.20it/s]Training CobwebTree:  96%|| 9627/10000 [04:12<00:10, 35.88it/s]Training CobwebTree:  96%|| 9631/10000 [04:13<00:10, 35.61it/s]Training CobwebTree:  96%|| 9635/10000 [04:13<00:10, 35.22it/s]Training CobwebTree:  96%|| 9639/10000 [04:13<00:10, 33.13it/s]Training CobwebTree:  96%|| 9643/10000 [04:13<00:10, 33.03it/s]Training CobwebTree:  96%|| 9647/10000 [04:13<00:10, 34.19it/s]Training CobwebTree:  97%|| 9651/10000 [04:13<00:10, 34.14it/s]Training CobwebTree:  97%|| 9655/10000 [04:13<00:09, 35.67it/s]Training CobwebTree:  97%|| 9659/10000 [04:13<00:09, 36.12it/s]Training CobwebTree:  97%|| 9663/10000 [04:13<00:09, 35.69it/s]Training CobwebTree:  97%|| 9667/10000 [04:14<00:09, 35.71it/s]Training CobwebTree:  97%|| 9672/10000 [04:14<00:09, 36.13it/s]Training CobwebTree:  97%|| 9676/10000 [04:14<00:09, 35.15it/s]Training CobwebTree:  97%|| 9680/10000 [04:14<00:08, 36.03it/s]Training CobwebTree:  97%|| 9684/10000 [04:14<00:08, 35.30it/s]Training CobwebTree:  97%|| 9688/10000 [04:14<00:09, 34.44it/s]Training CobwebTree:  97%|| 9692/10000 [04:14<00:08, 35.68it/s]Training CobwebTree:  97%|| 9696/10000 [04:14<00:08, 35.73it/s]Training CobwebTree:  97%|| 9700/10000 [04:15<00:08, 35.19it/s]Training CobwebTree:  97%|| 9704/10000 [04:15<00:08, 34.81it/s]Training CobwebTree:  97%|| 9708/10000 [04:15<00:08, 35.12it/s]Training CobwebTree:  97%|| 9712/10000 [04:15<00:08, 34.26it/s]Training CobwebTree:  97%|| 9716/10000 [04:15<00:08, 32.67it/s]Training CobwebTree:  97%|| 9720/10000 [04:15<00:08, 33.45it/s]Training CobwebTree:  97%|| 9724/10000 [04:15<00:08, 33.58it/s]Training CobwebTree:  97%|| 9728/10000 [04:15<00:08, 33.35it/s]Training CobwebTree:  97%|| 9732/10000 [04:15<00:08, 32.79it/s]Training CobwebTree:  97%|| 9736/10000 [04:16<00:08, 32.07it/s]Training CobwebTree:  97%|| 9740/10000 [04:16<00:07, 33.32it/s]Training CobwebTree:  97%|| 9744/10000 [04:16<00:07, 32.96it/s]Training CobwebTree:  97%|| 9748/10000 [04:16<00:07, 31.79it/s]Training CobwebTree:  98%|| 9752/10000 [04:16<00:07, 31.63it/s]Training CobwebTree:  98%|| 9756/10000 [04:16<00:07, 32.44it/s]Training CobwebTree:  98%|| 9760/10000 [04:16<00:07, 33.16it/s]Training CobwebTree:  98%|| 9764/10000 [04:16<00:07, 32.65it/s]Training CobwebTree:  98%|| 9768/10000 [04:17<00:07, 31.98it/s]Training CobwebTree:  98%|| 9772/10000 [04:17<00:07, 32.36it/s]Training CobwebTree:  98%|| 9776/10000 [04:17<00:06, 33.31it/s]Training CobwebTree:  98%|| 9780/10000 [04:17<00:06, 33.90it/s]Training CobwebTree:  98%|| 9784/10000 [04:17<00:06, 34.07it/s]Training CobwebTree:  98%|| 9788/10000 [04:17<00:06, 34.42it/s]Training CobwebTree:  98%|| 9792/10000 [04:17<00:05, 34.77it/s]Training CobwebTree:  98%|| 9796/10000 [04:17<00:05, 34.72it/s]Training CobwebTree:  98%|| 9800/10000 [04:18<00:05, 33.95it/s]Training CobwebTree:  98%|| 9804/10000 [04:18<00:05, 33.27it/s]Training CobwebTree:  98%|| 9808/10000 [04:18<00:05, 32.94it/s]Training CobwebTree:  98%|| 9812/10000 [04:18<00:05, 32.96it/s]Training CobwebTree:  98%|| 9816/10000 [04:18<00:05, 32.13it/s]Training CobwebTree:  98%|| 9820/10000 [04:18<00:05, 31.84it/s]Training CobwebTree:  98%|| 9824/10000 [04:18<00:05, 32.89it/s]Training CobwebTree:  98%|| 9828/10000 [04:18<00:05, 34.04it/s]Training CobwebTree:  98%|| 9832/10000 [04:18<00:04, 35.35it/s]Training CobwebTree:  98%|| 9836/10000 [04:19<00:04, 34.85it/s]Training CobwebTree:  98%|| 9840/10000 [04:19<00:04, 34.37it/s]Training CobwebTree:  98%|| 9844/10000 [04:19<00:04, 34.13it/s]Training CobwebTree:  98%|| 9848/10000 [04:19<00:04, 33.08it/s]Training CobwebTree:  99%|| 9852/10000 [04:19<00:04, 34.10it/s]Training CobwebTree:  99%|| 9856/10000 [04:19<00:04, 34.46it/s]Training CobwebTree:  99%|| 9860/10000 [04:19<00:03, 35.05it/s]Training CobwebTree:  99%|| 9864/10000 [04:19<00:03, 35.04it/s]Training CobwebTree:  99%|| 9868/10000 [04:20<00:03, 34.07it/s]Training CobwebTree:  99%|| 9872/10000 [04:20<00:03, 34.20it/s]Training CobwebTree:  99%|| 9876/10000 [04:20<00:03, 33.48it/s]Training CobwebTree:  99%|| 9880/10000 [04:20<00:03, 31.37it/s]Training CobwebTree:  99%|| 9884/10000 [04:20<00:03, 33.03it/s]Training CobwebTree:  99%|| 9888/10000 [04:20<00:03, 34.60it/s]Training CobwebTree:  99%|| 9892/10000 [04:20<00:03, 34.87it/s]Training CobwebTree:  99%|| 9896/10000 [04:20<00:03, 34.51it/s]Training CobwebTree:  99%|| 9900/10000 [04:20<00:02, 33.71it/s]Training CobwebTree:  99%|| 9904/10000 [04:21<00:02, 33.75it/s]Training CobwebTree:  99%|| 9908/10000 [04:21<00:02, 33.04it/s]Training CobwebTree:  99%|| 9912/10000 [04:21<00:02, 33.36it/s]Training CobwebTree:  99%|| 9916/10000 [04:21<00:02, 32.68it/s]Training CobwebTree:  99%|| 9920/10000 [04:21<00:02, 33.21it/s]Training CobwebTree:  99%|| 9924/10000 [04:21<00:02, 33.69it/s]Training CobwebTree:  99%|| 9929/10000 [04:21<00:01, 35.66it/s]Training CobwebTree:  99%|| 9933/10000 [04:21<00:01, 35.04it/s]Training CobwebTree:  99%|| 9937/10000 [04:22<00:01, 35.74it/s]Training CobwebTree:  99%|| 9941/10000 [04:22<00:01, 34.46it/s]Training CobwebTree:  99%|| 9945/10000 [04:22<00:01, 33.03it/s]Training CobwebTree:  99%|| 9949/10000 [04:22<00:01, 33.12it/s]Training CobwebTree: 100%|| 9953/10000 [04:22<00:01, 33.86it/s]Training CobwebTree: 100%|| 9957/10000 [04:22<00:01, 34.16it/s]Training CobwebTree: 100%|| 9961/10000 [04:22<00:01, 33.33it/s]Training CobwebTree: 100%|| 9965/10000 [04:22<00:01, 33.24it/s]Training CobwebTree: 100%|| 9969/10000 [04:23<00:00, 33.39it/s]Training CobwebTree: 100%|| 9973/10000 [04:23<00:00, 32.66it/s]Training CobwebTree: 100%|| 9977/10000 [04:23<00:00, 32.34it/s]Training CobwebTree: 100%|| 9981/10000 [04:23<00:00, 33.30it/s]Training CobwebTree: 100%|| 9985/10000 [04:23<00:00, 32.56it/s]Training CobwebTree: 100%|| 9989/10000 [04:23<00:00, 33.61it/s]Training CobwebTree: 100%|| 9993/10000 [04:23<00:00, 33.05it/s]Training CobwebTree: 100%|| 9997/10000 [04:23<00:00, 33.14it/s]Training CobwebTree: 100%|| 10000/10000 [04:23<00:00, 37.88it/s]
2025-12-23 23:49:45,036 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:49:51,496 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (-70568 virtual)
2025-12-23 23:49:52,481 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (-99218 virtual)
2025-12-23 23:49:53,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,242 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,242 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,244 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,244 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,249 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,261 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,269 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,301 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,321 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,326 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,783 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,798 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,070 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,085 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,478 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:00,121 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:50:00,428 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 86452 virtual documents
2025-12-23 23:50:01,089 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:50:07,250 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5038 virtual)
2025-12-23 23:50:07,253 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (9939 virtual)
2025-12-23 23:50:07,255 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15346 virtual)
2025-12-23 23:50:07,257 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19775 virtual)
2025-12-23 23:50:07,259 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (24513 virtual)
2025-12-23 23:50:07,261 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30004 virtual)
2025-12-23 23:50:07,264 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35561 virtual)
2025-12-23 23:50:07,266 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (40599 virtual)
2025-12-23 23:50:07,268 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45821 virtual)
2025-12-23 23:50:07,270 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51004 virtual)
2025-12-23 23:50:07,272 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (56880 virtual)
2025-12-23 23:50:07,275 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62578 virtual)
2025-12-23 23:50:07,277 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (68501 virtual)
2025-12-23 23:50:07,279 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (74285 virtual)
2025-12-23 23:50:07,281 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (78940 virtual)
2025-12-23 23:50:07,283 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (84436 virtual)
2025-12-23 23:50:07,285 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (90137 virtual)
2025-12-23 23:50:07,287 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (95470 virtual)
2025-12-23 23:50:07,289 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (100900 virtual)
2025-12-23 23:50:07,292 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (106659 virtual)
2025-12-23 23:50:07,295 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (111500 virtual)
2025-12-23 23:50:07,297 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (116869 virtual)
2025-12-23 23:50:07,299 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (122293 virtual)
2025-12-23 23:50:07,301 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (127898 virtual)
2025-12-23 23:50:07,303 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (132828 virtual)
2025-12-23 23:50:07,306 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (138085 virtual)
2025-12-23 23:50:07,308 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (143343 virtual)
2025-12-23 23:50:07,309 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (148098 virtual)
2025-12-23 23:50:07,311 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (152948 virtual)
2025-12-23 23:50:07,313 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (157644 virtual)
2025-12-23 23:50:07,315 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (162784 virtual)
2025-12-23 23:50:07,318 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (167970 virtual)
2025-12-23 23:50:07,320 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (173334 virtual)
2025-12-23 23:50:07,322 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (179046 virtual)
2025-12-23 23:50:07,324 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (184503 virtual)
2025-12-23 23:50:07,327 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (190482 virtual)
2025-12-23 23:50:07,329 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (195734 virtual)
2025-12-23 23:50:07,331 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (201130 virtual)
2025-12-23 23:50:07,334 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (206629 virtual)
2025-12-23 23:50:07,336 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (212054 virtual)
2025-12-23 23:50:07,339 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (217483 virtual)
2025-12-23 23:50:07,342 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (222543 virtual)
2025-12-23 23:50:07,344 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (227517 virtual)
2025-12-23 23:50:07,347 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (232518 virtual)
2025-12-23 23:50:07,349 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (237974 virtual)
2025-12-23 23:50:07,351 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (243778 virtual)
2025-12-23 23:50:07,354 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (249684 virtual)
2025-12-23 23:50:07,356 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (255577 virtual)
2025-12-23 23:50:07,359 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (261521 virtual)
2025-12-23 23:50:07,362 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (267029 virtual)
2025-12-23 23:50:07,364 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (272359 virtual)
2025-12-23 23:50:07,367 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (278302 virtual)
2025-12-23 23:50:07,370 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (284138 virtual)
2025-12-23 23:50:07,372 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (289694 virtual)
2025-12-23 23:50:07,375 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (295460 virtual)
2025-12-23 23:50:07,378 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (301061 virtual)
2025-12-23 23:50:07,380 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (306311 virtual)
2025-12-23 23:50:07,383 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (312194 virtual)
2025-12-23 23:50:07,386 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (318140 virtual)
2025-12-23 23:50:07,388 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (323661 virtual)
2025-12-23 23:50:07,390 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (328872 virtual)
2025-12-23 23:50:07,393 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (334616 virtual)
2025-12-23 23:50:07,396 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (340760 virtual)
2025-12-23 23:50:07,399 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (347024 virtual)
2025-12-23 23:50:07,401 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (353005 virtual)
2025-12-23 23:50:07,403 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (358895 virtual)
2025-12-23 23:50:07,406 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (364790 virtual)
2025-12-23 23:50:07,409 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (370572 virtual)
2025-12-23 23:50:07,411 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (376025 virtual)
2025-12-23 23:50:07,414 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (381884 virtual)
2025-12-23 23:50:07,417 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (387336 virtual)
2025-12-23 23:50:07,419 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (393046 virtual)
2025-12-23 23:50:07,422 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (398587 virtual)
2025-12-23 23:50:07,424 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404515 virtual)
2025-12-23 23:50:07,427 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (409840 virtual)
2025-12-23 23:50:07,430 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (415807 virtual)
2025-12-23 23:50:07,432 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (422232 virtual)
2025-12-23 23:50:07,435 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427794 virtual)
2025-12-23 23:50:07,438 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (433810 virtual)
2025-12-23 23:50:07,440 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (439773 virtual)
2025-12-23 23:50:07,443 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (445625 virtual)
2025-12-23 23:50:07,446 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (451425 virtual)
2025-12-23 23:50:07,451 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (457024 virtual)
2025-12-23 23:50:07,459 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (462670 virtual)
2025-12-23 23:50:07,462 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (468463 virtual)
2025-12-23 23:50:07,464 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (474212 virtual)
2025-12-23 23:50:07,486 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (479446 virtual)
2025-12-23 23:50:07,494 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (484627 virtual)
2025-12-23 23:50:07,513 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (490059 virtual)
2025-12-23 23:50:07,544 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (495865 virtual)
2025-12-23 23:50:07,569 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (501609 virtual)
2025-12-23 23:50:07,585 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (507454 virtual)
2025-12-23 23:50:07,601 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (512888 virtual)
2025-12-23 23:50:07,673 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (518689 virtual)
2025-12-23 23:50:07,698 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (524392 virtual)
2025-12-23 23:50:07,713 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (530209 virtual)
2025-12-23 23:50:07,769 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (536088 virtual)
2025-12-23 23:50:07,821 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (542046 virtual)
2025-12-23 23:50:07,878 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (548028 virtual)
2025-12-23 23:50:07,893 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (553932 virtual)
2025-12-23 23:50:07,954 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (560021 virtual)
2025-12-23 23:50:07,969 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (566327 virtual)
2025-12-23 23:50:08,059 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (572119 virtual)
2025-12-23 23:50:08,073 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (577590 virtual)
2025-12-23 23:50:08,170 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (583031 virtual)
2025-12-23 23:50:08,185 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (588967 virtual)
2025-12-23 23:50:08,259 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (594321 virtual)
2025-12-23 23:50:08,273 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (599788 virtual)
2025-12-23 23:50:08,385 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (605199 virtual)
2025-12-23 23:50:08,437 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (611045 virtual)
2025-12-23 23:50:08,510 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (616902 virtual)
2025-12-23 23:50:08,628 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (622705 virtual)
2025-12-23 23:50:08,735 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (628573 virtual)
2025-12-23 23:50:08,749 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (634432 virtual)
2025-12-23 23:50:08,801 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (640090 virtual)
2025-12-23 23:50:08,803 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (645815 virtual)
2025-12-23 23:50:08,942 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (651571 virtual)
2025-12-23 23:50:08,945 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (657387 virtual)
2025-12-23 23:50:08,947 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (663236 virtual)
2025-12-23 23:50:09,054 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (669362 virtual)
2025-12-23 23:50:09,057 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (675610 virtual)
2025-12-23 23:50:09,141 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (681771 virtual)
2025-12-23 23:50:09,229 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (687925 virtual)
2025-12-23 23:50:09,282 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (694382 virtual)
2025-12-23 23:50:09,369 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (700400 virtual)
2025-12-23 23:50:09,421 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (706047 virtual)
2025-12-23 23:50:09,498 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (712150 virtual)
2025-12-23 23:50:09,513 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (718089 virtual)
2025-12-23 23:50:09,625 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (723908 virtual)
2025-12-23 23:50:09,686 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (729792 virtual)
2025-12-23 23:50:09,689 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (735737 virtual)
2025-12-23 23:50:09,691 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (740988 virtual)
2025-12-23 23:50:09,826 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (746714 virtual)
2025-12-23 23:50:09,841 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (752115 virtual)
2025-12-23 23:50:09,846 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (757747 virtual)
2025-12-23 23:50:09,958 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (763660 virtual)
2025-12-23 23:50:09,961 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (769419 virtual)
2025-12-23 23:50:09,963 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (774931 virtual)
2025-12-23 23:50:10,086 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (780589 virtual)
2025-12-23 23:50:10,101 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (786081 virtual)
2025-12-23 23:50:10,206 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (791777 virtual)
2025-12-23 23:50:10,221 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (798138 virtual)
2025-12-23 23:50:10,319 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (803809 virtual)
2025-12-23 23:50:10,321 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (809775 virtual)
2025-12-23 23:50:10,323 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (815761 virtual)
2025-12-23 23:50:10,458 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (821960 virtual)
2025-12-23 23:50:10,522 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (827580 virtual)
2025-12-23 23:50:10,525 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (833163 virtual)
2025-12-23 23:50:10,649 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (838905 virtual)
2025-12-23 23:50:10,694 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (844537 virtual)
2025-12-23 23:50:10,709 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (850171 virtual)
2025-12-23 23:50:10,834 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (855698 virtual)
2025-12-23 23:50:10,849 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (861596 virtual)
2025-12-23 23:50:10,966 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (867137 virtual)
2025-12-23 23:50:10,981 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (873138 virtual)
2025-12-23 23:50:11,078 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (878914 virtual)
2025-12-23 23:50:11,080 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (880327 virtual)
2025-12-23 23:50:11,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,091 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,092 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,092 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,095 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,095 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,096 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,097 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,097 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,098 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,098 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,101 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,102 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,102 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,103 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,105 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,111 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,115 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,116 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,116 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,117 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,119 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,120 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,127 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,188 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,188 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,210 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,222 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,242 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,242 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,262 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,267 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,288 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,290 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,316 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,242 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,406 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,462 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,865 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:11,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:11,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,449 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,660 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,661 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,768 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,779 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,850 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,908 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:12,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:12,982 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,133 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,273 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,526 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,548 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,562 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,580 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,616 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,630 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,640 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:13,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:50:13,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:50:18,021 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:50:18,252 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 880332 virtual documents
2025-12-23 23:50:18,735 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.5203767655023454, 'coherence_npmi': 0.048469885219015386, 'topic_diversity': 0.7058823529411765, 'inter_topic_similarity': 0.5060652494430542}
2025-12-23 23:50:18,735 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6138126106941303, 'coherence_npmi': 0.11704270851477915, 'topic_diversity': 0.644, 'inter_topic_similarity': 0.6761587262153625}
2025-12-23 23:50:18,735 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.5989325234340358, 'coherence_npmi': 0.10625356050420925, 'topic_diversity': 0.6134328358208955, 'inter_topic_similarity': 0.6709443926811218}
