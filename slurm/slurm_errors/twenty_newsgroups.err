2025-12-21 12:57:23,863 INFO __main__: Starting benchmark for dataset=20newsgroups
2025-12-21 12:57:25,120 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-21 12:57:25,611 INFO gensim.corpora.dictionary: built Dictionary<70709 unique tokens: ['88', '89', 'best', 'bonnevilles', 'book']...> from 7317 documents (total 691444 corpus positions)
2025-12-21 12:57:25,614 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<70709 unique tokens: ['88', '89', 'best', 'bonnevilles', 'book']...> from 7317 documents (total 691444 corpus positions)", 'datetime': '2025-12-21T12:57:25.612124', 'gensim': '4.4.0', 'python': '3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-200-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-21 12:57:26,128 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-21 12:57:26,128 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-21 12:57:28,144 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 7317 docs
2025-12-21 12:58:50,000 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:58:53,208 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-21 12:58:53,212 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-21 12:58:53,214 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-21 12:58:53,217 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-21 12:58:53,222 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-21 12:58:53,227 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-21 12:58:53,230 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-21 12:58:53,248 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-21 12:58:53,255 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-21 12:58:53,260 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-21 12:58:53,276 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-21 12:58:53,306 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-21 12:58:53,319 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-21 12:58:53,331 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-21 12:58:53,426 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-21 12:58:53,476 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-21 12:58:53,480 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-21 12:58:53,555 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-21 12:58:53,560 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-21 12:58:53,599 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-21 12:58:53,629 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-21 12:58:53,976 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-21 12:58:54,056 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-21 12:58:54,105 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-21 12:58:54,136 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-21 12:58:54,142 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-21 12:58:54,162 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-21 12:58:54,308 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-21 12:58:54,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,326 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,326 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,327 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,330 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,330 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,359 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,359 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,364 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,364 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,367 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,381 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,398 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,402 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,416 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,418 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,424 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,442 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,482 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,758 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:54,935 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:54,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:55,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:55,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:55,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:55,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:55,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:55,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:55,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:55,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:55,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:55,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:55,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:55,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:55,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:55,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:56,244 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:56,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:56,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:56,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:56,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:56,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:58:56,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:58:56,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:00,569 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 12:59:00,601 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-21 12:59:01,051 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 12:59:05,195 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-21 12:59:05,197 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-21 12:59:05,198 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-21 12:59:05,199 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-21 12:59:05,201 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-21 12:59:05,203 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-21 12:59:05,204 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-21 12:59:05,206 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-21 12:59:05,208 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-21 12:59:05,209 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-21 12:59:05,211 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-21 12:59:05,212 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-21 12:59:05,214 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-21 12:59:05,216 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-21 12:59:05,217 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-21 12:59:05,220 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-21 12:59:05,222 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-21 12:59:05,226 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-21 12:59:05,227 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-21 12:59:05,229 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-21 12:59:05,231 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-21 12:59:05,233 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-21 12:59:05,234 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-21 12:59:05,235 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-21 12:59:05,236 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-21 12:59:05,239 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-21 12:59:05,240 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-21 12:59:05,242 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-21 12:59:05,244 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-21 12:59:05,245 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-21 12:59:05,252 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-21 12:59:05,253 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-21 12:59:05,257 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-21 12:59:05,259 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-21 12:59:05,260 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-21 12:59:05,261 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-21 12:59:05,262 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-21 12:59:05,265 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-21 12:59:05,266 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-21 12:59:05,268 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-21 12:59:05,270 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-21 12:59:05,271 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-21 12:59:05,289 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-21 12:59:05,290 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-21 12:59:05,292 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-21 12:59:05,294 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-21 12:59:05,295 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-21 12:59:05,312 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-21 12:59:05,352 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-21 12:59:05,353 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-21 12:59:05,355 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-21 12:59:05,356 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-21 12:59:05,364 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-21 12:59:05,380 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-21 12:59:05,388 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-21 12:59:05,404 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-21 12:59:05,423 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-21 12:59:05,425 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-21 12:59:05,444 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-21 12:59:05,464 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-21 12:59:05,480 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-21 12:59:05,496 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-21 12:59:05,512 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-21 12:59:05,576 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-21 12:59:05,577 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-21 12:59:05,578 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-21 12:59:05,676 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-21 12:59:05,721 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-21 12:59:05,723 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-21 12:59:05,784 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-21 12:59:05,786 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-21 12:59:05,862 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-21 12:59:05,865 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-21 12:59:05,866 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-21 12:59:05,950 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-21 12:59:05,952 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-21 12:59:05,954 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-21 12:59:05,955 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-21 12:59:06,018 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-21 12:59:06,021 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-21 12:59:06,023 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-21 12:59:06,058 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-21 12:59:06,093 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-21 12:59:06,095 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-21 12:59:06,108 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-21 12:59:06,114 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-21 12:59:06,170 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-21 12:59:06,172 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-21 12:59:06,174 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-21 12:59:06,176 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-21 12:59:06,233 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-21 12:59:06,235 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-21 12:59:06,236 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-21 12:59:06,281 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-21 12:59:06,283 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-21 12:59:06,285 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-21 12:59:06,343 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-21 12:59:06,346 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-21 12:59:06,347 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-21 12:59:06,349 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-21 12:59:06,389 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-21 12:59:06,419 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-21 12:59:06,436 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-21 12:59:06,459 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-21 12:59:06,462 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-21 12:59:06,463 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-21 12:59:06,490 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-21 12:59:06,492 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-21 12:59:06,535 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-21 12:59:06,536 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-21 12:59:06,538 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-21 12:59:06,541 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-21 12:59:06,578 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-21 12:59:06,581 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-21 12:59:06,586 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-21 12:59:06,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,636 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,638 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,672 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,673 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,682 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,700 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,702 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,705 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,888 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:06,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:06,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,037 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,119 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,121 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,716 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:07,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:07,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:08,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:08,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:08,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:08,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:08,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:08,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:08,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:08,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 12:59:08,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:08,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 12:59:13,083 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 12:59:13,114 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-21 12:59:13,539 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 7317 docs
2025-12-21 13:00:15,410 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:00:19,794 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-21 13:00:19,799 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-21 13:00:19,802 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-21 13:00:19,806 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-21 13:00:19,812 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-21 13:00:19,820 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-21 13:00:19,824 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-21 13:00:19,827 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-21 13:00:19,835 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-21 13:00:19,841 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-21 13:00:19,849 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-21 13:00:19,855 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-21 13:00:19,862 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-21 13:00:19,867 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-21 13:00:19,926 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-21 13:00:20,164 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-21 13:00:20,192 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-21 13:00:20,467 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-21 13:00:20,515 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-21 13:00:20,658 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-21 13:00:20,773 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-21 13:00:21,218 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-21 13:00:21,321 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-21 13:00:21,441 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-21 13:00:21,492 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-21 13:00:21,555 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-21 13:00:21,630 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-21 13:00:21,792 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-21 13:00:21,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,901 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,906 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,906 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,907 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,907 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,908 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,908 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,908 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,909 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:21,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:21,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,060 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,689 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,710 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:22,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:22,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,053 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:23,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:23,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:24,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:24,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:24,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:24,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:24,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:24,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:24,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:24,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:25,054 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,069 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:25,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:25,291 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:25,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:25,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:25,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:25,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:25,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:30,272 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:00:30,476 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-21 13:00:31,401 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:00:35,854 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-21 13:00:35,856 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-21 13:00:35,857 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-21 13:00:35,858 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-21 13:00:35,859 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-21 13:00:35,861 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-21 13:00:35,861 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-21 13:00:35,862 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-21 13:00:35,864 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-21 13:00:35,865 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-21 13:00:35,866 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-21 13:00:35,867 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-21 13:00:35,868 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-21 13:00:35,870 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-21 13:00:35,871 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-21 13:00:35,873 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-21 13:00:35,874 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-21 13:00:35,879 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-21 13:00:35,881 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-21 13:00:35,882 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-21 13:00:35,885 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-21 13:00:35,887 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-21 13:00:35,888 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-21 13:00:35,890 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-21 13:00:35,891 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-21 13:00:35,894 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-21 13:00:35,895 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-21 13:00:35,898 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-21 13:00:35,900 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-21 13:00:35,902 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-21 13:00:35,908 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-21 13:00:35,910 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-21 13:00:35,912 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-21 13:00:35,914 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-21 13:00:35,916 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-21 13:00:35,917 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-21 13:00:35,919 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-21 13:00:35,921 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-21 13:00:35,923 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-21 13:00:35,925 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-21 13:00:35,927 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-21 13:00:35,929 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-21 13:00:35,931 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-21 13:00:35,933 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-21 13:00:35,935 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-21 13:00:35,938 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-21 13:00:35,956 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-21 13:00:35,958 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-21 13:00:35,959 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-21 13:00:35,961 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-21 13:00:35,963 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-21 13:00:35,964 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-21 13:00:35,966 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-21 13:00:35,967 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-21 13:00:35,970 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-21 13:00:35,972 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-21 13:00:35,988 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-21 13:00:35,990 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-21 13:00:35,992 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-21 13:00:35,994 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-21 13:00:35,995 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-21 13:00:35,999 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-21 13:00:36,001 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-21 13:00:36,002 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-21 13:00:36,012 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-21 13:00:36,014 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-21 13:00:36,016 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-21 13:00:36,019 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-21 13:00:36,024 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-21 13:00:36,026 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-21 13:00:36,027 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-21 13:00:36,129 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-21 13:00:36,227 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-21 13:00:36,229 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-21 13:00:36,232 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-21 13:00:36,380 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-21 13:00:36,461 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-21 13:00:36,463 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-21 13:00:36,571 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-21 13:00:36,573 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-21 13:00:36,576 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-21 13:00:36,689 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-21 13:00:36,691 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-21 13:00:36,693 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-21 13:00:36,698 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-21 13:00:36,833 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-21 13:00:36,890 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-21 13:00:36,904 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-21 13:00:36,906 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-21 13:00:37,027 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-21 13:00:37,029 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-21 13:00:37,031 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-21 13:00:37,123 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-21 13:00:37,185 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-21 13:00:37,201 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-21 13:00:37,358 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-21 13:00:37,365 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-21 13:00:37,421 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-21 13:00:37,479 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-21 13:00:37,589 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-21 13:00:37,592 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-21 13:00:37,594 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-21 13:00:37,682 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-21 13:00:37,739 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-21 13:00:37,743 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-21 13:00:37,745 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-21 13:00:37,866 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-21 13:00:37,880 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-21 13:00:37,963 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-21 13:00:37,976 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-21 13:00:38,049 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-21 13:00:38,053 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-21 13:00:38,055 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-21 13:00:38,131 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-21 13:00:38,132 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-21 13:00:38,151 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,153 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,154 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,156 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,156 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,172 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,172 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,197 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,779 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,789 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:38,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:38,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,047 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,210 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,281 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,770 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:39,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:39,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,075 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,562 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,580 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,766 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,845 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:40,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:40,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:00:41,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:00:45,662 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:00:45,810 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-21 13:00:46,522 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 7317 docs
Training CobwebTree:   0%|          | 0/7317 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 17/7317 [00:00<00:48, 152.06it/s]Training CobwebTree:   0%|          | 33/7317 [00:00<00:59, 122.20it/s]Training CobwebTree:   1%|          | 46/7317 [00:00<01:10, 103.23it/s]Training CobwebTree:   1%|          | 57/7317 [00:00<01:16, 94.36it/s] Training CobwebTree:   1%|          | 67/7317 [00:00<01:23, 87.24it/s]Training CobwebTree:   1%|          | 76/7317 [00:00<01:26, 83.59it/s]Training CobwebTree:   1%|          | 85/7317 [00:00<01:33, 77.19it/s]Training CobwebTree:   1%|         | 93/7317 [00:01<01:39, 72.35it/s]Training CobwebTree:   1%|         | 101/7317 [00:01<01:43, 69.89it/s]Training CobwebTree:   1%|         | 109/7317 [00:01<01:43, 69.43it/s]Training CobwebTree:   2%|         | 116/7317 [00:01<01:46, 67.52it/s]Training CobwebTree:   2%|         | 124/7317 [00:01<01:43, 69.42it/s]Training CobwebTree:   2%|         | 131/7317 [00:01<01:53, 63.38it/s]Training CobwebTree:   2%|         | 139/7317 [00:01<01:48, 66.09it/s]Training CobwebTree:   2%|         | 147/7317 [00:01<01:46, 67.52it/s]Training CobwebTree:   2%|         | 154/7317 [00:02<01:50, 64.71it/s]Training CobwebTree:   2%|         | 161/7317 [00:02<01:53, 63.15it/s]Training CobwebTree:   2%|         | 168/7317 [00:02<01:52, 63.58it/s]Training CobwebTree:   2%|         | 175/7317 [00:02<01:55, 61.95it/s]Training CobwebTree:   2%|         | 182/7317 [00:02<01:56, 61.33it/s]Training CobwebTree:   3%|         | 189/7317 [00:02<01:58, 60.38it/s]Training CobwebTree:   3%|         | 196/7317 [00:02<02:03, 57.86it/s]Training CobwebTree:   3%|         | 202/7317 [00:02<02:08, 55.35it/s]Training CobwebTree:   3%|         | 208/7317 [00:02<02:09, 54.86it/s]Training CobwebTree:   3%|         | 215/7317 [00:03<02:01, 58.40it/s]Training CobwebTree:   3%|         | 221/7317 [00:03<02:02, 57.73it/s]Training CobwebTree:   3%|         | 228/7317 [00:03<01:56, 60.61it/s]Training CobwebTree:   3%|         | 235/7317 [00:03<02:01, 58.50it/s]Training CobwebTree:   3%|         | 241/7317 [00:03<02:00, 58.64it/s]Training CobwebTree:   3%|         | 247/7317 [00:03<02:10, 54.03it/s]Training CobwebTree:   3%|         | 253/7317 [00:03<02:12, 53.39it/s]Training CobwebTree:   4%|         | 260/7317 [00:03<02:06, 55.81it/s]Training CobwebTree:   4%|         | 266/7317 [00:03<02:07, 55.27it/s]Training CobwebTree:   4%|         | 272/7317 [00:04<02:10, 54.11it/s]Training CobwebTree:   4%|         | 278/7317 [00:04<02:06, 55.55it/s]Training CobwebTree:   4%|         | 284/7317 [00:04<02:12, 53.28it/s]Training CobwebTree:   4%|         | 291/7317 [00:04<02:05, 56.20it/s]Training CobwebTree:   4%|         | 297/7317 [00:04<02:03, 56.64it/s]Training CobwebTree:   4%|         | 303/7317 [00:04<02:04, 56.13it/s]Training CobwebTree:   4%|         | 309/7317 [00:04<02:03, 56.71it/s]Training CobwebTree:   4%|         | 315/7317 [00:04<02:06, 55.17it/s]Training CobwebTree:   4%|         | 322/7317 [00:04<02:01, 57.49it/s]Training CobwebTree:   4%|         | 328/7317 [00:05<02:06, 55.12it/s]Training CobwebTree:   5%|         | 334/7317 [00:05<02:09, 53.99it/s]Training CobwebTree:   5%|         | 340/7317 [00:05<02:08, 54.19it/s]Training CobwebTree:   5%|         | 346/7317 [00:05<02:11, 52.91it/s]Training CobwebTree:   5%|         | 352/7317 [00:05<02:14, 51.96it/s]Training CobwebTree:   5%|         | 358/7317 [00:05<02:11, 52.79it/s]Training CobwebTree:   5%|         | 364/7317 [00:05<02:21, 49.29it/s]Training CobwebTree:   5%|         | 369/7317 [00:05<02:22, 48.89it/s]Training CobwebTree:   5%|         | 375/7317 [00:06<02:17, 50.48it/s]Training CobwebTree:   5%|         | 381/7317 [00:06<02:14, 51.62it/s]Training CobwebTree:   5%|         | 387/7317 [00:06<02:11, 52.85it/s]Training CobwebTree:   5%|         | 393/7317 [00:06<02:14, 51.63it/s]Training CobwebTree:   5%|         | 399/7317 [00:06<02:17, 50.25it/s]Training CobwebTree:   6%|         | 405/7317 [00:06<02:19, 49.57it/s]Training CobwebTree:   6%|         | 410/7317 [00:06<02:25, 47.34it/s]Training CobwebTree:   6%|         | 415/7317 [00:06<02:24, 47.85it/s]Training CobwebTree:   6%|         | 421/7317 [00:06<02:19, 49.46it/s]Training CobwebTree:   6%|         | 427/7317 [00:07<02:15, 50.74it/s]Training CobwebTree:   6%|         | 433/7317 [00:07<02:11, 52.51it/s]Training CobwebTree:   6%|         | 439/7317 [00:07<02:14, 51.04it/s]Training CobwebTree:   6%|         | 445/7317 [00:07<02:16, 50.35it/s]Training CobwebTree:   6%|         | 451/7317 [00:07<02:25, 47.30it/s]Training CobwebTree:   6%|         | 456/7317 [00:07<02:23, 47.73it/s]Training CobwebTree:   6%|         | 461/7317 [00:07<02:22, 48.19it/s]Training CobwebTree:   6%|         | 466/7317 [00:07<02:28, 46.04it/s]Training CobwebTree:   6%|         | 471/7317 [00:08<02:30, 45.59it/s]Training CobwebTree:   7%|         | 476/7317 [00:08<02:28, 46.17it/s]Training CobwebTree:   7%|         | 481/7317 [00:08<02:27, 46.37it/s]Training CobwebTree:   7%|         | 486/7317 [00:08<02:30, 45.28it/s]Training CobwebTree:   7%|         | 491/7317 [00:08<02:30, 45.50it/s]Training CobwebTree:   7%|         | 496/7317 [00:08<02:29, 45.68it/s]Training CobwebTree:   7%|         | 501/7317 [00:08<02:28, 46.03it/s]Training CobwebTree:   7%|         | 507/7317 [00:08<02:27, 46.30it/s]Training CobwebTree:   7%|         | 513/7317 [00:08<02:18, 49.26it/s]Training CobwebTree:   7%|         | 518/7317 [00:08<02:20, 48.38it/s]Training CobwebTree:   7%|         | 524/7317 [00:09<02:14, 50.41it/s]Training CobwebTree:   7%|         | 530/7317 [00:09<02:17, 49.51it/s]Training CobwebTree:   7%|         | 535/7317 [00:09<02:17, 49.37it/s]Training CobwebTree:   7%|         | 540/7317 [00:09<02:22, 47.65it/s]Training CobwebTree:   7%|         | 545/7317 [00:09<02:30, 44.99it/s]Training CobwebTree:   8%|         | 550/7317 [00:09<02:29, 45.38it/s]Training CobwebTree:   8%|         | 555/7317 [00:09<02:27, 45.90it/s]Training CobwebTree:   8%|         | 560/7317 [00:09<02:24, 46.67it/s]Training CobwebTree:   8%|         | 565/7317 [00:09<02:25, 46.45it/s]Training CobwebTree:   8%|         | 571/7317 [00:10<02:21, 47.60it/s]Training CobwebTree:   8%|         | 577/7317 [00:10<02:17, 49.08it/s]Training CobwebTree:   8%|         | 582/7317 [00:10<02:18, 48.68it/s]Training CobwebTree:   8%|         | 588/7317 [00:10<02:14, 50.07it/s]Training CobwebTree:   8%|         | 594/7317 [00:10<02:08, 52.31it/s]Training CobwebTree:   8%|         | 600/7317 [00:10<02:03, 54.17it/s]Training CobwebTree:   8%|         | 606/7317 [00:10<02:09, 51.95it/s]Training CobwebTree:   8%|         | 612/7317 [00:10<02:08, 52.15it/s]Training CobwebTree:   8%|         | 618/7317 [00:11<02:10, 51.48it/s]Training CobwebTree:   9%|         | 624/7317 [00:11<02:09, 51.79it/s]Training CobwebTree:   9%|         | 630/7317 [00:11<02:08, 52.04it/s]Training CobwebTree:   9%|         | 636/7317 [00:11<02:15, 49.16it/s]Training CobwebTree:   9%|         | 641/7317 [00:11<02:18, 48.12it/s]Training CobwebTree:   9%|         | 647/7317 [00:11<02:16, 48.98it/s]Training CobwebTree:   9%|         | 652/7317 [00:11<02:16, 48.75it/s]Training CobwebTree:   9%|         | 658/7317 [00:11<02:16, 48.73it/s]Training CobwebTree:   9%|         | 663/7317 [00:11<02:17, 48.32it/s]Training CobwebTree:   9%|         | 668/7317 [00:12<02:19, 47.61it/s]Training CobwebTree:   9%|         | 674/7317 [00:12<02:14, 49.49it/s]Training CobwebTree:   9%|         | 679/7317 [00:12<02:16, 48.47it/s]Training CobwebTree:   9%|         | 684/7317 [00:12<02:20, 47.09it/s]Training CobwebTree:   9%|         | 690/7317 [00:12<02:16, 48.41it/s]Training CobwebTree:  10%|         | 696/7317 [00:12<02:14, 49.12it/s]Training CobwebTree:  10%|         | 701/7317 [00:12<02:19, 47.33it/s]Training CobwebTree:  10%|         | 706/7317 [00:12<02:22, 46.53it/s]Training CobwebTree:  10%|         | 711/7317 [00:12<02:20, 47.09it/s]Training CobwebTree:  10%|         | 716/7317 [00:13<02:24, 45.61it/s]Training CobwebTree:  10%|         | 722/7317 [00:13<02:15, 48.55it/s]Training CobwebTree:  10%|         | 727/7317 [00:13<02:15, 48.71it/s]Training CobwebTree:  10%|         | 733/7317 [00:13<02:11, 50.15it/s]Training CobwebTree:  10%|         | 739/7317 [00:13<02:17, 47.81it/s]Training CobwebTree:  10%|         | 745/7317 [00:13<02:15, 48.40it/s]Training CobwebTree:  10%|         | 750/7317 [00:13<02:17, 47.77it/s]Training CobwebTree:  10%|         | 755/7317 [00:13<02:21, 46.52it/s]Training CobwebTree:  10%|         | 760/7317 [00:13<02:24, 45.49it/s]Training CobwebTree:  10%|         | 765/7317 [00:14<02:24, 45.41it/s]Training CobwebTree:  11%|         | 770/7317 [00:14<02:25, 44.90it/s]Training CobwebTree:  11%|         | 775/7317 [00:14<02:24, 45.12it/s]Training CobwebTree:  11%|         | 781/7317 [00:14<02:21, 46.31it/s]Training CobwebTree:  11%|         | 787/7317 [00:14<02:17, 47.60it/s]Training CobwebTree:  11%|         | 792/7317 [00:14<02:19, 46.76it/s]Training CobwebTree:  11%|         | 797/7317 [00:14<02:24, 44.99it/s]Training CobwebTree:  11%|         | 802/7317 [00:14<02:24, 45.01it/s]Training CobwebTree:  11%|         | 807/7317 [00:15<02:24, 45.05it/s]Training CobwebTree:  11%|         | 812/7317 [00:15<02:28, 43.69it/s]Training CobwebTree:  11%|         | 817/7317 [00:15<02:23, 45.24it/s]Training CobwebTree:  11%|         | 823/7317 [00:15<02:16, 47.56it/s]Training CobwebTree:  11%|        | 828/7317 [00:15<02:22, 45.49it/s]Training CobwebTree:  11%|        | 834/7317 [00:15<02:18, 46.94it/s]Training CobwebTree:  11%|        | 839/7317 [00:15<02:16, 47.61it/s]Training CobwebTree:  12%|        | 845/7317 [00:15<02:09, 50.07it/s]Training CobwebTree:  12%|        | 851/7317 [00:15<02:12, 48.70it/s]Training CobwebTree:  12%|        | 856/7317 [00:16<02:16, 47.36it/s]Training CobwebTree:  12%|        | 861/7317 [00:16<02:20, 46.09it/s]Training CobwebTree:  12%|        | 866/7317 [00:16<02:22, 45.14it/s]Training CobwebTree:  12%|        | 871/7317 [00:16<02:24, 44.71it/s]Training CobwebTree:  12%|        | 876/7317 [00:16<02:20, 45.69it/s]Training CobwebTree:  12%|        | 881/7317 [00:16<02:22, 45.11it/s]Training CobwebTree:  12%|        | 886/7317 [00:16<02:22, 45.26it/s]Training CobwebTree:  12%|        | 891/7317 [00:16<02:24, 44.33it/s]Training CobwebTree:  12%|        | 897/7317 [00:16<02:17, 46.64it/s]Training CobwebTree:  12%|        | 902/7317 [00:17<02:18, 46.18it/s]Training CobwebTree:  12%|        | 907/7317 [00:17<02:17, 46.77it/s]Training CobwebTree:  12%|        | 912/7317 [00:17<02:18, 46.09it/s]Training CobwebTree:  13%|        | 917/7317 [00:17<02:27, 43.48it/s]Training CobwebTree:  13%|        | 922/7317 [00:17<02:27, 43.43it/s]Training CobwebTree:  13%|        | 927/7317 [00:17<02:24, 44.32it/s]Training CobwebTree:  13%|        | 932/7317 [00:17<02:26, 43.64it/s]Training CobwebTree:  13%|        | 937/7317 [00:17<02:25, 43.90it/s]Training CobwebTree:  13%|        | 942/7317 [00:17<02:28, 42.82it/s]Training CobwebTree:  13%|        | 947/7317 [00:18<02:22, 44.73it/s]Training CobwebTree:  13%|        | 952/7317 [00:18<02:22, 44.72it/s]Training CobwebTree:  13%|        | 957/7317 [00:18<02:23, 44.39it/s]Training CobwebTree:  13%|        | 962/7317 [00:18<02:18, 45.74it/s]Training CobwebTree:  13%|        | 967/7317 [00:18<02:19, 45.47it/s]Training CobwebTree:  13%|        | 972/7317 [00:18<02:19, 45.34it/s]Training CobwebTree:  13%|        | 977/7317 [00:18<02:17, 46.02it/s]Training CobwebTree:  13%|        | 983/7317 [00:18<02:14, 47.06it/s]Training CobwebTree:  14%|        | 988/7317 [00:19<02:27, 43.03it/s]Training CobwebTree:  14%|        | 993/7317 [00:19<02:24, 43.70it/s]Training CobwebTree:  14%|        | 998/7317 [00:19<02:20, 45.02it/s]Training CobwebTree:  14%|        | 1003/7317 [00:19<02:18, 45.64it/s]Training CobwebTree:  14%|        | 1008/7317 [00:19<02:16, 46.14it/s]Training CobwebTree:  14%|        | 1013/7317 [00:19<02:16, 46.10it/s]Training CobwebTree:  14%|        | 1018/7317 [00:19<02:19, 45.19it/s]Training CobwebTree:  14%|        | 1023/7317 [00:19<02:22, 44.12it/s]Training CobwebTree:  14%|        | 1028/7317 [00:19<02:18, 45.51it/s]Training CobwebTree:  14%|        | 1033/7317 [00:20<02:23, 43.66it/s]Training CobwebTree:  14%|        | 1038/7317 [00:20<02:30, 41.60it/s]Training CobwebTree:  14%|        | 1043/7317 [00:20<02:30, 41.69it/s]Training CobwebTree:  14%|        | 1048/7317 [00:20<02:31, 41.49it/s]Training CobwebTree:  14%|        | 1053/7317 [00:20<02:33, 40.89it/s]Training CobwebTree:  14%|        | 1058/7317 [00:20<02:33, 40.87it/s]Training CobwebTree:  15%|        | 1063/7317 [00:20<02:31, 41.36it/s]Training CobwebTree:  15%|        | 1068/7317 [00:20<02:26, 42.64it/s]Training CobwebTree:  15%|        | 1074/7317 [00:20<02:15, 45.95it/s]Training CobwebTree:  15%|        | 1079/7317 [00:21<02:23, 43.60it/s]Training CobwebTree:  15%|        | 1084/7317 [00:21<02:26, 42.50it/s]Training CobwebTree:  15%|        | 1089/7317 [00:21<02:25, 42.92it/s]Training CobwebTree:  15%|        | 1094/7317 [00:21<02:31, 40.95it/s]Training CobwebTree:  15%|        | 1099/7317 [00:21<02:29, 41.65it/s]Training CobwebTree:  15%|        | 1104/7317 [00:21<02:36, 39.59it/s]Training CobwebTree:  15%|        | 1109/7317 [00:21<02:31, 41.06it/s]Training CobwebTree:  15%|        | 1114/7317 [00:21<02:28, 41.74it/s]Training CobwebTree:  15%|        | 1119/7317 [00:22<02:24, 42.91it/s]Training CobwebTree:  15%|        | 1125/7317 [00:22<02:15, 45.57it/s]Training CobwebTree:  15%|        | 1130/7317 [00:22<02:13, 46.25it/s]Training CobwebTree:  16%|        | 1135/7317 [00:22<02:19, 44.40it/s]Training CobwebTree:  16%|        | 1140/7317 [00:22<02:20, 44.03it/s]Training CobwebTree:  16%|        | 1145/7317 [00:22<02:24, 42.78it/s]Training CobwebTree:  16%|        | 1150/7317 [00:22<02:31, 40.59it/s]Training CobwebTree:  16%|        | 1155/7317 [00:22<02:30, 40.89it/s]Training CobwebTree:  16%|        | 1160/7317 [00:23<02:30, 40.96it/s]Training CobwebTree:  16%|        | 1165/7317 [00:23<02:34, 39.92it/s]Training CobwebTree:  16%|        | 1170/7317 [00:23<02:28, 41.33it/s]Training CobwebTree:  16%|        | 1175/7317 [00:23<02:26, 41.88it/s]Training CobwebTree:  16%|        | 1180/7317 [00:23<02:27, 41.65it/s]Training CobwebTree:  16%|        | 1185/7317 [00:23<02:25, 42.18it/s]Training CobwebTree:  16%|        | 1190/7317 [00:23<02:27, 41.56it/s]Training CobwebTree:  16%|        | 1195/7317 [00:23<02:24, 42.29it/s]Training CobwebTree:  16%|        | 1201/7317 [00:23<02:17, 44.57it/s]Training CobwebTree:  16%|        | 1206/7317 [00:24<02:17, 44.39it/s]Training CobwebTree:  17%|        | 1211/7317 [00:24<02:18, 44.22it/s]Training CobwebTree:  17%|        | 1216/7317 [00:24<02:33, 39.79it/s]Training CobwebTree:  17%|        | 1221/7317 [00:24<02:28, 41.07it/s]Training CobwebTree:  17%|        | 1227/7317 [00:24<02:19, 43.80it/s]Training CobwebTree:  17%|        | 1232/7317 [00:24<02:19, 43.68it/s]Training CobwebTree:  17%|        | 1237/7317 [00:24<02:17, 44.22it/s]Training CobwebTree:  17%|        | 1242/7317 [00:24<02:15, 44.77it/s]Training CobwebTree:  17%|        | 1247/7317 [00:25<02:21, 43.02it/s]Training CobwebTree:  17%|        | 1252/7317 [00:25<02:26, 41.34it/s]Training CobwebTree:  17%|        | 1257/7317 [00:25<02:25, 41.76it/s]Training CobwebTree:  17%|        | 1262/7317 [00:25<02:29, 40.44it/s]Training CobwebTree:  17%|        | 1267/7317 [00:25<02:34, 39.07it/s]Training CobwebTree:  17%|        | 1272/7317 [00:25<02:29, 40.55it/s]Training CobwebTree:  17%|        | 1277/7317 [00:25<02:25, 41.51it/s]Training CobwebTree:  18%|        | 1282/7317 [00:25<02:24, 41.90it/s]Training CobwebTree:  18%|        | 1287/7317 [00:26<02:20, 42.87it/s]Training CobwebTree:  18%|        | 1292/7317 [00:26<02:24, 41.78it/s]Training CobwebTree:  18%|        | 1297/7317 [00:26<02:22, 42.34it/s]Training CobwebTree:  18%|        | 1302/7317 [00:26<02:18, 43.54it/s]Training CobwebTree:  18%|        | 1307/7317 [00:26<02:15, 44.22it/s]Training CobwebTree:  18%|        | 1312/7317 [00:26<02:20, 42.67it/s]Training CobwebTree:  18%|        | 1317/7317 [00:26<02:19, 42.97it/s]Training CobwebTree:  18%|        | 1322/7317 [00:26<02:28, 40.32it/s]Training CobwebTree:  18%|        | 1327/7317 [00:26<02:26, 40.86it/s]Training CobwebTree:  18%|        | 1332/7317 [00:27<02:22, 41.88it/s]Training CobwebTree:  18%|        | 1337/7317 [00:27<02:24, 41.42it/s]Training CobwebTree:  18%|        | 1342/7317 [00:27<02:22, 41.84it/s]Training CobwebTree:  18%|        | 1347/7317 [00:27<02:24, 41.29it/s]Training CobwebTree:  18%|        | 1352/7317 [00:27<02:19, 42.65it/s]Training CobwebTree:  19%|        | 1357/7317 [00:27<02:20, 42.51it/s]Training CobwebTree:  19%|        | 1362/7317 [00:27<02:22, 41.70it/s]Training CobwebTree:  19%|        | 1367/7317 [00:27<02:22, 41.80it/s]Training CobwebTree:  19%|        | 1372/7317 [00:28<02:19, 42.52it/s]Training CobwebTree:  19%|        | 1377/7317 [00:28<02:17, 43.34it/s]Training CobwebTree:  19%|        | 1382/7317 [00:28<02:18, 42.88it/s]Training CobwebTree:  19%|        | 1387/7317 [00:28<02:16, 43.36it/s]Training CobwebTree:  19%|        | 1392/7317 [00:28<02:11, 44.89it/s]Training CobwebTree:  19%|        | 1397/7317 [00:28<02:11, 44.91it/s]Training CobwebTree:  19%|        | 1402/7317 [00:28<02:10, 45.40it/s]Training CobwebTree:  19%|        | 1407/7317 [00:28<02:14, 43.91it/s]Training CobwebTree:  19%|        | 1412/7317 [00:28<02:16, 43.34it/s]Training CobwebTree:  19%|        | 1417/7317 [00:29<02:21, 41.71it/s]Training CobwebTree:  19%|        | 1422/7317 [00:29<02:21, 41.77it/s]Training CobwebTree:  20%|        | 1427/7317 [00:29<02:24, 40.87it/s]Training CobwebTree:  20%|        | 1432/7317 [00:29<02:20, 41.98it/s]Training CobwebTree:  20%|        | 1437/7317 [00:29<02:15, 43.40it/s]Training CobwebTree:  20%|        | 1442/7317 [00:29<02:16, 42.92it/s]Training CobwebTree:  20%|        | 1447/7317 [00:29<02:21, 41.56it/s]Training CobwebTree:  20%|        | 1452/7317 [00:29<02:19, 42.02it/s]Training CobwebTree:  20%|        | 1457/7317 [00:30<02:20, 41.66it/s]Training CobwebTree:  20%|        | 1462/7317 [00:30<02:21, 41.29it/s]Training CobwebTree:  20%|        | 1467/7317 [00:30<02:28, 39.43it/s]Training CobwebTree:  20%|        | 1472/7317 [00:30<02:25, 40.10it/s]Training CobwebTree:  20%|        | 1477/7317 [00:30<02:26, 39.88it/s]Training CobwebTree:  20%|        | 1482/7317 [00:30<02:19, 41.89it/s]Training CobwebTree:  20%|        | 1487/7317 [00:30<02:12, 43.99it/s]Training CobwebTree:  20%|        | 1492/7317 [00:30<02:12, 44.00it/s]Training CobwebTree:  20%|        | 1497/7317 [00:30<02:13, 43.75it/s]Training CobwebTree:  21%|        | 1502/7317 [00:31<02:17, 42.16it/s]Training CobwebTree:  21%|        | 1507/7317 [00:31<02:17, 42.16it/s]Training CobwebTree:  21%|        | 1512/7317 [00:31<02:20, 41.43it/s]Training CobwebTree:  21%|        | 1517/7317 [00:31<02:15, 42.69it/s]Training CobwebTree:  21%|        | 1522/7317 [00:31<02:16, 42.48it/s]Training CobwebTree:  21%|        | 1527/7317 [00:31<02:17, 42.01it/s]Training CobwebTree:  21%|        | 1532/7317 [00:31<02:20, 41.09it/s]Training CobwebTree:  21%|        | 1537/7317 [00:31<02:22, 40.53it/s]Training CobwebTree:  21%|        | 1542/7317 [00:32<02:33, 37.61it/s]Training CobwebTree:  21%|        | 1547/7317 [00:32<02:33, 37.64it/s]Training CobwebTree:  21%|        | 1552/7317 [00:32<02:23, 40.09it/s]Training CobwebTree:  21%|       | 1557/7317 [00:32<02:23, 40.25it/s]Training CobwebTree:  21%|       | 1562/7317 [00:32<02:27, 39.10it/s]Training CobwebTree:  21%|       | 1568/7317 [00:32<02:13, 43.07it/s]Training CobwebTree:  21%|       | 1573/7317 [00:32<02:17, 41.78it/s]Training CobwebTree:  22%|       | 1578/7317 [00:32<02:17, 41.85it/s]Training CobwebTree:  22%|       | 1583/7317 [00:33<02:24, 39.67it/s]Training CobwebTree:  22%|       | 1588/7317 [00:33<02:25, 39.43it/s]Training CobwebTree:  22%|       | 1592/7317 [00:33<02:29, 38.39it/s]Training CobwebTree:  22%|       | 1596/7317 [00:33<02:27, 38.71it/s]Training CobwebTree:  22%|       | 1600/7317 [00:33<02:27, 38.88it/s]Training CobwebTree:  22%|       | 1605/7317 [00:33<02:19, 41.01it/s]Training CobwebTree:  22%|       | 1610/7317 [00:33<02:24, 39.56it/s]Training CobwebTree:  22%|       | 1615/7317 [00:33<02:21, 40.20it/s]Training CobwebTree:  22%|       | 1620/7317 [00:34<02:18, 41.13it/s]Training CobwebTree:  22%|       | 1625/7317 [00:34<02:17, 41.43it/s]Training CobwebTree:  22%|       | 1630/7317 [00:34<02:17, 41.42it/s]Training CobwebTree:  22%|       | 1635/7317 [00:34<02:18, 40.95it/s]Training CobwebTree:  22%|       | 1640/7317 [00:34<02:21, 39.99it/s]Training CobwebTree:  22%|       | 1645/7317 [00:34<02:21, 40.20it/s]Training CobwebTree:  23%|       | 1650/7317 [00:34<02:24, 39.11it/s]Training CobwebTree:  23%|       | 1654/7317 [00:34<02:27, 38.47it/s]Training CobwebTree:  23%|       | 1658/7317 [00:35<02:30, 37.64it/s]Training CobwebTree:  23%|       | 1663/7317 [00:35<02:27, 38.46it/s]Training CobwebTree:  23%|       | 1668/7317 [00:35<02:18, 40.82it/s]Training CobwebTree:  23%|       | 1673/7317 [00:35<02:15, 41.60it/s]Training CobwebTree:  23%|       | 1678/7317 [00:35<02:16, 41.32it/s]Training CobwebTree:  23%|       | 1683/7317 [00:35<02:11, 42.89it/s]Training CobwebTree:  23%|       | 1688/7317 [00:35<02:21, 39.73it/s]Training CobwebTree:  23%|       | 1693/7317 [00:35<02:23, 39.28it/s]Training CobwebTree:  23%|       | 1698/7317 [00:35<02:20, 40.10it/s]Training CobwebTree:  23%|       | 1703/7317 [00:36<02:20, 40.01it/s]Training CobwebTree:  23%|       | 1708/7317 [00:36<02:20, 39.96it/s]Training CobwebTree:  23%|       | 1713/7317 [00:36<02:16, 40.91it/s]Training CobwebTree:  23%|       | 1718/7317 [00:36<02:12, 42.22it/s]Training CobwebTree:  24%|       | 1723/7317 [00:36<02:11, 42.40it/s]Training CobwebTree:  24%|       | 1728/7317 [00:36<02:12, 42.15it/s]Training CobwebTree:  24%|       | 1733/7317 [00:36<02:12, 42.07it/s]Training CobwebTree:  24%|       | 1738/7317 [00:36<02:19, 39.93it/s]Training CobwebTree:  24%|       | 1743/7317 [00:37<02:22, 39.23it/s]Training CobwebTree:  24%|       | 1747/7317 [00:37<02:26, 38.02it/s]Training CobwebTree:  24%|       | 1751/7317 [00:37<02:25, 38.20it/s]Training CobwebTree:  24%|       | 1755/7317 [00:37<02:23, 38.67it/s]Training CobwebTree:  24%|       | 1759/7317 [00:37<02:27, 37.75it/s]Training CobwebTree:  24%|       | 1763/7317 [00:37<02:26, 37.99it/s]Training CobwebTree:  24%|       | 1768/7317 [00:37<02:21, 39.35it/s]Training CobwebTree:  24%|       | 1773/7317 [00:37<02:19, 39.81it/s]Training CobwebTree:  24%|       | 1777/7317 [00:37<02:22, 38.97it/s]Training CobwebTree:  24%|       | 1782/7317 [00:38<02:17, 40.32it/s]Training CobwebTree:  24%|       | 1787/7317 [00:38<02:19, 39.71it/s]Training CobwebTree:  24%|       | 1792/7317 [00:38<02:15, 40.69it/s]Training CobwebTree:  25%|       | 1797/7317 [00:38<02:22, 38.85it/s]Training CobwebTree:  25%|       | 1801/7317 [00:38<02:23, 38.37it/s]Training CobwebTree:  25%|       | 1805/7317 [00:38<02:25, 38.00it/s]Training CobwebTree:  25%|       | 1810/7317 [00:38<02:23, 38.48it/s]Training CobwebTree:  25%|       | 1814/7317 [00:38<02:23, 38.27it/s]Training CobwebTree:  25%|       | 1818/7317 [00:39<02:26, 37.51it/s]Training CobwebTree:  25%|       | 1823/7317 [00:39<02:23, 38.21it/s]Training CobwebTree:  25%|       | 1828/7317 [00:39<02:21, 38.67it/s]Training CobwebTree:  25%|       | 1832/7317 [00:39<02:27, 37.18it/s]Training CobwebTree:  25%|       | 1837/7317 [00:39<02:20, 38.94it/s]Training CobwebTree:  25%|       | 1842/7317 [00:39<02:17, 39.79it/s]Training CobwebTree:  25%|       | 1846/7317 [00:39<02:19, 39.24it/s]Training CobwebTree:  25%|       | 1851/7317 [00:39<02:12, 41.12it/s]Training CobwebTree:  25%|       | 1856/7317 [00:39<02:13, 40.98it/s]Training CobwebTree:  25%|       | 1861/7317 [00:40<02:11, 41.61it/s]Training CobwebTree:  26%|       | 1866/7317 [00:40<02:15, 40.33it/s]Training CobwebTree:  26%|       | 1871/7317 [00:40<02:17, 39.56it/s]Training CobwebTree:  26%|       | 1875/7317 [00:40<02:24, 37.56it/s]Training CobwebTree:  26%|       | 1880/7317 [00:40<02:23, 37.88it/s]Training CobwebTree:  26%|       | 1885/7317 [00:40<02:14, 40.30it/s]Training CobwebTree:  26%|       | 1890/7317 [00:40<02:14, 40.46it/s]Training CobwebTree:  26%|       | 1895/7317 [00:40<02:16, 39.72it/s]Training CobwebTree:  26%|       | 1900/7317 [00:41<02:15, 40.07it/s]Training CobwebTree:  26%|       | 1905/7317 [00:41<02:17, 39.42it/s]Training CobwebTree:  26%|       | 1910/7317 [00:41<02:13, 40.62it/s]Training CobwebTree:  26%|       | 1915/7317 [00:41<02:16, 39.70it/s]Training CobwebTree:  26%|       | 1919/7317 [00:41<02:17, 39.30it/s]Training CobwebTree:  26%|       | 1924/7317 [00:41<02:13, 40.30it/s]Training CobwebTree:  26%|       | 1929/7317 [00:41<02:16, 39.52it/s]Training CobwebTree:  26%|       | 1933/7317 [00:41<02:20, 38.39it/s]Training CobwebTree:  26%|       | 1938/7317 [00:42<02:13, 40.20it/s]Training CobwebTree:  27%|       | 1943/7317 [00:42<02:17, 39.02it/s]Training CobwebTree:  27%|       | 1948/7317 [00:42<02:13, 40.10it/s]Training CobwebTree:  27%|       | 1953/7317 [00:42<02:15, 39.54it/s]Training CobwebTree:  27%|       | 1957/7317 [00:42<02:17, 39.12it/s]Training CobwebTree:  27%|       | 1961/7317 [00:42<02:20, 38.00it/s]Training CobwebTree:  27%|       | 1965/7317 [00:42<02:25, 36.78it/s]Training CobwebTree:  27%|       | 1970/7317 [00:42<02:20, 38.05it/s]Training CobwebTree:  27%|       | 1975/7317 [00:43<02:19, 38.42it/s]Training CobwebTree:  27%|       | 1979/7317 [00:43<02:19, 38.22it/s]Training CobwebTree:  27%|       | 1983/7317 [00:43<02:26, 36.41it/s]Training CobwebTree:  27%|       | 1988/7317 [00:43<02:23, 37.26it/s]Training CobwebTree:  27%|       | 1992/7317 [00:43<02:24, 36.86it/s]Training CobwebTree:  27%|       | 1996/7317 [00:43<02:24, 36.79it/s]Training CobwebTree:  27%|       | 2001/7317 [00:43<02:20, 37.71it/s]Training CobwebTree:  27%|       | 2005/7317 [00:43<02:22, 37.23it/s]Training CobwebTree:  27%|       | 2009/7317 [00:43<02:22, 37.27it/s]Training CobwebTree:  28%|       | 2013/7317 [00:44<02:24, 36.65it/s]Training CobwebTree:  28%|       | 2018/7317 [00:44<02:18, 38.23it/s]Training CobwebTree:  28%|       | 2022/7317 [00:44<02:21, 37.35it/s]Training CobwebTree:  28%|       | 2026/7317 [00:44<02:20, 37.59it/s]Training CobwebTree:  28%|       | 2031/7317 [00:44<02:18, 38.19it/s]Training CobwebTree:  28%|       | 2036/7317 [00:44<02:16, 38.82it/s]Training CobwebTree:  28%|       | 2041/7317 [00:44<02:14, 39.15it/s]Training CobwebTree:  28%|       | 2046/7317 [00:44<02:14, 39.30it/s]Training CobwebTree:  28%|       | 2051/7317 [00:45<02:09, 40.76it/s]Training CobwebTree:  28%|       | 2056/7317 [00:45<02:09, 40.48it/s]Training CobwebTree:  28%|       | 2061/7317 [00:45<02:12, 39.59it/s]Training CobwebTree:  28%|       | 2065/7317 [00:45<02:15, 38.63it/s]Training CobwebTree:  28%|       | 2069/7317 [00:45<02:16, 38.55it/s]Training CobwebTree:  28%|       | 2074/7317 [00:45<02:13, 39.41it/s]Training CobwebTree:  28%|       | 2078/7317 [00:45<02:12, 39.42it/s]Training CobwebTree:  28%|       | 2082/7317 [00:45<02:15, 38.64it/s]Training CobwebTree:  29%|       | 2087/7317 [00:45<02:12, 39.32it/s]Training CobwebTree:  29%|       | 2091/7317 [00:46<02:15, 38.48it/s]Training CobwebTree:  29%|       | 2096/7317 [00:46<02:12, 39.37it/s]Training CobwebTree:  29%|       | 2100/7317 [00:46<02:17, 37.99it/s]Training CobwebTree:  29%|       | 2104/7317 [00:46<02:24, 35.96it/s]Training CobwebTree:  29%|       | 2108/7317 [00:46<02:24, 36.04it/s]Training CobwebTree:  29%|       | 2112/7317 [00:46<02:26, 35.51it/s]Training CobwebTree:  29%|       | 2117/7317 [00:46<02:20, 36.92it/s]Training CobwebTree:  29%|       | 2122/7317 [00:46<02:15, 38.23it/s]Training CobwebTree:  29%|       | 2127/7317 [00:47<02:15, 38.34it/s]Training CobwebTree:  29%|       | 2132/7317 [00:47<02:10, 39.76it/s]Training CobwebTree:  29%|       | 2137/7317 [00:47<02:09, 40.07it/s]Training CobwebTree:  29%|       | 2142/7317 [00:47<02:07, 40.64it/s]Training CobwebTree:  29%|       | 2147/7317 [00:47<02:10, 39.70it/s]Training CobwebTree:  29%|       | 2152/7317 [00:47<02:05, 41.13it/s]Training CobwebTree:  29%|       | 2157/7317 [00:47<02:07, 40.45it/s]Training CobwebTree:  30%|       | 2162/7317 [00:47<02:06, 40.86it/s]Training CobwebTree:  30%|       | 2167/7317 [00:48<02:16, 37.79it/s]Training CobwebTree:  30%|       | 2171/7317 [00:48<02:20, 36.62it/s]Training CobwebTree:  30%|       | 2175/7317 [00:48<02:18, 37.12it/s]Training CobwebTree:  30%|       | 2179/7317 [00:48<02:25, 35.26it/s]Training CobwebTree:  30%|       | 2183/7317 [00:48<02:24, 35.63it/s]Training CobwebTree:  30%|       | 2187/7317 [00:48<02:20, 36.39it/s]Training CobwebTree:  30%|       | 2192/7317 [00:48<02:15, 37.85it/s]Training CobwebTree:  30%|       | 2196/7317 [00:48<02:14, 38.03it/s]Training CobwebTree:  30%|       | 2201/7317 [00:48<02:09, 39.61it/s]Training CobwebTree:  30%|       | 2205/7317 [00:49<02:12, 38.49it/s]Training CobwebTree:  30%|       | 2209/7317 [00:49<02:14, 37.88it/s]Training CobwebTree:  30%|       | 2213/7317 [00:49<02:13, 38.27it/s]Training CobwebTree:  30%|       | 2217/7317 [00:49<02:17, 37.21it/s]Training CobwebTree:  30%|       | 2221/7317 [00:49<02:14, 37.84it/s]Training CobwebTree:  30%|       | 2226/7317 [00:49<02:10, 38.93it/s]Training CobwebTree:  30%|       | 2231/7317 [00:49<02:07, 40.01it/s]Training CobwebTree:  31%|       | 2235/7317 [00:49<02:07, 39.76it/s]Training CobwebTree:  31%|       | 2240/7317 [00:49<02:07, 39.79it/s]Training CobwebTree:  31%|       | 2244/7317 [00:50<02:07, 39.80it/s]Training CobwebTree:  31%|       | 2249/7317 [00:50<02:05, 40.31it/s]Training CobwebTree:  31%|       | 2254/7317 [00:50<02:11, 38.56it/s]Training CobwebTree:  31%|       | 2258/7317 [00:50<02:14, 37.49it/s]Training CobwebTree:  31%|       | 2262/7317 [00:50<02:16, 36.94it/s]Training CobwebTree:  31%|       | 2267/7317 [00:50<02:11, 38.36it/s]Training CobwebTree:  31%|       | 2272/7317 [00:50<02:08, 39.30it/s]Training CobwebTree:  31%|       | 2277/7317 [00:50<02:05, 40.16it/s]Training CobwebTree:  31%|       | 2282/7317 [00:51<02:07, 39.49it/s]Training CobwebTree:  31%|       | 2286/7317 [00:51<02:09, 38.90it/s]Training CobwebTree:  31%|      | 2290/7317 [00:51<02:12, 38.04it/s]Training CobwebTree:  31%|      | 2295/7317 [00:51<02:09, 38.82it/s]Training CobwebTree:  31%|      | 2300/7317 [00:51<02:09, 38.75it/s]Training CobwebTree:  31%|      | 2304/7317 [00:51<02:10, 38.42it/s]Training CobwebTree:  32%|      | 2308/7317 [00:51<02:16, 36.83it/s]Training CobwebTree:  32%|      | 2312/7317 [00:51<02:13, 37.49it/s]Training CobwebTree:  32%|      | 2316/7317 [00:51<02:11, 38.00it/s]Training CobwebTree:  32%|      | 2320/7317 [00:52<02:10, 38.15it/s]Training CobwebTree:  32%|      | 2324/7317 [00:52<02:12, 37.67it/s]Training CobwebTree:  32%|      | 2329/7317 [00:52<02:07, 39.03it/s]Training CobwebTree:  32%|      | 2334/7317 [00:52<02:06, 39.38it/s]Training CobwebTree:  32%|      | 2339/7317 [00:52<02:03, 40.22it/s]Training CobwebTree:  32%|      | 2344/7317 [00:52<02:02, 40.57it/s]Training CobwebTree:  32%|      | 2349/7317 [00:52<02:07, 39.10it/s]Training CobwebTree:  32%|      | 2353/7317 [00:52<02:08, 38.66it/s]Training CobwebTree:  32%|      | 2357/7317 [00:52<02:10, 38.06it/s]Training CobwebTree:  32%|      | 2361/7317 [00:53<02:13, 37.08it/s]Training CobwebTree:  32%|      | 2365/7317 [00:53<02:15, 36.57it/s]Training CobwebTree:  32%|      | 2369/7317 [00:53<02:13, 36.98it/s]Training CobwebTree:  32%|      | 2374/7317 [00:53<02:06, 39.15it/s]Training CobwebTree:  32%|      | 2378/7317 [00:53<02:05, 39.30it/s]Training CobwebTree:  33%|      | 2383/7317 [00:53<02:00, 40.91it/s]Training CobwebTree:  33%|      | 2388/7317 [00:53<01:54, 42.88it/s]Training CobwebTree:  33%|      | 2393/7317 [00:53<01:58, 41.53it/s]Training CobwebTree:  33%|      | 2398/7317 [00:53<02:01, 40.53it/s]Training CobwebTree:  33%|      | 2403/7317 [00:54<02:03, 39.69it/s]Training CobwebTree:  33%|      | 2407/7317 [00:54<02:03, 39.68it/s]Training CobwebTree:  33%|      | 2411/7317 [00:54<02:04, 39.42it/s]Training CobwebTree:  33%|      | 2416/7317 [00:54<02:00, 40.60it/s]Training CobwebTree:  33%|      | 2421/7317 [00:54<02:01, 40.16it/s]Training CobwebTree:  33%|      | 2426/7317 [00:54<02:03, 39.58it/s]Training CobwebTree:  33%|      | 2430/7317 [00:54<02:04, 39.26it/s]Training CobwebTree:  33%|      | 2434/7317 [00:54<02:04, 39.09it/s]Training CobwebTree:  33%|      | 2438/7317 [00:55<02:05, 38.75it/s]Training CobwebTree:  33%|      | 2443/7317 [00:55<02:07, 38.09it/s]Training CobwebTree:  33%|      | 2447/7317 [00:55<02:09, 37.75it/s]Training CobwebTree:  33%|      | 2451/7317 [00:55<02:11, 37.08it/s]Training CobwebTree:  34%|      | 2455/7317 [00:55<02:12, 36.69it/s]Training CobwebTree:  34%|      | 2459/7317 [00:55<02:13, 36.39it/s]Training CobwebTree:  34%|      | 2464/7317 [00:55<02:07, 38.18it/s]Training CobwebTree:  34%|      | 2468/7317 [00:55<02:11, 36.82it/s]Training CobwebTree:  34%|      | 2473/7317 [00:55<02:05, 38.62it/s]Training CobwebTree:  34%|      | 2477/7317 [00:56<02:05, 38.70it/s]Training CobwebTree:  34%|      | 2481/7317 [00:56<02:07, 37.95it/s]Training CobwebTree:  34%|      | 2485/7317 [00:56<02:07, 38.02it/s]Training CobwebTree:  34%|      | 2489/7317 [00:56<02:06, 38.12it/s]Training CobwebTree:  34%|      | 2493/7317 [00:56<02:12, 36.53it/s]Training CobwebTree:  34%|      | 2498/7317 [00:56<02:04, 38.63it/s]Training CobwebTree:  34%|      | 2503/7317 [00:56<02:01, 39.69it/s]Training CobwebTree:  34%|      | 2508/7317 [00:56<01:57, 40.97it/s]Training CobwebTree:  34%|      | 2513/7317 [00:56<02:02, 39.08it/s]Training CobwebTree:  34%|      | 2517/7317 [00:57<02:09, 36.93it/s]Training CobwebTree:  34%|      | 2521/7317 [00:57<02:07, 37.47it/s]Training CobwebTree:  35%|      | 2526/7317 [00:57<02:06, 37.99it/s]Training CobwebTree:  35%|      | 2531/7317 [00:57<02:02, 39.12it/s]Training CobwebTree:  35%|      | 2535/7317 [00:57<02:02, 39.18it/s]Training CobwebTree:  35%|      | 2539/7317 [00:57<02:04, 38.40it/s]Training CobwebTree:  35%|      | 2543/7317 [00:57<02:03, 38.75it/s]Training CobwebTree:  35%|      | 2547/7317 [00:57<02:06, 37.80it/s]Training CobwebTree:  35%|      | 2551/7317 [00:57<02:04, 38.26it/s]Training CobwebTree:  35%|      | 2555/7317 [00:58<02:06, 37.67it/s]Training CobwebTree:  35%|      | 2559/7317 [00:58<02:06, 37.57it/s]Training CobwebTree:  35%|      | 2563/7317 [00:58<02:10, 36.31it/s]Training CobwebTree:  35%|      | 2568/7317 [00:58<02:03, 38.55it/s]Training CobwebTree:  35%|      | 2572/7317 [00:58<02:02, 38.78it/s]Training CobwebTree:  35%|      | 2577/7317 [00:58<02:00, 39.48it/s]Training CobwebTree:  35%|      | 2581/7317 [00:58<02:03, 38.22it/s]Training CobwebTree:  35%|      | 2585/7317 [00:58<02:03, 38.20it/s]Training CobwebTree:  35%|      | 2589/7317 [00:58<02:07, 36.98it/s]Training CobwebTree:  35%|      | 2594/7317 [00:59<02:05, 37.76it/s]Training CobwebTree:  36%|      | 2599/7317 [00:59<02:03, 38.21it/s]Training CobwebTree:  36%|      | 2603/7317 [00:59<02:05, 37.50it/s]Training CobwebTree:  36%|      | 2608/7317 [00:59<02:00, 39.03it/s]Training CobwebTree:  36%|      | 2613/7317 [00:59<01:56, 40.48it/s]Training CobwebTree:  36%|      | 2618/7317 [00:59<01:56, 40.33it/s]Training CobwebTree:  36%|      | 2623/7317 [00:59<02:01, 38.69it/s]Training CobwebTree:  36%|      | 2627/7317 [00:59<02:00, 38.95it/s]Training CobwebTree:  36%|      | 2631/7317 [01:00<02:00, 39.00it/s]Training CobwebTree:  36%|      | 2635/7317 [01:00<02:00, 38.70it/s]Training CobwebTree:  36%|      | 2639/7317 [01:00<02:04, 37.65it/s]Training CobwebTree:  36%|      | 2643/7317 [01:00<02:02, 38.21it/s]Training CobwebTree:  36%|      | 2647/7317 [01:00<02:01, 38.50it/s]Training CobwebTree:  36%|      | 2651/7317 [01:00<02:01, 38.54it/s]Training CobwebTree:  36%|      | 2655/7317 [01:00<02:00, 38.65it/s]Training CobwebTree:  36%|      | 2659/7317 [01:00<02:03, 37.64it/s]Training CobwebTree:  36%|      | 2664/7317 [01:00<01:57, 39.74it/s]Training CobwebTree:  36%|      | 2669/7317 [01:01<01:56, 40.00it/s]Training CobwebTree:  37%|      | 2673/7317 [01:01<02:00, 38.61it/s]Training CobwebTree:  37%|      | 2678/7317 [01:01<01:58, 39.12it/s]Training CobwebTree:  37%|      | 2682/7317 [01:01<02:00, 38.38it/s]Training CobwebTree:  37%|      | 2687/7317 [01:01<01:58, 38.92it/s]Training CobwebTree:  37%|      | 2692/7317 [01:01<01:51, 41.30it/s]Training CobwebTree:  37%|      | 2697/7317 [01:01<01:54, 40.20it/s]Training CobwebTree:  37%|      | 2702/7317 [01:01<01:58, 38.90it/s]Training CobwebTree:  37%|      | 2706/7317 [01:02<02:03, 37.22it/s]Training CobwebTree:  37%|      | 2710/7317 [01:02<02:01, 37.81it/s]Training CobwebTree:  37%|      | 2714/7317 [01:02<02:05, 36.63it/s]Training CobwebTree:  37%|      | 2719/7317 [01:02<02:00, 38.17it/s]Training CobwebTree:  37%|      | 2724/7317 [01:02<01:54, 40.14it/s]Training CobwebTree:  37%|      | 2729/7317 [01:02<01:55, 39.79it/s]Training CobwebTree:  37%|      | 2733/7317 [01:02<01:59, 38.51it/s]Training CobwebTree:  37%|      | 2738/7317 [01:02<01:57, 38.85it/s]Training CobwebTree:  37%|      | 2742/7317 [01:02<01:58, 38.51it/s]Training CobwebTree:  38%|      | 2747/7317 [01:03<01:56, 39.07it/s]Training CobwebTree:  38%|      | 2751/7317 [01:03<01:57, 38.73it/s]Training CobwebTree:  38%|      | 2756/7317 [01:03<01:57, 38.80it/s]Training CobwebTree:  38%|      | 2761/7317 [01:03<01:56, 39.10it/s]Training CobwebTree:  38%|      | 2766/7317 [01:03<01:54, 39.58it/s]Training CobwebTree:  38%|      | 2770/7317 [01:03<01:58, 38.22it/s]Training CobwebTree:  38%|      | 2774/7317 [01:03<01:59, 38.00it/s]Training CobwebTree:  38%|      | 2778/7317 [01:03<02:00, 37.79it/s]Training CobwebTree:  38%|      | 2782/7317 [01:03<02:03, 36.85it/s]Training CobwebTree:  38%|      | 2787/7317 [01:04<01:58, 38.34it/s]Training CobwebTree:  38%|      | 2792/7317 [01:04<01:55, 39.32it/s]Training CobwebTree:  38%|      | 2797/7317 [01:04<01:50, 40.97it/s]Training CobwebTree:  38%|      | 2802/7317 [01:04<01:51, 40.42it/s]Training CobwebTree:  38%|      | 2807/7317 [01:04<01:56, 38.74it/s]Training CobwebTree:  38%|      | 2811/7317 [01:04<01:58, 38.02it/s]Training CobwebTree:  38%|      | 2815/7317 [01:04<01:57, 38.47it/s]Training CobwebTree:  39%|      | 2819/7317 [01:04<01:58, 37.97it/s]Training CobwebTree:  39%|      | 2823/7317 [01:05<02:01, 37.10it/s]Training CobwebTree:  39%|      | 2827/7317 [01:05<01:58, 37.76it/s]Training CobwebTree:  39%|      | 2831/7317 [01:05<02:00, 37.12it/s]Training CobwebTree:  39%|      | 2835/7317 [01:05<01:59, 37.61it/s]Training CobwebTree:  39%|      | 2840/7317 [01:05<01:57, 38.00it/s]Training CobwebTree:  39%|      | 2844/7317 [01:05<01:57, 37.92it/s]Training CobwebTree:  39%|      | 2849/7317 [01:05<01:59, 37.24it/s]Training CobwebTree:  39%|      | 2853/7317 [01:05<02:00, 36.93it/s]Training CobwebTree:  39%|      | 2858/7317 [01:05<01:57, 37.89it/s]Training CobwebTree:  39%|      | 2863/7317 [01:06<01:50, 40.33it/s]Training CobwebTree:  39%|      | 2868/7317 [01:06<01:57, 38.00it/s]Training CobwebTree:  39%|      | 2872/7317 [01:06<01:59, 37.28it/s]Training CobwebTree:  39%|      | 2877/7317 [01:06<01:56, 38.06it/s]Training CobwebTree:  39%|      | 2881/7317 [01:06<01:56, 38.10it/s]Training CobwebTree:  39%|      | 2885/7317 [01:06<01:57, 37.84it/s]Training CobwebTree:  39%|      | 2889/7317 [01:06<01:55, 38.39it/s]Training CobwebTree:  40%|      | 2893/7317 [01:06<01:54, 38.49it/s]Training CobwebTree:  40%|      | 2898/7317 [01:06<01:48, 40.59it/s]Training CobwebTree:  40%|      | 2903/7317 [01:07<01:57, 37.63it/s]Training CobwebTree:  40%|      | 2907/7317 [01:07<01:57, 37.54it/s]Training CobwebTree:  40%|      | 2912/7317 [01:07<01:56, 37.82it/s]Training CobwebTree:  40%|      | 2917/7317 [01:07<01:56, 37.71it/s]Training CobwebTree:  40%|      | 2922/7317 [01:07<01:55, 38.10it/s]Training CobwebTree:  40%|      | 2927/7317 [01:07<01:52, 38.89it/s]Training CobwebTree:  40%|      | 2931/7317 [01:07<01:52, 38.95it/s]Training CobwebTree:  40%|      | 2936/7317 [01:07<01:49, 40.15it/s]Training CobwebTree:  40%|      | 2941/7317 [01:08<01:49, 40.04it/s]Training CobwebTree:  40%|      | 2946/7317 [01:08<01:52, 38.82it/s]Training CobwebTree:  40%|      | 2950/7317 [01:08<01:52, 38.83it/s]Training CobwebTree:  40%|      | 2954/7317 [01:08<01:56, 37.47it/s]Training CobwebTree:  40%|      | 2959/7317 [01:08<01:52, 38.85it/s]Training CobwebTree:  40%|      | 2963/7317 [01:08<01:55, 37.62it/s]Training CobwebTree:  41%|      | 2967/7317 [01:08<01:54, 37.83it/s]Training CobwebTree:  41%|      | 2971/7317 [01:08<01:54, 38.00it/s]Training CobwebTree:  41%|      | 2975/7317 [01:09<01:54, 37.90it/s]Training CobwebTree:  41%|      | 2980/7317 [01:09<01:51, 38.97it/s]Training CobwebTree:  41%|      | 2984/7317 [01:09<01:56, 37.09it/s]Training CobwebTree:  41%|      | 2988/7317 [01:09<01:56, 37.24it/s]Training CobwebTree:  41%|      | 2992/7317 [01:09<01:55, 37.53it/s]Training CobwebTree:  41%|      | 2997/7317 [01:09<01:53, 37.90it/s]Training CobwebTree:  41%|      | 3001/7317 [01:09<01:54, 37.71it/s]Training CobwebTree:  41%|      | 3005/7317 [01:09<01:54, 37.61it/s]Training CobwebTree:  41%|      | 3009/7317 [01:09<01:56, 36.85it/s]Training CobwebTree:  41%|      | 3014/7317 [01:10<01:52, 38.19it/s]Training CobwebTree:  41%|      | 3018/7317 [01:10<01:53, 37.99it/s]Training CobwebTree:  41%|     | 3022/7317 [01:10<01:52, 38.04it/s]Training CobwebTree:  41%|     | 3026/7317 [01:10<01:51, 38.39it/s]Training CobwebTree:  41%|     | 3030/7317 [01:10<01:54, 37.49it/s]Training CobwebTree:  41%|     | 3034/7317 [01:10<01:52, 38.13it/s]Training CobwebTree:  42%|     | 3038/7317 [01:10<01:52, 38.07it/s]Training CobwebTree:  42%|     | 3042/7317 [01:10<01:51, 38.51it/s]Training CobwebTree:  42%|     | 3046/7317 [01:10<01:54, 37.23it/s]Training CobwebTree:  42%|     | 3050/7317 [01:10<01:53, 37.63it/s]Training CobwebTree:  42%|     | 3054/7317 [01:11<01:52, 38.05it/s]Training CobwebTree:  42%|     | 3059/7317 [01:11<01:49, 39.02it/s]Training CobwebTree:  42%|     | 3063/7317 [01:11<01:48, 39.16it/s]Training CobwebTree:  42%|     | 3067/7317 [01:11<01:56, 36.38it/s]Training CobwebTree:  42%|     | 3072/7317 [01:11<01:52, 37.73it/s]Training CobwebTree:  42%|     | 3076/7317 [01:11<01:57, 35.97it/s]Training CobwebTree:  42%|     | 3080/7317 [01:11<01:58, 35.75it/s]Training CobwebTree:  42%|     | 3084/7317 [01:11<01:59, 35.46it/s]Training CobwebTree:  42%|     | 3088/7317 [01:12<02:00, 35.06it/s]Training CobwebTree:  42%|     | 3092/7317 [01:12<01:59, 35.32it/s]Training CobwebTree:  42%|     | 3096/7317 [01:12<01:57, 35.99it/s]Training CobwebTree:  42%|     | 3100/7317 [01:12<01:56, 36.30it/s]Training CobwebTree:  42%|     | 3104/7317 [01:12<01:59, 35.38it/s]Training CobwebTree:  42%|     | 3108/7317 [01:12<02:03, 34.07it/s]Training CobwebTree:  43%|     | 3113/7317 [01:12<01:53, 37.02it/s]Training CobwebTree:  43%|     | 3117/7317 [01:12<01:51, 37.53it/s]Training CobwebTree:  43%|     | 3121/7317 [01:12<01:54, 36.64it/s]Training CobwebTree:  43%|     | 3125/7317 [01:13<01:52, 37.25it/s]Training CobwebTree:  43%|     | 3130/7317 [01:13<01:51, 37.43it/s]Training CobwebTree:  43%|     | 3134/7317 [01:13<01:53, 36.89it/s]Training CobwebTree:  43%|     | 3138/7317 [01:13<01:52, 37.30it/s]Training CobwebTree:  43%|     | 3142/7317 [01:13<01:51, 37.39it/s]Training CobwebTree:  43%|     | 3146/7317 [01:13<01:56, 35.81it/s]Training CobwebTree:  43%|     | 3150/7317 [01:13<01:55, 36.23it/s]Training CobwebTree:  43%|     | 3154/7317 [01:13<01:52, 37.05it/s]Training CobwebTree:  43%|     | 3158/7317 [01:13<01:53, 36.56it/s]Training CobwebTree:  43%|     | 3162/7317 [01:14<01:51, 37.36it/s]Training CobwebTree:  43%|     | 3166/7317 [01:14<01:51, 37.17it/s]Training CobwebTree:  43%|     | 3170/7317 [01:14<01:50, 37.54it/s]Training CobwebTree:  43%|     | 3174/7317 [01:14<01:49, 37.68it/s]Training CobwebTree:  43%|     | 3179/7317 [01:14<01:46, 38.69it/s]Training CobwebTree:  44%|     | 3183/7317 [01:14<01:48, 38.23it/s]Training CobwebTree:  44%|     | 3188/7317 [01:14<01:44, 39.45it/s]Training CobwebTree:  44%|     | 3192/7317 [01:14<01:44, 39.49it/s]Training CobwebTree:  44%|     | 3196/7317 [01:14<01:46, 38.52it/s]Training CobwebTree:  44%|     | 3200/7317 [01:15<01:46, 38.77it/s]Training CobwebTree:  44%|     | 3204/7317 [01:15<01:48, 38.07it/s]Training CobwebTree:  44%|     | 3209/7317 [01:15<01:43, 39.60it/s]Training CobwebTree:  44%|     | 3213/7317 [01:15<01:44, 39.28it/s]Training CobwebTree:  44%|     | 3217/7317 [01:15<01:45, 38.89it/s]Training CobwebTree:  44%|     | 3221/7317 [01:15<01:46, 38.55it/s]Training CobwebTree:  44%|     | 3225/7317 [01:15<01:46, 38.26it/s]Training CobwebTree:  44%|     | 3230/7317 [01:15<01:47, 38.11it/s]Training CobwebTree:  44%|     | 3234/7317 [01:15<01:49, 37.36it/s]Training CobwebTree:  44%|     | 3238/7317 [01:16<01:55, 35.28it/s]Training CobwebTree:  44%|     | 3243/7317 [01:16<01:48, 37.61it/s]Training CobwebTree:  44%|     | 3247/7317 [01:16<01:47, 37.83it/s]Training CobwebTree:  44%|     | 3251/7317 [01:16<01:50, 36.79it/s]Training CobwebTree:  44%|     | 3256/7317 [01:16<01:45, 38.47it/s]Training CobwebTree:  45%|     | 3261/7317 [01:16<01:39, 40.91it/s]Training CobwebTree:  45%|     | 3266/7317 [01:16<01:41, 39.83it/s]Training CobwebTree:  45%|     | 3271/7317 [01:16<01:43, 39.06it/s]Training CobwebTree:  45%|     | 3275/7317 [01:16<01:44, 38.82it/s]Training CobwebTree:  45%|     | 3279/7317 [01:17<01:43, 39.00it/s]Training CobwebTree:  45%|     | 3283/7317 [01:17<01:43, 38.94it/s]Training CobwebTree:  45%|     | 3287/7317 [01:17<01:50, 36.54it/s]Training CobwebTree:  45%|     | 3291/7317 [01:17<01:49, 36.84it/s]Training CobwebTree:  45%|     | 3295/7317 [01:17<01:51, 36.00it/s]Training CobwebTree:  45%|     | 3299/7317 [01:17<01:50, 36.39it/s]Training CobwebTree:  45%|     | 3304/7317 [01:17<01:46, 37.74it/s]Training CobwebTree:  45%|     | 3308/7317 [01:17<01:48, 36.82it/s]Training CobwebTree:  45%|     | 3312/7317 [01:18<01:54, 34.93it/s]Training CobwebTree:  45%|     | 3316/7317 [01:18<01:52, 35.53it/s]Training CobwebTree:  45%|     | 3320/7317 [01:18<01:56, 34.39it/s]Training CobwebTree:  45%|     | 3324/7317 [01:18<01:51, 35.70it/s]Training CobwebTree:  45%|     | 3328/7317 [01:18<01:53, 35.20it/s]Training CobwebTree:  46%|     | 3332/7317 [01:18<01:49, 36.34it/s]Training CobwebTree:  46%|     | 3336/7317 [01:18<01:54, 34.87it/s]Training CobwebTree:  46%|     | 3340/7317 [01:18<01:50, 35.89it/s]Training CobwebTree:  46%|     | 3344/7317 [01:18<01:47, 36.97it/s]Training CobwebTree:  46%|     | 3348/7317 [01:19<01:48, 36.65it/s]Training CobwebTree:  46%|     | 3352/7317 [01:19<01:46, 37.08it/s]Training CobwebTree:  46%|     | 3356/7317 [01:19<01:47, 37.01it/s]Training CobwebTree:  46%|     | 3360/7317 [01:19<01:47, 36.76it/s]Training CobwebTree:  46%|     | 3365/7317 [01:19<01:43, 38.26it/s]Training CobwebTree:  46%|     | 3369/7317 [01:19<01:44, 37.96it/s]Training CobwebTree:  46%|     | 3373/7317 [01:19<01:47, 36.74it/s]Training CobwebTree:  46%|     | 3378/7317 [01:19<01:44, 37.86it/s]Training CobwebTree:  46%|     | 3383/7317 [01:19<01:39, 39.43it/s]Training CobwebTree:  46%|     | 3387/7317 [01:20<01:43, 38.10it/s]Training CobwebTree:  46%|     | 3392/7317 [01:20<01:40, 39.13it/s]Training CobwebTree:  46%|     | 3396/7317 [01:20<01:42, 38.19it/s]Training CobwebTree:  46%|     | 3400/7317 [01:20<01:48, 36.04it/s]Training CobwebTree:  47%|     | 3404/7317 [01:20<01:52, 34.79it/s]Training CobwebTree:  47%|     | 3409/7317 [01:20<01:48, 36.05it/s]Training CobwebTree:  47%|     | 3413/7317 [01:20<01:48, 35.88it/s]Training CobwebTree:  47%|     | 3417/7317 [01:20<01:48, 35.90it/s]Training CobwebTree:  47%|     | 3421/7317 [01:20<01:48, 35.79it/s]Training CobwebTree:  47%|     | 3425/7317 [01:21<01:46, 36.53it/s]Training CobwebTree:  47%|     | 3429/7317 [01:21<01:53, 34.32it/s]Training CobwebTree:  47%|     | 3433/7317 [01:21<01:53, 34.09it/s]Training CobwebTree:  47%|     | 3437/7317 [01:21<01:48, 35.63it/s]Training CobwebTree:  47%|     | 3441/7317 [01:21<01:49, 35.30it/s]Training CobwebTree:  47%|     | 3446/7317 [01:21<01:48, 35.64it/s]Training CobwebTree:  47%|     | 3450/7317 [01:21<01:49, 35.29it/s]Training CobwebTree:  47%|     | 3454/7317 [01:21<01:50, 34.93it/s]Training CobwebTree:  47%|     | 3458/7317 [01:22<01:55, 33.36it/s]Training CobwebTree:  47%|     | 3462/7317 [01:22<01:54, 33.52it/s]Training CobwebTree:  47%|     | 3466/7317 [01:22<01:52, 34.10it/s]Training CobwebTree:  47%|     | 3470/7317 [01:22<01:50, 34.73it/s]Training CobwebTree:  47%|     | 3474/7317 [01:22<01:56, 33.04it/s]Training CobwebTree:  48%|     | 3478/7317 [01:22<01:53, 33.79it/s]Training CobwebTree:  48%|     | 3482/7317 [01:22<01:49, 35.08it/s]Training CobwebTree:  48%|     | 3486/7317 [01:22<01:47, 35.62it/s]Training CobwebTree:  48%|     | 3490/7317 [01:22<01:44, 36.55it/s]Training CobwebTree:  48%|     | 3494/7317 [01:23<01:45, 36.09it/s]Training CobwebTree:  48%|     | 3498/7317 [01:23<01:48, 35.32it/s]Training CobwebTree:  48%|     | 3502/7317 [01:23<01:50, 34.67it/s]Training CobwebTree:  48%|     | 3506/7317 [01:23<01:49, 34.95it/s]Training CobwebTree:  48%|     | 3510/7317 [01:23<01:47, 35.36it/s]Training CobwebTree:  48%|     | 3514/7317 [01:23<01:46, 35.87it/s]Training CobwebTree:  48%|     | 3518/7317 [01:23<01:50, 34.25it/s]Training CobwebTree:  48%|     | 3522/7317 [01:23<01:48, 34.86it/s]Training CobwebTree:  48%|     | 3526/7317 [01:23<01:49, 34.54it/s]Training CobwebTree:  48%|     | 3531/7317 [01:24<01:46, 35.64it/s]Training CobwebTree:  48%|     | 3535/7317 [01:24<01:44, 36.23it/s]Training CobwebTree:  48%|     | 3539/7317 [01:24<01:43, 36.46it/s]Training CobwebTree:  48%|     | 3543/7317 [01:24<01:47, 35.13it/s]Training CobwebTree:  48%|     | 3547/7317 [01:24<01:43, 36.41it/s]Training CobwebTree:  49%|     | 3551/7317 [01:24<01:45, 35.72it/s]Training CobwebTree:  49%|     | 3555/7317 [01:24<01:48, 34.72it/s]Training CobwebTree:  49%|     | 3559/7317 [01:24<01:45, 35.57it/s]Training CobwebTree:  49%|     | 3563/7317 [01:25<01:43, 36.41it/s]Training CobwebTree:  49%|     | 3567/7317 [01:25<01:41, 37.12it/s]Training CobwebTree:  49%|     | 3571/7317 [01:25<01:43, 36.30it/s]Training CobwebTree:  49%|     | 3575/7317 [01:25<01:40, 37.23it/s]Training CobwebTree:  49%|     | 3580/7317 [01:25<01:35, 38.99it/s]Training CobwebTree:  49%|     | 3584/7317 [01:25<01:39, 37.46it/s]Training CobwebTree:  49%|     | 3589/7317 [01:25<01:32, 40.14it/s]Training CobwebTree:  49%|     | 3594/7317 [01:25<01:36, 38.38it/s]Training CobwebTree:  49%|     | 3598/7317 [01:25<01:36, 38.58it/s]Training CobwebTree:  49%|     | 3602/7317 [01:26<01:38, 37.88it/s]Training CobwebTree:  49%|     | 3606/7317 [01:26<01:36, 38.31it/s]Training CobwebTree:  49%|     | 3610/7317 [01:26<01:37, 37.98it/s]Training CobwebTree:  49%|     | 3615/7317 [01:26<01:34, 39.18it/s]Training CobwebTree:  49%|     | 3619/7317 [01:26<01:36, 38.41it/s]Training CobwebTree:  50%|     | 3623/7317 [01:26<01:40, 36.84it/s]Training CobwebTree:  50%|     | 3627/7317 [01:26<01:39, 37.01it/s]Training CobwebTree:  50%|     | 3631/7317 [01:26<01:37, 37.62it/s]Training CobwebTree:  50%|     | 3636/7317 [01:26<01:37, 37.75it/s]Training CobwebTree:  50%|     | 3640/7317 [01:27<01:40, 36.65it/s]Training CobwebTree:  50%|     | 3644/7317 [01:27<01:40, 36.51it/s]Training CobwebTree:  50%|     | 3648/7317 [01:27<01:41, 36.13it/s]Training CobwebTree:  50%|     | 3652/7317 [01:27<01:40, 36.62it/s]Training CobwebTree:  50%|     | 3656/7317 [01:27<01:40, 36.57it/s]Training CobwebTree:  50%|     | 3660/7317 [01:27<01:39, 36.80it/s]Training CobwebTree:  50%|     | 3664/7317 [01:27<01:40, 36.40it/s]Training CobwebTree:  50%|     | 3668/7317 [01:27<01:43, 35.27it/s]Training CobwebTree:  50%|     | 3673/7317 [01:27<01:36, 37.73it/s]Training CobwebTree:  50%|     | 3677/7317 [01:28<01:37, 37.39it/s]Training CobwebTree:  50%|     | 3681/7317 [01:28<01:36, 37.75it/s]Training CobwebTree:  50%|     | 3685/7317 [01:28<01:37, 37.07it/s]Training CobwebTree:  50%|     | 3689/7317 [01:28<01:42, 35.57it/s]Training CobwebTree:  50%|     | 3693/7317 [01:28<01:40, 36.05it/s]Training CobwebTree:  51%|     | 3697/7317 [01:28<01:37, 36.96it/s]Training CobwebTree:  51%|     | 3701/7317 [01:28<01:40, 36.02it/s]Training CobwebTree:  51%|     | 3706/7317 [01:28<01:35, 38.00it/s]Training CobwebTree:  51%|     | 3710/7317 [01:28<01:34, 38.03it/s]Training CobwebTree:  51%|     | 3714/7317 [01:29<01:34, 37.97it/s]Training CobwebTree:  51%|     | 3718/7317 [01:29<01:33, 38.33it/s]Training CobwebTree:  51%|     | 3722/7317 [01:29<01:39, 36.01it/s]Training CobwebTree:  51%|     | 3726/7317 [01:29<01:38, 36.39it/s]Training CobwebTree:  51%|     | 3730/7317 [01:29<01:36, 37.14it/s]Training CobwebTree:  51%|     | 3735/7317 [01:29<01:33, 38.15it/s]Training CobwebTree:  51%|     | 3739/7317 [01:29<01:34, 37.73it/s]Training CobwebTree:  51%|     | 3744/7317 [01:29<01:30, 39.52it/s]Training CobwebTree:  51%|     | 3748/7317 [01:29<01:32, 38.70it/s]Training CobwebTree:  51%|    | 3752/7317 [01:30<01:33, 38.29it/s]Training CobwebTree:  51%|    | 3756/7317 [01:30<01:33, 38.24it/s]Training CobwebTree:  51%|    | 3761/7317 [01:30<01:30, 39.35it/s]Training CobwebTree:  51%|    | 3765/7317 [01:30<01:31, 38.68it/s]Training CobwebTree:  52%|    | 3769/7317 [01:30<01:35, 37.11it/s]Training CobwebTree:  52%|    | 3773/7317 [01:30<01:35, 37.16it/s]Training CobwebTree:  52%|    | 3777/7317 [01:30<01:41, 34.71it/s]Training CobwebTree:  52%|    | 3781/7317 [01:30<01:43, 34.24it/s]Training CobwebTree:  52%|    | 3785/7317 [01:30<01:44, 33.95it/s]Training CobwebTree:  52%|    | 3789/7317 [01:31<01:42, 34.58it/s]Training CobwebTree:  52%|    | 3794/7317 [01:31<01:34, 37.29it/s]Training CobwebTree:  52%|    | 3798/7317 [01:31<01:35, 36.88it/s]Training CobwebTree:  52%|    | 3802/7317 [01:31<01:36, 36.38it/s]Training CobwebTree:  52%|    | 3806/7317 [01:31<01:35, 36.59it/s]Training CobwebTree:  52%|    | 3810/7317 [01:31<01:33, 37.48it/s]Training CobwebTree:  52%|    | 3814/7317 [01:31<01:35, 36.79it/s]Training CobwebTree:  52%|    | 3818/7317 [01:31<01:38, 35.43it/s]Training CobwebTree:  52%|    | 3822/7317 [01:31<01:39, 34.98it/s]Training CobwebTree:  52%|    | 3826/7317 [01:32<01:40, 34.75it/s]Training CobwebTree:  52%|    | 3830/7317 [01:32<01:40, 34.57it/s]Training CobwebTree:  52%|    | 3834/7317 [01:32<01:40, 34.61it/s]Training CobwebTree:  52%|    | 3838/7317 [01:32<01:44, 33.34it/s]Training CobwebTree:  53%|    | 3842/7317 [01:32<01:43, 33.65it/s]Training CobwebTree:  53%|    | 3846/7317 [01:32<01:46, 32.72it/s]Training CobwebTree:  53%|    | 3850/7317 [01:32<01:42, 33.85it/s]Training CobwebTree:  53%|    | 3854/7317 [01:32<01:42, 33.70it/s]Training CobwebTree:  53%|    | 3858/7317 [01:33<01:39, 34.81it/s]Training CobwebTree:  53%|    | 3862/7317 [01:33<01:37, 35.40it/s]Training CobwebTree:  53%|    | 3867/7317 [01:33<01:29, 38.45it/s]Training CobwebTree:  53%|    | 3871/7317 [01:33<01:37, 35.18it/s]Training CobwebTree:  53%|    | 3875/7317 [01:33<01:36, 35.62it/s]Training CobwebTree:  53%|    | 3879/7317 [01:33<01:36, 35.55it/s]Training CobwebTree:  53%|    | 3884/7317 [01:33<01:31, 37.32it/s]Training CobwebTree:  53%|    | 3889/7317 [01:33<01:32, 37.04it/s]Training CobwebTree:  53%|    | 3893/7317 [01:34<01:34, 36.27it/s]Training CobwebTree:  53%|    | 3897/7317 [01:34<01:35, 35.73it/s]Training CobwebTree:  53%|    | 3901/7317 [01:34<01:34, 36.18it/s]Training CobwebTree:  53%|    | 3905/7317 [01:34<01:32, 36.70it/s]Training CobwebTree:  53%|    | 3909/7317 [01:34<01:34, 36.02it/s]Training CobwebTree:  53%|    | 3913/7317 [01:34<01:38, 34.49it/s]Training CobwebTree:  54%|    | 3917/7317 [01:34<01:36, 35.29it/s]Training CobwebTree:  54%|    | 3921/7317 [01:34<01:35, 35.59it/s]Training CobwebTree:  54%|    | 3925/7317 [01:34<01:35, 35.70it/s]Training CobwebTree:  54%|    | 3929/7317 [01:35<01:35, 35.65it/s]Training CobwebTree:  54%|    | 3933/7317 [01:35<01:39, 33.99it/s]Training CobwebTree:  54%|    | 3937/7317 [01:35<01:39, 34.05it/s]Training CobwebTree:  54%|    | 3941/7317 [01:35<01:35, 35.42it/s]Training CobwebTree:  54%|    | 3945/7317 [01:35<01:38, 34.27it/s]Training CobwebTree:  54%|    | 3949/7317 [01:35<01:38, 34.34it/s]Training CobwebTree:  54%|    | 3953/7317 [01:35<01:40, 33.42it/s]Training CobwebTree:  54%|    | 3957/7317 [01:35<01:36, 34.84it/s]Training CobwebTree:  54%|    | 3961/7317 [01:35<01:33, 35.71it/s]Training CobwebTree:  54%|    | 3965/7317 [01:36<01:35, 35.21it/s]Training CobwebTree:  54%|    | 3969/7317 [01:36<01:31, 36.44it/s]Training CobwebTree:  54%|    | 3973/7317 [01:36<01:31, 36.51it/s]Training CobwebTree:  54%|    | 3977/7317 [01:36<01:32, 36.06it/s]Training CobwebTree:  54%|    | 3981/7317 [01:36<01:31, 36.34it/s]Training CobwebTree:  54%|    | 3985/7317 [01:36<01:31, 36.44it/s]Training CobwebTree:  55%|    | 3989/7317 [01:36<01:29, 37.18it/s]Training CobwebTree:  55%|    | 3993/7317 [01:36<01:30, 36.72it/s]Training CobwebTree:  55%|    | 3998/7317 [01:36<01:27, 37.72it/s]Training CobwebTree:  55%|    | 4002/7317 [01:37<01:30, 36.55it/s]Training CobwebTree:  55%|    | 4006/7317 [01:37<01:34, 34.88it/s]Training CobwebTree:  55%|    | 4010/7317 [01:37<01:35, 34.75it/s]Training CobwebTree:  55%|    | 4014/7317 [01:37<01:36, 34.27it/s]Training CobwebTree:  55%|    | 4018/7317 [01:37<01:36, 34.30it/s]Training CobwebTree:  55%|    | 4023/7317 [01:37<01:30, 36.21it/s]Training CobwebTree:  55%|    | 4028/7317 [01:37<01:25, 38.46it/s]Training CobwebTree:  55%|    | 4032/7317 [01:37<01:30, 36.35it/s]Training CobwebTree:  55%|    | 4037/7317 [01:38<01:26, 37.99it/s]Training CobwebTree:  55%|    | 4042/7317 [01:38<01:25, 38.40it/s]Training CobwebTree:  55%|    | 4046/7317 [01:38<01:25, 38.17it/s]Training CobwebTree:  55%|    | 4050/7317 [01:38<01:27, 37.44it/s]Training CobwebTree:  55%|    | 4054/7317 [01:38<01:26, 37.91it/s]Training CobwebTree:  55%|    | 4058/7317 [01:38<01:26, 37.52it/s]Training CobwebTree:  56%|    | 4062/7317 [01:38<01:27, 37.39it/s]Training CobwebTree:  56%|    | 4067/7317 [01:38<01:24, 38.45it/s]Training CobwebTree:  56%|    | 4071/7317 [01:38<01:24, 38.57it/s]Training CobwebTree:  56%|    | 4075/7317 [01:39<01:26, 37.28it/s]Training CobwebTree:  56%|    | 4079/7317 [01:39<01:27, 36.82it/s]Training CobwebTree:  56%|    | 4083/7317 [01:39<01:30, 35.64it/s]Training CobwebTree:  56%|    | 4087/7317 [01:39<01:34, 34.35it/s]Training CobwebTree:  56%|    | 4091/7317 [01:39<01:32, 34.69it/s]Training CobwebTree:  56%|    | 4095/7317 [01:39<01:32, 34.97it/s]Training CobwebTree:  56%|    | 4099/7317 [01:39<01:35, 33.71it/s]Training CobwebTree:  56%|    | 4103/7317 [01:39<01:35, 33.76it/s]Training CobwebTree:  56%|    | 4107/7317 [01:39<01:32, 34.55it/s]Training CobwebTree:  56%|    | 4111/7317 [01:40<01:32, 34.68it/s]Training CobwebTree:  56%|    | 4115/7317 [01:40<01:30, 35.20it/s]Training CobwebTree:  56%|    | 4119/7317 [01:40<01:30, 35.34it/s]Training CobwebTree:  56%|    | 4124/7317 [01:40<01:26, 36.88it/s]Training CobwebTree:  56%|    | 4128/7317 [01:40<01:25, 37.36it/s]Training CobwebTree:  56%|    | 4132/7317 [01:40<01:25, 37.05it/s]Training CobwebTree:  57%|    | 4136/7317 [01:40<01:24, 37.67it/s]Training CobwebTree:  57%|    | 4140/7317 [01:40<01:26, 36.80it/s]Training CobwebTree:  57%|    | 4144/7317 [01:40<01:28, 35.90it/s]Training CobwebTree:  57%|    | 4148/7317 [01:41<01:25, 36.97it/s]Training CobwebTree:  57%|    | 4152/7317 [01:41<01:23, 37.77it/s]Training CobwebTree:  57%|    | 4156/7317 [01:41<01:23, 37.68it/s]Training CobwebTree:  57%|    | 4160/7317 [01:41<01:24, 37.30it/s]Training CobwebTree:  57%|    | 4165/7317 [01:41<01:22, 38.01it/s]Training CobwebTree:  57%|    | 4169/7317 [01:41<01:22, 38.36it/s]Training CobwebTree:  57%|    | 4173/7317 [01:41<01:21, 38.58it/s]Training CobwebTree:  57%|    | 4177/7317 [01:41<01:26, 36.50it/s]Training CobwebTree:  57%|    | 4182/7317 [01:41<01:21, 38.46it/s]Training CobwebTree:  57%|    | 4186/7317 [01:42<01:24, 36.89it/s]Training CobwebTree:  57%|    | 4190/7317 [01:42<01:27, 35.62it/s]Training CobwebTree:  57%|    | 4194/7317 [01:42<01:27, 35.83it/s]Training CobwebTree:  57%|    | 4198/7317 [01:42<01:24, 36.87it/s]Training CobwebTree:  57%|    | 4202/7317 [01:42<01:26, 36.02it/s]Training CobwebTree:  57%|    | 4206/7317 [01:42<01:23, 37.12it/s]Training CobwebTree:  58%|    | 4210/7317 [01:42<01:25, 36.48it/s]Training CobwebTree:  58%|    | 4214/7317 [01:42<01:24, 36.63it/s]Training CobwebTree:  58%|    | 4218/7317 [01:42<01:22, 37.49it/s]Training CobwebTree:  58%|    | 4222/7317 [01:43<01:24, 36.63it/s]Training CobwebTree:  58%|    | 4226/7317 [01:43<01:24, 36.44it/s]Training CobwebTree:  58%|    | 4230/7317 [01:43<01:28, 34.81it/s]Training CobwebTree:  58%|    | 4234/7317 [01:43<01:28, 34.76it/s]Training CobwebTree:  58%|    | 4238/7317 [01:43<01:29, 34.29it/s]Training CobwebTree:  58%|    | 4242/7317 [01:43<01:31, 33.49it/s]Training CobwebTree:  58%|    | 4247/7317 [01:43<01:26, 35.57it/s]Training CobwebTree:  58%|    | 4252/7317 [01:43<01:25, 36.04it/s]Training CobwebTree:  58%|    | 4256/7317 [01:44<01:24, 36.04it/s]Training CobwebTree:  58%|    | 4260/7317 [01:44<01:24, 36.25it/s]Training CobwebTree:  58%|    | 4264/7317 [01:44<01:25, 35.80it/s]Training CobwebTree:  58%|    | 4268/7317 [01:44<01:27, 34.84it/s]Training CobwebTree:  58%|    | 4272/7317 [01:44<01:26, 35.05it/s]Training CobwebTree:  58%|    | 4276/7317 [01:44<01:28, 34.36it/s]Training CobwebTree:  58%|    | 4280/7317 [01:44<01:26, 34.99it/s]Training CobwebTree:  59%|    | 4284/7317 [01:44<01:25, 35.34it/s]Training CobwebTree:  59%|    | 4288/7317 [01:44<01:27, 34.67it/s]Training CobwebTree:  59%|    | 4292/7317 [01:45<01:29, 33.65it/s]Training CobwebTree:  59%|    | 4296/7317 [01:45<01:26, 35.12it/s]Training CobwebTree:  59%|    | 4300/7317 [01:45<01:27, 34.48it/s]Training CobwebTree:  59%|    | 4304/7317 [01:45<01:28, 34.15it/s]Training CobwebTree:  59%|    | 4309/7317 [01:45<01:22, 36.46it/s]Training CobwebTree:  59%|    | 4313/7317 [01:45<01:21, 36.83it/s]Training CobwebTree:  59%|    | 4317/7317 [01:45<01:24, 35.38it/s]Training CobwebTree:  59%|    | 4321/7317 [01:45<01:25, 35.11it/s]Training CobwebTree:  59%|    | 4325/7317 [01:46<01:25, 35.02it/s]Training CobwebTree:  59%|    | 4329/7317 [01:46<01:25, 35.15it/s]Training CobwebTree:  59%|    | 4333/7317 [01:46<01:24, 35.36it/s]Training CobwebTree:  59%|    | 4337/7317 [01:46<01:23, 35.75it/s]Training CobwebTree:  59%|    | 4341/7317 [01:46<01:20, 36.80it/s]Training CobwebTree:  59%|    | 4345/7317 [01:46<01:21, 36.60it/s]Training CobwebTree:  59%|    | 4349/7317 [01:46<01:22, 35.97it/s]Training CobwebTree:  59%|    | 4353/7317 [01:46<01:23, 35.56it/s]Training CobwebTree:  60%|    | 4357/7317 [01:46<01:21, 36.50it/s]Training CobwebTree:  60%|    | 4361/7317 [01:47<01:19, 37.13it/s]Training CobwebTree:  60%|    | 4366/7317 [01:47<01:17, 37.99it/s]Training CobwebTree:  60%|    | 4370/7317 [01:47<01:18, 37.36it/s]Training CobwebTree:  60%|    | 4374/7317 [01:47<01:18, 37.73it/s]Training CobwebTree:  60%|    | 4378/7317 [01:47<01:19, 36.74it/s]Training CobwebTree:  60%|    | 4383/7317 [01:47<01:17, 37.85it/s]Training CobwebTree:  60%|    | 4387/7317 [01:47<01:19, 37.07it/s]Training CobwebTree:  60%|    | 4391/7317 [01:47<01:18, 37.04it/s]Training CobwebTree:  60%|    | 4395/7317 [01:47<01:18, 37.29it/s]Training CobwebTree:  60%|    | 4399/7317 [01:48<01:21, 35.82it/s]Training CobwebTree:  60%|    | 4403/7317 [01:48<01:19, 36.64it/s]Training CobwebTree:  60%|    | 4407/7317 [01:48<01:21, 35.59it/s]Training CobwebTree:  60%|    | 4411/7317 [01:48<01:21, 35.65it/s]Training CobwebTree:  60%|    | 4415/7317 [01:48<01:19, 36.58it/s]Training CobwebTree:  60%|    | 4419/7317 [01:48<01:18, 36.75it/s]Training CobwebTree:  60%|    | 4423/7317 [01:48<01:19, 36.30it/s]Training CobwebTree:  61%|    | 4427/7317 [01:48<01:19, 36.52it/s]Training CobwebTree:  61%|    | 4431/7317 [01:48<01:18, 36.68it/s]Training CobwebTree:  61%|    | 4435/7317 [01:49<01:19, 36.48it/s]Training CobwebTree:  61%|    | 4439/7317 [01:49<01:21, 35.28it/s]Training CobwebTree:  61%|    | 4444/7317 [01:49<01:18, 36.70it/s]Training CobwebTree:  61%|    | 4448/7317 [01:49<01:16, 37.35it/s]Training CobwebTree:  61%|    | 4452/7317 [01:49<01:19, 36.09it/s]Training CobwebTree:  61%|    | 4457/7317 [01:49<01:16, 37.63it/s]Training CobwebTree:  61%|    | 4461/7317 [01:49<01:18, 36.38it/s]Training CobwebTree:  61%|    | 4465/7317 [01:49<01:20, 35.45it/s]Training CobwebTree:  61%|    | 4469/7317 [01:49<01:20, 35.42it/s]Training CobwebTree:  61%|    | 4473/7317 [01:50<01:17, 36.63it/s]Training CobwebTree:  61%|    | 4477/7317 [01:50<01:23, 34.06it/s]Training CobwebTree:  61%|    | 4481/7317 [01:50<01:20, 35.30it/s]Training CobwebTree:  61%|   | 4485/7317 [01:50<01:19, 35.57it/s]Training CobwebTree:  61%|   | 4489/7317 [01:50<01:17, 36.27it/s]Training CobwebTree:  61%|   | 4493/7317 [01:50<01:22, 34.03it/s]Training CobwebTree:  61%|   | 4498/7317 [01:50<01:17, 36.17it/s]Training CobwebTree:  62%|   | 4502/7317 [01:50<01:17, 36.24it/s]Training CobwebTree:  62%|   | 4506/7317 [01:51<01:17, 36.31it/s]Training CobwebTree:  62%|   | 4511/7317 [01:51<01:16, 36.56it/s]Training CobwebTree:  62%|   | 4516/7317 [01:51<01:15, 37.00it/s]Training CobwebTree:  62%|   | 4521/7317 [01:51<01:14, 37.48it/s]Training CobwebTree:  62%|   | 4525/7317 [01:51<01:15, 37.17it/s]Training CobwebTree:  62%|   | 4529/7317 [01:51<01:13, 37.70it/s]Training CobwebTree:  62%|   | 4533/7317 [01:51<01:15, 36.73it/s]Training CobwebTree:  62%|   | 4537/7317 [01:51<01:15, 37.04it/s]Training CobwebTree:  62%|   | 4541/7317 [01:51<01:16, 36.52it/s]Training CobwebTree:  62%|   | 4545/7317 [01:52<01:20, 34.48it/s]Training CobwebTree:  62%|   | 4549/7317 [01:52<01:17, 35.61it/s]Training CobwebTree:  62%|   | 4553/7317 [01:52<01:22, 33.35it/s]Training CobwebTree:  62%|   | 4557/7317 [01:52<01:19, 34.80it/s]Training CobwebTree:  62%|   | 4561/7317 [01:52<01:17, 35.51it/s]Training CobwebTree:  62%|   | 4565/7317 [01:52<01:19, 34.67it/s]Training CobwebTree:  62%|   | 4569/7317 [01:52<01:17, 35.50it/s]Training CobwebTree:  62%|   | 4573/7317 [01:52<01:17, 35.37it/s]Training CobwebTree:  63%|   | 4577/7317 [01:52<01:16, 35.78it/s]Training CobwebTree:  63%|   | 4581/7317 [01:53<01:16, 35.82it/s]Training CobwebTree:  63%|   | 4586/7317 [01:53<01:11, 37.98it/s]Training CobwebTree:  63%|   | 4590/7317 [01:53<01:14, 36.42it/s]Training CobwebTree:  63%|   | 4594/7317 [01:53<01:18, 34.63it/s]Training CobwebTree:  63%|   | 4599/7317 [01:53<01:15, 36.15it/s]Training CobwebTree:  63%|   | 4603/7317 [01:53<01:15, 35.90it/s]Training CobwebTree:  63%|   | 4607/7317 [01:53<01:17, 34.85it/s]Training CobwebTree:  63%|   | 4611/7317 [01:53<01:20, 33.53it/s]Training CobwebTree:  63%|   | 4615/7317 [01:54<01:17, 34.93it/s]Training CobwebTree:  63%|   | 4619/7317 [01:54<01:18, 34.18it/s]Training CobwebTree:  63%|   | 4623/7317 [01:54<01:18, 34.43it/s]Training CobwebTree:  63%|   | 4627/7317 [01:54<01:15, 35.45it/s]Training CobwebTree:  63%|   | 4631/7317 [01:54<01:15, 35.35it/s]Training CobwebTree:  63%|   | 4635/7317 [01:54<01:14, 35.89it/s]Training CobwebTree:  63%|   | 4639/7317 [01:54<01:18, 34.14it/s]Training CobwebTree:  63%|   | 4643/7317 [01:54<01:16, 35.05it/s]Training CobwebTree:  64%|   | 4647/7317 [01:54<01:16, 34.70it/s]Training CobwebTree:  64%|   | 4651/7317 [01:55<01:15, 35.38it/s]Training CobwebTree:  64%|   | 4655/7317 [01:55<01:17, 34.48it/s]Training CobwebTree:  64%|   | 4659/7317 [01:55<01:14, 35.48it/s]Training CobwebTree:  64%|   | 4663/7317 [01:55<01:16, 34.77it/s]Training CobwebTree:  64%|   | 4667/7317 [01:55<01:16, 34.44it/s]Training CobwebTree:  64%|   | 4671/7317 [01:55<01:16, 34.57it/s]Training CobwebTree:  64%|   | 4675/7317 [01:55<01:17, 34.27it/s]Training CobwebTree:  64%|   | 4679/7317 [01:55<01:15, 34.86it/s]Training CobwebTree:  64%|   | 4683/7317 [01:56<01:14, 35.15it/s]Training CobwebTree:  64%|   | 4687/7317 [01:56<01:14, 35.22it/s]Training CobwebTree:  64%|   | 4691/7317 [01:56<01:13, 35.56it/s]Training CobwebTree:  64%|   | 4695/7317 [01:56<01:18, 33.39it/s]Training CobwebTree:  64%|   | 4699/7317 [01:56<01:15, 34.75it/s]Training CobwebTree:  64%|   | 4703/7317 [01:56<01:14, 35.04it/s]Training CobwebTree:  64%|   | 4707/7317 [01:56<01:13, 35.66it/s]Training CobwebTree:  64%|   | 4711/7317 [01:56<01:11, 36.61it/s]Training CobwebTree:  64%|   | 4715/7317 [01:56<01:09, 37.50it/s]Training CobwebTree:  64%|   | 4719/7317 [01:57<01:09, 37.16it/s]Training CobwebTree:  65%|   | 4723/7317 [01:57<01:11, 36.15it/s]Training CobwebTree:  65%|   | 4727/7317 [01:57<01:12, 35.88it/s]Training CobwebTree:  65%|   | 4731/7317 [01:57<01:11, 36.05it/s]Training CobwebTree:  65%|   | 4735/7317 [01:57<01:11, 35.90it/s]Training CobwebTree:  65%|   | 4739/7317 [01:57<01:11, 35.83it/s]Training CobwebTree:  65%|   | 4743/7317 [01:57<01:11, 35.96it/s]Training CobwebTree:  65%|   | 4747/7317 [01:57<01:13, 34.77it/s]Training CobwebTree:  65%|   | 4751/7317 [01:57<01:14, 34.63it/s]Training CobwebTree:  65%|   | 4756/7317 [01:58<01:09, 36.62it/s]Training CobwebTree:  65%|   | 4760/7317 [01:58<01:11, 35.98it/s]Training CobwebTree:  65%|   | 4764/7317 [01:58<01:13, 34.64it/s]Training CobwebTree:  65%|   | 4768/7317 [01:58<01:12, 35.25it/s]Training CobwebTree:  65%|   | 4772/7317 [01:58<01:12, 34.95it/s]Training CobwebTree:  65%|   | 4776/7317 [01:58<01:16, 33.34it/s]Training CobwebTree:  65%|   | 4780/7317 [01:58<01:14, 34.00it/s]Training CobwebTree:  65%|   | 4784/7317 [01:58<01:13, 34.58it/s]Training CobwebTree:  65%|   | 4788/7317 [01:58<01:11, 35.31it/s]Training CobwebTree:  65%|   | 4792/7317 [01:59<01:10, 35.73it/s]Training CobwebTree:  66%|   | 4796/7317 [01:59<01:09, 36.05it/s]Training CobwebTree:  66%|   | 4800/7317 [01:59<01:10, 35.94it/s]Training CobwebTree:  66%|   | 4804/7317 [01:59<01:10, 35.73it/s]Training CobwebTree:  66%|   | 4808/7317 [01:59<01:08, 36.62it/s]Training CobwebTree:  66%|   | 4812/7317 [01:59<01:09, 36.17it/s]Training CobwebTree:  66%|   | 4816/7317 [01:59<01:07, 36.84it/s]Training CobwebTree:  66%|   | 4820/7317 [01:59<01:06, 37.55it/s]Training CobwebTree:  66%|   | 4824/7317 [01:59<01:09, 36.10it/s]Training CobwebTree:  66%|   | 4828/7317 [02:00<01:09, 35.66it/s]Training CobwebTree:  66%|   | 4832/7317 [02:00<01:10, 35.32it/s]Training CobwebTree:  66%|   | 4836/7317 [02:00<01:09, 35.84it/s]Training CobwebTree:  66%|   | 4840/7317 [02:00<01:10, 35.32it/s]Training CobwebTree:  66%|   | 4844/7317 [02:00<01:08, 36.11it/s]Training CobwebTree:  66%|   | 4848/7317 [02:00<01:09, 35.43it/s]Training CobwebTree:  66%|   | 4852/7317 [02:00<01:09, 35.36it/s]Training CobwebTree:  66%|   | 4856/7317 [02:00<01:08, 35.94it/s]Training CobwebTree:  66%|   | 4860/7317 [02:00<01:07, 36.19it/s]Training CobwebTree:  66%|   | 4864/7317 [02:01<01:10, 35.00it/s]Training CobwebTree:  67%|   | 4868/7317 [02:01<01:10, 34.96it/s]Training CobwebTree:  67%|   | 4872/7317 [02:01<01:12, 33.57it/s]Training CobwebTree:  67%|   | 4876/7317 [02:01<01:13, 33.43it/s]Training CobwebTree:  67%|   | 4880/7317 [02:01<01:12, 33.74it/s]Training CobwebTree:  67%|   | 4884/7317 [02:01<01:11, 34.24it/s]Training CobwebTree:  67%|   | 4888/7317 [02:01<01:08, 35.33it/s]Training CobwebTree:  67%|   | 4892/7317 [02:01<01:09, 34.95it/s]Training CobwebTree:  67%|   | 4897/7317 [02:02<01:06, 36.33it/s]Training CobwebTree:  67%|   | 4901/7317 [02:02<01:06, 36.14it/s]Training CobwebTree:  67%|   | 4905/7317 [02:02<01:08, 35.32it/s]Training CobwebTree:  67%|   | 4909/7317 [02:02<01:06, 36.33it/s]Training CobwebTree:  67%|   | 4913/7317 [02:02<01:05, 36.58it/s]Training CobwebTree:  67%|   | 4917/7317 [02:02<01:06, 36.22it/s]Training CobwebTree:  67%|   | 4921/7317 [02:02<01:05, 36.36it/s]Training CobwebTree:  67%|   | 4925/7317 [02:02<01:04, 37.27it/s]Training CobwebTree:  67%|   | 4929/7317 [02:02<01:05, 36.58it/s]Training CobwebTree:  67%|   | 4933/7317 [02:03<01:07, 35.11it/s]Training CobwebTree:  67%|   | 4937/7317 [02:03<01:08, 34.63it/s]Training CobwebTree:  68%|   | 4941/7317 [02:03<01:09, 34.07it/s]Training CobwebTree:  68%|   | 4945/7317 [02:03<01:09, 34.18it/s]Training CobwebTree:  68%|   | 4949/7317 [02:03<01:11, 33.07it/s]Training CobwebTree:  68%|   | 4954/7317 [02:03<01:09, 33.84it/s]Training CobwebTree:  68%|   | 4959/7317 [02:03<01:05, 36.09it/s]Training CobwebTree:  68%|   | 4963/7317 [02:03<01:06, 35.53it/s]Training CobwebTree:  68%|   | 4967/7317 [02:04<01:05, 35.74it/s]Training CobwebTree:  68%|   | 4971/7317 [02:04<01:06, 35.20it/s]Training CobwebTree:  68%|   | 4976/7317 [02:04<01:03, 36.67it/s]Training CobwebTree:  68%|   | 4980/7317 [02:04<01:03, 36.67it/s]Training CobwebTree:  68%|   | 4984/7317 [02:04<01:03, 36.72it/s]Training CobwebTree:  68%|   | 4988/7317 [02:04<01:04, 35.90it/s]Training CobwebTree:  68%|   | 4992/7317 [02:04<01:03, 36.45it/s]Training CobwebTree:  68%|   | 4996/7317 [02:04<01:04, 36.04it/s]Training CobwebTree:  68%|   | 5000/7317 [02:04<01:04, 36.12it/s]Training CobwebTree:  68%|   | 5004/7317 [02:05<01:05, 35.18it/s]Training CobwebTree:  68%|   | 5008/7317 [02:05<01:06, 34.84it/s]Training CobwebTree:  68%|   | 5012/7317 [02:05<01:03, 36.03it/s]Training CobwebTree:  69%|   | 5017/7317 [02:05<01:02, 36.92it/s]Training CobwebTree:  69%|   | 5021/7317 [02:05<01:02, 36.93it/s]Training CobwebTree:  69%|   | 5025/7317 [02:05<01:03, 35.94it/s]Training CobwebTree:  69%|   | 5030/7317 [02:05<01:02, 36.83it/s]Training CobwebTree:  69%|   | 5034/7317 [02:05<01:03, 35.86it/s]Training CobwebTree:  69%|   | 5038/7317 [02:05<01:03, 35.96it/s]Training CobwebTree:  69%|   | 5042/7317 [02:06<01:03, 35.76it/s]Training CobwebTree:  69%|   | 5047/7317 [02:06<01:01, 36.96it/s]Training CobwebTree:  69%|   | 5051/7317 [02:06<01:00, 37.40it/s]Training CobwebTree:  69%|   | 5055/7317 [02:06<01:04, 35.05it/s]Training CobwebTree:  69%|   | 5059/7317 [02:06<01:03, 35.77it/s]Training CobwebTree:  69%|   | 5063/7317 [02:06<01:03, 35.43it/s]Training CobwebTree:  69%|   | 5067/7317 [02:06<01:03, 35.45it/s]Training CobwebTree:  69%|   | 5071/7317 [02:06<01:08, 32.89it/s]Training CobwebTree:  69%|   | 5075/7317 [02:07<01:05, 34.29it/s]Training CobwebTree:  69%|   | 5079/7317 [02:07<01:04, 34.75it/s]Training CobwebTree:  69%|   | 5083/7317 [02:07<01:03, 35.32it/s]Training CobwebTree:  70%|   | 5087/7317 [02:07<01:05, 33.82it/s]Training CobwebTree:  70%|   | 5091/7317 [02:07<01:05, 34.12it/s]Training CobwebTree:  70%|   | 5095/7317 [02:07<01:03, 35.09it/s]Training CobwebTree:  70%|   | 5099/7317 [02:07<01:03, 34.66it/s]Training CobwebTree:  70%|   | 5103/7317 [02:07<01:03, 34.96it/s]Training CobwebTree:  70%|   | 5107/7317 [02:07<01:03, 35.02it/s]Training CobwebTree:  70%|   | 5111/7317 [02:08<01:03, 34.52it/s]Training CobwebTree:  70%|   | 5115/7317 [02:08<01:03, 34.78it/s]Training CobwebTree:  70%|   | 5119/7317 [02:08<01:05, 33.78it/s]Training CobwebTree:  70%|   | 5124/7317 [02:08<00:58, 37.24it/s]Training CobwebTree:  70%|   | 5128/7317 [02:08<00:59, 36.75it/s]Training CobwebTree:  70%|   | 5132/7317 [02:08<01:01, 35.71it/s]Training CobwebTree:  70%|   | 5136/7317 [02:08<01:00, 36.00it/s]Training CobwebTree:  70%|   | 5140/7317 [02:08<01:01, 35.12it/s]Training CobwebTree:  70%|   | 5144/7317 [02:08<01:02, 34.84it/s]Training CobwebTree:  70%|   | 5148/7317 [02:09<00:59, 36.22it/s]Training CobwebTree:  70%|   | 5152/7317 [02:09<01:00, 36.02it/s]Training CobwebTree:  70%|   | 5156/7317 [02:09<01:00, 35.58it/s]Training CobwebTree:  71%|   | 5160/7317 [02:09<00:58, 36.72it/s]Training CobwebTree:  71%|   | 5164/7317 [02:09<00:59, 36.10it/s]Training CobwebTree:  71%|   | 5168/7317 [02:09<00:58, 36.99it/s]Training CobwebTree:  71%|   | 5172/7317 [02:09<01:00, 35.67it/s]Training CobwebTree:  71%|   | 5176/7317 [02:09<01:01, 34.80it/s]Training CobwebTree:  71%|   | 5181/7317 [02:10<00:58, 36.38it/s]Training CobwebTree:  71%|   | 5185/7317 [02:10<01:02, 34.24it/s]Training CobwebTree:  71%|   | 5190/7317 [02:10<00:59, 35.65it/s]Training CobwebTree:  71%|   | 5194/7317 [02:10<01:01, 34.31it/s]Training CobwebTree:  71%|   | 5198/7317 [02:10<01:01, 34.62it/s]Training CobwebTree:  71%|   | 5202/7317 [02:10<01:00, 34.94it/s]Training CobwebTree:  71%|   | 5206/7317 [02:10<00:59, 35.50it/s]Training CobwebTree:  71%|   | 5210/7317 [02:10<00:58, 35.83it/s]Training CobwebTree:  71%|  | 5214/7317 [02:10<00:59, 35.44it/s]Training CobwebTree:  71%|  | 5218/7317 [02:11<00:58, 35.65it/s]Training CobwebTree:  71%|  | 5222/7317 [02:11<01:00, 34.75it/s]Training CobwebTree:  71%|  | 5226/7317 [02:11<00:58, 35.84it/s]Training CobwebTree:  71%|  | 5230/7317 [02:11<00:57, 36.27it/s]Training CobwebTree:  72%|  | 5234/7317 [02:11<00:56, 36.63it/s]Training CobwebTree:  72%|  | 5238/7317 [02:11<00:55, 37.32it/s]Training CobwebTree:  72%|  | 5242/7317 [02:11<00:58, 35.53it/s]Training CobwebTree:  72%|  | 5246/7317 [02:11<00:58, 35.19it/s]Training CobwebTree:  72%|  | 5250/7317 [02:11<00:57, 35.80it/s]Training CobwebTree:  72%|  | 5254/7317 [02:12<00:58, 35.52it/s]Training CobwebTree:  72%|  | 5258/7317 [02:12<01:00, 34.19it/s]Training CobwebTree:  72%|  | 5262/7317 [02:12<00:59, 34.66it/s]Training CobwebTree:  72%|  | 5266/7317 [02:12<00:58, 35.29it/s]Training CobwebTree:  72%|  | 5270/7317 [02:12<00:59, 34.44it/s]Training CobwebTree:  72%|  | 5274/7317 [02:12<00:59, 34.36it/s]Training CobwebTree:  72%|  | 5278/7317 [02:12<00:57, 35.25it/s]Training CobwebTree:  72%|  | 5282/7317 [02:12<00:56, 35.96it/s]Training CobwebTree:  72%|  | 5286/7317 [02:12<00:56, 35.90it/s]Training CobwebTree:  72%|  | 5290/7317 [02:13<00:57, 35.18it/s]Training CobwebTree:  72%|  | 5294/7317 [02:13<00:59, 33.78it/s]Training CobwebTree:  72%|  | 5298/7317 [02:13<00:58, 34.60it/s]Training CobwebTree:  72%|  | 5302/7317 [02:13<00:57, 35.30it/s]Training CobwebTree:  73%|  | 5306/7317 [02:13<00:58, 34.54it/s]Training CobwebTree:  73%|  | 5310/7317 [02:13<00:55, 36.00it/s]Training CobwebTree:  73%|  | 5314/7317 [02:13<00:56, 35.52it/s]Training CobwebTree:  73%|  | 5318/7317 [02:13<00:58, 34.03it/s]Training CobwebTree:  73%|  | 5322/7317 [02:14<00:59, 33.67it/s]Training CobwebTree:  73%|  | 5326/7317 [02:14<00:59, 33.42it/s]Training CobwebTree:  73%|  | 5330/7317 [02:14<00:58, 33.70it/s]Training CobwebTree:  73%|  | 5334/7317 [02:14<00:59, 33.07it/s]Training CobwebTree:  73%|  | 5338/7317 [02:14<00:57, 34.22it/s]Training CobwebTree:  73%|  | 5342/7317 [02:14<00:58, 33.98it/s]Training CobwebTree:  73%|  | 5346/7317 [02:14<00:57, 34.18it/s]Training CobwebTree:  73%|  | 5350/7317 [02:14<00:56, 34.96it/s]Training CobwebTree:  73%|  | 5354/7317 [02:14<00:57, 34.02it/s]Training CobwebTree:  73%|  | 5358/7317 [02:15<00:57, 33.79it/s]Training CobwebTree:  73%|  | 5362/7317 [02:15<00:58, 33.33it/s]Training CobwebTree:  73%|  | 5366/7317 [02:15<00:57, 34.14it/s]Training CobwebTree:  73%|  | 5370/7317 [02:15<00:59, 32.56it/s]Training CobwebTree:  73%|  | 5374/7317 [02:15<01:01, 31.85it/s]Training CobwebTree:  74%|  | 5378/7317 [02:15<01:00, 32.09it/s]Training CobwebTree:  74%|  | 5382/7317 [02:15<00:58, 33.06it/s]Training CobwebTree:  74%|  | 5386/7317 [02:15<00:55, 34.55it/s]Training CobwebTree:  74%|  | 5390/7317 [02:16<00:55, 34.58it/s]Training CobwebTree:  74%|  | 5394/7317 [02:16<00:56, 34.11it/s]Training CobwebTree:  74%|  | 5398/7317 [02:16<00:55, 34.29it/s]Training CobwebTree:  74%|  | 5402/7317 [02:16<00:59, 32.44it/s]Training CobwebTree:  74%|  | 5406/7317 [02:16<00:55, 34.14it/s]Training CobwebTree:  74%|  | 5410/7317 [02:16<00:53, 35.41it/s]Training CobwebTree:  74%|  | 5414/7317 [02:16<00:52, 36.35it/s]Training CobwebTree:  74%|  | 5418/7317 [02:16<00:55, 33.97it/s]Training CobwebTree:  74%|  | 5423/7317 [02:17<00:53, 35.70it/s]Training CobwebTree:  74%|  | 5427/7317 [02:17<00:52, 35.73it/s]Training CobwebTree:  74%|  | 5431/7317 [02:17<00:52, 36.11it/s]Training CobwebTree:  74%|  | 5435/7317 [02:17<00:52, 35.66it/s]Training CobwebTree:  74%|  | 5439/7317 [02:17<00:52, 35.69it/s]Training CobwebTree:  74%|  | 5443/7317 [02:17<00:52, 35.65it/s]Training CobwebTree:  74%|  | 5447/7317 [02:17<00:53, 34.93it/s]Training CobwebTree:  74%|  | 5451/7317 [02:17<00:55, 33.74it/s]Training CobwebTree:  75%|  | 5455/7317 [02:17<00:54, 34.48it/s]Training CobwebTree:  75%|  | 5459/7317 [02:18<00:52, 35.41it/s]Training CobwebTree:  75%|  | 5463/7317 [02:18<00:53, 34.55it/s]Training CobwebTree:  75%|  | 5467/7317 [02:18<00:53, 34.59it/s]Training CobwebTree:  75%|  | 5471/7317 [02:18<00:52, 35.12it/s]Training CobwebTree:  75%|  | 5475/7317 [02:18<00:52, 34.90it/s]Training CobwebTree:  75%|  | 5479/7317 [02:18<00:52, 34.77it/s]Training CobwebTree:  75%|  | 5483/7317 [02:18<00:53, 34.22it/s]Training CobwebTree:  75%|  | 5487/7317 [02:18<00:53, 34.44it/s]Training CobwebTree:  75%|  | 5491/7317 [02:18<00:55, 33.05it/s]Training CobwebTree:  75%|  | 5495/7317 [02:19<00:55, 32.97it/s]Training CobwebTree:  75%|  | 5499/7317 [02:19<00:55, 32.59it/s]Training CobwebTree:  75%|  | 5503/7317 [02:19<00:54, 33.36it/s]Training CobwebTree:  75%|  | 5507/7317 [02:19<00:54, 33.42it/s]Training CobwebTree:  75%|  | 5511/7317 [02:19<00:52, 34.17it/s]Training CobwebTree:  75%|  | 5515/7317 [02:19<00:53, 33.66it/s]Training CobwebTree:  75%|  | 5519/7317 [02:19<00:52, 34.34it/s]Training CobwebTree:  75%|  | 5523/7317 [02:19<00:50, 35.63it/s]Training CobwebTree:  76%|  | 5527/7317 [02:20<00:52, 34.02it/s]Training CobwebTree:  76%|  | 5531/7317 [02:20<00:51, 34.47it/s]Training CobwebTree:  76%|  | 5535/7317 [02:20<00:52, 33.71it/s]Training CobwebTree:  76%|  | 5539/7317 [02:20<00:51, 34.85it/s]Training CobwebTree:  76%|  | 5543/7317 [02:20<00:50, 35.25it/s]Training CobwebTree:  76%|  | 5547/7317 [02:20<00:49, 35.74it/s]Training CobwebTree:  76%|  | 5551/7317 [02:20<00:50, 35.05it/s]Training CobwebTree:  76%|  | 5555/7317 [02:20<00:51, 34.21it/s]Training CobwebTree:  76%|  | 5559/7317 [02:20<00:51, 33.81it/s]Training CobwebTree:  76%|  | 5563/7317 [02:21<00:51, 33.96it/s]Training CobwebTree:  76%|  | 5567/7317 [02:21<00:53, 32.98it/s]Training CobwebTree:  76%|  | 5571/7317 [02:21<00:52, 33.19it/s]Training CobwebTree:  76%|  | 5575/7317 [02:21<00:51, 33.62it/s]Training CobwebTree:  76%|  | 5579/7317 [02:21<00:51, 33.90it/s]Training CobwebTree:  76%|  | 5583/7317 [02:21<00:50, 34.63it/s]Training CobwebTree:  76%|  | 5587/7317 [02:21<00:48, 35.78it/s]Training CobwebTree:  76%|  | 5591/7317 [02:21<00:48, 35.85it/s]Training CobwebTree:  76%|  | 5595/7317 [02:21<00:48, 35.49it/s]Training CobwebTree:  77%|  | 5599/7317 [02:22<00:50, 33.82it/s]Training CobwebTree:  77%|  | 5603/7317 [02:22<00:49, 34.56it/s]Training CobwebTree:  77%|  | 5607/7317 [02:22<00:48, 35.41it/s]Training CobwebTree:  77%|  | 5611/7317 [02:22<00:50, 33.99it/s]Training CobwebTree:  77%|  | 5615/7317 [02:22<00:49, 34.32it/s]Training CobwebTree:  77%|  | 5619/7317 [02:22<00:51, 33.02it/s]Training CobwebTree:  77%|  | 5623/7317 [02:22<00:49, 34.07it/s]Training CobwebTree:  77%|  | 5628/7317 [02:22<00:45, 37.04it/s]Training CobwebTree:  77%|  | 5633/7317 [02:23<00:43, 38.55it/s]Training CobwebTree:  77%|  | 5637/7317 [02:23<00:46, 35.97it/s]Training CobwebTree:  77%|  | 5642/7317 [02:23<00:45, 37.13it/s]Training CobwebTree:  77%|  | 5647/7317 [02:23<00:43, 38.77it/s]Training CobwebTree:  77%|  | 5651/7317 [02:23<00:43, 38.72it/s]Training CobwebTree:  77%|  | 5655/7317 [02:23<00:43, 38.53it/s]Training CobwebTree:  77%|  | 5659/7317 [02:23<00:44, 37.11it/s]Training CobwebTree:  77%|  | 5663/7317 [02:23<00:44, 37.43it/s]Training CobwebTree:  77%|  | 5667/7317 [02:23<00:44, 36.86it/s]Training CobwebTree:  78%|  | 5671/7317 [02:24<00:44, 37.30it/s]Training CobwebTree:  78%|  | 5675/7317 [02:24<00:45, 35.81it/s]Training CobwebTree:  78%|  | 5679/7317 [02:24<00:46, 35.31it/s]Training CobwebTree:  78%|  | 5683/7317 [02:24<00:48, 33.79it/s]Training CobwebTree:  78%|  | 5687/7317 [02:24<00:46, 35.35it/s]Training CobwebTree:  78%|  | 5691/7317 [02:24<00:45, 36.10it/s]Training CobwebTree:  78%|  | 5695/7317 [02:24<00:45, 35.54it/s]Training CobwebTree:  78%|  | 5699/7317 [02:24<00:44, 36.20it/s]Training CobwebTree:  78%|  | 5703/7317 [02:25<00:46, 34.87it/s]Training CobwebTree:  78%|  | 5707/7317 [02:25<00:44, 36.10it/s]Training CobwebTree:  78%|  | 5711/7317 [02:25<00:44, 35.75it/s]Training CobwebTree:  78%|  | 5715/7317 [02:25<00:44, 36.23it/s]Training CobwebTree:  78%|  | 5719/7317 [02:25<00:43, 37.02it/s]Training CobwebTree:  78%|  | 5723/7317 [02:25<00:45, 34.84it/s]Training CobwebTree:  78%|  | 5727/7317 [02:25<00:44, 35.40it/s]Training CobwebTree:  78%|  | 5731/7317 [02:25<00:47, 33.62it/s]Training CobwebTree:  78%|  | 5735/7317 [02:25<00:48, 32.71it/s]Training CobwebTree:  78%|  | 5739/7317 [02:26<00:47, 33.44it/s]Training CobwebTree:  78%|  | 5743/7317 [02:26<00:49, 31.86it/s]Training CobwebTree:  79%|  | 5747/7317 [02:26<00:48, 32.40it/s]Training CobwebTree:  79%|  | 5751/7317 [02:26<00:46, 33.74it/s]Training CobwebTree:  79%|  | 5756/7317 [02:26<00:44, 34.98it/s]Training CobwebTree:  79%|  | 5760/7317 [02:26<00:46, 33.31it/s]Training CobwebTree:  79%|  | 5764/7317 [02:26<00:46, 33.63it/s]Training CobwebTree:  79%|  | 5768/7317 [02:26<00:44, 34.66it/s]Training CobwebTree:  79%|  | 5772/7317 [02:27<00:44, 34.47it/s]Training CobwebTree:  79%|  | 5776/7317 [02:27<00:43, 35.72it/s]Training CobwebTree:  79%|  | 5780/7317 [02:27<00:43, 34.98it/s]Training CobwebTree:  79%|  | 5784/7317 [02:27<00:44, 34.17it/s]Training CobwebTree:  79%|  | 5788/7317 [02:27<00:44, 34.72it/s]Training CobwebTree:  79%|  | 5792/7317 [02:27<00:44, 34.30it/s]Training CobwebTree:  79%|  | 5796/7317 [02:27<00:43, 35.34it/s]Training CobwebTree:  79%|  | 5800/7317 [02:27<00:42, 35.88it/s]Training CobwebTree:  79%|  | 5804/7317 [02:27<00:41, 36.74it/s]Training CobwebTree:  79%|  | 5808/7317 [02:28<00:43, 35.05it/s]Training CobwebTree:  79%|  | 5812/7317 [02:28<00:42, 35.50it/s]Training CobwebTree:  79%|  | 5816/7317 [02:28<00:43, 34.49it/s]Training CobwebTree:  80%|  | 5820/7317 [02:28<00:44, 33.99it/s]Training CobwebTree:  80%|  | 5824/7317 [02:28<00:42, 35.29it/s]Training CobwebTree:  80%|  | 5828/7317 [02:28<00:41, 36.15it/s]Training CobwebTree:  80%|  | 5832/7317 [02:28<00:41, 35.83it/s]Training CobwebTree:  80%|  | 5836/7317 [02:28<00:43, 34.24it/s]Training CobwebTree:  80%|  | 5840/7317 [02:28<00:45, 32.30it/s]Training CobwebTree:  80%|  | 5844/7317 [02:29<00:47, 31.29it/s]Training CobwebTree:  80%|  | 5848/7317 [02:29<00:46, 31.68it/s]Training CobwebTree:  80%|  | 5852/7317 [02:29<00:45, 32.46it/s]Training CobwebTree:  80%|  | 5856/7317 [02:29<00:43, 33.78it/s]Training CobwebTree:  80%|  | 5860/7317 [02:29<00:42, 34.11it/s]Training CobwebTree:  80%|  | 5864/7317 [02:29<00:42, 34.00it/s]Training CobwebTree:  80%|  | 5868/7317 [02:29<00:42, 34.00it/s]Training CobwebTree:  80%|  | 5872/7317 [02:29<00:41, 34.43it/s]Training CobwebTree:  80%|  | 5876/7317 [02:30<00:44, 32.32it/s]Training CobwebTree:  80%|  | 5880/7317 [02:30<00:46, 31.02it/s]Training CobwebTree:  80%|  | 5884/7317 [02:30<00:45, 31.78it/s]Training CobwebTree:  80%|  | 5888/7317 [02:30<00:43, 32.49it/s]Training CobwebTree:  81%|  | 5892/7317 [02:30<00:43, 32.59it/s]Training CobwebTree:  81%|  | 5896/7317 [02:30<00:43, 32.92it/s]Training CobwebTree:  81%|  | 5900/7317 [02:30<00:40, 34.57it/s]Training CobwebTree:  81%|  | 5904/7317 [02:30<00:40, 34.74it/s]Training CobwebTree:  81%|  | 5908/7317 [02:31<00:44, 31.79it/s]Training CobwebTree:  81%|  | 5912/7317 [02:31<00:41, 33.71it/s]Training CobwebTree:  81%|  | 5916/7317 [02:31<00:43, 32.58it/s]Training CobwebTree:  81%|  | 5920/7317 [02:31<00:41, 33.40it/s]Training CobwebTree:  81%|  | 5924/7317 [02:31<00:41, 33.44it/s]Training CobwebTree:  81%|  | 5928/7317 [02:31<00:42, 32.64it/s]Training CobwebTree:  81%|  | 5932/7317 [02:31<00:42, 32.63it/s]Training CobwebTree:  81%|  | 5936/7317 [02:31<00:41, 33.46it/s]Training CobwebTree:  81%|  | 5940/7317 [02:32<00:41, 33.48it/s]Training CobwebTree:  81%|  | 5944/7317 [02:32<00:39, 34.58it/s]Training CobwebTree:  81%| | 5948/7317 [02:32<00:38, 35.28it/s]Training CobwebTree:  81%| | 5952/7317 [02:32<00:40, 33.97it/s]Training CobwebTree:  81%| | 5956/7317 [02:32<00:40, 33.68it/s]Training CobwebTree:  81%| | 5960/7317 [02:32<00:39, 34.31it/s]Training CobwebTree:  82%| | 5964/7317 [02:32<00:38, 34.71it/s]Training CobwebTree:  82%| | 5968/7317 [02:32<00:39, 34.34it/s]Training CobwebTree:  82%| | 5972/7317 [02:32<00:39, 33.99it/s]Training CobwebTree:  82%| | 5976/7317 [02:33<00:40, 33.48it/s]Training CobwebTree:  82%| | 5980/7317 [02:33<00:39, 33.75it/s]Training CobwebTree:  82%| | 5984/7317 [02:33<00:38, 34.28it/s]Training CobwebTree:  82%| | 5988/7317 [02:33<00:39, 33.54it/s]Training CobwebTree:  82%| | 5992/7317 [02:33<00:40, 32.95it/s]Training CobwebTree:  82%| | 5996/7317 [02:33<00:39, 33.51it/s]Training CobwebTree:  82%| | 6000/7317 [02:33<00:40, 32.92it/s]Training CobwebTree:  82%| | 6004/7317 [02:33<00:40, 32.82it/s]Training CobwebTree:  82%| | 6008/7317 [02:34<00:38, 34.11it/s]Training CobwebTree:  82%| | 6012/7317 [02:34<00:39, 33.44it/s]Training CobwebTree:  82%| | 6016/7317 [02:34<00:39, 33.20it/s]Training CobwebTree:  82%| | 6020/7317 [02:34<00:37, 34.40it/s]Training CobwebTree:  82%| | 6024/7317 [02:34<00:37, 34.87it/s]Training CobwebTree:  82%| | 6028/7317 [02:34<00:37, 34.15it/s]Training CobwebTree:  82%| | 6032/7317 [02:34<00:38, 33.70it/s]Training CobwebTree:  82%| | 6036/7317 [02:34<00:37, 34.44it/s]Training CobwebTree:  83%| | 6040/7317 [02:34<00:36, 35.34it/s]Training CobwebTree:  83%| | 6044/7317 [02:35<00:36, 35.16it/s]Training CobwebTree:  83%| | 6048/7317 [02:35<00:37, 33.99it/s]Training CobwebTree:  83%| | 6052/7317 [02:35<00:37, 33.50it/s]Training CobwebTree:  83%| | 6056/7317 [02:35<00:37, 33.67it/s]Training CobwebTree:  83%| | 6061/7317 [02:35<00:35, 35.77it/s]Training CobwebTree:  83%| | 6065/7317 [02:35<00:34, 36.26it/s]Training CobwebTree:  83%| | 6069/7317 [02:35<00:35, 35.54it/s]Training CobwebTree:  83%| | 6073/7317 [02:35<00:34, 35.68it/s]Training CobwebTree:  83%| | 6077/7317 [02:35<00:34, 35.85it/s]Training CobwebTree:  83%| | 6081/7317 [02:36<00:36, 33.42it/s]Training CobwebTree:  83%| | 6085/7317 [02:36<00:38, 32.28it/s]Training CobwebTree:  83%| | 6089/7317 [02:36<00:37, 33.08it/s]Training CobwebTree:  83%| | 6093/7317 [02:36<00:35, 34.71it/s]Training CobwebTree:  83%| | 6097/7317 [02:36<00:34, 34.88it/s]Training CobwebTree:  83%| | 6101/7317 [02:36<00:34, 35.20it/s]Training CobwebTree:  83%| | 6105/7317 [02:36<00:35, 34.28it/s]Training CobwebTree:  83%| | 6109/7317 [02:36<00:34, 35.01it/s]Training CobwebTree:  84%| | 6113/7317 [02:37<00:34, 34.63it/s]Training CobwebTree:  84%| | 6117/7317 [02:37<00:36, 33.27it/s]Training CobwebTree:  84%| | 6121/7317 [02:37<00:34, 34.55it/s]Training CobwebTree:  84%| | 6125/7317 [02:37<00:34, 34.35it/s]Training CobwebTree:  84%| | 6129/7317 [02:37<00:33, 35.39it/s]Training CobwebTree:  84%| | 6133/7317 [02:37<00:33, 35.44it/s]Training CobwebTree:  84%| | 6137/7317 [02:37<00:34, 34.45it/s]Training CobwebTree:  84%| | 6141/7317 [02:37<00:33, 34.74it/s]Training CobwebTree:  84%| | 6145/7317 [02:37<00:34, 34.04it/s]Training CobwebTree:  84%| | 6149/7317 [02:38<00:33, 34.87it/s]Training CobwebTree:  84%| | 6153/7317 [02:38<00:34, 33.77it/s]Training CobwebTree:  84%| | 6157/7317 [02:38<00:34, 33.95it/s]Training CobwebTree:  84%| | 6161/7317 [02:38<00:34, 33.51it/s]Training CobwebTree:  84%| | 6165/7317 [02:38<00:33, 34.13it/s]Training CobwebTree:  84%| | 6169/7317 [02:38<00:33, 34.57it/s]Training CobwebTree:  84%| | 6173/7317 [02:38<00:33, 34.66it/s]Training CobwebTree:  84%| | 6177/7317 [02:38<00:34, 33.28it/s]Training CobwebTree:  84%| | 6181/7317 [02:39<00:34, 33.38it/s]Training CobwebTree:  85%| | 6185/7317 [02:39<00:34, 32.97it/s]Training CobwebTree:  85%| | 6189/7317 [02:39<00:33, 33.22it/s]Training CobwebTree:  85%| | 6193/7317 [02:39<00:33, 33.85it/s]Training CobwebTree:  85%| | 6197/7317 [02:39<00:33, 33.14it/s]Training CobwebTree:  85%| | 6201/7317 [02:39<00:33, 33.57it/s]Training CobwebTree:  85%| | 6205/7317 [02:39<00:31, 35.09it/s]Training CobwebTree:  85%| | 6209/7317 [02:39<00:32, 34.61it/s]Training CobwebTree:  85%| | 6213/7317 [02:39<00:31, 35.26it/s]Training CobwebTree:  85%| | 6217/7317 [02:40<00:30, 36.19it/s]Training CobwebTree:  85%| | 6221/7317 [02:40<00:30, 36.03it/s]Training CobwebTree:  85%| | 6225/7317 [02:40<00:31, 34.77it/s]Training CobwebTree:  85%| | 6230/7317 [02:40<00:30, 35.59it/s]Training CobwebTree:  85%| | 6235/7317 [02:40<00:29, 36.80it/s]Training CobwebTree:  85%| | 6239/7317 [02:40<00:29, 36.43it/s]Training CobwebTree:  85%| | 6243/7317 [02:40<00:30, 35.38it/s]Training CobwebTree:  85%| | 6247/7317 [02:40<00:29, 35.91it/s]Training CobwebTree:  85%| | 6251/7317 [02:41<00:30, 34.78it/s]Training CobwebTree:  85%| | 6255/7317 [02:41<00:31, 33.81it/s]Training CobwebTree:  86%| | 6259/7317 [02:41<00:30, 34.88it/s]Training CobwebTree:  86%| | 6263/7317 [02:41<00:31, 33.95it/s]Training CobwebTree:  86%| | 6268/7317 [02:41<00:28, 36.40it/s]Training CobwebTree:  86%| | 6273/7317 [02:41<00:27, 37.83it/s]Training CobwebTree:  86%| | 6277/7317 [02:41<00:27, 38.22it/s]Training CobwebTree:  86%| | 6281/7317 [02:41<00:27, 38.03it/s]Training CobwebTree:  86%| | 6285/7317 [02:41<00:28, 35.87it/s]Training CobwebTree:  86%| | 6289/7317 [02:42<00:28, 36.38it/s]Training CobwebTree:  86%| | 6293/7317 [02:42<00:29, 35.18it/s]Training CobwebTree:  86%| | 6297/7317 [02:42<00:29, 34.97it/s]Training CobwebTree:  86%| | 6301/7317 [02:42<00:30, 33.75it/s]Training CobwebTree:  86%| | 6305/7317 [02:42<00:30, 33.71it/s]Training CobwebTree:  86%| | 6309/7317 [02:42<00:29, 33.62it/s]Training CobwebTree:  86%| | 6313/7317 [02:42<00:29, 33.83it/s]Training CobwebTree:  86%| | 6317/7317 [02:42<00:30, 32.71it/s]Training CobwebTree:  86%| | 6321/7317 [02:43<00:30, 33.04it/s]Training CobwebTree:  86%| | 6326/7317 [02:43<00:28, 35.20it/s]Training CobwebTree:  87%| | 6330/7317 [02:43<00:27, 35.78it/s]Training CobwebTree:  87%| | 6334/7317 [02:43<00:27, 35.71it/s]Training CobwebTree:  87%| | 6338/7317 [02:43<00:27, 36.14it/s]Training CobwebTree:  87%| | 6342/7317 [02:43<00:27, 35.83it/s]Training CobwebTree:  87%| | 6346/7317 [02:43<00:26, 36.73it/s]Training CobwebTree:  87%| | 6350/7317 [02:43<00:26, 37.14it/s]Training CobwebTree:  87%| | 6354/7317 [02:43<00:27, 35.57it/s]Training CobwebTree:  87%| | 6358/7317 [02:44<00:26, 35.54it/s]Training CobwebTree:  87%| | 6362/7317 [02:44<00:27, 34.50it/s]Training CobwebTree:  87%| | 6366/7317 [02:44<00:27, 34.64it/s]Training CobwebTree:  87%| | 6370/7317 [02:44<00:26, 35.12it/s]Training CobwebTree:  87%| | 6374/7317 [02:44<00:27, 34.24it/s]Training CobwebTree:  87%| | 6378/7317 [02:44<00:27, 34.09it/s]Training CobwebTree:  87%| | 6382/7317 [02:44<00:27, 34.20it/s]Training CobwebTree:  87%| | 6386/7317 [02:44<00:26, 35.70it/s]Training CobwebTree:  87%| | 6390/7317 [02:44<00:26, 34.92it/s]Training CobwebTree:  87%| | 6394/7317 [02:45<00:28, 32.81it/s]Training CobwebTree:  87%| | 6398/7317 [02:45<00:27, 33.81it/s]Training CobwebTree:  87%| | 6402/7317 [02:45<00:26, 34.15it/s]Training CobwebTree:  88%| | 6406/7317 [02:45<00:26, 34.84it/s]Training CobwebTree:  88%| | 6410/7317 [02:45<00:26, 34.27it/s]Training CobwebTree:  88%| | 6414/7317 [02:45<00:26, 33.56it/s]Training CobwebTree:  88%| | 6418/7317 [02:45<00:25, 34.76it/s]Training CobwebTree:  88%| | 6422/7317 [02:45<00:24, 36.05it/s]Training CobwebTree:  88%| | 6426/7317 [02:46<00:26, 33.98it/s]Training CobwebTree:  88%| | 6430/7317 [02:46<00:25, 35.13it/s]Training CobwebTree:  88%| | 6434/7317 [02:46<00:25, 34.57it/s]Training CobwebTree:  88%| | 6438/7317 [02:46<00:25, 34.45it/s]Training CobwebTree:  88%| | 6442/7317 [02:46<00:25, 33.69it/s]Training CobwebTree:  88%| | 6446/7317 [02:46<00:26, 32.92it/s]Training CobwebTree:  88%| | 6450/7317 [02:46<00:25, 33.76it/s]Training CobwebTree:  88%| | 6454/7317 [02:46<00:25, 33.98it/s]Training CobwebTree:  88%| | 6458/7317 [02:46<00:24, 34.49it/s]Training CobwebTree:  88%| | 6462/7317 [02:47<00:24, 34.90it/s]Training CobwebTree:  88%| | 6466/7317 [02:47<00:23, 35.74it/s]Training CobwebTree:  88%| | 6470/7317 [02:47<00:24, 35.28it/s]Training CobwebTree:  88%| | 6474/7317 [02:47<00:23, 36.21it/s]Training CobwebTree:  89%| | 6478/7317 [02:47<00:23, 35.95it/s]Training CobwebTree:  89%| | 6482/7317 [02:47<00:23, 36.18it/s]Training CobwebTree:  89%| | 6486/7317 [02:47<00:23, 36.01it/s]Training CobwebTree:  89%| | 6490/7317 [02:47<00:22, 36.63it/s]Training CobwebTree:  89%| | 6494/7317 [02:47<00:23, 35.76it/s]Training CobwebTree:  89%| | 6498/7317 [02:48<00:22, 35.62it/s]Training CobwebTree:  89%| | 6502/7317 [02:48<00:22, 36.36it/s]Training CobwebTree:  89%| | 6506/7317 [02:48<00:22, 36.13it/s]Training CobwebTree:  89%| | 6510/7317 [02:48<00:23, 35.02it/s]Training CobwebTree:  89%| | 6514/7317 [02:48<00:23, 34.54it/s]Training CobwebTree:  89%| | 6518/7317 [02:48<00:22, 35.38it/s]Training CobwebTree:  89%| | 6522/7317 [02:48<00:22, 35.54it/s]Training CobwebTree:  89%| | 6526/7317 [02:48<00:22, 34.82it/s]Training CobwebTree:  89%| | 6530/7317 [02:49<00:22, 34.64it/s]Training CobwebTree:  89%| | 6534/7317 [02:49<00:22, 34.19it/s]Training CobwebTree:  89%| | 6538/7317 [02:49<00:22, 35.40it/s]Training CobwebTree:  89%| | 6542/7317 [02:49<00:23, 33.54it/s]Training CobwebTree:  89%| | 6546/7317 [02:49<00:22, 34.90it/s]Training CobwebTree:  90%| | 6550/7317 [02:49<00:23, 33.20it/s]Training CobwebTree:  90%| | 6554/7317 [02:49<00:24, 31.61it/s]Training CobwebTree:  90%| | 6559/7317 [02:49<00:22, 33.60it/s]Training CobwebTree:  90%| | 6563/7317 [02:49<00:21, 34.87it/s]Training CobwebTree:  90%| | 6567/7317 [02:50<00:21, 34.93it/s]Training CobwebTree:  90%| | 6571/7317 [02:50<00:21, 34.34it/s]Training CobwebTree:  90%| | 6575/7317 [02:50<00:21, 35.11it/s]Training CobwebTree:  90%| | 6579/7317 [02:50<00:21, 34.96it/s]Training CobwebTree:  90%| | 6583/7317 [02:50<00:20, 35.77it/s]Training CobwebTree:  90%| | 6587/7317 [02:50<00:19, 36.66it/s]Training CobwebTree:  90%| | 6591/7317 [02:50<00:19, 36.78it/s]Training CobwebTree:  90%| | 6595/7317 [02:50<00:19, 36.72it/s]Training CobwebTree:  90%| | 6599/7317 [02:50<00:20, 34.93it/s]Training CobwebTree:  90%| | 6603/7317 [02:51<00:20, 35.23it/s]Training CobwebTree:  90%| | 6607/7317 [02:51<00:20, 33.83it/s]Training CobwebTree:  90%| | 6612/7317 [02:51<00:20, 35.23it/s]Training CobwebTree:  90%| | 6616/7317 [02:51<00:20, 34.64it/s]Training CobwebTree:  90%| | 6620/7317 [02:51<00:20, 34.85it/s]Training CobwebTree:  91%| | 6624/7317 [02:51<00:20, 33.26it/s]Training CobwebTree:  91%| | 6628/7317 [02:51<00:20, 33.05it/s]Training CobwebTree:  91%| | 6632/7317 [02:51<00:19, 34.38it/s]Training CobwebTree:  91%| | 6636/7317 [02:52<00:19, 34.64it/s]Training CobwebTree:  91%| | 6640/7317 [02:52<00:19, 34.42it/s]Training CobwebTree:  91%| | 6644/7317 [02:52<00:19, 34.51it/s]Training CobwebTree:  91%| | 6648/7317 [02:52<00:19, 34.68it/s]Training CobwebTree:  91%| | 6652/7317 [02:52<00:19, 34.38it/s]Training CobwebTree:  91%| | 6656/7317 [02:52<00:20, 32.79it/s]Training CobwebTree:  91%| | 6660/7317 [02:52<00:19, 33.23it/s]Training CobwebTree:  91%| | 6664/7317 [02:52<00:19, 33.63it/s]Training CobwebTree:  91%| | 6668/7317 [02:53<00:19, 33.58it/s]Training CobwebTree:  91%| | 6672/7317 [02:53<00:19, 33.06it/s]Training CobwebTree:  91%| | 6676/7317 [02:53<00:19, 33.20it/s]Training CobwebTree:  91%|| 6680/7317 [02:53<00:18, 34.19it/s]Training CobwebTree:  91%|| 6684/7317 [02:53<00:19, 32.73it/s]Training CobwebTree:  91%|| 6688/7317 [02:53<00:18, 33.48it/s]Training CobwebTree:  91%|| 6692/7317 [02:53<00:18, 33.37it/s]Training CobwebTree:  92%|| 6696/7317 [02:53<00:18, 33.23it/s]Training CobwebTree:  92%|| 6700/7317 [02:53<00:17, 34.73it/s]Training CobwebTree:  92%|| 6704/7317 [02:54<00:18, 33.49it/s]Training CobwebTree:  92%|| 6708/7317 [02:54<00:18, 32.83it/s]Training CobwebTree:  92%|| 6712/7317 [02:54<00:17, 34.05it/s]Training CobwebTree:  92%|| 6716/7317 [02:54<00:17, 34.76it/s]Training CobwebTree:  92%|| 6720/7317 [02:54<00:17, 34.94it/s]Training CobwebTree:  92%|| 6724/7317 [02:54<00:17, 34.66it/s]Training CobwebTree:  92%|| 6728/7317 [02:54<00:17, 34.06it/s]Training CobwebTree:  92%|| 6732/7317 [02:54<00:17, 33.12it/s]Training CobwebTree:  92%|| 6736/7317 [02:55<00:17, 33.64it/s]Training CobwebTree:  92%|| 6740/7317 [02:55<00:16, 34.78it/s]Training CobwebTree:  92%|| 6744/7317 [02:55<00:16, 35.30it/s]Training CobwebTree:  92%|| 6748/7317 [02:55<00:16, 34.39it/s]Training CobwebTree:  92%|| 6752/7317 [02:55<00:16, 33.83it/s]Training CobwebTree:  92%|| 6756/7317 [02:55<00:16, 33.16it/s]Training CobwebTree:  92%|| 6760/7317 [02:55<00:17, 32.73it/s]Training CobwebTree:  92%|| 6765/7317 [02:55<00:15, 34.77it/s]Training CobwebTree:  93%|| 6769/7317 [02:55<00:16, 34.01it/s]Training CobwebTree:  93%|| 6773/7317 [02:56<00:15, 34.09it/s]Training CobwebTree:  93%|| 6777/7317 [02:56<00:15, 34.08it/s]Training CobwebTree:  93%|| 6781/7317 [02:56<00:15, 34.66it/s]Training CobwebTree:  93%|| 6785/7317 [02:56<00:15, 33.69it/s]Training CobwebTree:  93%|| 6789/7317 [02:56<00:15, 33.39it/s]Training CobwebTree:  93%|| 6793/7317 [02:56<00:15, 34.02it/s]Training CobwebTree:  93%|| 6797/7317 [02:56<00:15, 33.66it/s]Training CobwebTree:  93%|| 6801/7317 [02:56<00:14, 34.47it/s]Training CobwebTree:  93%|| 6805/7317 [02:57<00:14, 34.15it/s]Training CobwebTree:  93%|| 6809/7317 [02:57<00:14, 34.16it/s]Training CobwebTree:  93%|| 6813/7317 [02:57<00:14, 34.10it/s]Training CobwebTree:  93%|| 6817/7317 [02:57<00:15, 33.31it/s]Training CobwebTree:  93%|| 6821/7317 [02:57<00:15, 32.63it/s]Training CobwebTree:  93%|| 6825/7317 [02:57<00:14, 34.12it/s]Training CobwebTree:  93%|| 6830/7317 [02:57<00:13, 35.85it/s]Training CobwebTree:  93%|| 6834/7317 [02:57<00:13, 35.36it/s]Training CobwebTree:  93%|| 6838/7317 [02:57<00:13, 36.13it/s]Training CobwebTree:  94%|| 6842/7317 [02:58<00:13, 35.99it/s]Training CobwebTree:  94%|| 6846/7317 [02:58<00:13, 35.61it/s]Training CobwebTree:  94%|| 6850/7317 [02:58<00:13, 34.67it/s]Training CobwebTree:  94%|| 6854/7317 [02:58<00:13, 33.90it/s]Training CobwebTree:  94%|| 6858/7317 [02:58<00:13, 33.69it/s]Training CobwebTree:  94%|| 6862/7317 [02:58<00:13, 34.24it/s]Training CobwebTree:  94%|| 6866/7317 [02:58<00:13, 33.87it/s]Training CobwebTree:  94%|| 6870/7317 [02:58<00:13, 34.19it/s]Training CobwebTree:  94%|| 6874/7317 [02:59<00:13, 33.49it/s]Training CobwebTree:  94%|| 6878/7317 [02:59<00:12, 34.02it/s]Training CobwebTree:  94%|| 6882/7317 [02:59<00:12, 34.24it/s]Training CobwebTree:  94%|| 6886/7317 [02:59<00:12, 33.38it/s]Training CobwebTree:  94%|| 6890/7317 [02:59<00:13, 32.30it/s]Training CobwebTree:  94%|| 6894/7317 [02:59<00:12, 32.94it/s]Training CobwebTree:  94%|| 6898/7317 [02:59<00:13, 31.49it/s]Training CobwebTree:  94%|| 6902/7317 [02:59<00:12, 32.09it/s]Training CobwebTree:  94%|| 6906/7317 [03:00<00:12, 32.94it/s]Training CobwebTree:  94%|| 6910/7317 [03:00<00:12, 33.57it/s]Training CobwebTree:  94%|| 6914/7317 [03:00<00:11, 34.42it/s]Training CobwebTree:  95%|| 6918/7317 [03:00<00:11, 33.68it/s]Training CobwebTree:  95%|| 6922/7317 [03:00<00:12, 32.42it/s]Training CobwebTree:  95%|| 6926/7317 [03:00<00:12, 31.49it/s]Training CobwebTree:  95%|| 6930/7317 [03:00<00:12, 32.22it/s]Training CobwebTree:  95%|| 6934/7317 [03:00<00:11, 32.58it/s]Training CobwebTree:  95%|| 6939/7317 [03:01<00:10, 35.49it/s]Training CobwebTree:  95%|| 6943/7317 [03:01<00:10, 35.21it/s]Training CobwebTree:  95%|| 6947/7317 [03:01<00:10, 36.41it/s]Training CobwebTree:  95%|| 6951/7317 [03:01<00:10, 35.24it/s]Training CobwebTree:  95%|| 6956/7317 [03:01<00:09, 36.63it/s]Training CobwebTree:  95%|| 6961/7317 [03:01<00:09, 37.10it/s]Training CobwebTree:  95%|| 6965/7317 [03:01<00:09, 36.23it/s]Training CobwebTree:  95%|| 6969/7317 [03:01<00:09, 35.98it/s]Training CobwebTree:  95%|| 6973/7317 [03:01<00:09, 35.22it/s]Training CobwebTree:  95%|| 6977/7317 [03:02<00:09, 34.88it/s]Training CobwebTree:  95%|| 6981/7317 [03:02<00:09, 34.88it/s]Training CobwebTree:  95%|| 6985/7317 [03:02<00:09, 33.72it/s]Training CobwebTree:  96%|| 6989/7317 [03:02<00:09, 33.74it/s]Training CobwebTree:  96%|| 6993/7317 [03:02<00:09, 33.59it/s]Training CobwebTree:  96%|| 6997/7317 [03:02<00:09, 34.50it/s]Training CobwebTree:  96%|| 7001/7317 [03:02<00:09, 33.85it/s]Training CobwebTree:  96%|| 7005/7317 [03:02<00:09, 33.31it/s]Training CobwebTree:  96%|| 7009/7317 [03:03<00:09, 33.40it/s]Training CobwebTree:  96%|| 7014/7317 [03:03<00:08, 35.81it/s]Training CobwebTree:  96%|| 7018/7317 [03:03<00:08, 33.31it/s]Training CobwebTree:  96%|| 7022/7317 [03:03<00:08, 33.68it/s]Training CobwebTree:  96%|| 7026/7317 [03:03<00:08, 33.29it/s]Training CobwebTree:  96%|| 7030/7317 [03:03<00:08, 34.26it/s]Training CobwebTree:  96%|| 7034/7317 [03:03<00:08, 33.26it/s]Training CobwebTree:  96%|| 7038/7317 [03:03<00:08, 33.54it/s]Training CobwebTree:  96%|| 7042/7317 [03:03<00:07, 34.56it/s]Training CobwebTree:  96%|| 7046/7317 [03:04<00:07, 35.24it/s]Training CobwebTree:  96%|| 7051/7317 [03:04<00:07, 36.89it/s]Training CobwebTree:  96%|| 7056/7317 [03:04<00:06, 38.32it/s]Training CobwebTree:  96%|| 7060/7317 [03:04<00:07, 35.69it/s]Training CobwebTree:  97%|| 7064/7317 [03:04<00:06, 36.26it/s]Training CobwebTree:  97%|| 7068/7317 [03:04<00:06, 35.97it/s]Training CobwebTree:  97%|| 7072/7317 [03:04<00:07, 34.29it/s]Training CobwebTree:  97%|| 7076/7317 [03:04<00:06, 35.27it/s]Training CobwebTree:  97%|| 7080/7317 [03:05<00:06, 34.72it/s]Training CobwebTree:  97%|| 7084/7317 [03:05<00:06, 33.58it/s]Training CobwebTree:  97%|| 7088/7317 [03:05<00:07, 31.67it/s]Training CobwebTree:  97%|| 7092/7317 [03:05<00:06, 32.43it/s]Training CobwebTree:  97%|| 7096/7317 [03:05<00:07, 31.11it/s]Training CobwebTree:  97%|| 7100/7317 [03:05<00:06, 31.73it/s]Training CobwebTree:  97%|| 7104/7317 [03:05<00:06, 33.03it/s]Training CobwebTree:  97%|| 7108/7317 [03:05<00:06, 32.93it/s]Training CobwebTree:  97%|| 7112/7317 [03:06<00:06, 32.17it/s]Training CobwebTree:  97%|| 7116/7317 [03:06<00:06, 32.71it/s]Training CobwebTree:  97%|| 7120/7317 [03:06<00:05, 32.93it/s]Training CobwebTree:  97%|| 7124/7317 [03:06<00:06, 31.87it/s]Training CobwebTree:  97%|| 7128/7317 [03:06<00:05, 33.54it/s]Training CobwebTree:  97%|| 7132/7317 [03:06<00:05, 33.04it/s]Training CobwebTree:  98%|| 7136/7317 [03:06<00:05, 33.39it/s]Training CobwebTree:  98%|| 7140/7317 [03:06<00:05, 33.36it/s]Training CobwebTree:  98%|| 7144/7317 [03:07<00:05, 34.26it/s]Training CobwebTree:  98%|| 7149/7317 [03:07<00:04, 36.03it/s]Training CobwebTree:  98%|| 7153/7317 [03:07<00:04, 35.25it/s]Training CobwebTree:  98%|| 7157/7317 [03:07<00:04, 33.71it/s]Training CobwebTree:  98%|| 7161/7317 [03:07<00:04, 33.49it/s]Training CobwebTree:  98%|| 7165/7317 [03:07<00:04, 34.11it/s]Training CobwebTree:  98%|| 7169/7317 [03:07<00:04, 34.42it/s]Training CobwebTree:  98%|| 7173/7317 [03:07<00:04, 32.94it/s]Training CobwebTree:  98%|| 7177/7317 [03:07<00:04, 33.01it/s]Training CobwebTree:  98%|| 7181/7317 [03:08<00:04, 31.81it/s]Training CobwebTree:  98%|| 7185/7317 [03:08<00:03, 33.61it/s]Training CobwebTree:  98%|| 7189/7317 [03:08<00:03, 34.95it/s]Training CobwebTree:  98%|| 7193/7317 [03:08<00:03, 34.65it/s]Training CobwebTree:  98%|| 7197/7317 [03:08<00:03, 33.62it/s]Training CobwebTree:  98%|| 7201/7317 [03:08<00:03, 33.01it/s]Training CobwebTree:  98%|| 7205/7317 [03:08<00:03, 33.10it/s]Training CobwebTree:  99%|| 7209/7317 [03:08<00:03, 33.45it/s]Training CobwebTree:  99%|| 7213/7317 [03:09<00:03, 33.98it/s]Training CobwebTree:  99%|| 7217/7317 [03:09<00:02, 34.32it/s]Training CobwebTree:  99%|| 7221/7317 [03:09<00:02, 34.27it/s]Training CobwebTree:  99%|| 7225/7317 [03:09<00:02, 35.12it/s]Training CobwebTree:  99%|| 7229/7317 [03:09<00:05, 17.37it/s]Training CobwebTree:  99%|| 7233/7317 [03:10<00:04, 20.57it/s]Training CobwebTree:  99%|| 7237/7317 [03:10<00:03, 23.12it/s]Training CobwebTree:  99%|| 7242/7317 [03:10<00:02, 27.18it/s]Training CobwebTree:  99%|| 7246/7317 [03:10<00:02, 29.05it/s]Training CobwebTree:  99%|| 7251/7317 [03:10<00:02, 32.86it/s]Training CobwebTree:  99%|| 7255/7317 [03:10<00:01, 32.72it/s]Training CobwebTree:  99%|| 7259/7317 [03:10<00:01, 34.20it/s]Training CobwebTree:  99%|| 7263/7317 [03:10<00:01, 35.24it/s]Training CobwebTree:  99%|| 7267/7317 [03:10<00:01, 33.36it/s]Training CobwebTree:  99%|| 7271/7317 [03:11<00:01, 33.84it/s]Training CobwebTree:  99%|| 7275/7317 [03:11<00:01, 34.14it/s]Training CobwebTree:  99%|| 7279/7317 [03:11<00:01, 33.44it/s]Training CobwebTree: 100%|| 7283/7317 [03:11<00:01, 33.94it/s]Training CobwebTree: 100%|| 7287/7317 [03:11<00:00, 34.26it/s]Training CobwebTree: 100%|| 7291/7317 [03:11<00:00, 33.84it/s]Training CobwebTree: 100%|| 7295/7317 [03:11<00:00, 34.21it/s]Training CobwebTree: 100%|| 7299/7317 [03:11<00:00, 33.56it/s]Training CobwebTree: 100%|| 7303/7317 [03:12<00:00, 34.50it/s]Training CobwebTree: 100%|| 7307/7317 [03:12<00:00, 33.68it/s]Training CobwebTree: 100%|| 7311/7317 [03:12<00:00, 32.04it/s]Training CobwebTree: 100%|| 7315/7317 [03:12<00:00, 33.08it/s]Training CobwebTree: 100%|| 7317/7317 [03:12<00:00, 38.02it/s]
2025-12-21 13:05:15,139 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:05:19,515 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-21 13:05:19,520 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-21 13:05:19,524 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-21 13:05:19,528 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-21 13:05:19,533 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-21 13:05:19,542 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-21 13:05:19,546 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-21 13:05:19,548 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-21 13:05:19,556 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-21 13:05:19,593 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-21 13:05:19,625 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-21 13:05:19,654 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-21 13:05:19,667 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-21 13:05:19,748 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-21 13:05:19,965 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-21 13:05:20,074 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-21 13:05:20,132 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-21 13:05:20,276 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-21 13:05:20,297 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-21 13:05:20,457 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-21 13:05:20,532 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-21 13:05:20,912 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-21 13:05:21,017 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-21 13:05:21,087 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-21 13:05:21,104 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-21 13:05:21,219 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-21 13:05:21,242 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-21 13:05:21,427 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-21 13:05:21,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,505 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,505 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,505 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,512 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,512 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,517 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,606 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,641 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:21,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:21,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,043 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,058 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,077 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,153 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,154 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,858 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,863 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:22,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:22,968 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:23,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,049 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:23,082 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:23,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:23,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:23,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,593 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:23,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:23,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,017 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,102 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,277 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,396 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,418 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,476 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:24,868 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:24,868 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:28,877 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:05:29,021 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-21 13:05:29,811 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:05:35,592 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-21 13:05:35,594 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-21 13:05:35,595 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-21 13:05:35,596 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-21 13:05:35,597 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-21 13:05:35,599 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-21 13:05:35,599 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-21 13:05:35,601 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-21 13:05:35,602 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-21 13:05:35,603 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-21 13:05:35,604 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-21 13:05:35,606 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-21 13:05:35,608 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-21 13:05:35,611 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-21 13:05:35,613 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-21 13:05:35,616 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-21 13:05:35,619 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-21 13:05:35,623 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-21 13:05:35,625 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-21 13:05:35,626 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-21 13:05:35,629 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-21 13:05:35,631 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-21 13:05:35,632 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-21 13:05:35,634 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-21 13:05:35,635 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-21 13:05:35,637 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-21 13:05:35,639 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-21 13:05:35,642 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-21 13:05:35,644 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-21 13:05:35,646 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-21 13:05:35,652 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-21 13:05:35,653 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-21 13:05:35,655 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-21 13:05:35,657 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-21 13:05:35,659 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-21 13:05:35,661 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-21 13:05:35,684 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-21 13:05:35,687 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-21 13:05:35,690 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-21 13:05:35,693 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-21 13:05:35,749 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-21 13:05:35,756 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-21 13:05:35,770 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-21 13:05:35,772 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-21 13:05:35,774 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-21 13:05:35,787 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-21 13:05:35,804 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-21 13:05:35,825 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-21 13:05:35,848 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-21 13:05:35,885 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-21 13:05:35,926 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-21 13:05:35,940 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-21 13:05:35,986 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-21 13:05:36,001 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-21 13:05:36,078 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-21 13:05:36,093 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-21 13:05:36,108 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-21 13:05:36,186 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-21 13:05:36,213 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-21 13:05:36,215 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-21 13:05:36,216 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-21 13:05:36,290 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-21 13:05:36,304 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-21 13:05:36,320 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-21 13:05:36,418 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-21 13:05:36,420 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-21 13:05:36,473 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-21 13:05:36,541 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-21 13:05:36,573 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-21 13:05:36,575 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-21 13:05:36,577 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-21 13:05:36,655 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-21 13:05:36,657 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-21 13:05:36,659 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-21 13:05:36,733 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-21 13:05:36,754 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-21 13:05:36,757 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-21 13:05:36,759 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-21 13:05:36,838 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-21 13:05:36,853 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-21 13:05:36,869 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-21 13:05:36,940 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-21 13:05:36,952 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-21 13:05:37,014 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-21 13:05:37,016 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-21 13:05:37,018 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-21 13:05:37,082 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-21 13:05:37,106 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-21 13:05:37,120 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-21 13:05:37,122 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-21 13:05:37,190 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-21 13:05:37,204 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-21 13:05:37,220 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-21 13:05:37,292 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-21 13:05:37,350 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-21 13:05:37,365 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-21 13:05:37,370 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-21 13:05:37,463 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-21 13:05:37,477 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-21 13:05:37,482 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-21 13:05:37,535 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-21 13:05:37,537 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-21 13:05:37,539 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-21 13:05:37,603 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-21 13:05:37,606 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-21 13:05:37,608 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-21 13:05:37,655 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-21 13:05:37,657 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-21 13:05:37,659 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-21 13:05:37,691 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-21 13:05:37,705 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-21 13:05:37,747 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-21 13:05:37,749 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-21 13:05:37,751 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-21 13:05:37,783 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-21 13:05:37,787 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,787 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,789 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,789 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,796 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,798 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,798 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,798 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,798 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,893 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,913 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,918 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,934 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,934 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,960 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:37,973 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:37,854 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,101 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,867 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,909 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:38,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:38,993 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,265 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,769 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,787 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,788 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:39,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:39,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:40,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:40,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:40,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:40,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:40,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:40,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:40,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:40,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:40,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:40,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:40,762 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:41,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:05:41,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:05:45,589 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:05:45,706 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-21 13:05:46,347 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.5766001011205771, 'coherence_npmi': 0.04804915517678855, 'topic_diversity': 0.9, 'inter_topic_similarity': 0.4513057470321655}
2025-12-21 13:05:46,347 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6552319124955125, 'coherence_npmi': 0.08096113062702878, 'topic_diversity': 0.826, 'inter_topic_similarity': 0.27366361021995544}
2025-12-21 13:05:46,348 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6412495546833605, 'coherence_npmi': 0.0930649428394367, 'topic_diversity': 0.8317073170731707, 'inter_topic_similarity': 0.2847086489200592}
2025-12-21 13:05:46,348 INFO src.utils.hierarchical_utils: Fitting BERTopic model HDBSCAN on 7317 docs (hierarchical)
2025-12-21 13:07:02,289 INFO src.utils.hierarchical_utils: Fitting BERTopic model KMeans on 7317 docs (hierarchical)
2025-12-21 13:08:23,306 INFO src.utils.hierarchical_utils: Fitting BERTopic model BERTopicCobwebWrapper on 7317 docs (hierarchical)
Training CobwebTree:   0%|          | 0/7317 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 15/7317 [00:00<00:54, 134.98it/s]Training CobwebTree:   0%|          | 29/7317 [00:00<01:02, 117.30it/s]Training CobwebTree:   1%|          | 41/7317 [00:00<01:06, 108.76it/s]Training CobwebTree:   1%|          | 52/7317 [00:00<01:15, 96.49it/s] Training CobwebTree:   1%|          | 62/7317 [00:00<01:17, 94.19it/s]Training CobwebTree:   1%|          | 72/7317 [00:00<01:24, 85.50it/s]Training CobwebTree:   1%|          | 81/7317 [00:00<01:29, 80.61it/s]Training CobwebTree:   1%|          | 90/7317 [00:01<01:31, 78.93it/s]Training CobwebTree:   1%|         | 98/7317 [00:01<01:33, 77.55it/s]Training CobwebTree:   1%|         | 107/7317 [00:01<01:32, 78.31it/s]Training CobwebTree:   2%|         | 115/7317 [00:01<01:37, 73.67it/s]Training CobwebTree:   2%|         | 123/7317 [00:01<01:47, 66.99it/s]Training CobwebTree:   2%|         | 130/7317 [00:01<01:49, 65.50it/s]Training CobwebTree:   2%|         | 137/7317 [00:01<01:55, 62.29it/s]Training CobwebTree:   2%|         | 144/7317 [00:01<01:51, 64.05it/s]Training CobwebTree:   2%|         | 151/7317 [00:01<01:53, 62.87it/s]Training CobwebTree:   2%|         | 158/7317 [00:02<01:54, 62.65it/s]Training CobwebTree:   2%|         | 165/7317 [00:02<01:54, 62.49it/s]Training CobwebTree:   2%|         | 172/7317 [00:02<01:54, 62.57it/s]Training CobwebTree:   2%|         | 179/7317 [00:02<01:53, 62.95it/s]Training CobwebTree:   3%|         | 186/7317 [00:02<01:55, 61.53it/s]Training CobwebTree:   3%|         | 193/7317 [00:02<02:00, 59.19it/s]Training CobwebTree:   3%|         | 199/7317 [00:02<02:04, 57.39it/s]Training CobwebTree:   3%|         | 205/7317 [00:02<02:06, 56.41it/s]Training CobwebTree:   3%|         | 212/7317 [00:02<02:01, 58.41it/s]Training CobwebTree:   3%|         | 218/7317 [00:03<02:01, 58.23it/s]Training CobwebTree:   3%|         | 224/7317 [00:03<02:05, 56.72it/s]Training CobwebTree:   3%|         | 231/7317 [00:03<02:01, 58.15it/s]Training CobwebTree:   3%|         | 238/7317 [00:03<02:00, 58.65it/s]Training CobwebTree:   3%|         | 244/7317 [00:03<02:00, 58.91it/s]Training CobwebTree:   3%|         | 250/7317 [00:03<02:01, 57.98it/s]Training CobwebTree:   4%|         | 257/7317 [00:03<02:00, 58.53it/s]Training CobwebTree:   4%|         | 263/7317 [00:03<02:07, 55.39it/s]Training CobwebTree:   4%|         | 269/7317 [00:03<02:07, 55.13it/s]Training CobwebTree:   4%|         | 275/7317 [00:04<02:08, 54.81it/s]Training CobwebTree:   4%|         | 281/7317 [00:04<02:05, 56.01it/s]Training CobwebTree:   4%|         | 287/7317 [00:04<02:04, 56.40it/s]Training CobwebTree:   4%|         | 294/7317 [00:04<02:00, 58.46it/s]Training CobwebTree:   4%|         | 301/7317 [00:04<01:57, 59.71it/s]Training CobwebTree:   4%|         | 307/7317 [00:04<01:58, 58.93it/s]Training CobwebTree:   4%|         | 313/7317 [00:04<02:05, 55.67it/s]Training CobwebTree:   4%|         | 319/7317 [00:04<02:13, 52.40it/s]Training CobwebTree:   4%|         | 325/7317 [00:05<02:15, 51.63it/s]Training CobwebTree:   5%|         | 331/7317 [00:05<02:15, 51.48it/s]Training CobwebTree:   5%|         | 337/7317 [00:05<02:13, 52.38it/s]Training CobwebTree:   5%|         | 343/7317 [00:05<02:09, 53.85it/s]Training CobwebTree:   5%|         | 349/7317 [00:05<02:18, 50.37it/s]Training CobwebTree:   5%|         | 355/7317 [00:05<02:19, 49.83it/s]Training CobwebTree:   5%|         | 361/7317 [00:05<02:20, 49.45it/s]Training CobwebTree:   5%|         | 367/7317 [00:05<02:18, 50.11it/s]Training CobwebTree:   5%|         | 373/7317 [00:05<02:13, 51.90it/s]Training CobwebTree:   5%|         | 379/7317 [00:06<02:11, 52.78it/s]Training CobwebTree:   5%|         | 385/7317 [00:06<02:11, 52.83it/s]Training CobwebTree:   5%|         | 391/7317 [00:06<02:14, 51.34it/s]Training CobwebTree:   5%|         | 397/7317 [00:06<02:17, 50.20it/s]Training CobwebTree:   6%|         | 403/7317 [00:06<02:28, 46.61it/s]Training CobwebTree:   6%|         | 409/7317 [00:06<02:21, 48.66it/s]Training CobwebTree:   6%|         | 415/7317 [00:06<02:18, 49.93it/s]Training CobwebTree:   6%|         | 421/7317 [00:06<02:18, 49.82it/s]Training CobwebTree:   6%|         | 427/7317 [00:07<02:24, 47.69it/s]Training CobwebTree:   6%|         | 432/7317 [00:07<02:23, 48.01it/s]Training CobwebTree:   6%|         | 438/7317 [00:07<02:18, 49.83it/s]Training CobwebTree:   6%|         | 444/7317 [00:07<02:22, 48.24it/s]Training CobwebTree:   6%|         | 450/7317 [00:07<02:16, 50.27it/s]Training CobwebTree:   6%|         | 456/7317 [00:07<02:12, 51.67it/s]Training CobwebTree:   6%|         | 462/7317 [00:07<02:11, 52.24it/s]Training CobwebTree:   6%|         | 468/7317 [00:07<02:15, 50.64it/s]Training CobwebTree:   6%|         | 474/7317 [00:07<02:19, 49.16it/s]Training CobwebTree:   7%|         | 479/7317 [00:08<02:25, 46.93it/s]Training CobwebTree:   7%|         | 485/7317 [00:08<02:17, 49.53it/s]Training CobwebTree:   7%|         | 491/7317 [00:08<02:11, 52.05it/s]Training CobwebTree:   7%|         | 497/7317 [00:08<02:12, 51.36it/s]Training CobwebTree:   7%|         | 503/7317 [00:08<02:09, 52.42it/s]Training CobwebTree:   7%|         | 509/7317 [00:08<02:14, 50.50it/s]Training CobwebTree:   7%|         | 515/7317 [00:08<02:16, 49.88it/s]Training CobwebTree:   7%|         | 521/7317 [00:08<02:17, 49.30it/s]Training CobwebTree:   7%|         | 528/7317 [00:09<02:10, 52.12it/s]Training CobwebTree:   7%|         | 534/7317 [00:09<02:11, 51.57it/s]Training CobwebTree:   7%|         | 540/7317 [00:09<02:11, 51.52it/s]Training CobwebTree:   7%|         | 546/7317 [00:09<02:13, 50.80it/s]Training CobwebTree:   8%|         | 552/7317 [00:09<02:11, 51.34it/s]Training CobwebTree:   8%|         | 558/7317 [00:09<02:14, 50.23it/s]Training CobwebTree:   8%|         | 564/7317 [00:09<02:13, 50.58it/s]Training CobwebTree:   8%|         | 570/7317 [00:09<02:09, 52.02it/s]Training CobwebTree:   8%|         | 576/7317 [00:09<02:12, 50.97it/s]Training CobwebTree:   8%|         | 582/7317 [00:10<02:12, 50.72it/s]Training CobwebTree:   8%|         | 588/7317 [00:10<02:12, 50.83it/s]Training CobwebTree:   8%|         | 594/7317 [00:10<02:11, 50.97it/s]Training CobwebTree:   8%|         | 600/7317 [00:10<02:17, 48.91it/s]Training CobwebTree:   8%|         | 605/7317 [00:10<02:21, 47.46it/s]Training CobwebTree:   8%|         | 610/7317 [00:10<02:20, 47.66it/s]Training CobwebTree:   8%|         | 615/7317 [00:10<02:19, 47.95it/s]Training CobwebTree:   8%|         | 620/7317 [00:10<02:26, 45.72it/s]Training CobwebTree:   9%|         | 625/7317 [00:11<02:22, 46.87it/s]Training CobwebTree:   9%|         | 631/7317 [00:11<02:16, 48.92it/s]Training CobwebTree:   9%|         | 636/7317 [00:11<02:24, 46.38it/s]Training CobwebTree:   9%|         | 642/7317 [00:11<02:17, 48.42it/s]Training CobwebTree:   9%|         | 647/7317 [00:11<02:25, 45.81it/s]Training CobwebTree:   9%|         | 652/7317 [00:11<02:28, 44.98it/s]Training CobwebTree:   9%|         | 657/7317 [00:11<02:24, 46.24it/s]Training CobwebTree:   9%|         | 662/7317 [00:11<02:26, 45.28it/s]Training CobwebTree:   9%|         | 667/7317 [00:11<02:26, 45.52it/s]Training CobwebTree:   9%|         | 672/7317 [00:12<02:25, 45.74it/s]Training CobwebTree:   9%|         | 677/7317 [00:12<02:24, 45.96it/s]Training CobwebTree:   9%|         | 683/7317 [00:12<02:18, 48.01it/s]Training CobwebTree:   9%|         | 688/7317 [00:12<02:17, 48.28it/s]Training CobwebTree:   9%|         | 693/7317 [00:12<02:22, 46.63it/s]Training CobwebTree:  10%|         | 698/7317 [00:12<02:21, 46.82it/s]Training CobwebTree:  10%|         | 703/7317 [00:12<02:19, 47.40it/s]Training CobwebTree:  10%|         | 709/7317 [00:12<02:16, 48.57it/s]Training CobwebTree:  10%|         | 715/7317 [00:12<02:17, 47.84it/s]Training CobwebTree:  10%|         | 720/7317 [00:13<02:17, 48.15it/s]Training CobwebTree:  10%|         | 725/7317 [00:13<02:17, 48.09it/s]Training CobwebTree:  10%|         | 730/7317 [00:13<02:29, 44.07it/s]Training CobwebTree:  10%|         | 735/7317 [00:13<02:24, 45.45it/s]Training CobwebTree:  10%|         | 740/7317 [00:13<02:24, 45.51it/s]Training CobwebTree:  10%|         | 745/7317 [00:13<02:28, 44.35it/s]Training CobwebTree:  10%|         | 751/7317 [00:13<02:19, 47.02it/s]Training CobwebTree:  10%|         | 756/7317 [00:13<02:25, 45.19it/s]Training CobwebTree:  10%|         | 761/7317 [00:13<02:24, 45.46it/s]Training CobwebTree:  10%|         | 767/7317 [00:14<02:17, 47.67it/s]Training CobwebTree:  11%|         | 772/7317 [00:14<02:21, 46.19it/s]Training CobwebTree:  11%|         | 777/7317 [00:14<02:25, 44.81it/s]Training CobwebTree:  11%|         | 782/7317 [00:14<02:28, 44.15it/s]Training CobwebTree:  11%|         | 787/7317 [00:14<02:34, 42.32it/s]Training CobwebTree:  11%|         | 792/7317 [00:14<02:30, 43.32it/s]Training CobwebTree:  11%|         | 797/7317 [00:14<02:31, 42.98it/s]Training CobwebTree:  11%|         | 802/7317 [00:14<02:27, 44.32it/s]Training CobwebTree:  11%|         | 807/7317 [00:14<02:24, 45.19it/s]Training CobwebTree:  11%|         | 812/7317 [00:15<02:21, 45.85it/s]Training CobwebTree:  11%|         | 817/7317 [00:15<02:25, 44.55it/s]Training CobwebTree:  11%|         | 822/7317 [00:15<02:29, 43.47it/s]Training CobwebTree:  11%|        | 827/7317 [00:15<02:30, 43.00it/s]Training CobwebTree:  11%|        | 832/7317 [00:15<02:30, 42.95it/s]Training CobwebTree:  11%|        | 837/7317 [00:15<02:25, 44.39it/s]Training CobwebTree:  12%|        | 842/7317 [00:15<02:27, 44.04it/s]Training CobwebTree:  12%|        | 847/7317 [00:15<02:26, 44.10it/s]Training CobwebTree:  12%|        | 852/7317 [00:16<02:28, 43.48it/s]Training CobwebTree:  12%|        | 857/7317 [00:16<02:29, 43.11it/s]Training CobwebTree:  12%|        | 862/7317 [00:16<02:35, 41.47it/s]Training CobwebTree:  12%|        | 867/7317 [00:16<02:36, 41.30it/s]Training CobwebTree:  12%|        | 872/7317 [00:16<02:35, 41.51it/s]Training CobwebTree:  12%|        | 877/7317 [00:16<02:29, 43.05it/s]Training CobwebTree:  12%|        | 883/7317 [00:16<02:22, 45.14it/s]Training CobwebTree:  12%|        | 888/7317 [00:16<02:22, 45.08it/s]Training CobwebTree:  12%|        | 893/7317 [00:16<02:19, 45.90it/s]Training CobwebTree:  12%|        | 898/7317 [00:17<02:18, 46.31it/s]Training CobwebTree:  12%|        | 903/7317 [00:17<02:21, 45.31it/s]Training CobwebTree:  12%|        | 908/7317 [00:17<02:25, 44.16it/s]Training CobwebTree:  12%|        | 913/7317 [00:17<02:26, 43.85it/s]Training CobwebTree:  13%|        | 918/7317 [00:17<02:22, 44.78it/s]Training CobwebTree:  13%|        | 923/7317 [00:17<02:20, 45.48it/s]Training CobwebTree:  13%|        | 928/7317 [00:17<02:20, 45.56it/s]Training CobwebTree:  13%|        | 933/7317 [00:17<02:29, 42.58it/s]Training CobwebTree:  13%|        | 938/7317 [00:17<02:31, 42.12it/s]Training CobwebTree:  13%|        | 943/7317 [00:18<02:27, 43.21it/s]Training CobwebTree:  13%|        | 948/7317 [00:18<02:33, 41.49it/s]Training CobwebTree:  13%|        | 953/7317 [00:18<02:31, 41.94it/s]Training CobwebTree:  13%|        | 958/7317 [00:18<02:30, 42.14it/s]Training CobwebTree:  13%|        | 963/7317 [00:18<02:34, 41.16it/s]Training CobwebTree:  13%|        | 968/7317 [00:18<02:29, 42.46it/s]Training CobwebTree:  13%|        | 973/7317 [00:18<02:24, 43.77it/s]Training CobwebTree:  13%|        | 978/7317 [00:18<02:24, 43.77it/s]Training CobwebTree:  13%|        | 983/7317 [00:19<02:21, 44.79it/s]Training CobwebTree:  14%|        | 988/7317 [00:19<02:26, 43.10it/s]Training CobwebTree:  14%|        | 993/7317 [00:19<02:27, 42.96it/s]Training CobwebTree:  14%|        | 998/7317 [00:19<02:28, 42.61it/s]Training CobwebTree:  14%|        | 1003/7317 [00:19<02:27, 42.69it/s]Training CobwebTree:  14%|        | 1008/7317 [00:19<02:23, 44.10it/s]Training CobwebTree:  14%|        | 1013/7317 [00:19<02:29, 42.14it/s]Training CobwebTree:  14%|        | 1018/7317 [00:19<02:22, 44.20it/s]Training CobwebTree:  14%|        | 1023/7317 [00:19<02:24, 43.50it/s]Training CobwebTree:  14%|        | 1028/7317 [00:20<02:21, 44.42it/s]Training CobwebTree:  14%|        | 1033/7317 [00:20<02:22, 44.05it/s]Training CobwebTree:  14%|        | 1038/7317 [00:20<02:28, 42.23it/s]Training CobwebTree:  14%|        | 1043/7317 [00:20<02:26, 42.84it/s]Training CobwebTree:  14%|        | 1048/7317 [00:20<02:25, 43.10it/s]Training CobwebTree:  14%|        | 1053/7317 [00:20<02:21, 44.30it/s]Training CobwebTree:  14%|        | 1058/7317 [00:20<02:28, 42.20it/s]Training CobwebTree:  15%|        | 1063/7317 [00:20<02:28, 42.00it/s]Training CobwebTree:  15%|        | 1068/7317 [00:21<02:25, 42.92it/s]Training CobwebTree:  15%|        | 1073/7317 [00:21<02:23, 43.36it/s]Training CobwebTree:  15%|        | 1078/7317 [00:21<02:22, 43.66it/s]Training CobwebTree:  15%|        | 1083/7317 [00:21<02:27, 42.13it/s]Training CobwebTree:  15%|        | 1088/7317 [00:21<02:24, 43.24it/s]Training CobwebTree:  15%|        | 1093/7317 [00:21<02:27, 42.27it/s]Training CobwebTree:  15%|        | 1098/7317 [00:21<02:21, 43.88it/s]Training CobwebTree:  15%|        | 1103/7317 [00:21<02:21, 44.06it/s]Training CobwebTree:  15%|        | 1108/7317 [00:21<02:22, 43.48it/s]Training CobwebTree:  15%|        | 1113/7317 [00:22<02:27, 42.03it/s]Training CobwebTree:  15%|        | 1118/7317 [00:22<02:25, 42.74it/s]Training CobwebTree:  15%|        | 1124/7317 [00:22<02:16, 45.49it/s]Training CobwebTree:  15%|        | 1129/7317 [00:22<02:17, 45.01it/s]Training CobwebTree:  15%|        | 1134/7317 [00:22<02:16, 45.22it/s]Training CobwebTree:  16%|        | 1139/7317 [00:22<02:15, 45.66it/s]Training CobwebTree:  16%|        | 1144/7317 [00:22<02:19, 44.29it/s]Training CobwebTree:  16%|        | 1149/7317 [00:22<02:24, 42.78it/s]Training CobwebTree:  16%|        | 1154/7317 [00:22<02:29, 41.14it/s]Training CobwebTree:  16%|        | 1159/7317 [00:23<02:25, 42.27it/s]Training CobwebTree:  16%|        | 1164/7317 [00:23<02:24, 42.60it/s]Training CobwebTree:  16%|        | 1169/7317 [00:23<02:33, 40.11it/s]Training CobwebTree:  16%|        | 1174/7317 [00:23<02:39, 38.61it/s]Training CobwebTree:  16%|        | 1178/7317 [00:23<02:38, 38.67it/s]Training CobwebTree:  16%|        | 1183/7317 [00:23<02:36, 39.28it/s]Training CobwebTree:  16%|        | 1188/7317 [00:23<02:34, 39.63it/s]Training CobwebTree:  16%|        | 1192/7317 [00:23<02:39, 38.51it/s]Training CobwebTree:  16%|        | 1197/7317 [00:24<02:35, 39.27it/s]Training CobwebTree:  16%|        | 1201/7317 [00:24<02:35, 39.30it/s]Training CobwebTree:  16%|        | 1206/7317 [00:24<02:31, 40.25it/s]Training CobwebTree:  17%|        | 1211/7317 [00:24<02:23, 42.70it/s]Training CobwebTree:  17%|        | 1216/7317 [00:24<02:23, 42.54it/s]Training CobwebTree:  17%|        | 1221/7317 [00:24<02:19, 43.69it/s]Training CobwebTree:  17%|        | 1226/7317 [00:24<02:20, 43.31it/s]Training CobwebTree:  17%|        | 1231/7317 [00:24<02:18, 44.10it/s]Training CobwebTree:  17%|        | 1236/7317 [00:24<02:21, 43.00it/s]Training CobwebTree:  17%|        | 1241/7317 [00:25<02:22, 42.64it/s]Training CobwebTree:  17%|        | 1246/7317 [00:25<02:25, 41.62it/s]Training CobwebTree:  17%|        | 1251/7317 [00:25<02:34, 39.35it/s]Training CobwebTree:  17%|        | 1255/7317 [00:25<02:39, 37.94it/s]Training CobwebTree:  17%|        | 1260/7317 [00:25<02:34, 39.25it/s]Training CobwebTree:  17%|        | 1265/7317 [00:25<02:27, 41.09it/s]Training CobwebTree:  17%|        | 1270/7317 [00:25<02:19, 43.36it/s]Training CobwebTree:  17%|        | 1275/7317 [00:25<02:16, 44.37it/s]Training CobwebTree:  17%|        | 1280/7317 [00:26<02:19, 43.34it/s]Training CobwebTree:  18%|        | 1285/7317 [00:26<02:19, 43.34it/s]Training CobwebTree:  18%|        | 1290/7317 [00:26<02:18, 43.40it/s]Training CobwebTree:  18%|        | 1295/7317 [00:26<02:18, 43.48it/s]Training CobwebTree:  18%|        | 1300/7317 [00:26<02:15, 44.26it/s]Training CobwebTree:  18%|        | 1305/7317 [00:26<02:19, 43.13it/s]Training CobwebTree:  18%|        | 1310/7317 [00:26<02:18, 43.42it/s]Training CobwebTree:  18%|        | 1315/7317 [00:26<02:22, 42.23it/s]Training CobwebTree:  18%|        | 1320/7317 [00:26<02:27, 40.66it/s]Training CobwebTree:  18%|        | 1325/7317 [00:27<02:24, 41.46it/s]Training CobwebTree:  18%|        | 1330/7317 [00:27<02:23, 41.82it/s]Training CobwebTree:  18%|        | 1335/7317 [00:27<02:24, 41.46it/s]Training CobwebTree:  18%|        | 1340/7317 [00:27<02:24, 41.27it/s]Training CobwebTree:  18%|        | 1345/7317 [00:27<02:25, 41.08it/s]Training CobwebTree:  18%|        | 1350/7317 [00:27<02:28, 40.30it/s]Training CobwebTree:  19%|        | 1355/7317 [00:27<02:23, 41.65it/s]Training CobwebTree:  19%|        | 1360/7317 [00:27<02:25, 40.89it/s]Training CobwebTree:  19%|        | 1365/7317 [00:28<02:22, 41.75it/s]Training CobwebTree:  19%|        | 1370/7317 [00:28<02:24, 41.06it/s]Training CobwebTree:  19%|        | 1375/7317 [00:28<02:27, 40.22it/s]Training CobwebTree:  19%|        | 1380/7317 [00:28<02:23, 41.51it/s]Training CobwebTree:  19%|        | 1385/7317 [00:28<02:25, 40.85it/s]Training CobwebTree:  19%|        | 1390/7317 [00:28<02:19, 42.40it/s]Training CobwebTree:  19%|        | 1395/7317 [00:28<02:21, 41.83it/s]Training CobwebTree:  19%|        | 1400/7317 [00:28<02:16, 43.45it/s]Training CobwebTree:  19%|        | 1405/7317 [00:29<02:16, 43.18it/s]Training CobwebTree:  19%|        | 1410/7317 [00:29<02:18, 42.53it/s]Training CobwebTree:  19%|        | 1416/7317 [00:29<02:12, 44.51it/s]Training CobwebTree:  19%|        | 1421/7317 [00:29<02:11, 44.85it/s]Training CobwebTree:  19%|        | 1426/7317 [00:29<02:16, 43.24it/s]Training CobwebTree:  20%|        | 1431/7317 [00:29<02:14, 43.78it/s]Training CobwebTree:  20%|        | 1436/7317 [00:29<02:14, 43.86it/s]Training CobwebTree:  20%|        | 1441/7317 [00:29<02:15, 43.50it/s]Training CobwebTree:  20%|        | 1446/7317 [00:29<02:20, 41.85it/s]Training CobwebTree:  20%|        | 1451/7317 [00:30<02:17, 42.75it/s]Training CobwebTree:  20%|        | 1456/7317 [00:30<02:19, 41.96it/s]Training CobwebTree:  20%|        | 1461/7317 [00:30<02:17, 42.60it/s]Training CobwebTree:  20%|        | 1466/7317 [00:30<02:23, 40.74it/s]Training CobwebTree:  20%|        | 1471/7317 [00:30<02:23, 40.60it/s]Training CobwebTree:  20%|        | 1476/7317 [00:30<02:26, 40.00it/s]Training CobwebTree:  20%|        | 1481/7317 [00:30<02:20, 41.66it/s]Training CobwebTree:  20%|        | 1486/7317 [00:30<02:21, 41.13it/s]Training CobwebTree:  20%|        | 1491/7317 [00:31<02:21, 41.07it/s]Training CobwebTree:  20%|        | 1496/7317 [00:31<02:25, 39.93it/s]Training CobwebTree:  21%|        | 1501/7317 [00:31<02:22, 40.94it/s]Training CobwebTree:  21%|        | 1506/7317 [00:31<02:17, 42.37it/s]Training CobwebTree:  21%|        | 1511/7317 [00:31<02:20, 41.40it/s]Training CobwebTree:  21%|        | 1516/7317 [00:31<02:21, 41.13it/s]Training CobwebTree:  21%|        | 1521/7317 [00:31<02:13, 43.35it/s]Training CobwebTree:  21%|        | 1526/7317 [00:31<02:15, 42.77it/s]Training CobwebTree:  21%|        | 1531/7317 [00:32<02:17, 42.17it/s]Training CobwebTree:  21%|        | 1536/7317 [00:32<02:15, 42.54it/s]Training CobwebTree:  21%|        | 1541/7317 [00:32<02:19, 41.45it/s]Training CobwebTree:  21%|        | 1546/7317 [00:32<02:20, 41.01it/s]Training CobwebTree:  21%|        | 1551/7317 [00:32<02:13, 43.07it/s]Training CobwebTree:  21%|       | 1556/7317 [00:32<02:12, 43.53it/s]Training CobwebTree:  21%|       | 1561/7317 [00:32<02:11, 43.72it/s]Training CobwebTree:  21%|       | 1566/7317 [00:32<02:12, 43.55it/s]Training CobwebTree:  21%|       | 1571/7317 [00:32<02:12, 43.44it/s]Training CobwebTree:  22%|       | 1576/7317 [00:33<02:15, 42.28it/s]Training CobwebTree:  22%|       | 1581/7317 [00:33<02:20, 40.88it/s]Training CobwebTree:  22%|       | 1586/7317 [00:33<02:22, 40.12it/s]Training CobwebTree:  22%|       | 1591/7317 [00:33<02:25, 39.29it/s]Training CobwebTree:  22%|       | 1596/7317 [00:33<02:21, 40.56it/s]Training CobwebTree:  22%|       | 1601/7317 [00:33<02:20, 40.64it/s]Training CobwebTree:  22%|       | 1606/7317 [00:33<02:22, 40.20it/s]Training CobwebTree:  22%|       | 1611/7317 [00:33<02:19, 40.93it/s]Training CobwebTree:  22%|       | 1616/7317 [00:34<02:22, 40.11it/s]Training CobwebTree:  22%|       | 1621/7317 [00:34<02:26, 38.95it/s]Training CobwebTree:  22%|       | 1626/7317 [00:34<02:21, 40.34it/s]Training CobwebTree:  22%|       | 1631/7317 [00:34<02:27, 38.45it/s]Training CobwebTree:  22%|       | 1636/7317 [00:34<02:25, 39.14it/s]Training CobwebTree:  22%|       | 1640/7317 [00:34<02:26, 38.76it/s]Training CobwebTree:  22%|       | 1645/7317 [00:34<02:23, 39.50it/s]Training CobwebTree:  23%|       | 1650/7317 [00:34<02:20, 40.21it/s]Training CobwebTree:  23%|       | 1655/7317 [00:35<02:19, 40.55it/s]Training CobwebTree:  23%|       | 1660/7317 [00:35<02:18, 40.99it/s]Training CobwebTree:  23%|       | 1665/7317 [00:35<02:19, 40.65it/s]Training CobwebTree:  23%|       | 1670/7317 [00:35<02:27, 38.38it/s]Training CobwebTree:  23%|       | 1675/7317 [00:35<02:24, 38.92it/s]Training CobwebTree:  23%|       | 1679/7317 [00:35<02:25, 38.80it/s]Training CobwebTree:  23%|       | 1684/7317 [00:35<02:21, 39.79it/s]Training CobwebTree:  23%|       | 1688/7317 [00:35<02:22, 39.40it/s]Training CobwebTree:  23%|       | 1693/7317 [00:36<02:22, 39.42it/s]Training CobwebTree:  23%|       | 1698/7317 [00:36<02:18, 40.50it/s]Training CobwebTree:  23%|       | 1703/7317 [00:36<02:16, 41.14it/s]Training CobwebTree:  23%|       | 1708/7317 [00:36<02:16, 41.21it/s]Training CobwebTree:  23%|       | 1713/7317 [00:36<02:11, 42.77it/s]Training CobwebTree:  23%|       | 1718/7317 [00:36<02:11, 42.49it/s]Training CobwebTree:  24%|       | 1724/7317 [00:36<02:07, 43.92it/s]Training CobwebTree:  24%|       | 1729/7317 [00:36<02:07, 43.98it/s]Training CobwebTree:  24%|       | 1734/7317 [00:36<02:07, 43.74it/s]Training CobwebTree:  24%|       | 1739/7317 [00:37<02:13, 41.90it/s]Training CobwebTree:  24%|       | 1744/7317 [00:37<02:17, 40.40it/s]Training CobwebTree:  24%|       | 1749/7317 [00:37<02:19, 39.95it/s]Training CobwebTree:  24%|       | 1754/7317 [00:37<02:16, 40.65it/s]Training CobwebTree:  24%|       | 1759/7317 [00:37<02:14, 41.23it/s]Training CobwebTree:  24%|       | 1764/7317 [00:37<02:17, 40.46it/s]Training CobwebTree:  24%|       | 1769/7317 [00:37<02:16, 40.67it/s]Training CobwebTree:  24%|       | 1774/7317 [00:37<02:14, 41.32it/s]Training CobwebTree:  24%|       | 1779/7317 [00:38<02:20, 39.41it/s]Training CobwebTree:  24%|       | 1784/7317 [00:38<02:16, 40.40it/s]Training CobwebTree:  24%|       | 1789/7317 [00:38<02:12, 41.58it/s]Training CobwebTree:  25%|       | 1794/7317 [00:38<02:13, 41.46it/s]Training CobwebTree:  25%|       | 1799/7317 [00:38<02:12, 41.66it/s]Training CobwebTree:  25%|       | 1804/7317 [00:38<02:14, 41.12it/s]Training CobwebTree:  25%|       | 1809/7317 [00:38<02:16, 40.42it/s]Training CobwebTree:  25%|       | 1814/7317 [00:38<02:10, 42.26it/s]Training CobwebTree:  25%|       | 1819/7317 [00:39<02:14, 40.81it/s]Training CobwebTree:  25%|       | 1824/7317 [00:39<02:22, 38.56it/s]Training CobwebTree:  25%|       | 1828/7317 [00:39<02:21, 38.66it/s]Training CobwebTree:  25%|       | 1832/7317 [00:39<02:24, 37.83it/s]Training CobwebTree:  25%|       | 1836/7317 [00:39<02:25, 37.66it/s]Training CobwebTree:  25%|       | 1840/7317 [00:39<02:23, 38.07it/s]Training CobwebTree:  25%|       | 1845/7317 [00:39<02:19, 39.21it/s]Training CobwebTree:  25%|       | 1850/7317 [00:39<02:16, 40.18it/s]Training CobwebTree:  25%|       | 1855/7317 [00:40<02:19, 39.13it/s]Training CobwebTree:  25%|       | 1859/7317 [00:40<02:20, 38.80it/s]Training CobwebTree:  25%|       | 1863/7317 [00:40<02:23, 38.03it/s]Training CobwebTree:  26%|       | 1868/7317 [00:40<02:24, 37.74it/s]Training CobwebTree:  26%|       | 1872/7317 [00:40<02:26, 37.10it/s]Training CobwebTree:  26%|       | 1877/7317 [00:40<02:19, 38.98it/s]Training CobwebTree:  26%|       | 1881/7317 [00:40<02:19, 39.03it/s]Training CobwebTree:  26%|       | 1885/7317 [00:40<02:20, 38.67it/s]Training CobwebTree:  26%|       | 1890/7317 [00:40<02:19, 38.89it/s]Training CobwebTree:  26%|       | 1894/7317 [00:41<02:21, 38.29it/s]Training CobwebTree:  26%|       | 1898/7317 [00:41<02:22, 37.97it/s]Training CobwebTree:  26%|       | 1902/7317 [00:41<02:25, 37.16it/s]Training CobwebTree:  26%|       | 1906/7317 [00:41<02:22, 37.88it/s]Training CobwebTree:  26%|       | 1911/7317 [00:41<02:14, 40.23it/s]Training CobwebTree:  26%|       | 1916/7317 [00:41<02:09, 41.76it/s]Training CobwebTree:  26%|       | 1921/7317 [00:41<02:13, 40.54it/s]Training CobwebTree:  26%|       | 1926/7317 [00:41<02:14, 40.19it/s]Training CobwebTree:  26%|       | 1931/7317 [00:41<02:12, 40.52it/s]Training CobwebTree:  26%|       | 1936/7317 [00:42<02:10, 41.37it/s]Training CobwebTree:  27%|       | 1941/7317 [00:42<02:04, 43.25it/s]Training CobwebTree:  27%|       | 1946/7317 [00:42<02:06, 42.58it/s]Training CobwebTree:  27%|       | 1951/7317 [00:42<02:06, 42.58it/s]Training CobwebTree:  27%|       | 1956/7317 [00:42<02:08, 41.64it/s]Training CobwebTree:  27%|       | 1961/7317 [00:42<02:10, 41.07it/s]Training CobwebTree:  27%|       | 1966/7317 [00:42<02:11, 40.56it/s]Training CobwebTree:  27%|       | 1971/7317 [00:42<02:15, 39.56it/s]Training CobwebTree:  27%|       | 1976/7317 [00:43<02:12, 40.41it/s]Training CobwebTree:  27%|       | 1981/7317 [00:43<02:18, 38.42it/s]Training CobwebTree:  27%|       | 1986/7317 [00:43<02:16, 39.13it/s]Training CobwebTree:  27%|       | 1991/7317 [00:43<02:14, 39.49it/s]Training CobwebTree:  27%|       | 1995/7317 [00:43<02:15, 39.27it/s]Training CobwebTree:  27%|       | 1999/7317 [00:43<02:20, 37.83it/s]Training CobwebTree:  27%|       | 2003/7317 [00:43<02:25, 36.63it/s]Training CobwebTree:  27%|       | 2008/7317 [00:43<02:15, 39.10it/s]Training CobwebTree:  27%|       | 2012/7317 [00:43<02:15, 39.28it/s]Training CobwebTree:  28%|       | 2016/7317 [00:44<02:16, 38.77it/s]Training CobwebTree:  28%|       | 2021/7317 [00:44<02:12, 40.05it/s]Training CobwebTree:  28%|       | 2026/7317 [00:44<02:13, 39.78it/s]Training CobwebTree:  28%|       | 2031/7317 [00:44<02:10, 40.61it/s]Training CobwebTree:  28%|       | 2036/7317 [00:44<02:12, 39.99it/s]Training CobwebTree:  28%|       | 2041/7317 [00:44<02:16, 38.77it/s]Training CobwebTree:  28%|       | 2045/7317 [00:44<02:15, 38.83it/s]Training CobwebTree:  28%|       | 2049/7317 [00:44<02:17, 38.38it/s]Training CobwebTree:  28%|       | 2054/7317 [00:45<02:13, 39.41it/s]Training CobwebTree:  28%|       | 2059/7317 [00:45<02:12, 39.82it/s]Training CobwebTree:  28%|       | 2063/7317 [00:45<02:15, 38.73it/s]Training CobwebTree:  28%|       | 2068/7317 [00:45<02:17, 38.17it/s]Training CobwebTree:  28%|       | 2072/7317 [00:45<02:18, 37.95it/s]Training CobwebTree:  28%|       | 2077/7317 [00:45<02:12, 39.52it/s]Training CobwebTree:  28%|       | 2081/7317 [00:45<02:12, 39.40it/s]Training CobwebTree:  28%|       | 2085/7317 [00:45<02:12, 39.55it/s]Training CobwebTree:  29%|       | 2090/7317 [00:45<02:09, 40.41it/s]Training CobwebTree:  29%|       | 2095/7317 [00:46<02:10, 39.92it/s]Training CobwebTree:  29%|       | 2100/7317 [00:46<02:10, 39.93it/s]Training CobwebTree:  29%|       | 2104/7317 [00:46<02:12, 39.44it/s]Training CobwebTree:  29%|       | 2108/7317 [00:46<02:12, 39.32it/s]Training CobwebTree:  29%|       | 2113/7317 [00:46<02:08, 40.38it/s]Training CobwebTree:  29%|       | 2118/7317 [00:46<02:10, 39.91it/s]Training CobwebTree:  29%|       | 2122/7317 [00:46<02:10, 39.91it/s]Training CobwebTree:  29%|       | 2126/7317 [00:46<02:13, 38.96it/s]Training CobwebTree:  29%|       | 2130/7317 [00:46<02:13, 38.82it/s]Training CobwebTree:  29%|       | 2134/7317 [00:47<02:14, 38.43it/s]Training CobwebTree:  29%|       | 2138/7317 [00:47<02:23, 36.06it/s]Training CobwebTree:  29%|       | 2142/7317 [00:47<02:22, 36.22it/s]Training CobwebTree:  29%|       | 2146/7317 [00:47<02:21, 36.60it/s]Training CobwebTree:  29%|       | 2151/7317 [00:47<02:18, 37.28it/s]Training CobwebTree:  29%|       | 2155/7317 [00:47<02:21, 36.58it/s]Training CobwebTree:  30%|       | 2160/7317 [00:47<02:16, 37.84it/s]Training CobwebTree:  30%|       | 2165/7317 [00:47<02:15, 38.02it/s]Training CobwebTree:  30%|       | 2169/7317 [00:48<02:19, 37.01it/s]Training CobwebTree:  30%|       | 2174/7317 [00:48<02:14, 38.13it/s]Training CobwebTree:  30%|       | 2179/7317 [00:48<02:11, 38.93it/s]Training CobwebTree:  30%|       | 2183/7317 [00:48<02:12, 38.68it/s]Training CobwebTree:  30%|       | 2188/7317 [00:48<02:10, 39.31it/s]Training CobwebTree:  30%|       | 2192/7317 [00:48<02:10, 39.40it/s]Training CobwebTree:  30%|       | 2196/7317 [00:48<02:12, 38.79it/s]Training CobwebTree:  30%|       | 2200/7317 [00:48<02:11, 39.04it/s]Training CobwebTree:  30%|       | 2204/7317 [00:48<02:11, 38.97it/s]Training CobwebTree:  30%|       | 2209/7317 [00:49<02:08, 39.90it/s]Training CobwebTree:  30%|       | 2214/7317 [00:49<02:01, 42.10it/s]Training CobwebTree:  30%|       | 2219/7317 [00:49<02:06, 40.36it/s]Training CobwebTree:  30%|       | 2224/7317 [00:49<02:03, 41.22it/s]Training CobwebTree:  30%|       | 2229/7317 [00:49<02:06, 40.27it/s]Training CobwebTree:  31%|       | 2234/7317 [00:49<02:10, 38.95it/s]Training CobwebTree:  31%|       | 2238/7317 [00:49<02:10, 39.05it/s]Training CobwebTree:  31%|       | 2242/7317 [00:49<02:13, 38.11it/s]Training CobwebTree:  31%|       | 2247/7317 [00:49<02:07, 39.68it/s]Training CobwebTree:  31%|       | 2251/7317 [00:50<02:13, 38.04it/s]Training CobwebTree:  31%|       | 2256/7317 [00:50<02:05, 40.30it/s]Training CobwebTree:  31%|       | 2261/7317 [00:50<02:13, 38.01it/s]Training CobwebTree:  31%|       | 2266/7317 [00:50<02:10, 38.59it/s]Training CobwebTree:  31%|       | 2270/7317 [00:50<02:09, 38.91it/s]Training CobwebTree:  31%|       | 2275/7317 [00:50<02:03, 40.69it/s]Training CobwebTree:  31%|       | 2280/7317 [00:50<02:03, 40.90it/s]Training CobwebTree:  31%|       | 2285/7317 [00:50<01:59, 41.98it/s]Training CobwebTree:  31%|      | 2290/7317 [00:51<02:04, 40.35it/s]Training CobwebTree:  31%|      | 2295/7317 [00:51<01:59, 41.94it/s]Training CobwebTree:  31%|      | 2300/7317 [00:51<02:02, 40.90it/s]Training CobwebTree:  32%|      | 2305/7317 [00:51<02:01, 41.27it/s]Training CobwebTree:  32%|      | 2310/7317 [00:51<02:06, 39.66it/s]Training CobwebTree:  32%|      | 2315/7317 [00:51<02:04, 40.30it/s]Training CobwebTree:  32%|      | 2320/7317 [00:51<02:03, 40.43it/s]Training CobwebTree:  32%|      | 2325/7317 [00:51<02:03, 40.35it/s]Training CobwebTree:  32%|      | 2330/7317 [00:52<02:02, 40.81it/s]Training CobwebTree:  32%|      | 2335/7317 [00:52<02:06, 39.39it/s]Training CobwebTree:  32%|      | 2340/7317 [00:52<02:02, 40.54it/s]Training CobwebTree:  32%|      | 2345/7317 [00:52<02:06, 39.30it/s]Training CobwebTree:  32%|      | 2349/7317 [00:52<02:06, 39.24it/s]Training CobwebTree:  32%|      | 2354/7317 [00:52<02:01, 40.96it/s]Training CobwebTree:  32%|      | 2359/7317 [00:52<02:05, 39.44it/s]Training CobwebTree:  32%|      | 2363/7317 [00:52<02:06, 39.24it/s]Training CobwebTree:  32%|      | 2368/7317 [00:53<02:01, 40.74it/s]Training CobwebTree:  32%|      | 2373/7317 [00:53<01:59, 41.27it/s]Training CobwebTree:  32%|      | 2378/7317 [00:53<02:02, 40.36it/s]Training CobwebTree:  33%|      | 2383/7317 [00:53<02:07, 38.82it/s]Training CobwebTree:  33%|      | 2388/7317 [00:53<02:01, 40.69it/s]Training CobwebTree:  33%|      | 2393/7317 [00:53<02:04, 39.40it/s]Training CobwebTree:  33%|      | 2398/7317 [00:53<02:03, 39.74it/s]Training CobwebTree:  33%|      | 2403/7317 [00:53<02:05, 39.03it/s]Training CobwebTree:  33%|      | 2408/7317 [00:54<02:06, 38.87it/s]Training CobwebTree:  33%|      | 2412/7317 [00:54<02:08, 38.23it/s]Training CobwebTree:  33%|      | 2416/7317 [00:54<02:07, 38.47it/s]Training CobwebTree:  33%|      | 2421/7317 [00:54<02:03, 39.50it/s]Training CobwebTree:  33%|      | 2426/7317 [00:54<02:00, 40.64it/s]Training CobwebTree:  33%|      | 2431/7317 [00:54<02:04, 39.37it/s]Training CobwebTree:  33%|      | 2435/7317 [00:54<02:05, 39.05it/s]Training CobwebTree:  33%|      | 2439/7317 [00:54<02:07, 38.30it/s]Training CobwebTree:  33%|      | 2444/7317 [00:54<02:03, 39.41it/s]Training CobwebTree:  33%|      | 2448/7317 [00:55<02:08, 37.81it/s]Training CobwebTree:  34%|      | 2453/7317 [00:55<02:04, 39.03it/s]Training CobwebTree:  34%|      | 2457/7317 [00:55<02:04, 38.95it/s]Training CobwebTree:  34%|      | 2462/7317 [00:55<01:59, 40.61it/s]Training CobwebTree:  34%|      | 2467/7317 [00:55<02:00, 40.27it/s]Training CobwebTree:  34%|      | 2472/7317 [00:55<01:57, 41.06it/s]Training CobwebTree:  34%|      | 2477/7317 [00:55<02:01, 39.93it/s]Training CobwebTree:  34%|      | 2482/7317 [00:55<02:01, 39.71it/s]Training CobwebTree:  34%|      | 2487/7317 [00:56<01:56, 41.63it/s]Training CobwebTree:  34%|      | 2492/7317 [00:56<01:59, 40.34it/s]Training CobwebTree:  34%|      | 2497/7317 [00:56<01:59, 40.18it/s]Training CobwebTree:  34%|      | 2502/7317 [00:56<01:55, 41.76it/s]Training CobwebTree:  34%|      | 2507/7317 [00:56<01:53, 42.45it/s]Training CobwebTree:  34%|      | 2512/7317 [00:56<01:54, 41.87it/s]Training CobwebTree:  34%|      | 2517/7317 [00:56<02:01, 39.57it/s]Training CobwebTree:  34%|      | 2521/7317 [00:56<02:02, 39.22it/s]Training CobwebTree:  35%|      | 2526/7317 [00:56<02:00, 39.79it/s]Training CobwebTree:  35%|      | 2530/7317 [00:57<02:00, 39.73it/s]Training CobwebTree:  35%|      | 2535/7317 [00:57<02:01, 39.45it/s]Training CobwebTree:  35%|      | 2540/7317 [00:57<01:57, 40.69it/s]Training CobwebTree:  35%|      | 2545/7317 [00:57<01:57, 40.68it/s]Training CobwebTree:  35%|      | 2550/7317 [00:57<01:57, 40.43it/s]Training CobwebTree:  35%|      | 2555/7317 [00:57<02:00, 39.53it/s]Training CobwebTree:  35%|      | 2559/7317 [00:57<02:08, 37.05it/s]Training CobwebTree:  35%|      | 2563/7317 [00:57<02:10, 36.52it/s]Training CobwebTree:  35%|      | 2568/7317 [00:58<02:03, 38.41it/s]Training CobwebTree:  35%|      | 2573/7317 [00:58<02:00, 39.41it/s]Training CobwebTree:  35%|      | 2578/7317 [00:58<01:57, 40.45it/s]Training CobwebTree:  35%|      | 2583/7317 [00:58<01:57, 40.20it/s]Training CobwebTree:  35%|      | 2588/7317 [00:58<02:02, 38.61it/s]Training CobwebTree:  35%|      | 2593/7317 [00:58<01:58, 39.80it/s]Training CobwebTree:  36%|      | 2598/7317 [00:58<02:05, 37.47it/s]Training CobwebTree:  36%|      | 2603/7317 [00:58<02:02, 38.35it/s]Training CobwebTree:  36%|      | 2607/7317 [00:59<02:02, 38.36it/s]Training CobwebTree:  36%|      | 2612/7317 [00:59<01:57, 40.04it/s]Training CobwebTree:  36%|      | 2617/7317 [00:59<01:58, 39.67it/s]Training CobwebTree:  36%|      | 2621/7317 [00:59<02:00, 38.87it/s]Training CobwebTree:  36%|      | 2625/7317 [00:59<02:03, 38.07it/s]Training CobwebTree:  36%|      | 2630/7317 [00:59<02:00, 38.89it/s]Training CobwebTree:  36%|      | 2635/7317 [00:59<01:58, 39.51it/s]Training CobwebTree:  36%|      | 2639/7317 [00:59<02:01, 38.65it/s]Training CobwebTree:  36%|      | 2643/7317 [00:59<02:01, 38.54it/s]Training CobwebTree:  36%|      | 2647/7317 [01:00<02:00, 38.76it/s]Training CobwebTree:  36%|      | 2652/7317 [01:00<01:57, 39.75it/s]Training CobwebTree:  36%|      | 2656/7317 [01:00<01:57, 39.63it/s]Training CobwebTree:  36%|      | 2660/7317 [01:00<02:03, 37.85it/s]Training CobwebTree:  36%|      | 2665/7317 [01:00<02:02, 38.03it/s]Training CobwebTree:  36%|      | 2669/7317 [01:00<02:01, 38.37it/s]Training CobwebTree:  37%|      | 2673/7317 [01:00<02:07, 36.52it/s]Training CobwebTree:  37%|      | 2677/7317 [01:00<02:04, 37.20it/s]Training CobwebTree:  37%|      | 2682/7317 [01:00<01:59, 38.87it/s]Training CobwebTree:  37%|      | 2686/7317 [01:01<02:02, 37.83it/s]Training CobwebTree:  37%|      | 2691/7317 [01:01<01:57, 39.30it/s]Training CobwebTree:  37%|      | 2696/7317 [01:01<01:56, 39.71it/s]Training CobwebTree:  37%|      | 2700/7317 [01:01<01:57, 39.46it/s]Training CobwebTree:  37%|      | 2705/7317 [01:01<01:56, 39.45it/s]Training CobwebTree:  37%|      | 2709/7317 [01:01<01:59, 38.68it/s]Training CobwebTree:  37%|      | 2713/7317 [01:01<02:02, 37.73it/s]Training CobwebTree:  37%|      | 2717/7317 [01:01<02:03, 37.33it/s]Training CobwebTree:  37%|      | 2722/7317 [01:02<01:57, 39.11it/s]Training CobwebTree:  37%|      | 2727/7317 [01:02<01:55, 39.79it/s]Training CobwebTree:  37%|      | 2731/7317 [01:02<01:57, 39.02it/s]Training CobwebTree:  37%|      | 2736/7317 [01:02<01:51, 41.21it/s]Training CobwebTree:  37%|      | 2741/7317 [01:02<01:53, 40.24it/s]Training CobwebTree:  38%|      | 2746/7317 [01:02<02:03, 36.87it/s]Training CobwebTree:  38%|      | 2750/7317 [01:02<02:01, 37.53it/s]Training CobwebTree:  38%|      | 2754/7317 [01:02<02:03, 36.82it/s]Training CobwebTree:  38%|      | 2758/7317 [01:02<02:02, 37.16it/s]Training CobwebTree:  38%|      | 2763/7317 [01:03<02:03, 36.87it/s]Training CobwebTree:  38%|      | 2767/7317 [01:03<02:00, 37.67it/s]Training CobwebTree:  38%|      | 2771/7317 [01:03<02:05, 36.11it/s]Training CobwebTree:  38%|      | 2775/7317 [01:03<02:05, 36.29it/s]Training CobwebTree:  38%|      | 2780/7317 [01:03<02:01, 37.43it/s]Training CobwebTree:  38%|      | 2785/7317 [01:03<01:58, 38.17it/s]Training CobwebTree:  38%|      | 2790/7317 [01:03<01:54, 39.39it/s]Training CobwebTree:  38%|      | 2794/7317 [01:03<01:56, 38.80it/s]Training CobwebTree:  38%|      | 2798/7317 [01:04<01:57, 38.54it/s]Training CobwebTree:  38%|      | 2803/7317 [01:04<01:51, 40.52it/s]Training CobwebTree:  38%|      | 2808/7317 [01:04<01:50, 40.75it/s]Training CobwebTree:  38%|      | 2813/7317 [01:04<01:53, 39.62it/s]Training CobwebTree:  39%|      | 2818/7317 [01:04<01:51, 40.21it/s]Training CobwebTree:  39%|      | 2823/7317 [01:04<01:53, 39.66it/s]Training CobwebTree:  39%|      | 2827/7317 [01:04<01:56, 38.69it/s]Training CobwebTree:  39%|      | 2831/7317 [01:04<02:00, 37.11it/s]Training CobwebTree:  39%|      | 2835/7317 [01:04<02:01, 36.96it/s]Training CobwebTree:  39%|      | 2840/7317 [01:05<01:57, 38.20it/s]Training CobwebTree:  39%|      | 2844/7317 [01:05<02:00, 37.19it/s]Training CobwebTree:  39%|      | 2848/7317 [01:05<01:58, 37.81it/s]Training CobwebTree:  39%|      | 2852/7317 [01:05<01:59, 37.30it/s]Training CobwebTree:  39%|      | 2857/7317 [01:05<01:56, 38.39it/s]Training CobwebTree:  39%|      | 2861/7317 [01:05<01:58, 37.56it/s]Training CobwebTree:  39%|      | 2866/7317 [01:05<01:53, 39.05it/s]Training CobwebTree:  39%|      | 2871/7317 [01:05<01:51, 39.81it/s]Training CobwebTree:  39%|      | 2876/7317 [01:06<01:48, 41.09it/s]Training CobwebTree:  39%|      | 2881/7317 [01:06<01:48, 40.71it/s]Training CobwebTree:  39%|      | 2886/7317 [01:06<01:52, 39.56it/s]Training CobwebTree:  39%|      | 2890/7317 [01:06<01:52, 39.37it/s]Training CobwebTree:  40%|      | 2894/7317 [01:06<01:55, 38.17it/s]Training CobwebTree:  40%|      | 2899/7317 [01:06<01:52, 39.38it/s]Training CobwebTree:  40%|      | 2903/7317 [01:06<01:55, 38.10it/s]Training CobwebTree:  40%|      | 2907/7317 [01:06<01:54, 38.50it/s]Training CobwebTree:  40%|      | 2911/7317 [01:06<01:56, 37.75it/s]Training CobwebTree:  40%|      | 2915/7317 [01:07<01:56, 37.79it/s]Training CobwebTree:  40%|      | 2919/7317 [01:07<01:56, 37.70it/s]Training CobwebTree:  40%|      | 2924/7317 [01:07<01:53, 38.79it/s]Training CobwebTree:  40%|      | 2929/7317 [01:07<01:52, 39.06it/s]Training CobwebTree:  40%|      | 2934/7317 [01:07<01:47, 40.58it/s]Training CobwebTree:  40%|      | 2939/7317 [01:07<01:50, 39.76it/s]Training CobwebTree:  40%|      | 2943/7317 [01:07<01:50, 39.60it/s]Training CobwebTree:  40%|      | 2947/7317 [01:07<01:52, 38.95it/s]Training CobwebTree:  40%|      | 2951/7317 [01:07<01:55, 37.96it/s]Training CobwebTree:  40%|      | 2955/7317 [01:08<01:54, 38.21it/s]Training CobwebTree:  40%|      | 2960/7317 [01:08<01:50, 39.26it/s]Training CobwebTree:  41%|      | 2964/7317 [01:08<01:50, 39.42it/s]Training CobwebTree:  41%|      | 2969/7317 [01:08<01:48, 40.07it/s]Training CobwebTree:  41%|      | 2974/7317 [01:08<01:49, 39.49it/s]Training CobwebTree:  41%|      | 2979/7317 [01:08<01:43, 41.76it/s]Training CobwebTree:  41%|      | 2984/7317 [01:08<01:50, 39.14it/s]Training CobwebTree:  41%|      | 2989/7317 [01:08<01:46, 40.47it/s]Training CobwebTree:  41%|      | 2994/7317 [01:09<01:51, 38.82it/s]Training CobwebTree:  41%|      | 2998/7317 [01:09<02:01, 35.54it/s]Training CobwebTree:  41%|      | 3002/7317 [01:09<02:04, 34.55it/s]Training CobwebTree:  41%|      | 3007/7317 [01:09<01:56, 37.01it/s]Training CobwebTree:  41%|      | 3011/7317 [01:09<01:54, 37.65it/s]Training CobwebTree:  41%|      | 3016/7317 [01:09<01:50, 38.91it/s]Training CobwebTree:  41%|     | 3020/7317 [01:09<01:54, 37.57it/s]Training CobwebTree:  41%|     | 3025/7317 [01:09<01:50, 38.72it/s]Training CobwebTree:  41%|     | 3030/7317 [01:10<01:50, 38.66it/s]Training CobwebTree:  41%|     | 3034/7317 [01:10<01:52, 37.96it/s]Training CobwebTree:  42%|     | 3038/7317 [01:10<01:52, 38.07it/s]Training CobwebTree:  42%|     | 3043/7317 [01:10<01:50, 38.63it/s]Training CobwebTree:  42%|     | 3048/7317 [01:10<01:49, 39.04it/s]Training CobwebTree:  42%|     | 3052/7317 [01:10<01:52, 38.04it/s]Training CobwebTree:  42%|     | 3056/7317 [01:10<01:54, 37.30it/s]Training CobwebTree:  42%|     | 3060/7317 [01:10<01:53, 37.39it/s]Training CobwebTree:  42%|     | 3064/7317 [01:10<01:53, 37.31it/s]Training CobwebTree:  42%|     | 3068/7317 [01:11<01:59, 35.69it/s]Training CobwebTree:  42%|     | 3073/7317 [01:11<01:51, 38.12it/s]Training CobwebTree:  42%|     | 3077/7317 [01:11<01:51, 38.09it/s]Training CobwebTree:  42%|     | 3081/7317 [01:11<01:52, 37.59it/s]Training CobwebTree:  42%|     | 3085/7317 [01:11<01:52, 37.63it/s]Training CobwebTree:  42%|     | 3089/7317 [01:11<01:50, 38.22it/s]Training CobwebTree:  42%|     | 3093/7317 [01:11<01:51, 37.76it/s]Training CobwebTree:  42%|     | 3098/7317 [01:11<01:47, 39.19it/s]Training CobwebTree:  42%|     | 3103/7317 [01:11<01:47, 39.04it/s]Training CobwebTree:  42%|     | 3108/7317 [01:12<01:44, 40.19it/s]Training CobwebTree:  43%|     | 3113/7317 [01:12<01:47, 39.29it/s]Training CobwebTree:  43%|     | 3118/7317 [01:12<01:44, 40.02it/s]Training CobwebTree:  43%|     | 3123/7317 [01:12<01:51, 37.56it/s]Training CobwebTree:  43%|     | 3127/7317 [01:12<01:51, 37.67it/s]Training CobwebTree:  43%|     | 3131/7317 [01:12<01:55, 36.14it/s]Training CobwebTree:  43%|     | 3135/7317 [01:12<01:56, 36.00it/s]Training CobwebTree:  43%|     | 3139/7317 [01:12<01:53, 36.96it/s]Training CobwebTree:  43%|     | 3143/7317 [01:13<01:59, 34.87it/s]Training CobwebTree:  43%|     | 3147/7317 [01:13<01:56, 35.68it/s]Training CobwebTree:  43%|     | 3152/7317 [01:13<01:52, 37.02it/s]Training CobwebTree:  43%|     | 3156/7317 [01:13<01:54, 36.42it/s]Training CobwebTree:  43%|     | 3160/7317 [01:13<01:55, 35.91it/s]Training CobwebTree:  43%|     | 3164/7317 [01:13<01:54, 36.28it/s]Training CobwebTree:  43%|     | 3168/7317 [01:13<01:52, 36.88it/s]Training CobwebTree:  43%|     | 3172/7317 [01:13<01:52, 36.98it/s]Training CobwebTree:  43%|     | 3177/7317 [01:13<01:49, 37.91it/s]Training CobwebTree:  43%|     | 3181/7317 [01:14<01:48, 38.28it/s]Training CobwebTree:  44%|     | 3186/7317 [01:14<01:43, 39.76it/s]Training CobwebTree:  44%|     | 3191/7317 [01:14<01:42, 40.34it/s]Training CobwebTree:  44%|     | 3196/7317 [01:14<01:43, 39.83it/s]Training CobwebTree:  44%|     | 3200/7317 [01:14<01:45, 38.93it/s]Training CobwebTree:  44%|     | 3204/7317 [01:14<01:48, 37.74it/s]Training CobwebTree:  44%|     | 3209/7317 [01:14<01:46, 38.66it/s]Training CobwebTree:  44%|     | 3213/7317 [01:14<01:46, 38.47it/s]Training CobwebTree:  44%|     | 3218/7317 [01:14<01:46, 38.46it/s]Training CobwebTree:  44%|     | 3223/7317 [01:15<01:45, 38.77it/s]Training CobwebTree:  44%|     | 3227/7317 [01:15<01:48, 37.55it/s]Training CobwebTree:  44%|     | 3231/7317 [01:15<01:57, 34.91it/s]Training CobwebTree:  44%|     | 3236/7317 [01:15<01:52, 36.19it/s]Training CobwebTree:  44%|     | 3240/7317 [01:15<01:50, 36.92it/s]Training CobwebTree:  44%|     | 3245/7317 [01:15<01:44, 38.86it/s]Training CobwebTree:  44%|     | 3249/7317 [01:15<01:50, 36.96it/s]Training CobwebTree:  44%|     | 3253/7317 [01:15<01:51, 36.61it/s]Training CobwebTree:  45%|     | 3258/7317 [01:16<01:43, 39.12it/s]Training CobwebTree:  45%|     | 3262/7317 [01:16<01:43, 39.18it/s]Training CobwebTree:  45%|     | 3266/7317 [01:16<01:44, 38.87it/s]Training CobwebTree:  45%|     | 3270/7317 [01:16<01:45, 38.35it/s]Training CobwebTree:  45%|     | 3275/7317 [01:16<01:43, 39.11it/s]Training CobwebTree:  45%|     | 3279/7317 [01:16<01:46, 37.93it/s]Training CobwebTree:  45%|     | 3284/7317 [01:16<01:44, 38.74it/s]Training CobwebTree:  45%|     | 3288/7317 [01:16<01:45, 38.02it/s]Training CobwebTree:  45%|     | 3292/7317 [01:16<01:51, 36.09it/s]Training CobwebTree:  45%|     | 3296/7317 [01:17<01:49, 36.74it/s]Training CobwebTree:  45%|     | 3300/7317 [01:17<01:48, 36.86it/s]Training CobwebTree:  45%|     | 3304/7317 [01:17<01:52, 35.56it/s]Training CobwebTree:  45%|     | 3308/7317 [01:17<01:50, 36.26it/s]Training CobwebTree:  45%|     | 3312/7317 [01:17<01:55, 34.82it/s]Training CobwebTree:  45%|     | 3316/7317 [01:17<01:58, 33.88it/s]Training CobwebTree:  45%|     | 3320/7317 [01:17<01:57, 34.02it/s]Training CobwebTree:  45%|     | 3325/7317 [01:17<01:50, 36.04it/s]Training CobwebTree:  45%|     | 3329/7317 [01:17<01:48, 36.63it/s]Training CobwebTree:  46%|     | 3334/7317 [01:18<01:43, 38.50it/s]Training CobwebTree:  46%|     | 3339/7317 [01:18<01:39, 39.85it/s]Training CobwebTree:  46%|     | 3343/7317 [01:18<01:47, 37.13it/s]Training CobwebTree:  46%|     | 3347/7317 [01:18<01:45, 37.51it/s]Training CobwebTree:  46%|     | 3352/7317 [01:18<01:41, 38.97it/s]Training CobwebTree:  46%|     | 3356/7317 [01:18<01:43, 38.12it/s]Training CobwebTree:  46%|     | 3360/7317 [01:18<01:45, 37.64it/s]Training CobwebTree:  46%|     | 3364/7317 [01:18<01:46, 37.20it/s]Training CobwebTree:  46%|     | 3368/7317 [01:19<01:46, 37.12it/s]Training CobwebTree:  46%|     | 3372/7317 [01:19<01:44, 37.67it/s]Training CobwebTree:  46%|     | 3376/7317 [01:19<01:47, 36.77it/s]Training CobwebTree:  46%|     | 3380/7317 [01:19<01:47, 36.67it/s]Training CobwebTree:  46%|     | 3384/7317 [01:19<01:46, 37.03it/s]Training CobwebTree:  46%|     | 3388/7317 [01:19<01:45, 37.17it/s]Training CobwebTree:  46%|     | 3392/7317 [01:19<01:43, 37.80it/s]Training CobwebTree:  46%|     | 3396/7317 [01:19<01:43, 37.84it/s]Training CobwebTree:  46%|     | 3400/7317 [01:19<01:50, 35.47it/s]Training CobwebTree:  47%|     | 3405/7317 [01:20<01:45, 36.93it/s]Training CobwebTree:  47%|     | 3409/7317 [01:20<01:43, 37.62it/s]Training CobwebTree:  47%|     | 3413/7317 [01:20<01:44, 37.29it/s]Training CobwebTree:  47%|     | 3417/7317 [01:20<01:46, 36.67it/s]Training CobwebTree:  47%|     | 3421/7317 [01:20<01:44, 37.29it/s]Training CobwebTree:  47%|     | 3425/7317 [01:20<01:43, 37.70it/s]Training CobwebTree:  47%|     | 3429/7317 [01:20<01:46, 36.62it/s]Training CobwebTree:  47%|     | 3433/7317 [01:20<01:44, 37.23it/s]Training CobwebTree:  47%|     | 3437/7317 [01:20<01:42, 37.72it/s]Training CobwebTree:  47%|     | 3441/7317 [01:20<01:43, 37.44it/s]Training CobwebTree:  47%|     | 3446/7317 [01:21<01:42, 37.64it/s]Training CobwebTree:  47%|     | 3450/7317 [01:21<01:45, 36.82it/s]Training CobwebTree:  47%|     | 3454/7317 [01:21<01:49, 35.41it/s]Training CobwebTree:  47%|     | 3458/7317 [01:21<01:50, 34.79it/s]Training CobwebTree:  47%|     | 3462/7317 [01:21<01:50, 34.89it/s]Training CobwebTree:  47%|     | 3466/7317 [01:21<01:50, 34.98it/s]Training CobwebTree:  47%|     | 3470/7317 [01:21<01:53, 34.04it/s]Training CobwebTree:  47%|     | 3474/7317 [01:21<01:48, 35.56it/s]Training CobwebTree:  48%|     | 3478/7317 [01:22<01:45, 36.26it/s]Training CobwebTree:  48%|     | 3482/7317 [01:22<01:43, 37.06it/s]Training CobwebTree:  48%|     | 3486/7317 [01:22<01:44, 36.77it/s]Training CobwebTree:  48%|     | 3490/7317 [01:22<01:42, 37.24it/s]Training CobwebTree:  48%|     | 3494/7317 [01:22<01:48, 35.35it/s]Training CobwebTree:  48%|     | 3498/7317 [01:22<01:47, 35.44it/s]Training CobwebTree:  48%|     | 3502/7317 [01:22<01:45, 36.00it/s]Training CobwebTree:  48%|     | 3506/7317 [01:22<01:46, 35.77it/s]Training CobwebTree:  48%|     | 3510/7317 [01:22<01:44, 36.38it/s]Training CobwebTree:  48%|     | 3515/7317 [01:23<01:40, 37.94it/s]Training CobwebTree:  48%|     | 3519/7317 [01:23<01:43, 36.57it/s]Training CobwebTree:  48%|     | 3523/7317 [01:23<01:49, 34.81it/s]Training CobwebTree:  48%|     | 3527/7317 [01:23<01:45, 35.77it/s]Training CobwebTree:  48%|     | 3531/7317 [01:23<01:49, 34.46it/s]Training CobwebTree:  48%|     | 3535/7317 [01:23<01:48, 34.70it/s]Training CobwebTree:  48%|     | 3539/7317 [01:23<01:46, 35.59it/s]Training CobwebTree:  48%|     | 3543/7317 [01:23<01:47, 34.96it/s]Training CobwebTree:  48%|     | 3547/7317 [01:23<01:48, 34.79it/s]Training CobwebTree:  49%|     | 3551/7317 [01:24<01:44, 36.04it/s]Training CobwebTree:  49%|     | 3556/7317 [01:24<01:39, 37.76it/s]Training CobwebTree:  49%|     | 3560/7317 [01:24<01:42, 36.81it/s]Training CobwebTree:  49%|     | 3565/7317 [01:24<01:39, 37.88it/s]Training CobwebTree:  49%|     | 3569/7317 [01:24<01:42, 36.50it/s]Training CobwebTree:  49%|     | 3573/7317 [01:24<01:44, 35.76it/s]Training CobwebTree:  49%|     | 3577/7317 [01:24<01:46, 35.17it/s]Training CobwebTree:  49%|     | 3581/7317 [01:24<01:43, 36.18it/s]Training CobwebTree:  49%|     | 3585/7317 [01:24<01:41, 36.73it/s]Training CobwebTree:  49%|     | 3589/7317 [01:25<01:42, 36.31it/s]Training CobwebTree:  49%|     | 3593/7317 [01:25<01:45, 35.42it/s]Training CobwebTree:  49%|     | 3597/7317 [01:25<01:47, 34.57it/s]Training CobwebTree:  49%|     | 3601/7317 [01:25<01:48, 34.27it/s]Training CobwebTree:  49%|     | 3605/7317 [01:25<01:44, 35.57it/s]Training CobwebTree:  49%|     | 3610/7317 [01:25<01:40, 37.03it/s]Training CobwebTree:  49%|     | 3615/7317 [01:25<01:37, 37.81it/s]Training CobwebTree:  49%|     | 3619/7317 [01:25<01:37, 37.97it/s]Training CobwebTree:  50%|     | 3623/7317 [01:26<01:42, 35.93it/s]Training CobwebTree:  50%|     | 3627/7317 [01:26<01:42, 36.15it/s]Training CobwebTree:  50%|     | 3631/7317 [01:26<01:42, 36.04it/s]Training CobwebTree:  50%|     | 3635/7317 [01:26<01:43, 35.53it/s]Training CobwebTree:  50%|     | 3639/7317 [01:26<01:40, 36.63it/s]Training CobwebTree:  50%|     | 3643/7317 [01:26<01:38, 37.20it/s]Training CobwebTree:  50%|     | 3647/7317 [01:26<01:38, 37.34it/s]Training CobwebTree:  50%|     | 3651/7317 [01:26<01:37, 37.75it/s]Training CobwebTree:  50%|     | 3655/7317 [01:26<01:38, 37.25it/s]Training CobwebTree:  50%|     | 3659/7317 [01:27<01:37, 37.60it/s]Training CobwebTree:  50%|     | 3664/7317 [01:27<01:34, 38.73it/s]Training CobwebTree:  50%|     | 3668/7317 [01:27<01:34, 38.54it/s]Training CobwebTree:  50%|     | 3672/7317 [01:27<01:34, 38.57it/s]Training CobwebTree:  50%|     | 3676/7317 [01:27<01:36, 37.92it/s]Training CobwebTree:  50%|     | 3680/7317 [01:27<01:41, 35.77it/s]Training CobwebTree:  50%|     | 3684/7317 [01:27<01:40, 36.26it/s]Training CobwebTree:  50%|     | 3688/7317 [01:27<01:47, 33.83it/s]Training CobwebTree:  50%|     | 3692/7317 [01:27<01:49, 33.06it/s]Training CobwebTree:  51%|     | 3696/7317 [01:28<01:46, 34.00it/s]Training CobwebTree:  51%|     | 3700/7317 [01:28<01:45, 34.16it/s]Training CobwebTree:  51%|     | 3704/7317 [01:28<01:41, 35.60it/s]Training CobwebTree:  51%|     | 3708/7317 [01:28<01:42, 35.05it/s]Training CobwebTree:  51%|     | 3712/7317 [01:28<01:41, 35.52it/s]Training CobwebTree:  51%|     | 3716/7317 [01:28<01:44, 34.43it/s]Training CobwebTree:  51%|     | 3721/7317 [01:28<01:41, 35.48it/s]Training CobwebTree:  51%|     | 3725/7317 [01:28<01:41, 35.27it/s]Training CobwebTree:  51%|     | 3729/7317 [01:28<01:39, 36.15it/s]Training CobwebTree:  51%|     | 3733/7317 [01:29<01:40, 35.76it/s]Training CobwebTree:  51%|     | 3737/7317 [01:29<01:42, 35.07it/s]Training CobwebTree:  51%|     | 3741/7317 [01:29<01:39, 35.77it/s]Training CobwebTree:  51%|     | 3746/7317 [01:29<01:35, 37.49it/s]Training CobwebTree:  51%|    | 3750/7317 [01:29<01:40, 35.64it/s]Training CobwebTree:  51%|    | 3754/7317 [01:29<01:39, 35.81it/s]Training CobwebTree:  51%|    | 3758/7317 [01:29<01:42, 34.86it/s]Training CobwebTree:  51%|    | 3762/7317 [01:29<01:43, 34.51it/s]Training CobwebTree:  51%|    | 3766/7317 [01:30<01:38, 35.94it/s]Training CobwebTree:  52%|    | 3771/7317 [01:30<01:33, 37.88it/s]Training CobwebTree:  52%|    | 3775/7317 [01:30<01:34, 37.53it/s]Training CobwebTree:  52%|    | 3779/7317 [01:30<01:33, 37.77it/s]Training CobwebTree:  52%|    | 3783/7317 [01:30<01:33, 37.68it/s]Training CobwebTree:  52%|    | 3787/7317 [01:30<01:36, 36.60it/s]Training CobwebTree:  52%|    | 3791/7317 [01:30<01:34, 37.21it/s]Training CobwebTree:  52%|    | 3796/7317 [01:30<01:34, 37.06it/s]Training CobwebTree:  52%|    | 3800/7317 [01:30<01:33, 37.75it/s]Training CobwebTree:  52%|    | 3804/7317 [01:31<01:35, 36.72it/s]Training CobwebTree:  52%|    | 3808/7317 [01:31<01:40, 34.94it/s]Training CobwebTree:  52%|    | 3812/7317 [01:31<01:40, 34.85it/s]Training CobwebTree:  52%|    | 3816/7317 [01:31<01:38, 35.46it/s]Training CobwebTree:  52%|    | 3820/7317 [01:31<01:39, 35.24it/s]Training CobwebTree:  52%|    | 3824/7317 [01:31<01:41, 34.50it/s]Training CobwebTree:  52%|    | 3828/7317 [01:31<01:38, 35.37it/s]Training CobwebTree:  52%|    | 3832/7317 [01:31<01:36, 35.99it/s]Training CobwebTree:  52%|    | 3837/7317 [01:31<01:31, 37.91it/s]Training CobwebTree:  52%|    | 3841/7317 [01:32<01:36, 36.09it/s]Training CobwebTree:  53%|    | 3845/7317 [01:32<01:35, 36.54it/s]Training CobwebTree:  53%|    | 3849/7317 [01:32<01:35, 36.28it/s]Training CobwebTree:  53%|    | 3854/7317 [01:32<01:30, 38.10it/s]Training CobwebTree:  53%|    | 3858/7317 [01:32<01:31, 37.97it/s]Training CobwebTree:  53%|    | 3862/7317 [01:32<01:36, 35.70it/s]Training CobwebTree:  53%|    | 3866/7317 [01:32<01:34, 36.39it/s]Training CobwebTree:  53%|    | 3870/7317 [01:32<01:36, 35.81it/s]Training CobwebTree:  53%|    | 3874/7317 [01:32<01:34, 36.45it/s]Training CobwebTree:  53%|    | 3878/7317 [01:33<01:37, 35.10it/s]Training CobwebTree:  53%|    | 3883/7317 [01:33<01:34, 36.42it/s]Training CobwebTree:  53%|    | 3887/7317 [01:33<01:34, 36.25it/s]Training CobwebTree:  53%|    | 3891/7317 [01:33<01:39, 34.27it/s]Training CobwebTree:  53%|    | 3895/7317 [01:33<01:36, 35.52it/s]Training CobwebTree:  53%|    | 3899/7317 [01:33<01:37, 35.15it/s]Training CobwebTree:  53%|    | 3903/7317 [01:33<01:34, 35.95it/s]Training CobwebTree:  53%|    | 3907/7317 [01:33<01:35, 35.87it/s]Training CobwebTree:  53%|    | 3911/7317 [01:34<03:19, 17.09it/s]Training CobwebTree:  54%|    | 3915/7317 [01:34<02:47, 20.36it/s]Training CobwebTree:  54%|    | 3919/7317 [01:34<02:29, 22.71it/s]Training CobwebTree:  54%|    | 3923/7317 [01:34<02:12, 25.64it/s]Training CobwebTree:  54%|    | 3927/7317 [01:34<01:59, 28.39it/s]Training CobwebTree:  54%|    | 3931/7317 [01:34<01:48, 31.08it/s]Training CobwebTree:  54%|    | 3935/7317 [01:35<01:46, 31.90it/s]Training CobwebTree:  54%|    | 3939/7317 [01:35<01:41, 33.43it/s]Training CobwebTree:  54%|    | 3943/7317 [01:35<01:37, 34.44it/s]Training CobwebTree:  54%|    | 3948/7317 [01:35<01:32, 36.42it/s]Training CobwebTree:  54%|    | 3952/7317 [01:35<01:34, 35.54it/s]Training CobwebTree:  54%|    | 3956/7317 [01:35<01:40, 33.52it/s]Training CobwebTree:  54%|    | 3960/7317 [01:35<01:36, 34.79it/s]Training CobwebTree:  54%|    | 3964/7317 [01:35<01:36, 34.89it/s]Training CobwebTree:  54%|    | 3968/7317 [01:36<01:34, 35.36it/s]Training CobwebTree:  54%|    | 3972/7317 [01:36<01:34, 35.47it/s]Training CobwebTree:  54%|    | 3976/7317 [01:36<01:34, 35.27it/s]Training CobwebTree:  54%|    | 3980/7317 [01:36<01:35, 34.87it/s]Training CobwebTree:  54%|    | 3984/7317 [01:36<01:33, 35.47it/s]Training CobwebTree:  55%|    | 3988/7317 [01:36<01:33, 35.68it/s]Training CobwebTree:  55%|    | 3992/7317 [01:36<01:33, 35.65it/s]Training CobwebTree:  55%|    | 3996/7317 [01:36<01:30, 36.78it/s]Training CobwebTree:  55%|    | 4001/7317 [01:36<01:27, 38.11it/s]Training CobwebTree:  55%|    | 4005/7317 [01:37<01:26, 38.11it/s]Training CobwebTree:  55%|    | 4009/7317 [01:37<01:29, 36.98it/s]Training CobwebTree:  55%|    | 4013/7317 [01:37<01:32, 35.83it/s]Training CobwebTree:  55%|    | 4017/7317 [01:37<01:36, 34.28it/s]Training CobwebTree:  55%|    | 4021/7317 [01:37<01:32, 35.78it/s]Training CobwebTree:  55%|    | 4025/7317 [01:37<01:30, 36.49it/s]Training CobwebTree:  55%|    | 4029/7317 [01:37<01:32, 35.39it/s]Training CobwebTree:  55%|    | 4033/7317 [01:37<01:34, 34.78it/s]Training CobwebTree:  55%|    | 4037/7317 [01:37<01:32, 35.46it/s]Training CobwebTree:  55%|    | 4042/7317 [01:38<01:27, 37.42it/s]Training CobwebTree:  55%|    | 4046/7317 [01:38<01:27, 37.59it/s]Training CobwebTree:  55%|    | 4050/7317 [01:38<01:31, 35.82it/s]Training CobwebTree:  55%|    | 4054/7317 [01:38<01:29, 36.48it/s]Training CobwebTree:  55%|    | 4058/7317 [01:38<01:28, 36.65it/s]Training CobwebTree:  56%|    | 4062/7317 [01:38<01:29, 36.53it/s]Training CobwebTree:  56%|    | 4067/7317 [01:38<01:25, 38.21it/s]Training CobwebTree:  56%|    | 4071/7317 [01:38<01:27, 36.91it/s]Training CobwebTree:  56%|    | 4075/7317 [01:38<01:34, 34.31it/s]Training CobwebTree:  56%|    | 4079/7317 [01:39<01:40, 32.12it/s]Training CobwebTree:  56%|    | 4083/7317 [01:39<01:35, 34.04it/s]Training CobwebTree:  56%|    | 4087/7317 [01:39<01:32, 35.04it/s]Training CobwebTree:  56%|    | 4091/7317 [01:39<01:32, 34.86it/s]Training CobwebTree:  56%|    | 4095/7317 [01:39<01:36, 33.28it/s]Training CobwebTree:  56%|    | 4099/7317 [01:39<01:38, 32.52it/s]Training CobwebTree:  56%|    | 4103/7317 [01:39<01:36, 33.42it/s]Training CobwebTree:  56%|    | 4107/7317 [01:39<01:37, 32.95it/s]Training CobwebTree:  56%|    | 4111/7317 [01:40<01:36, 33.10it/s]Training CobwebTree:  56%|    | 4115/7317 [01:40<01:33, 34.08it/s]Training CobwebTree:  56%|    | 4119/7317 [01:40<01:30, 35.33it/s]Training CobwebTree:  56%|    | 4123/7317 [01:40<01:32, 34.68it/s]Training CobwebTree:  56%|    | 4127/7317 [01:40<01:29, 35.78it/s]Training CobwebTree:  56%|    | 4132/7317 [01:40<01:26, 36.90it/s]Training CobwebTree:  57%|    | 4137/7317 [01:40<01:23, 38.12it/s]Training CobwebTree:  57%|    | 4141/7317 [01:40<01:28, 35.86it/s]Training CobwebTree:  57%|    | 4145/7317 [01:41<01:27, 36.11it/s]Training CobwebTree:  57%|    | 4149/7317 [01:41<01:26, 36.45it/s]Training CobwebTree:  57%|    | 4153/7317 [01:41<01:30, 35.12it/s]Training CobwebTree:  57%|    | 4157/7317 [01:41<01:28, 35.54it/s]Training CobwebTree:  57%|    | 4161/7317 [01:41<01:28, 35.70it/s]Training CobwebTree:  57%|    | 4165/7317 [01:41<01:30, 34.99it/s]Training CobwebTree:  57%|    | 4169/7317 [01:41<01:30, 34.73it/s]Training CobwebTree:  57%|    | 4173/7317 [01:41<01:27, 35.90it/s]Training CobwebTree:  57%|    | 4177/7317 [01:41<01:27, 35.87it/s]Training CobwebTree:  57%|    | 4181/7317 [01:42<01:27, 35.90it/s]Training CobwebTree:  57%|    | 4185/7317 [01:42<01:29, 34.92it/s]Training CobwebTree:  57%|    | 4189/7317 [01:42<01:27, 35.91it/s]Training CobwebTree:  57%|    | 4193/7317 [01:42<01:30, 34.70it/s]Training CobwebTree:  57%|    | 4198/7317 [01:42<01:22, 37.80it/s]Training CobwebTree:  57%|    | 4202/7317 [01:42<01:23, 37.11it/s]Training CobwebTree:  57%|    | 4207/7317 [01:42<01:21, 37.94it/s]Training CobwebTree:  58%|    | 4212/7317 [01:42<01:18, 39.53it/s]Training CobwebTree:  58%|    | 4216/7317 [01:42<01:18, 39.43it/s]Training CobwebTree:  58%|    | 4221/7317 [01:43<01:15, 40.84it/s]Training CobwebTree:  58%|    | 4226/7317 [01:43<01:15, 41.04it/s]Training CobwebTree:  58%|    | 4231/7317 [01:43<01:18, 39.50it/s]Training CobwebTree:  58%|    | 4235/7317 [01:43<01:19, 38.70it/s]Training CobwebTree:  58%|    | 4239/7317 [01:43<01:20, 38.43it/s]Training CobwebTree:  58%|    | 4243/7317 [01:43<01:21, 37.52it/s]Training CobwebTree:  58%|    | 4248/7317 [01:43<01:18, 39.21it/s]Training CobwebTree:  58%|    | 4252/7317 [01:43<01:19, 38.71it/s]Training CobwebTree:  58%|    | 4256/7317 [01:43<01:19, 38.60it/s]Training CobwebTree:  58%|    | 4260/7317 [01:44<01:19, 38.65it/s]Training CobwebTree:  58%|    | 4265/7317 [01:44<01:18, 38.94it/s]Training CobwebTree:  58%|    | 4269/7317 [01:44<01:18, 38.71it/s]Training CobwebTree:  58%|    | 4273/7317 [01:44<01:21, 37.25it/s]Training CobwebTree:  58%|    | 4277/7317 [01:44<01:21, 37.48it/s]Training CobwebTree:  59%|    | 4281/7317 [01:44<01:21, 37.24it/s]Training CobwebTree:  59%|    | 4285/7317 [01:44<01:24, 35.92it/s]Training CobwebTree:  59%|    | 4289/7317 [01:44<01:24, 36.00it/s]Training CobwebTree:  59%|    | 4293/7317 [01:44<01:26, 35.14it/s]Training CobwebTree:  59%|    | 4297/7317 [01:45<01:28, 34.11it/s]Training CobwebTree:  59%|    | 4301/7317 [01:45<01:28, 34.02it/s]Training CobwebTree:  59%|    | 4305/7317 [01:45<01:26, 34.80it/s]Training CobwebTree:  59%|    | 4309/7317 [01:45<01:25, 35.33it/s]Training CobwebTree:  59%|    | 4314/7317 [01:45<01:21, 36.94it/s]Training CobwebTree:  59%|    | 4318/7317 [01:45<01:20, 37.24it/s]Training CobwebTree:  59%|    | 4322/7317 [01:45<01:20, 37.00it/s]Training CobwebTree:  59%|    | 4326/7317 [01:45<01:25, 34.89it/s]Training CobwebTree:  59%|    | 4330/7317 [01:46<01:23, 35.92it/s]Training CobwebTree:  59%|    | 4334/7317 [01:46<01:24, 35.40it/s]Training CobwebTree:  59%|    | 4338/7317 [01:46<01:22, 36.07it/s]Training CobwebTree:  59%|    | 4342/7317 [01:46<01:21, 36.49it/s]Training CobwebTree:  59%|    | 4346/7317 [01:46<01:21, 36.47it/s]Training CobwebTree:  59%|    | 4350/7317 [01:46<01:19, 37.13it/s]Training CobwebTree:  60%|    | 4354/7317 [01:46<01:21, 36.30it/s]Training CobwebTree:  60%|    | 4358/7317 [01:46<01:19, 37.10it/s]Training CobwebTree:  60%|    | 4362/7317 [01:46<01:20, 36.68it/s]Training CobwebTree:  60%|    | 4367/7317 [01:47<01:16, 38.38it/s]Training CobwebTree:  60%|    | 4372/7317 [01:47<01:16, 38.68it/s]Training CobwebTree:  60%|    | 4376/7317 [01:47<01:17, 37.96it/s]Training CobwebTree:  60%|    | 4380/7317 [01:47<01:19, 36.81it/s]Training CobwebTree:  60%|    | 4384/7317 [01:47<01:20, 36.34it/s]Training CobwebTree:  60%|    | 4388/7317 [01:47<01:26, 33.99it/s]Training CobwebTree:  60%|    | 4392/7317 [01:47<01:26, 33.68it/s]Training CobwebTree:  60%|    | 4396/7317 [01:47<01:23, 34.86it/s]Training CobwebTree:  60%|    | 4400/7317 [01:47<01:24, 34.53it/s]Training CobwebTree:  60%|    | 4404/7317 [01:48<01:24, 34.35it/s]Training CobwebTree:  60%|    | 4408/7317 [01:48<01:22, 35.07it/s]Training CobwebTree:  60%|    | 4413/7317 [01:48<01:20, 36.27it/s]Training CobwebTree:  60%|    | 4417/7317 [01:48<01:20, 36.24it/s]Training CobwebTree:  60%|    | 4422/7317 [01:48<01:15, 38.28it/s]Training CobwebTree:  60%|    | 4426/7317 [01:48<01:16, 37.57it/s]Training CobwebTree:  61%|    | 4430/7317 [01:48<01:20, 36.02it/s]Training CobwebTree:  61%|    | 4434/7317 [01:48<01:22, 35.05it/s]Training CobwebTree:  61%|    | 4438/7317 [01:48<01:20, 35.74it/s]Training CobwebTree:  61%|    | 4442/7317 [01:49<01:18, 36.72it/s]Training CobwebTree:  61%|    | 4446/7317 [01:49<01:19, 36.07it/s]Training CobwebTree:  61%|    | 4450/7317 [01:49<01:21, 35.16it/s]Training CobwebTree:  61%|    | 4454/7317 [01:49<01:23, 34.16it/s]Training CobwebTree:  61%|    | 4459/7317 [01:49<01:20, 35.55it/s]Training CobwebTree:  61%|    | 4463/7317 [01:49<01:20, 35.58it/s]Training CobwebTree:  61%|    | 4467/7317 [01:49<01:21, 35.06it/s]Training CobwebTree:  61%|    | 4472/7317 [01:49<01:16, 37.22it/s]Training CobwebTree:  61%|    | 4476/7317 [01:50<01:20, 35.39it/s]Training CobwebTree:  61%|    | 4480/7317 [01:50<01:18, 36.36it/s]Training CobwebTree:  61%|   | 4484/7317 [01:50<01:17, 36.52it/s]Training CobwebTree:  61%|   | 4488/7317 [01:50<01:20, 35.03it/s]Training CobwebTree:  61%|   | 4492/7317 [01:50<01:24, 33.39it/s]Training CobwebTree:  61%|   | 4496/7317 [01:50<01:24, 33.53it/s]Training CobwebTree:  62%|   | 4500/7317 [01:50<01:25, 33.09it/s]Training CobwebTree:  62%|   | 4504/7317 [01:50<01:21, 34.63it/s]Training CobwebTree:  62%|   | 4508/7317 [01:50<01:21, 34.61it/s]Training CobwebTree:  62%|   | 4512/7317 [01:51<01:20, 35.00it/s]Training CobwebTree:  62%|   | 4516/7317 [01:51<01:18, 35.78it/s]Training CobwebTree:  62%|   | 4520/7317 [01:51<01:17, 35.90it/s]Training CobwebTree:  62%|   | 4524/7317 [01:51<01:19, 35.30it/s]Training CobwebTree:  62%|   | 4528/7317 [01:51<01:18, 35.72it/s]Training CobwebTree:  62%|   | 4532/7317 [01:51<01:17, 35.94it/s]Training CobwebTree:  62%|   | 4536/7317 [01:51<01:20, 34.39it/s]Training CobwebTree:  62%|   | 4540/7317 [01:51<01:21, 34.13it/s]Training CobwebTree:  62%|   | 4544/7317 [01:52<01:22, 33.51it/s]Training CobwebTree:  62%|   | 4548/7317 [01:52<01:19, 34.97it/s]Training CobwebTree:  62%|   | 4552/7317 [01:52<01:18, 35.37it/s]Training CobwebTree:  62%|   | 4556/7317 [01:52<01:16, 36.14it/s]Training CobwebTree:  62%|   | 4560/7317 [01:52<01:14, 36.94it/s]Training CobwebTree:  62%|   | 4564/7317 [01:52<01:15, 36.54it/s]Training CobwebTree:  62%|   | 4568/7317 [01:52<01:14, 37.14it/s]Training CobwebTree:  62%|   | 4572/7317 [01:52<01:16, 35.74it/s]Training CobwebTree:  63%|   | 4576/7317 [01:52<01:17, 35.39it/s]Training CobwebTree:  63%|   | 4580/7317 [01:53<01:15, 36.08it/s]Training CobwebTree:  63%|   | 4584/7317 [01:53<01:17, 35.19it/s]Training CobwebTree:  63%|   | 4589/7317 [01:53<01:15, 35.93it/s]Training CobwebTree:  63%|   | 4593/7317 [01:53<01:15, 36.23it/s]Training CobwebTree:  63%|   | 4597/7317 [01:53<01:13, 36.98it/s]Training CobwebTree:  63%|   | 4601/7317 [01:53<01:14, 36.45it/s]Training CobwebTree:  63%|   | 4605/7317 [01:53<01:17, 34.96it/s]Training CobwebTree:  63%|   | 4609/7317 [01:53<01:19, 34.13it/s]Training CobwebTree:  63%|   | 4613/7317 [01:53<01:18, 34.29it/s]Training CobwebTree:  63%|   | 4617/7317 [01:54<01:17, 34.76it/s]Training CobwebTree:  63%|   | 4621/7317 [01:54<01:15, 35.66it/s]Training CobwebTree:  63%|   | 4625/7317 [01:54<01:16, 35.38it/s]Training CobwebTree:  63%|   | 4629/7317 [01:54<01:15, 35.59it/s]Training CobwebTree:  63%|   | 4634/7317 [01:54<01:12, 36.84it/s]Training CobwebTree:  63%|   | 4638/7317 [01:54<01:15, 35.41it/s]Training CobwebTree:  63%|   | 4642/7317 [01:54<01:13, 36.26it/s]Training CobwebTree:  64%|   | 4647/7317 [01:54<01:10, 37.75it/s]Training CobwebTree:  64%|   | 4651/7317 [01:54<01:10, 37.72it/s]Training CobwebTree:  64%|   | 4655/7317 [01:55<01:09, 38.26it/s]Training CobwebTree:  64%|   | 4659/7317 [01:55<01:10, 37.77it/s]Training CobwebTree:  64%|   | 4663/7317 [01:55<01:12, 36.79it/s]Training CobwebTree:  64%|   | 4667/7317 [01:55<01:10, 37.35it/s]Training CobwebTree:  64%|   | 4671/7317 [01:55<01:11, 36.83it/s]Training CobwebTree:  64%|   | 4675/7317 [01:55<01:13, 36.11it/s]Training CobwebTree:  64%|   | 4679/7317 [01:55<01:14, 35.37it/s]Training CobwebTree:  64%|   | 4683/7317 [01:55<01:16, 34.59it/s]Training CobwebTree:  64%|   | 4687/7317 [01:55<01:17, 33.84it/s]Training CobwebTree:  64%|   | 4691/7317 [01:56<01:15, 34.60it/s]Training CobwebTree:  64%|   | 4695/7317 [01:56<01:17, 34.01it/s]Training CobwebTree:  64%|   | 4699/7317 [01:56<01:15, 34.79it/s]Training CobwebTree:  64%|   | 4703/7317 [01:56<01:15, 34.84it/s]Training CobwebTree:  64%|   | 4707/7317 [01:56<01:13, 35.61it/s]Training CobwebTree:  64%|   | 4712/7317 [01:56<01:08, 37.80it/s]Training CobwebTree:  64%|   | 4716/7317 [01:56<01:09, 37.21it/s]Training CobwebTree:  65%|   | 4720/7317 [01:56<01:11, 36.54it/s]Training CobwebTree:  65%|   | 4724/7317 [01:57<01:13, 35.37it/s]Training CobwebTree:  65%|   | 4728/7317 [01:57<01:11, 36.14it/s]Training CobwebTree:  65%|   | 4732/7317 [01:57<01:12, 35.44it/s]Training CobwebTree:  65%|   | 4736/7317 [01:57<01:11, 35.98it/s]Training CobwebTree:  65%|   | 4741/7317 [01:57<01:09, 37.00it/s]Training CobwebTree:  65%|   | 4745/7317 [01:57<01:11, 35.86it/s]Training CobwebTree:  65%|   | 4749/7317 [01:57<01:10, 36.32it/s]Training CobwebTree:  65%|   | 4753/7317 [01:57<01:12, 35.25it/s]Training CobwebTree:  65%|   | 4758/7317 [01:57<01:09, 37.00it/s]Training CobwebTree:  65%|   | 4762/7317 [01:58<01:10, 36.49it/s]Training CobwebTree:  65%|   | 4766/7317 [01:58<01:11, 35.66it/s]Training CobwebTree:  65%|   | 4770/7317 [01:58<01:15, 33.67it/s]Training CobwebTree:  65%|   | 4774/7317 [01:58<01:14, 34.02it/s]Training CobwebTree:  65%|   | 4778/7317 [01:58<01:12, 34.94it/s]Training CobwebTree:  65%|   | 4782/7317 [01:58<01:10, 36.17it/s]Training CobwebTree:  65%|   | 4786/7317 [01:58<01:11, 35.34it/s]Training CobwebTree:  65%|   | 4790/7317 [01:58<01:09, 36.46it/s]Training CobwebTree:  66%|   | 4794/7317 [01:58<01:10, 35.71it/s]Training CobwebTree:  66%|   | 4798/7317 [01:59<01:11, 35.00it/s]Training CobwebTree:  66%|   | 4802/7317 [01:59<01:10, 35.51it/s]Training CobwebTree:  66%|   | 4806/7317 [01:59<01:11, 35.06it/s]Training CobwebTree:  66%|   | 4811/7317 [01:59<01:09, 35.93it/s]Training CobwebTree:  66%|   | 4815/7317 [01:59<01:09, 35.84it/s]Training CobwebTree:  66%|   | 4819/7317 [01:59<01:11, 35.18it/s]Training CobwebTree:  66%|   | 4823/7317 [01:59<01:11, 34.70it/s]Training CobwebTree:  66%|   | 4827/7317 [01:59<01:12, 34.42it/s]Training CobwebTree:  66%|   | 4831/7317 [02:00<01:10, 35.42it/s]Training CobwebTree:  66%|   | 4836/7317 [02:00<01:07, 36.84it/s]Training CobwebTree:  66%|   | 4840/7317 [02:00<01:06, 37.43it/s]Training CobwebTree:  66%|   | 4844/7317 [02:00<01:06, 36.96it/s]Training CobwebTree:  66%|   | 4848/7317 [02:00<01:11, 34.77it/s]Training CobwebTree:  66%|   | 4852/7317 [02:00<01:09, 35.44it/s]Training CobwebTree:  66%|   | 4856/7317 [02:00<01:10, 34.68it/s]Training CobwebTree:  66%|   | 4860/7317 [02:00<01:10, 34.79it/s]Training CobwebTree:  66%|   | 4864/7317 [02:00<01:10, 34.87it/s]Training CobwebTree:  67%|   | 4868/7317 [02:01<01:10, 34.92it/s]Training CobwebTree:  67%|   | 4872/7317 [02:01<01:09, 35.09it/s]Training CobwebTree:  67%|   | 4876/7317 [02:01<01:08, 35.55it/s]Training CobwebTree:  67%|   | 4880/7317 [02:01<01:08, 35.41it/s]Training CobwebTree:  67%|   | 4884/7317 [02:01<01:09, 35.02it/s]Training CobwebTree:  67%|   | 4888/7317 [02:01<01:10, 34.44it/s]Training CobwebTree:  67%|   | 4892/7317 [02:01<01:08, 35.33it/s]Training CobwebTree:  67%|   | 4896/7317 [02:01<01:06, 36.40it/s]Training CobwebTree:  67%|   | 4900/7317 [02:01<01:07, 35.82it/s]Training CobwebTree:  67%|   | 4904/7317 [02:02<01:06, 36.06it/s]Training CobwebTree:  67%|   | 4908/7317 [02:02<01:08, 35.30it/s]Training CobwebTree:  67%|   | 4912/7317 [02:02<01:09, 34.52it/s]Training CobwebTree:  67%|   | 4917/7317 [02:02<01:06, 35.84it/s]Training CobwebTree:  67%|   | 4922/7317 [02:02<01:03, 37.82it/s]Training CobwebTree:  67%|   | 4926/7317 [02:02<01:04, 37.34it/s]Training CobwebTree:  67%|   | 4931/7317 [02:02<01:01, 38.91it/s]Training CobwebTree:  67%|   | 4935/7317 [02:02<01:02, 38.21it/s]Training CobwebTree:  68%|   | 4939/7317 [02:03<01:04, 36.96it/s]Training CobwebTree:  68%|   | 4943/7317 [02:03<01:05, 35.98it/s]Training CobwebTree:  68%|   | 4947/7317 [02:03<01:06, 35.48it/s]Training CobwebTree:  68%|   | 4952/7317 [02:03<01:05, 35.95it/s]Training CobwebTree:  68%|   | 4957/7317 [02:03<01:02, 37.80it/s]Training CobwebTree:  68%|   | 4961/7317 [02:03<01:04, 36.31it/s]Training CobwebTree:  68%|   | 4965/7317 [02:03<01:04, 36.36it/s]Training CobwebTree:  68%|   | 4969/7317 [02:03<01:04, 36.54it/s]Training CobwebTree:  68%|   | 4973/7317 [02:03<01:04, 36.08it/s]Training CobwebTree:  68%|   | 4978/7317 [02:04<01:01, 37.92it/s]Training CobwebTree:  68%|   | 4982/7317 [02:04<01:02, 37.48it/s]Training CobwebTree:  68%|   | 4986/7317 [02:04<01:02, 37.34it/s]Training CobwebTree:  68%|   | 4990/7317 [02:04<01:03, 36.80it/s]Training CobwebTree:  68%|   | 4994/7317 [02:04<01:04, 35.78it/s]Training CobwebTree:  68%|   | 4998/7317 [02:04<01:03, 36.79it/s]Training CobwebTree:  68%|   | 5002/7317 [02:04<01:04, 36.15it/s]Training CobwebTree:  68%|   | 5006/7317 [02:04<01:05, 35.52it/s]Training CobwebTree:  68%|   | 5010/7317 [02:04<01:03, 36.12it/s]Training CobwebTree:  69%|   | 5014/7317 [02:05<01:04, 35.92it/s]Training CobwebTree:  69%|   | 5018/7317 [02:05<01:04, 35.40it/s]Training CobwebTree:  69%|   | 5022/7317 [02:05<01:04, 35.75it/s]Training CobwebTree:  69%|   | 5026/7317 [02:05<01:03, 36.34it/s]Training CobwebTree:  69%|   | 5030/7317 [02:05<01:03, 36.19it/s]Training CobwebTree:  69%|   | 5034/7317 [02:05<01:02, 36.61it/s]Training CobwebTree:  69%|   | 5038/7317 [02:05<01:01, 36.99it/s]Training CobwebTree:  69%|   | 5042/7317 [02:05<01:04, 35.29it/s]Training CobwebTree:  69%|   | 5046/7317 [02:05<01:02, 36.07it/s]Training CobwebTree:  69%|   | 5050/7317 [02:06<01:06, 34.31it/s]Training CobwebTree:  69%|   | 5055/7317 [02:06<01:02, 35.96it/s]Training CobwebTree:  69%|   | 5059/7317 [02:06<01:02, 36.03it/s]Training CobwebTree:  69%|   | 5063/7317 [02:06<01:04, 34.92it/s]Training CobwebTree:  69%|   | 5067/7317 [02:06<01:03, 35.59it/s]Training CobwebTree:  69%|   | 5071/7317 [02:06<01:09, 32.35it/s]Training CobwebTree:  69%|   | 5075/7317 [02:06<01:08, 32.89it/s]Training CobwebTree:  69%|   | 5079/7317 [02:06<01:07, 33.00it/s]Training CobwebTree:  69%|   | 5084/7317 [02:07<01:04, 34.52it/s]Training CobwebTree:  70%|   | 5088/7317 [02:07<01:02, 35.46it/s]Training CobwebTree:  70%|   | 5092/7317 [02:07<01:04, 34.33it/s]Training CobwebTree:  70%|   | 5096/7317 [02:07<01:02, 35.61it/s]Training CobwebTree:  70%|   | 5100/7317 [02:07<01:02, 35.28it/s]Training CobwebTree:  70%|   | 5104/7317 [02:07<01:02, 35.19it/s]Training CobwebTree:  70%|   | 5108/7317 [02:07<01:02, 35.36it/s]Training CobwebTree:  70%|   | 5112/7317 [02:07<01:01, 35.77it/s]Training CobwebTree:  70%|   | 5116/7317 [02:08<01:04, 34.12it/s]Training CobwebTree:  70%|   | 5120/7317 [02:08<01:08, 32.18it/s]Training CobwebTree:  70%|   | 5125/7317 [02:08<01:01, 35.37it/s]Training CobwebTree:  70%|   | 5129/7317 [02:08<01:01, 35.67it/s]Training CobwebTree:  70%|   | 5134/7317 [02:08<00:56, 38.44it/s]Training CobwebTree:  70%|   | 5138/7317 [02:08<00:56, 38.50it/s]Training CobwebTree:  70%|   | 5142/7317 [02:08<00:57, 37.78it/s]Training CobwebTree:  70%|   | 5147/7317 [02:08<00:56, 38.41it/s]Training CobwebTree:  70%|   | 5151/7317 [02:08<00:58, 37.23it/s]Training CobwebTree:  70%|   | 5156/7317 [02:09<00:55, 38.79it/s]Training CobwebTree:  71%|   | 5160/7317 [02:09<00:58, 37.15it/s]Training CobwebTree:  71%|   | 5164/7317 [02:09<00:59, 36.07it/s]Training CobwebTree:  71%|   | 5168/7317 [02:09<01:01, 34.80it/s]Training CobwebTree:  71%|   | 5172/7317 [02:09<01:01, 34.67it/s]Training CobwebTree:  71%|   | 5176/7317 [02:09<01:01, 34.68it/s]Training CobwebTree:  71%|   | 5181/7317 [02:09<00:58, 36.21it/s]Training CobwebTree:  71%|   | 5185/7317 [02:09<01:02, 34.02it/s]Training CobwebTree:  71%|   | 5190/7317 [02:10<00:59, 35.58it/s]Training CobwebTree:  71%|   | 5194/7317 [02:10<01:00, 35.04it/s]Training CobwebTree:  71%|   | 5198/7317 [02:10<01:01, 34.44it/s]Training CobwebTree:  71%|   | 5202/7317 [02:10<01:01, 34.35it/s]Training CobwebTree:  71%|   | 5206/7317 [02:10<01:02, 33.70it/s]Training CobwebTree:  71%|   | 5210/7317 [02:10<01:02, 33.96it/s]Training CobwebTree:  71%|  | 5214/7317 [02:10<01:00, 34.85it/s]Training CobwebTree:  71%|  | 5218/7317 [02:10<00:59, 35.41it/s]Training CobwebTree:  71%|  | 5222/7317 [02:10<01:01, 34.12it/s]Training CobwebTree:  71%|  | 5226/7317 [02:11<01:00, 34.62it/s]Training CobwebTree:  71%|  | 5231/7317 [02:11<00:56, 36.98it/s]Training CobwebTree:  72%|  | 5235/7317 [02:11<00:58, 35.71it/s]Training CobwebTree:  72%|  | 5240/7317 [02:11<00:55, 37.14it/s]Training CobwebTree:  72%|  | 5244/7317 [02:11<00:57, 36.25it/s]Training CobwebTree:  72%|  | 5248/7317 [02:11<00:57, 35.70it/s]Training CobwebTree:  72%|  | 5252/7317 [02:11<00:56, 36.53it/s]Training CobwebTree:  72%|  | 5256/7317 [02:11<00:57, 36.11it/s]Training CobwebTree:  72%|  | 5260/7317 [02:12<00:57, 35.56it/s]Training CobwebTree:  72%|  | 5264/7317 [02:12<00:57, 35.62it/s]Training CobwebTree:  72%|  | 5268/7317 [02:12<00:57, 35.92it/s]Training CobwebTree:  72%|  | 5272/7317 [02:12<00:59, 34.32it/s]Training CobwebTree:  72%|  | 5276/7317 [02:12<00:57, 35.74it/s]Training CobwebTree:  72%|  | 5280/7317 [02:12<00:59, 34.04it/s]Training CobwebTree:  72%|  | 5284/7317 [02:12<00:57, 35.52it/s]Training CobwebTree:  72%|  | 5288/7317 [02:12<00:57, 35.08it/s]Training CobwebTree:  72%|  | 5292/7317 [02:12<00:57, 35.50it/s]Training CobwebTree:  72%|  | 5297/7317 [02:13<00:53, 37.59it/s]Training CobwebTree:  72%|  | 5301/7317 [02:13<00:54, 36.76it/s]Training CobwebTree:  73%|  | 5305/7317 [02:13<00:55, 35.99it/s]Training CobwebTree:  73%|  | 5309/7317 [02:13<00:56, 35.69it/s]Training CobwebTree:  73%|  | 5314/7317 [02:13<00:53, 37.41it/s]Training CobwebTree:  73%|  | 5318/7317 [02:13<00:56, 35.61it/s]Training CobwebTree:  73%|  | 5322/7317 [02:13<00:56, 35.16it/s]Training CobwebTree:  73%|  | 5326/7317 [02:13<00:54, 36.36it/s]Training CobwebTree:  73%|  | 5330/7317 [02:13<00:55, 35.95it/s]Training CobwebTree:  73%|  | 5334/7317 [02:14<00:53, 36.92it/s]Training CobwebTree:  73%|  | 5338/7317 [02:14<00:55, 35.63it/s]Training CobwebTree:  73%|  | 5342/7317 [02:14<00:56, 34.87it/s]Training CobwebTree:  73%|  | 5346/7317 [02:14<00:58, 33.79it/s]Training CobwebTree:  73%|  | 5350/7317 [02:14<00:57, 34.44it/s]Training CobwebTree:  73%|  | 5354/7317 [02:14<00:59, 33.17it/s]Training CobwebTree:  73%|  | 5358/7317 [02:14<00:59, 32.82it/s]Training CobwebTree:  73%|  | 5362/7317 [02:14<00:59, 33.02it/s]Training CobwebTree:  73%|  | 5366/7317 [02:15<00:56, 34.71it/s]Training CobwebTree:  73%|  | 5370/7317 [02:15<00:55, 34.79it/s]Training CobwebTree:  73%|  | 5375/7317 [02:15<00:53, 36.57it/s]Training CobwebTree:  74%|  | 5379/7317 [02:15<00:53, 36.51it/s]Training CobwebTree:  74%|  | 5383/7317 [02:15<00:53, 36.06it/s]Training CobwebTree:  74%|  | 5387/7317 [02:15<00:52, 36.47it/s]Training CobwebTree:  74%|  | 5391/7317 [02:15<00:51, 37.36it/s]Training CobwebTree:  74%|  | 5395/7317 [02:15<00:53, 36.03it/s]Training CobwebTree:  74%|  | 5399/7317 [02:15<00:52, 36.65it/s]Training CobwebTree:  74%|  | 5403/7317 [02:16<00:55, 34.58it/s]Training CobwebTree:  74%|  | 5407/7317 [02:16<00:54, 34.94it/s]Training CobwebTree:  74%|  | 5411/7317 [02:16<00:53, 35.30it/s]Training CobwebTree:  74%|  | 5415/7317 [02:16<00:54, 34.65it/s]Training CobwebTree:  74%|  | 5419/7317 [02:16<00:56, 33.55it/s]Training CobwebTree:  74%|  | 5423/7317 [02:16<00:55, 34.12it/s]Training CobwebTree:  74%|  | 5427/7317 [02:16<00:53, 35.35it/s]Training CobwebTree:  74%|  | 5431/7317 [02:16<00:54, 34.66it/s]Training CobwebTree:  74%|  | 5435/7317 [02:16<00:53, 35.30it/s]Training CobwebTree:  74%|  | 5439/7317 [02:17<00:53, 34.97it/s]Training CobwebTree:  74%|  | 5443/7317 [02:17<00:53, 34.85it/s]Training CobwebTree:  74%|  | 5447/7317 [02:17<00:53, 34.96it/s]Training CobwebTree:  74%|  | 5451/7317 [02:17<00:52, 35.63it/s]Training CobwebTree:  75%|  | 5455/7317 [02:17<00:53, 34.90it/s]Training CobwebTree:  75%|  | 5459/7317 [02:17<00:52, 35.24it/s]Training CobwebTree:  75%|  | 5463/7317 [02:17<00:54, 34.13it/s]Training CobwebTree:  75%|  | 5467/7317 [02:17<00:54, 34.22it/s]Training CobwebTree:  75%|  | 5471/7317 [02:18<00:53, 34.24it/s]Training CobwebTree:  75%|  | 5475/7317 [02:18<00:52, 35.00it/s]Training CobwebTree:  75%|  | 5479/7317 [02:18<00:51, 35.55it/s]Training CobwebTree:  75%|  | 5483/7317 [02:18<00:52, 35.16it/s]Training CobwebTree:  75%|  | 5488/7317 [02:18<00:49, 37.25it/s]Training CobwebTree:  75%|  | 5492/7317 [02:18<00:51, 35.70it/s]Training CobwebTree:  75%|  | 5496/7317 [02:18<00:51, 35.60it/s]Training CobwebTree:  75%|  | 5500/7317 [02:18<00:50, 35.75it/s]Training CobwebTree:  75%|  | 5504/7317 [02:18<00:51, 35.38it/s]Training CobwebTree:  75%|  | 5508/7317 [02:19<00:49, 36.61it/s]Training CobwebTree:  75%|  | 5513/7317 [02:19<00:48, 37.21it/s]Training CobwebTree:  75%|  | 5517/7317 [02:19<00:48, 36.82it/s]Training CobwebTree:  75%|  | 5521/7317 [02:19<00:48, 37.02it/s]Training CobwebTree:  76%|  | 5525/7317 [02:19<00:51, 35.10it/s]Training CobwebTree:  76%|  | 5529/7317 [02:19<00:53, 33.35it/s]Training CobwebTree:  76%|  | 5533/7317 [02:19<00:52, 33.70it/s]Training CobwebTree:  76%|  | 5537/7317 [02:19<00:52, 33.73it/s]Training CobwebTree:  76%|  | 5541/7317 [02:20<00:53, 33.17it/s]Training CobwebTree:  76%|  | 5545/7317 [02:20<00:52, 34.01it/s]Training CobwebTree:  76%|  | 5549/7317 [02:20<00:50, 34.95it/s]Training CobwebTree:  76%|  | 5553/7317 [02:20<00:50, 34.68it/s]Training CobwebTree:  76%|  | 5557/7317 [02:20<00:50, 34.74it/s]Training CobwebTree:  76%|  | 5561/7317 [02:20<00:48, 35.91it/s]Training CobwebTree:  76%|  | 5565/7317 [02:20<00:49, 35.34it/s]Training CobwebTree:  76%|  | 5569/7317 [02:20<00:49, 35.42it/s]Training CobwebTree:  76%|  | 5573/7317 [02:20<00:50, 34.78it/s]Training CobwebTree:  76%|  | 5577/7317 [02:21<00:50, 34.58it/s]Training CobwebTree:  76%|  | 5581/7317 [02:21<00:50, 34.42it/s]Training CobwebTree:  76%|  | 5585/7317 [02:21<00:49, 34.93it/s]Training CobwebTree:  76%|  | 5589/7317 [02:21<00:51, 33.73it/s]Training CobwebTree:  76%|  | 5593/7317 [02:21<00:49, 34.58it/s]Training CobwebTree:  76%|  | 5597/7317 [02:21<00:49, 34.71it/s]Training CobwebTree:  77%|  | 5601/7317 [02:21<00:51, 33.64it/s]Training CobwebTree:  77%|  | 5606/7317 [02:21<00:47, 35.66it/s]Training CobwebTree:  77%|  | 5610/7317 [02:21<00:49, 34.75it/s]Training CobwebTree:  77%|  | 5614/7317 [02:22<00:49, 34.30it/s]Training CobwebTree:  77%|  | 5618/7317 [02:22<00:49, 34.08it/s]Training CobwebTree:  77%|  | 5622/7317 [02:22<00:52, 32.34it/s]Training CobwebTree:  77%|  | 5626/7317 [02:22<00:50, 33.22it/s]Training CobwebTree:  77%|  | 5630/7317 [02:22<00:50, 33.58it/s]Training CobwebTree:  77%|  | 5634/7317 [02:22<00:48, 34.68it/s]Training CobwebTree:  77%|  | 5638/7317 [02:22<00:49, 34.05it/s]Training CobwebTree:  77%|  | 5642/7317 [02:22<00:48, 34.82it/s]Training CobwebTree:  77%|  | 5646/7317 [02:23<00:47, 35.03it/s]Training CobwebTree:  77%|  | 5650/7317 [02:23<00:49, 33.91it/s]Training CobwebTree:  77%|  | 5654/7317 [02:23<00:52, 31.85it/s]Training CobwebTree:  77%|  | 5658/7317 [02:23<00:51, 32.43it/s]Training CobwebTree:  77%|  | 5662/7317 [02:23<00:48, 33.86it/s]Training CobwebTree:  77%|  | 5666/7317 [02:23<00:48, 33.90it/s]Training CobwebTree:  77%|  | 5670/7317 [02:23<00:49, 33.49it/s]Training CobwebTree:  78%|  | 5674/7317 [02:23<00:49, 33.39it/s]Training CobwebTree:  78%|  | 5678/7317 [02:24<00:48, 34.07it/s]Training CobwebTree:  78%|  | 5682/7317 [02:24<00:49, 32.88it/s]Training CobwebTree:  78%|  | 5686/7317 [02:24<00:48, 33.71it/s]Training CobwebTree:  78%|  | 5690/7317 [02:24<00:47, 34.58it/s]Training CobwebTree:  78%|  | 5694/7317 [02:24<00:47, 34.20it/s]Training CobwebTree:  78%|  | 5698/7317 [02:24<00:51, 31.39it/s]Training CobwebTree:  78%|  | 5702/7317 [02:24<00:51, 31.61it/s]Training CobwebTree:  78%|  | 5706/7317 [02:24<00:49, 32.86it/s]Training CobwebTree:  78%|  | 5710/7317 [02:24<00:48, 33.13it/s]Training CobwebTree:  78%|  | 5714/7317 [02:25<00:49, 32.66it/s]Training CobwebTree:  78%|  | 5718/7317 [02:25<00:46, 34.12it/s]Training CobwebTree:  78%|  | 5722/7317 [02:25<00:47, 33.72it/s]Training CobwebTree:  78%|  | 5726/7317 [02:25<00:46, 34.00it/s]Training CobwebTree:  78%|  | 5730/7317 [02:25<00:48, 33.00it/s]Training CobwebTree:  78%|  | 5734/7317 [02:25<00:49, 31.83it/s]Training CobwebTree:  78%|  | 5738/7317 [02:25<00:49, 32.10it/s]Training CobwebTree:  78%|  | 5742/7317 [02:25<00:49, 31.70it/s]Training CobwebTree:  79%|  | 5746/7317 [02:26<00:50, 31.17it/s]Training CobwebTree:  79%|  | 5750/7317 [02:26<00:47, 33.22it/s]Training CobwebTree:  79%|  | 5754/7317 [02:26<00:47, 32.69it/s]Training CobwebTree:  79%|  | 5758/7317 [02:26<00:48, 31.96it/s]Training CobwebTree:  79%|  | 5762/7317 [02:26<00:45, 33.91it/s]Training CobwebTree:  79%|  | 5766/7317 [02:26<00:46, 33.06it/s]Training CobwebTree:  79%|  | 5770/7317 [02:26<00:46, 33.36it/s]Training CobwebTree:  79%|  | 5774/7317 [02:26<00:46, 33.28it/s]Training CobwebTree:  79%|  | 5778/7317 [02:27<00:48, 31.58it/s]Training CobwebTree:  79%|  | 5782/7317 [02:27<00:47, 31.99it/s]Training CobwebTree:  79%|  | 5786/7317 [02:27<00:47, 32.23it/s]Training CobwebTree:  79%|  | 5790/7317 [02:27<00:46, 33.03it/s]Training CobwebTree:  79%|  | 5794/7317 [02:27<00:44, 34.20it/s]Training CobwebTree:  79%|  | 5798/7317 [02:27<00:44, 33.97it/s]Training CobwebTree:  79%|  | 5803/7317 [02:27<00:43, 34.84it/s]Training CobwebTree:  79%|  | 5807/7317 [02:27<00:43, 35.05it/s]Training CobwebTree:  79%|  | 5811/7317 [02:28<00:43, 34.49it/s]Training CobwebTree:  79%|  | 5815/7317 [02:28<00:42, 35.33it/s]Training CobwebTree:  80%|  | 5819/7317 [02:28<00:44, 33.51it/s]Training CobwebTree:  80%|  | 5823/7317 [02:28<00:44, 33.48it/s]Training CobwebTree:  80%|  | 5827/7317 [02:28<00:44, 33.39it/s]Training CobwebTree:  80%|  | 5831/7317 [02:28<00:43, 34.36it/s]Training CobwebTree:  80%|  | 5835/7317 [02:28<00:45, 32.39it/s]Training CobwebTree:  80%|  | 5839/7317 [02:28<00:46, 31.70it/s]Training CobwebTree:  80%|  | 5843/7317 [02:29<00:45, 32.39it/s]Training CobwebTree:  80%|  | 5847/7317 [02:29<00:46, 31.88it/s]Training CobwebTree:  80%|  | 5852/7317 [02:29<00:43, 33.76it/s]Training CobwebTree:  80%|  | 5856/7317 [02:29<00:45, 32.20it/s]Training CobwebTree:  80%|  | 5860/7317 [02:29<00:44, 32.56it/s]Training CobwebTree:  80%|  | 5864/7317 [02:29<00:43, 33.37it/s]Training CobwebTree:  80%|  | 5868/7317 [02:29<00:42, 34.02it/s]Training CobwebTree:  80%|  | 5872/7317 [02:29<00:43, 33.55it/s]Training CobwebTree:  80%|  | 5876/7317 [02:30<00:43, 33.00it/s]Training CobwebTree:  80%|  | 5880/7317 [02:30<00:42, 33.65it/s]Training CobwebTree:  80%|  | 5884/7317 [02:30<00:42, 33.36it/s]Training CobwebTree:  80%|  | 5888/7317 [02:30<00:44, 32.17it/s]Training CobwebTree:  81%|  | 5892/7317 [02:30<00:44, 31.67it/s]Training CobwebTree:  81%|  | 5896/7317 [02:30<00:44, 31.89it/s]Training CobwebTree:  81%|  | 5900/7317 [02:30<00:41, 33.86it/s]Training CobwebTree:  81%|  | 5904/7317 [02:30<00:42, 33.47it/s]Training CobwebTree:  81%|  | 5908/7317 [02:30<00:42, 32.85it/s]Training CobwebTree:  81%|  | 5912/7317 [02:31<00:41, 34.26it/s]Training CobwebTree:  81%|  | 5916/7317 [02:31<00:40, 34.25it/s]Training CobwebTree:  81%|  | 5920/7317 [02:31<00:40, 34.64it/s]Training CobwebTree:  81%|  | 5924/7317 [02:31<00:39, 35.24it/s]Training CobwebTree:  81%|  | 5928/7317 [02:31<00:40, 34.63it/s]Training CobwebTree:  81%|  | 5932/7317 [02:31<00:40, 34.39it/s]Training CobwebTree:  81%|  | 5936/7317 [02:31<00:41, 33.45it/s]Training CobwebTree:  81%|  | 5940/7317 [02:31<00:41, 33.13it/s]Training CobwebTree:  81%|  | 5944/7317 [02:32<00:39, 34.58it/s]Training CobwebTree:  81%| | 5948/7317 [02:32<00:41, 32.74it/s]Training CobwebTree:  81%| | 5952/7317 [02:32<00:42, 32.16it/s]Training CobwebTree:  81%| | 5956/7317 [02:32<00:43, 31.62it/s]Training CobwebTree:  81%| | 5960/7317 [02:32<00:42, 31.56it/s]Training CobwebTree:  82%| | 5964/7317 [02:32<00:41, 32.31it/s]Training CobwebTree:  82%| | 5968/7317 [02:32<00:41, 32.76it/s]Training CobwebTree:  82%| | 5972/7317 [02:32<00:40, 33.24it/s]Training CobwebTree:  82%| | 5976/7317 [02:32<00:38, 34.77it/s]Training CobwebTree:  82%| | 5980/7317 [02:33<00:38, 34.42it/s]Training CobwebTree:  82%| | 5984/7317 [02:33<00:38, 34.87it/s]Training CobwebTree:  82%| | 5988/7317 [02:33<00:40, 33.20it/s]Training CobwebTree:  82%| | 5992/7317 [02:33<00:40, 33.02it/s]Training CobwebTree:  82%| | 5996/7317 [02:33<00:42, 31.01it/s]Training CobwebTree:  82%| | 6000/7317 [02:33<00:43, 30.30it/s]Training CobwebTree:  82%| | 6004/7317 [02:33<00:42, 31.20it/s]Training CobwebTree:  82%| | 6008/7317 [02:34<00:41, 31.33it/s]Training CobwebTree:  82%| | 6012/7317 [02:34<00:43, 30.21it/s]Training CobwebTree:  82%| | 6016/7317 [02:34<00:41, 31.62it/s]Training CobwebTree:  82%| | 6021/7317 [02:34<00:36, 35.38it/s]Training CobwebTree:  82%| | 6025/7317 [02:34<00:35, 35.91it/s]Training CobwebTree:  82%| | 6029/7317 [02:34<00:38, 33.51it/s]Training CobwebTree:  82%| | 6033/7317 [02:34<00:39, 32.91it/s]Training CobwebTree:  83%| | 6037/7317 [02:34<00:39, 32.58it/s]Training CobwebTree:  83%| | 6042/7317 [02:35<00:37, 34.17it/s]Training CobwebTree:  83%| | 6046/7317 [02:35<00:37, 33.97it/s]Training CobwebTree:  83%| | 6050/7317 [02:35<00:38, 33.11it/s]Training CobwebTree:  83%| | 6054/7317 [02:35<00:37, 33.38it/s]Training CobwebTree:  83%| | 6058/7317 [02:35<00:37, 33.32it/s]Training CobwebTree:  83%| | 6062/7317 [02:35<00:36, 34.29it/s]Training CobwebTree:  83%| | 6066/7317 [02:35<00:36, 34.37it/s]Training CobwebTree:  83%| | 6070/7317 [02:35<00:35, 35.39it/s]Training CobwebTree:  83%| | 6074/7317 [02:35<00:35, 34.58it/s]Training CobwebTree:  83%| | 6078/7317 [02:36<00:35, 34.75it/s]Training CobwebTree:  83%| | 6082/7317 [02:36<00:34, 35.30it/s]Training CobwebTree:  83%| | 6087/7317 [02:36<00:33, 36.79it/s]Training CobwebTree:  83%| | 6091/7317 [02:36<00:32, 37.22it/s]Training CobwebTree:  83%| | 6095/7317 [02:36<00:33, 36.32it/s]Training CobwebTree:  83%| | 6099/7317 [02:36<00:33, 36.31it/s]Training CobwebTree:  83%| | 6103/7317 [02:36<00:33, 35.88it/s]Training CobwebTree:  83%| | 6107/7317 [02:36<00:34, 35.30it/s]Training CobwebTree:  84%| | 6111/7317 [02:36<00:34, 35.33it/s]Training CobwebTree:  84%| | 6115/7317 [02:37<00:35, 34.31it/s]Training CobwebTree:  84%| | 6119/7317 [02:37<00:35, 33.95it/s]Training CobwebTree:  84%| | 6123/7317 [02:37<00:34, 34.22it/s]Training CobwebTree:  84%| | 6127/7317 [02:37<00:34, 34.61it/s]Training CobwebTree:  84%| | 6131/7317 [02:37<00:33, 35.23it/s]Training CobwebTree:  84%| | 6135/7317 [02:37<00:33, 35.50it/s]Training CobwebTree:  84%| | 6139/7317 [02:37<00:32, 35.85it/s]Training CobwebTree:  84%| | 6143/7317 [02:37<00:32, 35.71it/s]Training CobwebTree:  84%| | 6147/7317 [02:38<00:34, 34.37it/s]Training CobwebTree:  84%| | 6151/7317 [02:38<00:32, 35.43it/s]Training CobwebTree:  84%| | 6155/7317 [02:38<00:33, 34.51it/s]Training CobwebTree:  84%| | 6159/7317 [02:38<00:36, 31.42it/s]Training CobwebTree:  84%| | 6163/7317 [02:38<00:37, 30.59it/s]Training CobwebTree:  84%| | 6167/7317 [02:38<00:35, 32.60it/s]Training CobwebTree:  84%| | 6171/7317 [02:38<00:33, 33.87it/s]Training CobwebTree:  84%| | 6175/7317 [02:38<00:33, 34.56it/s]Training CobwebTree:  84%| | 6180/7317 [02:38<00:31, 36.27it/s]Training CobwebTree:  85%| | 6184/7317 [02:39<00:32, 34.96it/s]Training CobwebTree:  85%| | 6188/7317 [02:39<00:32, 34.56it/s]Training CobwebTree:  85%| | 6192/7317 [02:39<00:31, 35.82it/s]Training CobwebTree:  85%| | 6196/7317 [02:39<00:30, 36.20it/s]Training CobwebTree:  85%| | 6200/7317 [02:39<00:31, 35.70it/s]Training CobwebTree:  85%| | 6204/7317 [02:39<00:30, 36.06it/s]Training CobwebTree:  85%| | 6209/7317 [02:39<00:30, 36.49it/s]Training CobwebTree:  85%| | 6213/7317 [02:39<00:30, 36.33it/s]Training CobwebTree:  85%| | 6217/7317 [02:40<00:30, 36.66it/s]Training CobwebTree:  85%| | 6221/7317 [02:40<00:30, 35.95it/s]Training CobwebTree:  85%| | 6225/7317 [02:40<00:30, 35.77it/s]Training CobwebTree:  85%| | 6229/7317 [02:40<00:30, 35.28it/s]Training CobwebTree:  85%| | 6233/7317 [02:40<00:32, 33.61it/s]Training CobwebTree:  85%| | 6237/7317 [02:40<00:32, 33.62it/s]Training CobwebTree:  85%| | 6242/7317 [02:40<00:30, 35.09it/s]Training CobwebTree:  85%| | 6246/7317 [02:40<00:30, 34.87it/s]Training CobwebTree:  85%| | 6250/7317 [02:40<00:30, 35.07it/s]Training CobwebTree:  85%| | 6254/7317 [02:41<00:31, 33.87it/s]Training CobwebTree:  86%| | 6258/7317 [02:41<00:30, 34.30it/s]Training CobwebTree:  86%| | 6262/7317 [02:41<00:30, 35.03it/s]Training CobwebTree:  86%| | 6266/7317 [02:41<00:30, 34.98it/s]Training CobwebTree:  86%| | 6270/7317 [02:41<00:29, 35.21it/s]Training CobwebTree:  86%| | 6274/7317 [02:41<00:29, 35.34it/s]Training CobwebTree:  86%| | 6278/7317 [02:41<00:29, 35.48it/s]Training CobwebTree:  86%| | 6282/7317 [02:41<00:28, 35.97it/s]Training CobwebTree:  86%| | 6286/7317 [02:41<00:28, 35.83it/s]Training CobwebTree:  86%| | 6290/7317 [02:42<00:28, 36.18it/s]Training CobwebTree:  86%| | 6294/7317 [02:42<00:27, 37.00it/s]Training CobwebTree:  86%| | 6298/7317 [02:42<00:27, 36.49it/s]Training CobwebTree:  86%| | 6302/7317 [02:42<00:27, 37.30it/s]Training CobwebTree:  86%| | 6306/7317 [02:42<00:28, 35.48it/s]Training CobwebTree:  86%| | 6310/7317 [02:42<00:29, 34.25it/s]Training CobwebTree:  86%| | 6314/7317 [02:42<00:30, 33.16it/s]Training CobwebTree:  86%| | 6318/7317 [02:42<00:30, 32.43it/s]Training CobwebTree:  86%| | 6322/7317 [02:43<00:29, 33.31it/s]Training CobwebTree:  86%| | 6326/7317 [02:43<00:29, 34.02it/s]Training CobwebTree:  87%| | 6330/7317 [02:43<00:29, 33.83it/s]Training CobwebTree:  87%| | 6334/7317 [02:43<00:28, 33.93it/s]Training CobwebTree:  87%| | 6338/7317 [02:43<00:27, 35.34it/s]Training CobwebTree:  87%| | 6342/7317 [02:43<00:27, 35.92it/s]Training CobwebTree:  87%| | 6346/7317 [02:43<00:27, 35.72it/s]Training CobwebTree:  87%| | 6350/7317 [02:43<00:27, 35.26it/s]Training CobwebTree:  87%| | 6354/7317 [02:43<00:27, 35.20it/s]Training CobwebTree:  87%| | 6358/7317 [02:44<00:26, 35.76it/s]Training CobwebTree:  87%| | 6362/7317 [02:44<00:26, 35.74it/s]Training CobwebTree:  87%| | 6366/7317 [02:44<00:26, 36.32it/s]Training CobwebTree:  87%| | 6370/7317 [02:44<00:26, 35.14it/s]Training CobwebTree:  87%| | 6374/7317 [02:44<00:27, 34.60it/s]Training CobwebTree:  87%| | 6378/7317 [02:44<00:28, 33.51it/s]Training CobwebTree:  87%| | 6382/7317 [02:44<00:27, 33.76it/s]Training CobwebTree:  87%| | 6386/7317 [02:44<00:27, 34.06it/s]Training CobwebTree:  87%| | 6390/7317 [02:44<00:26, 35.33it/s]Training CobwebTree:  87%| | 6394/7317 [02:45<00:26, 35.38it/s]Training CobwebTree:  87%| | 6398/7317 [02:45<00:25, 35.49it/s]Training CobwebTree:  87%| | 6402/7317 [02:45<00:25, 36.28it/s]Training CobwebTree:  88%| | 6406/7317 [02:45<00:25, 35.55it/s]Training CobwebTree:  88%| | 6410/7317 [02:45<00:24, 36.46it/s]Training CobwebTree:  88%| | 6414/7317 [02:45<00:24, 37.19it/s]Training CobwebTree:  88%| | 6418/7317 [02:45<00:23, 37.70it/s]Training CobwebTree:  88%| | 6422/7317 [02:45<00:25, 35.48it/s]Training CobwebTree:  88%| | 6426/7317 [02:45<00:26, 34.01it/s]Training CobwebTree:  88%| | 6431/7317 [02:46<00:24, 35.83it/s]Training CobwebTree:  88%| | 6435/7317 [02:46<00:24, 35.43it/s]Training CobwebTree:  88%| | 6439/7317 [02:46<00:24, 35.33it/s]Training CobwebTree:  88%| | 6443/7317 [02:46<00:24, 35.68it/s]Training CobwebTree:  88%| | 6447/7317 [02:46<00:25, 34.55it/s]Training CobwebTree:  88%| | 6451/7317 [02:46<00:24, 35.91it/s]Training CobwebTree:  88%| | 6455/7317 [02:46<00:24, 35.08it/s]Training CobwebTree:  88%| | 6459/7317 [02:46<00:24, 35.27it/s]Training CobwebTree:  88%| | 6463/7317 [02:47<00:24, 34.98it/s]Training CobwebTree:  88%| | 6467/7317 [02:47<00:26, 32.42it/s]Training CobwebTree:  88%| | 6471/7317 [02:47<00:25, 32.71it/s]Training CobwebTree:  89%| | 6476/7317 [02:47<00:24, 34.59it/s]Training CobwebTree:  89%| | 6480/7317 [02:47<00:23, 35.04it/s]Training CobwebTree:  89%| | 6484/7317 [02:47<00:24, 34.70it/s]Training CobwebTree:  89%| | 6488/7317 [02:47<00:23, 35.27it/s]Training CobwebTree:  89%| | 6492/7317 [02:47<00:23, 35.35it/s]Training CobwebTree:  89%| | 6496/7317 [02:47<00:23, 35.37it/s]Training CobwebTree:  89%| | 6500/7317 [02:48<00:23, 35.32it/s]Training CobwebTree:  89%| | 6504/7317 [02:48<00:22, 36.29it/s]Training CobwebTree:  89%| | 6508/7317 [02:48<00:22, 35.48it/s]Training CobwebTree:  89%| | 6512/7317 [02:48<00:22, 36.24it/s]Training CobwebTree:  89%| | 6516/7317 [02:48<00:22, 35.42it/s]Training CobwebTree:  89%| | 6520/7317 [02:48<00:23, 34.18it/s]Training CobwebTree:  89%| | 6524/7317 [02:48<00:24, 32.45it/s]Training CobwebTree:  89%| | 6528/7317 [02:48<00:24, 32.68it/s]Training CobwebTree:  89%| | 6532/7317 [02:49<00:23, 33.96it/s]Training CobwebTree:  89%| | 6536/7317 [02:49<00:23, 33.88it/s]Training CobwebTree:  89%| | 6540/7317 [02:49<00:22, 34.27it/s]Training CobwebTree:  89%| | 6544/7317 [02:49<00:22, 33.96it/s]Training CobwebTree:  89%| | 6548/7317 [02:49<00:22, 34.67it/s]Training CobwebTree:  90%| | 6552/7317 [02:49<00:22, 34.56it/s]Training CobwebTree:  90%| | 6556/7317 [02:49<00:21, 35.74it/s]Training CobwebTree:  90%| | 6560/7317 [02:49<00:23, 32.89it/s]Training CobwebTree:  90%| | 6564/7317 [02:49<00:22, 33.39it/s]Training CobwebTree:  90%| | 6568/7317 [02:50<00:21, 34.18it/s]Training CobwebTree:  90%| | 6572/7317 [02:50<00:22, 33.55it/s]Training CobwebTree:  90%| | 6576/7317 [02:50<00:21, 34.62it/s]Training CobwebTree:  90%| | 6580/7317 [02:50<00:20, 35.36it/s]Training CobwebTree:  90%| | 6584/7317 [02:50<00:21, 34.19it/s]Training CobwebTree:  90%| | 6589/7317 [02:50<00:20, 36.13it/s]Training CobwebTree:  90%| | 6593/7317 [02:50<00:20, 35.87it/s]Training CobwebTree:  90%| | 6597/7317 [02:50<00:20, 35.91it/s]Training CobwebTree:  90%| | 6601/7317 [02:51<00:20, 35.53it/s]Training CobwebTree:  90%| | 6605/7317 [02:51<00:20, 35.22it/s]Training CobwebTree:  90%| | 6609/7317 [02:51<00:19, 36.01it/s]Training CobwebTree:  90%| | 6614/7317 [02:51<00:18, 38.23it/s]Training CobwebTree:  90%| | 6618/7317 [02:51<00:18, 38.58it/s]Training CobwebTree:  91%| | 6622/7317 [02:51<00:18, 37.43it/s]Training CobwebTree:  91%| | 6626/7317 [02:51<00:19, 36.30it/s]Training CobwebTree:  91%| | 6630/7317 [02:51<00:18, 36.25it/s]Training CobwebTree:  91%| | 6634/7317 [02:51<00:20, 33.97it/s]Training CobwebTree:  91%| | 6638/7317 [02:52<00:20, 33.71it/s]Training CobwebTree:  91%| | 6642/7317 [02:52<00:19, 34.91it/s]Training CobwebTree:  91%| | 6647/7317 [02:52<00:18, 35.86it/s]Training CobwebTree:  91%| | 6651/7317 [02:52<00:18, 36.46it/s]Training CobwebTree:  91%| | 6655/7317 [02:52<00:19, 34.25it/s]Training CobwebTree:  91%| | 6659/7317 [02:52<00:19, 34.47it/s]Training CobwebTree:  91%| | 6663/7317 [02:52<00:19, 34.21it/s]Training CobwebTree:  91%| | 6667/7317 [02:52<00:18, 34.22it/s]Training CobwebTree:  91%| | 6671/7317 [02:53<00:19, 32.54it/s]Training CobwebTree:  91%| | 6675/7317 [02:53<00:20, 31.67it/s]Training CobwebTree:  91%|| 6679/7317 [02:53<00:20, 31.25it/s]Training CobwebTree:  91%|| 6683/7317 [02:53<00:20, 30.81it/s]Training CobwebTree:  91%|| 6687/7317 [02:53<00:19, 32.48it/s]Training CobwebTree:  91%|| 6691/7317 [02:53<00:18, 33.74it/s]Training CobwebTree:  91%|| 6695/7317 [02:53<00:18, 33.74it/s]Training CobwebTree:  92%|| 6699/7317 [02:53<00:18, 34.10it/s]Training CobwebTree:  92%|| 6703/7317 [02:53<00:18, 33.94it/s]Training CobwebTree:  92%|| 6707/7317 [02:54<00:17, 34.03it/s]Training CobwebTree:  92%|| 6711/7317 [02:54<00:18, 33.40it/s]Training CobwebTree:  92%|| 6715/7317 [02:54<00:17, 34.97it/s]Training CobwebTree:  92%|| 6719/7317 [02:54<00:17, 34.58it/s]Training CobwebTree:  92%|| 6723/7317 [02:54<00:17, 34.03it/s]Training CobwebTree:  92%|| 6727/7317 [02:54<00:16, 35.32it/s]Training CobwebTree:  92%|| 6731/7317 [02:54<00:17, 34.08it/s]Training CobwebTree:  92%|| 6735/7317 [02:54<00:17, 33.36it/s]Training CobwebTree:  92%|| 6739/7317 [02:55<00:17, 32.88it/s]Training CobwebTree:  92%|| 6743/7317 [02:55<00:16, 34.69it/s]Training CobwebTree:  92%|| 6747/7317 [02:55<00:16, 34.19it/s]Training CobwebTree:  92%|| 6751/7317 [02:55<00:16, 33.85it/s]Training CobwebTree:  92%|| 6755/7317 [02:55<00:16, 33.27it/s]Training CobwebTree:  92%|| 6759/7317 [02:55<00:17, 32.59it/s]Training CobwebTree:  92%|| 6763/7317 [02:55<00:16, 34.20it/s]Training CobwebTree:  92%|| 6767/7317 [02:55<00:15, 34.39it/s]Training CobwebTree:  93%|| 6771/7317 [02:55<00:16, 34.12it/s]Training CobwebTree:  93%|| 6776/7317 [02:56<00:15, 34.51it/s]Training CobwebTree:  93%|| 6780/7317 [02:56<00:15, 33.67it/s]Training CobwebTree:  93%|| 6785/7317 [02:56<00:14, 35.56it/s]Training CobwebTree:  93%|| 6789/7317 [02:56<00:14, 36.35it/s]Training CobwebTree:  93%|| 6793/7317 [02:56<00:14, 36.23it/s]Training CobwebTree:  93%|| 6797/7317 [02:56<00:14, 35.84it/s]Training CobwebTree:  93%|| 6801/7317 [02:56<00:15, 34.36it/s]Training CobwebTree:  93%|| 6805/7317 [02:56<00:15, 32.48it/s]Training CobwebTree:  93%|| 6809/7317 [02:57<00:15, 32.39it/s]Training CobwebTree:  93%|| 6813/7317 [02:57<00:15, 33.05it/s]Training CobwebTree:  93%|| 6817/7317 [02:57<00:14, 33.33it/s]Training CobwebTree:  93%|| 6821/7317 [02:57<00:14, 33.77it/s]Training CobwebTree:  93%|| 6825/7317 [02:57<00:14, 33.28it/s]Training CobwebTree:  93%|| 6830/7317 [02:57<00:14, 34.46it/s]Training CobwebTree:  93%|| 6834/7317 [02:57<00:13, 34.79it/s]Training CobwebTree:  93%|| 6838/7317 [02:57<00:13, 36.13it/s]Training CobwebTree:  94%|| 6842/7317 [02:58<00:13, 36.11it/s]Training CobwebTree:  94%|| 6846/7317 [02:58<00:13, 35.27it/s]Training CobwebTree:  94%|| 6850/7317 [02:58<00:12, 36.43it/s]Training CobwebTree:  94%|| 6854/7317 [02:58<00:13, 34.58it/s]Training CobwebTree:  94%|| 6858/7317 [02:58<00:13, 34.27it/s]Training CobwebTree:  94%|| 6862/7317 [02:58<00:13, 34.17it/s]Training CobwebTree:  94%|| 6866/7317 [02:58<00:13, 34.28it/s]Training CobwebTree:  94%|| 6870/7317 [02:58<00:13, 33.92it/s]Training CobwebTree:  94%|| 6874/7317 [02:58<00:12, 34.25it/s]Training CobwebTree:  94%|| 6878/7317 [02:59<00:12, 34.74it/s]Training CobwebTree:  94%|| 6882/7317 [02:59<00:12, 34.07it/s]Training CobwebTree:  94%|| 6886/7317 [02:59<00:12, 33.81it/s]Training CobwebTree:  94%|| 6890/7317 [02:59<00:12, 34.97it/s]Training CobwebTree:  94%|| 6894/7317 [02:59<00:12, 34.60it/s]Training CobwebTree:  94%|| 6899/7317 [02:59<00:11, 36.18it/s]Training CobwebTree:  94%|| 6904/7317 [02:59<00:11, 37.30it/s]Training CobwebTree:  94%|| 6908/7317 [02:59<00:11, 36.50it/s]Training CobwebTree:  94%|| 6912/7317 [03:00<00:11, 36.77it/s]Training CobwebTree:  95%|| 6916/7317 [03:00<00:10, 36.49it/s]Training CobwebTree:  95%|| 6920/7317 [03:00<00:10, 36.77it/s]Training CobwebTree:  95%|| 6924/7317 [03:00<00:10, 35.89it/s]Training CobwebTree:  95%|| 6928/7317 [03:00<00:10, 35.89it/s]Training CobwebTree:  95%|| 6932/7317 [03:00<00:11, 34.13it/s]Training CobwebTree:  95%|| 6936/7317 [03:00<00:10, 35.05it/s]Training CobwebTree:  95%|| 6940/7317 [03:00<00:10, 35.98it/s]Training CobwebTree:  95%|| 6944/7317 [03:00<00:10, 35.87it/s]Training CobwebTree:  95%|| 6948/7317 [03:01<00:10, 34.13it/s]Training CobwebTree:  95%|| 6952/7317 [03:01<00:10, 33.44it/s]Training CobwebTree:  95%|| 6956/7317 [03:01<00:11, 32.74it/s]Training CobwebTree:  95%|| 6960/7317 [03:01<00:10, 32.80it/s]Training CobwebTree:  95%|| 6964/7317 [03:01<00:10, 32.71it/s]Training CobwebTree:  95%|| 6968/7317 [03:01<00:10, 33.10it/s]Training CobwebTree:  95%|| 6972/7317 [03:01<00:10, 32.83it/s]Training CobwebTree:  95%|| 6976/7317 [03:01<00:10, 32.19it/s]Training CobwebTree:  95%|| 6980/7317 [03:02<00:10, 32.12it/s]Training CobwebTree:  95%|| 6984/7317 [03:02<00:10, 32.07it/s]Training CobwebTree:  96%|| 6988/7317 [03:02<00:10, 31.77it/s]Training CobwebTree:  96%|| 6992/7317 [03:02<00:10, 32.11it/s]Training CobwebTree:  96%|| 6996/7317 [03:02<00:10, 32.09it/s]Training CobwebTree:  96%|| 7000/7317 [03:02<00:09, 33.26it/s]Training CobwebTree:  96%|| 7004/7317 [03:02<00:09, 33.44it/s]Training CobwebTree:  96%|| 7008/7317 [03:02<00:09, 34.22it/s]Training CobwebTree:  96%|| 7012/7317 [03:02<00:08, 34.94it/s]Training CobwebTree:  96%|| 7016/7317 [03:03<00:08, 35.34it/s]Training CobwebTree:  96%|| 7020/7317 [03:03<00:08, 33.10it/s]Training CobwebTree:  96%|| 7024/7317 [03:03<00:08, 33.00it/s]Training CobwebTree:  96%|| 7028/7317 [03:03<00:08, 33.57it/s]Training CobwebTree:  96%|| 7032/7317 [03:03<00:08, 33.28it/s]Training CobwebTree:  96%|| 7036/7317 [03:03<00:08, 31.93it/s]Training CobwebTree:  96%|| 7040/7317 [03:03<00:08, 31.34it/s]Training CobwebTree:  96%|| 7044/7317 [03:04<00:08, 30.78it/s]Training CobwebTree:  96%|| 7048/7317 [03:04<00:08, 31.73it/s]Training CobwebTree:  96%|| 7052/7317 [03:04<00:07, 33.77it/s]Training CobwebTree:  96%|| 7057/7317 [03:04<00:07, 34.96it/s]Training CobwebTree:  97%|| 7061/7317 [03:04<00:07, 32.95it/s]Training CobwebTree:  97%|| 7065/7317 [03:04<00:07, 33.59it/s]Training CobwebTree:  97%|| 7069/7317 [03:04<00:07, 32.58it/s]Training CobwebTree:  97%|| 7073/7317 [03:04<00:07, 33.68it/s]Training CobwebTree:  97%|| 7077/7317 [03:04<00:07, 33.28it/s]Training CobwebTree:  97%|| 7081/7317 [03:05<00:07, 32.23it/s]Training CobwebTree:  97%|| 7085/7317 [03:05<00:06, 33.73it/s]Training CobwebTree:  97%|| 7089/7317 [03:05<00:06, 34.56it/s]Training CobwebTree:  97%|| 7093/7317 [03:05<00:06, 34.91it/s]Training CobwebTree:  97%|| 7097/7317 [03:05<00:06, 34.19it/s]Training CobwebTree:  97%|| 7101/7317 [03:05<00:06, 35.04it/s]Training CobwebTree:  97%|| 7105/7317 [03:05<00:05, 35.76it/s]Training CobwebTree:  97%|| 7109/7317 [03:05<00:05, 35.68it/s]Training CobwebTree:  97%|| 7113/7317 [03:05<00:05, 35.03it/s]Training CobwebTree:  97%|| 7117/7317 [03:06<00:05, 35.88it/s]Training CobwebTree:  97%|| 7121/7317 [03:06<00:05, 33.90it/s]Training CobwebTree:  97%|| 7125/7317 [03:06<00:05, 32.04it/s]Training CobwebTree:  97%|| 7129/7317 [03:06<00:05, 32.83it/s]Training CobwebTree:  97%|| 7133/7317 [03:06<00:05, 32.10it/s]Training CobwebTree:  98%|| 7137/7317 [03:06<00:05, 32.27it/s]Training CobwebTree:  98%|| 7141/7317 [03:06<00:05, 33.67it/s]Training CobwebTree:  98%|| 7145/7317 [03:06<00:04, 35.13it/s]Training CobwebTree:  98%|| 7149/7317 [03:07<00:04, 35.31it/s]Training CobwebTree:  98%|| 7153/7317 [03:07<00:04, 35.50it/s]Training CobwebTree:  98%|| 7157/7317 [03:07<00:04, 34.18it/s]Training CobwebTree:  98%|| 7161/7317 [03:07<00:04, 35.14it/s]Training CobwebTree:  98%|| 7165/7317 [03:07<00:04, 35.51it/s]Training CobwebTree:  98%|| 7169/7317 [03:07<00:04, 35.97it/s]Training CobwebTree:  98%|| 7173/7317 [03:07<00:04, 35.63it/s]Training CobwebTree:  98%|| 7177/7317 [03:07<00:03, 36.11it/s]Training CobwebTree:  98%|| 7181/7317 [03:07<00:04, 33.95it/s]Training CobwebTree:  98%|| 7186/7317 [03:08<00:03, 36.41it/s]Training CobwebTree:  98%|| 7190/7317 [03:08<00:03, 36.50it/s]Training CobwebTree:  98%|| 7194/7317 [03:08<00:03, 35.79it/s]Training CobwebTree:  98%|| 7198/7317 [03:08<00:03, 35.88it/s]Training CobwebTree:  98%|| 7203/7317 [03:08<00:03, 37.37it/s]Training CobwebTree:  98%|| 7207/7317 [03:08<00:03, 35.32it/s]Training CobwebTree:  99%|| 7211/7317 [03:08<00:02, 35.48it/s]Training CobwebTree:  99%|| 7215/7317 [03:08<00:02, 36.30it/s]Training CobwebTree:  99%|| 7219/7317 [03:09<00:02, 34.80it/s]Training CobwebTree:  99%|| 7223/7317 [03:09<00:02, 33.75it/s]Training CobwebTree:  99%|| 7227/7317 [03:09<00:02, 34.73it/s]Training CobwebTree:  99%|| 7231/7317 [03:09<00:02, 34.60it/s]Training CobwebTree:  99%|| 7235/7317 [03:09<00:02, 35.31it/s]Training CobwebTree:  99%|| 7239/7317 [03:09<00:02, 34.52it/s]Training CobwebTree:  99%|| 7243/7317 [03:09<00:02, 33.70it/s]Training CobwebTree:  99%|| 7247/7317 [03:09<00:02, 34.21it/s]Training CobwebTree:  99%|| 7251/7317 [03:09<00:01, 34.61it/s]Training CobwebTree:  99%|| 7255/7317 [03:10<00:01, 34.53it/s]Training CobwebTree:  99%|| 7259/7317 [03:10<00:01, 33.99it/s]Training CobwebTree:  99%|| 7263/7317 [03:10<00:01, 33.70it/s]Training CobwebTree:  99%|| 7267/7317 [03:10<00:01, 34.49it/s]Training CobwebTree:  99%|| 7271/7317 [03:10<00:01, 33.63it/s]Training CobwebTree:  99%|| 7275/7317 [03:10<00:01, 34.41it/s]Training CobwebTree:  99%|| 7279/7317 [03:10<00:01, 33.38it/s]Training CobwebTree: 100%|| 7283/7317 [03:10<00:01, 33.34it/s]Training CobwebTree: 100%|| 7287/7317 [03:11<00:00, 33.01it/s]Training CobwebTree: 100%|| 7291/7317 [03:11<00:00, 32.21it/s]Training CobwebTree: 100%|| 7295/7317 [03:11<00:00, 33.47it/s]Training CobwebTree: 100%|| 7299/7317 [03:11<00:00, 34.48it/s]Training CobwebTree: 100%|| 7303/7317 [03:11<00:00, 35.64it/s]Training CobwebTree: 100%|| 7307/7317 [03:11<00:00, 34.58it/s]Training CobwebTree: 100%|| 7311/7317 [03:11<00:00, 34.03it/s]Training CobwebTree: 100%|| 7315/7317 [03:11<00:00, 34.74it/s]Training CobwebTree: 100%|| 7317/7317 [03:11<00:00, 38.13it/s]
2025-12-21 13:13:06,928 INFO __main__: Hierarchical Model 0 (HDBSCAN) metrics: {'hier_coherence_npmi': 0.1751731256600117, 'hier_topic_uniqueness': 0.9, 'hier_topic_diversity': 0.9, 'hier_topic_specialization': 0.12080792324862649, 'hier_affinity_child': 0.8044986128807068, 'hier_affinity_non_child': 0.0, 'hier_coherence_clnpmi': 0.04493474961960795}
2025-12-21 13:13:06,928 INFO __main__: Hierarchical Model 1 (KMeans) metrics: {'hier_coherence_npmi': 0.19898695340124586, 'hier_topic_uniqueness': 0.8036587957497048, 'hier_topic_diversity': 0.8036587957497048, 'hier_topic_specialization': 0.2217128136054985, 'hier_affinity_child': 0.8017793893814087, 'hier_affinity_non_child': 0.35230135917663574, 'hier_coherence_clnpmi': 0.13751396655420006}
2025-12-21 13:13:06,928 INFO __main__: Hierarchical Model 2 (BERTopicCobwebWrapper) metrics: {'hier_coherence_npmi': 0.1990393391996834, 'hier_topic_uniqueness': 0.7278293928293929, 'hier_topic_diversity': 0.7278293928293929, 'hier_topic_specialization': 0.24104858732057063, 'hier_affinity_child': 0.8061278462409973, 'hier_affinity_non_child': 0.3296597898006439, 'hier_coherence_clnpmi': 0.12778231049912994}
