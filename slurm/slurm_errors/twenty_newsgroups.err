2025-12-24 09:06:13,369 INFO __main__: Starting benchmark for dataset=20newsgroups
2025-12-24 09:06:14,522 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-24 09:06:14,971 INFO gensim.corpora.dictionary: built Dictionary<92350 unique tokens: ['60s', '70s', 'addition', 'body', 'bricklin']...> from 9738 documents (total 1099997 corpus positions)
2025-12-24 09:06:14,974 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<92350 unique tokens: ['60s', '70s', 'addition', 'body', 'bricklin']...> from 9738 documents (total 1099997 corpus positions)", 'datetime': '2025-12-24T09:06:14.971433', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-161-generic-x86_64-with-glibc2.35', 'event': 'created'}
2025-12-24 09:06:15,456 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-24 09:06:15,456 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-24 09:06:17,665 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 9738 docs
/nethome/ksingara3/flash/miniconda3/envs/rag-cobweb/lib/python3.11/site-packages/numba/np/ufunc/parallel.py:373: NumbaWarning:

The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.

2025-12-24 09:08:41,052 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=47, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-24 09:08:43,027 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (7232 virtual)
2025-12-24 09:08:43,032 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (3127 virtual)
2025-12-24 09:08:43,036 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (1323 virtual)
2025-12-24 09:08:43,038 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (713 virtual)
2025-12-24 09:08:43,041 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-5372 virtual)
2025-12-24 09:08:43,044 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (-3523 virtual)
2025-12-24 09:08:43,048 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (12146 virtual)
2025-12-24 09:08:43,050 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (22148 virtual)
2025-12-24 09:08:43,052 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (19719 virtual)
2025-12-24 09:08:43,059 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (5184 virtual)
2025-12-24 09:08:43,061 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (6705 virtual)
2025-12-24 09:08:43,069 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (-5029 virtual)
2025-12-24 09:08:43,073 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (-3476 virtual)
2025-12-24 09:08:43,074 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (-2950 virtual)
2025-12-24 09:08:43,093 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (2109 virtual)
2025-12-24 09:08:43,107 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (2483 virtual)
2025-12-24 09:08:43,108 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (6024 virtual)
2025-12-24 09:08:43,115 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (8719 virtual)
2025-12-24 09:08:43,135 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (10782 virtual)
2025-12-24 09:08:43,161 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (12662 virtual)
2025-12-24 09:08:43,165 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (14417 virtual)
2025-12-24 09:08:43,175 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (16318 virtual)
2025-12-24 09:08:43,235 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (21460 virtual)
2025-12-24 09:08:43,257 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (36852 virtual)
2025-12-24 09:08:43,265 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (39766 virtual)
2025-12-24 09:08:43,300 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (41441 virtual)
2025-12-24 09:08:43,306 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (51322 virtual)
2025-12-24 09:08:43,361 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (52687 virtual)
2025-12-24 09:08:43,362 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (52913 virtual)
2025-12-24 09:08:43,385 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (55693 virtual)
2025-12-24 09:08:43,398 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (52992 virtual)
2025-12-24 09:08:43,430 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (50615 virtual)
2025-12-24 09:08:43,547 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (45525 virtual)
2025-12-24 09:08:43,558 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (45324 virtual)
2025-12-24 09:08:43,590 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (42113 virtual)
2025-12-24 09:08:43,621 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (43690 virtual)
2025-12-24 09:08:43,657 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (42608 virtual)
2025-12-24 09:08:43,716 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (39072 virtual)
2025-12-24 09:08:43,725 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (40044 virtual)
2025-12-24 09:08:43,760 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (41542 virtual)
2025-12-24 09:08:43,776 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (45793 virtual)
2025-12-24 09:08:43,796 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (47498 virtual)
2025-12-24 09:08:43,853 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (42550 virtual)
2025-12-24 09:08:43,979 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (37489 virtual)
2025-12-24 09:08:44,051 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (39918 virtual)
2025-12-24 09:08:44,096 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (31027 virtual)
2025-12-24 09:08:44,139 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (40008 virtual)
2025-12-24 09:08:44,301 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (41502 virtual)
2025-12-24 09:08:44,319 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (46622 virtual)
2025-12-24 09:08:44,339 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (48729 virtual)
2025-12-24 09:08:44,401 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (44346 virtual)
2025-12-24 09:08:44,477 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (48297 virtual)
2025-12-24 09:08:45,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:45,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:45,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,920 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:45,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:45,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:45,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:45,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:45,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:45,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,000 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,010 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,035 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,040 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,044 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,129 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,153 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,364 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:46,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:46,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:47,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:47,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,139 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:47,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,322 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:47,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:47,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,762 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:47,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:47,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:47,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:48,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:48,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:48,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:48,093 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:48,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:48,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:48,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:08:48,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:08:50,821 INFO gensim.topic_coherence.text_analysis: 47 accumulators retrieved from output queue
2025-12-24 09:08:51,369 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 604327 virtual documents
2025-12-24 09:08:57,459 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=47, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-24 09:08:59,605 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5593 virtual)
2025-12-24 09:08:59,608 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (10727 virtual)
2025-12-24 09:08:59,612 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (26432 virtual)
2025-12-24 09:08:59,613 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (30159 virtual)
2025-12-24 09:08:59,613 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (32725 virtual)
2025-12-24 09:08:59,614 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (36278 virtual)
2025-12-24 09:08:59,615 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (39220 virtual)
2025-12-24 09:08:59,617 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (54327 virtual)
2025-12-24 09:08:59,618 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (60577 virtual)
2025-12-24 09:08:59,619 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (63971 virtual)
2025-12-24 09:08:59,620 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (68747 virtual)
2025-12-24 09:08:59,621 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (78123 virtual)
2025-12-24 09:08:59,622 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (82521 virtual)
2025-12-24 09:08:59,624 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (90313 virtual)
2025-12-24 09:08:59,624 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (95725 virtual)
2025-12-24 09:08:59,625 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (100032 virtual)
2025-12-24 09:08:59,626 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (103002 virtual)
2025-12-24 09:08:59,627 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (109828 virtual)
2025-12-24 09:08:59,628 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (115400 virtual)
2025-12-24 09:08:59,630 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (124477 virtual)
2025-12-24 09:08:59,630 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (128588 virtual)
2025-12-24 09:08:59,631 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (131939 virtual)
2025-12-24 09:08:59,634 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (159346 virtual)
2025-12-24 09:08:59,636 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (175748 virtual)
2025-12-24 09:08:59,637 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (179483 virtual)
2025-12-24 09:08:59,638 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (186119 virtual)
2025-12-24 09:08:59,639 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (189134 virtual)
2025-12-24 09:08:59,640 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (194011 virtual)
2025-12-24 09:08:59,641 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (198338 virtual)
2025-12-24 09:08:59,641 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (201418 virtual)
2025-12-24 09:08:59,642 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (205336 virtual)
2025-12-24 09:08:59,643 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (210865 virtual)
2025-12-24 09:08:59,644 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (215487 virtual)
2025-12-24 09:08:59,645 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (222784 virtual)
2025-12-24 09:08:59,646 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (227542 virtual)
2025-12-24 09:08:59,647 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (237105 virtual)
2025-12-24 09:08:59,648 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (242098 virtual)
2025-12-24 09:08:59,649 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (246462 virtual)
2025-12-24 09:08:59,649 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (250096 virtual)
2025-12-24 09:08:59,650 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (255323 virtual)
2025-12-24 09:08:59,651 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (258628 virtual)
2025-12-24 09:08:59,651 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (263349 virtual)
2025-12-24 09:08:59,652 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (268233 virtual)
2025-12-24 09:08:59,653 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (274132 virtual)
2025-12-24 09:08:59,655 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (282971 virtual)
2025-12-24 09:08:59,655 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (286673 virtual)
2025-12-24 09:08:59,656 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (290264 virtual)
2025-12-24 09:08:59,657 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (296037 virtual)
2025-12-24 09:08:59,659 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (310124 virtual)
2025-12-24 09:08:59,661 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (317050 virtual)
2025-12-24 09:08:59,662 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (320598 virtual)
2025-12-24 09:08:59,664 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (334909 virtual)
2025-12-24 09:08:59,683 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (341683 virtual)
2025-12-24 09:08:59,685 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (351624 virtual)
2025-12-24 09:08:59,687 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (360719 virtual)
2025-12-24 09:08:59,688 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (365651 virtual)
2025-12-24 09:08:59,695 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (375582 virtual)
2025-12-24 09:08:59,696 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (380043 virtual)
2025-12-24 09:08:59,697 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (390262 virtual)
2025-12-24 09:08:59,703 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (394060 virtual)
2025-12-24 09:08:59,711 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (398804 virtual)
2025-12-24 09:08:59,712 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (402970 virtual)
2025-12-24 09:08:59,719 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (417617 virtual)
2025-12-24 09:08:59,720 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (425918 virtual)
2025-12-24 09:08:59,722 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (437460 virtual)
2025-12-24 09:08:59,723 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (441365 virtual)
2025-12-24 09:08:59,739 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (446589 virtual)
2025-12-24 09:08:59,740 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (450244 virtual)
2025-12-24 09:08:59,760 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (478452 virtual)
2025-12-24 09:08:59,761 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (482978 virtual)
2025-12-24 09:08:59,763 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (494166 virtual)
2025-12-24 09:08:59,772 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (502241 virtual)
2025-12-24 09:08:59,784 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (518522 virtual)
2025-12-24 09:08:59,785 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (526287 virtual)
2025-12-24 09:08:59,786 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (532913 virtual)
2025-12-24 09:08:59,787 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (536619 virtual)
2025-12-24 09:08:59,788 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (548493 virtual)
2025-12-24 09:08:59,812 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (551958 virtual)
2025-12-24 09:08:59,812 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (555379 virtual)
2025-12-24 09:08:59,814 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (564992 virtual)
2025-12-24 09:08:59,814 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (568990 virtual)
2025-12-24 09:08:59,815 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (574796 virtual)
2025-12-24 09:08:59,816 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (581815 virtual)
2025-12-24 09:08:59,823 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (585873 virtual)
2025-12-24 09:08:59,825 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (591771 virtual)
2025-12-24 09:08:59,827 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (597046 virtual)
2025-12-24 09:08:59,828 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (601849 virtual)
2025-12-24 09:08:59,831 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (608725 virtual)
2025-12-24 09:08:59,832 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (615021 virtual)
2025-12-24 09:08:59,833 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (621265 virtual)
2025-12-24 09:08:59,835 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (627724 virtual)
2025-12-24 09:08:59,836 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (631679 virtual)
2025-12-24 09:08:59,837 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (635736 virtual)
2025-12-24 09:08:59,843 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (643713 virtual)
2025-12-24 09:08:59,844 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (649874 virtual)
2025-12-24 09:08:59,912 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (658090 virtual)
2025-12-24 09:08:59,928 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (661695 virtual)
2025-12-24 09:08:59,932 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (669808 virtual)
2025-12-24 09:08:59,943 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (673686 virtual)
2025-12-24 09:08:59,948 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (678673 virtual)
2025-12-24 09:09:00,024 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (683677 virtual)
2025-12-24 09:09:00,058 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (687618 virtual)
2025-12-24 09:09:00,073 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (698272 virtual)
2025-12-24 09:09:00,081 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (703739 virtual)
2025-12-24 09:09:00,094 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (712044 virtual)
2025-12-24 09:09:00,128 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (719942 virtual)
2025-12-24 09:09:00,148 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (730593 virtual)
2025-12-24 09:09:00,158 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (738698 virtual)
2025-12-24 09:09:00,160 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (744918 virtual)
2025-12-24 09:09:00,166 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (748587 virtual)
2025-12-24 09:09:00,174 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (754283 virtual)
2025-12-24 09:09:00,178 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (759411 virtual)
2025-12-24 09:09:00,184 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (763074 virtual)
2025-12-24 09:09:00,205 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (769146 virtual)
2025-12-24 09:09:00,209 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (778550 virtual)
2025-12-24 09:09:00,213 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (782687 virtual)
2025-12-24 09:09:00,243 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (787370 virtual)
2025-12-24 09:09:00,250 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (792747 virtual)
2025-12-24 09:09:00,267 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (795834 virtual)
2025-12-24 09:09:00,277 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (799472 virtual)
2025-12-24 09:09:00,283 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (804251 virtual)
2025-12-24 09:09:00,291 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (818289 virtual)
2025-12-24 09:09:00,296 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (823742 virtual)
2025-12-24 09:09:00,329 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (829121 virtual)
2025-12-24 09:09:00,331 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (839918 virtual)
2025-12-24 09:09:00,345 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (843357 virtual)
2025-12-24 09:09:00,347 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (847194 virtual)
2025-12-24 09:09:00,382 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (850480 virtual)
2025-12-24 09:09:00,389 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (854672 virtual)
2025-12-24 09:09:00,400 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (863027 virtual)
2025-12-24 09:09:00,408 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (865772 virtual)
2025-12-24 09:09:00,418 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (884808 virtual)
2025-12-24 09:09:00,420 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (889004 virtual)
2025-12-24 09:09:00,458 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (894985 virtual)
2025-12-24 09:09:00,466 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (898703 virtual)
2025-12-24 09:09:00,523 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (901982 virtual)
2025-12-24 09:09:00,565 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (906587 virtual)
2025-12-24 09:09:00,571 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (910463 virtual)
2025-12-24 09:09:00,585 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (931102 virtual)
2025-12-24 09:09:00,602 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (942622 virtual)
2025-12-24 09:09:00,613 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (951129 virtual)
2025-12-24 09:09:00,627 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (955276 virtual)
2025-12-24 09:09:00,631 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (959539 virtual)
2025-12-24 09:09:00,648 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (965946 virtual)
2025-12-24 09:09:00,656 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (970853 virtual)
2025-12-24 09:09:00,658 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (973829 virtual)
2025-12-24 09:09:00,690 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (989097 virtual)
2025-12-24 09:09:00,716 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (994234 virtual)
2025-12-24 09:09:00,743 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (997762 virtual)
2025-12-24 09:09:00,747 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (1000793 virtual)
2025-12-24 09:09:00,748 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (1006413 virtual)
2025-12-24 09:09:00,767 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (1011923 virtual)
2025-12-24 09:09:00,771 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (1012355 virtual)
2025-12-24 09:09:01,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,496 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,525 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,525 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,573 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,594 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,678 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,691 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,694 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,705 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,793 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,794 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:01,950 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:01,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,029 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,050 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,101 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,908 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:02,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:02,954 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:03,103 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:03,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:03,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:09:03,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:09:05,762 INFO gensim.topic_coherence.text_analysis: 47 accumulators retrieved from output queue
2025-12-24 09:09:06,355 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1015742 virtual documents
2025-12-24 09:09:07,379 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 9738 docs
2025-12-24 09:10:53,886 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=47, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-24 09:10:55,388 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (7232 virtual)
2025-12-24 09:10:55,393 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (3127 virtual)
2025-12-24 09:10:55,397 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (1323 virtual)
2025-12-24 09:10:55,399 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (713 virtual)
2025-12-24 09:10:55,402 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-5372 virtual)
2025-12-24 09:10:55,404 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (-3523 virtual)
2025-12-24 09:10:55,409 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (12146 virtual)
2025-12-24 09:10:55,412 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (22148 virtual)
2025-12-24 09:10:55,413 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (19719 virtual)
2025-12-24 09:10:55,420 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (5184 virtual)
2025-12-24 09:10:55,422 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (6705 virtual)
2025-12-24 09:10:55,430 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (-5029 virtual)
2025-12-24 09:10:55,434 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (-3476 virtual)
2025-12-24 09:10:55,435 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (-2950 virtual)
2025-12-24 09:10:55,438 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (2109 virtual)
2025-12-24 09:10:55,443 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (2483 virtual)
2025-12-24 09:10:55,444 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (6024 virtual)
2025-12-24 09:10:55,455 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (8719 virtual)
2025-12-24 09:10:55,457 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (10782 virtual)
2025-12-24 09:10:55,471 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (12662 virtual)
2025-12-24 09:10:55,506 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (14417 virtual)
2025-12-24 09:10:55,515 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (16318 virtual)
2025-12-24 09:10:55,517 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (21460 virtual)
2025-12-24 09:10:55,540 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (36852 virtual)
2025-12-24 09:10:55,542 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (39766 virtual)
2025-12-24 09:10:55,543 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (41441 virtual)
2025-12-24 09:10:55,546 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (51322 virtual)
2025-12-24 09:10:55,552 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (52687 virtual)
2025-12-24 09:10:55,554 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (52913 virtual)
2025-12-24 09:10:55,567 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (55693 virtual)
2025-12-24 09:10:55,569 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (52992 virtual)
2025-12-24 09:10:55,692 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (50615 virtual)
2025-12-24 09:10:55,747 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (45525 virtual)
2025-12-24 09:10:55,750 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (45324 virtual)
2025-12-24 09:10:55,785 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (42113 virtual)
2025-12-24 09:10:55,813 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (43690 virtual)
2025-12-24 09:10:55,833 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (42608 virtual)
2025-12-24 09:10:55,900 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (39072 virtual)
2025-12-24 09:10:55,902 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (40044 virtual)
2025-12-24 09:10:55,924 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (41542 virtual)
2025-12-24 09:10:55,944 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (45793 virtual)
2025-12-24 09:10:55,968 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (47498 virtual)
2025-12-24 09:10:56,013 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (42550 virtual)
2025-12-24 09:10:56,079 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (37489 virtual)
2025-12-24 09:10:56,095 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (39918 virtual)
2025-12-24 09:10:56,142 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (31027 virtual)
2025-12-24 09:10:56,178 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (40008 virtual)
2025-12-24 09:10:56,275 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (41502 virtual)
2025-12-24 09:10:56,309 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (46622 virtual)
2025-12-24 09:10:56,324 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (48729 virtual)
2025-12-24 09:10:56,345 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (44346 virtual)
2025-12-24 09:10:56,378 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (48297 virtual)
2025-12-24 09:10:56,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,462 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,473 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,501 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,521 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,521 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,521 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,534 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,534 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,548 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,587 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,598 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,770 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,790 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,851 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,858 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,914 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:56,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:56,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:57,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:57,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:57,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:57,384 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:57,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:57,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:57,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:57,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:57,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:57,672 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:57,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:57,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:10:57,845 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:10:57,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:00,099 INFO gensim.topic_coherence.text_analysis: 47 accumulators retrieved from output queue
2025-12-24 09:11:00,125 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 604327 virtual documents
2025-12-24 09:11:00,466 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=47, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-24 09:11:02,001 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5593 virtual)
2025-12-24 09:11:02,004 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (10727 virtual)
2025-12-24 09:11:02,006 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (26432 virtual)
2025-12-24 09:11:02,007 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (30159 virtual)
2025-12-24 09:11:02,008 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (32725 virtual)
2025-12-24 09:11:02,008 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (36278 virtual)
2025-12-24 09:11:02,009 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (39220 virtual)
2025-12-24 09:11:02,011 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (54327 virtual)
2025-12-24 09:11:02,012 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (60577 virtual)
2025-12-24 09:11:02,013 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (63971 virtual)
2025-12-24 09:11:02,013 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (68747 virtual)
2025-12-24 09:11:02,015 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (78123 virtual)
2025-12-24 09:11:02,015 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (82521 virtual)
2025-12-24 09:11:02,017 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (90313 virtual)
2025-12-24 09:11:02,017 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (95725 virtual)
2025-12-24 09:11:02,018 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (100032 virtual)
2025-12-24 09:11:02,019 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (103002 virtual)
2025-12-24 09:11:02,020 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (109828 virtual)
2025-12-24 09:11:02,020 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (115400 virtual)
2025-12-24 09:11:02,022 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (124477 virtual)
2025-12-24 09:11:02,022 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (128588 virtual)
2025-12-24 09:11:02,023 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (131939 virtual)
2025-12-24 09:11:02,026 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (159346 virtual)
2025-12-24 09:11:02,028 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (175748 virtual)
2025-12-24 09:11:02,029 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (179483 virtual)
2025-12-24 09:11:02,030 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (186119 virtual)
2025-12-24 09:11:02,030 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (189134 virtual)
2025-12-24 09:11:02,031 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (194011 virtual)
2025-12-24 09:11:02,032 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (198338 virtual)
2025-12-24 09:11:02,032 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (201418 virtual)
2025-12-24 09:11:02,033 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (205336 virtual)
2025-12-24 09:11:02,034 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (210865 virtual)
2025-12-24 09:11:02,034 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (215487 virtual)
2025-12-24 09:11:02,035 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (222784 virtual)
2025-12-24 09:11:02,036 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (227542 virtual)
2025-12-24 09:11:02,038 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (237105 virtual)
2025-12-24 09:11:02,038 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (242098 virtual)
2025-12-24 09:11:02,039 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (246462 virtual)
2025-12-24 09:11:02,040 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (250096 virtual)
2025-12-24 09:11:02,040 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (255323 virtual)
2025-12-24 09:11:02,041 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (258628 virtual)
2025-12-24 09:11:02,042 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (263349 virtual)
2025-12-24 09:11:02,042 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (268233 virtual)
2025-12-24 09:11:02,043 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (274132 virtual)
2025-12-24 09:11:02,044 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (282971 virtual)
2025-12-24 09:11:02,045 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (286673 virtual)
2025-12-24 09:11:02,046 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (290264 virtual)
2025-12-24 09:11:02,046 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (296037 virtual)
2025-12-24 09:11:02,049 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (310124 virtual)
2025-12-24 09:11:02,050 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (317050 virtual)
2025-12-24 09:11:02,107 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (320598 virtual)
2025-12-24 09:11:02,109 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (334909 virtual)
2025-12-24 09:11:02,110 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (341683 virtual)
2025-12-24 09:11:02,251 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (351624 virtual)
2025-12-24 09:11:02,256 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (360719 virtual)
2025-12-24 09:11:02,257 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (365651 virtual)
2025-12-24 09:11:02,308 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (375582 virtual)
2025-12-24 09:11:02,335 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (380043 virtual)
2025-12-24 09:11:02,340 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (390262 virtual)
2025-12-24 09:11:02,351 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (394060 virtual)
2025-12-24 09:11:02,360 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (398804 virtual)
2025-12-24 09:11:02,371 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (402970 virtual)
2025-12-24 09:11:02,381 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (417617 virtual)
2025-12-24 09:11:02,400 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (425918 virtual)
2025-12-24 09:11:02,416 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (437460 virtual)
2025-12-24 09:11:02,423 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (441365 virtual)
2025-12-24 09:11:02,424 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (446589 virtual)
2025-12-24 09:11:02,438 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (450244 virtual)
2025-12-24 09:11:02,442 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (478452 virtual)
2025-12-24 09:11:02,468 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (482978 virtual)
2025-12-24 09:11:02,476 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (494166 virtual)
2025-12-24 09:11:02,496 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (502241 virtual)
2025-12-24 09:11:02,509 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (518522 virtual)
2025-12-24 09:11:02,531 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (526287 virtual)
2025-12-24 09:11:02,548 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (532913 virtual)
2025-12-24 09:11:02,567 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (536619 virtual)
2025-12-24 09:11:02,569 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (548493 virtual)
2025-12-24 09:11:02,579 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (551958 virtual)
2025-12-24 09:11:02,587 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (555379 virtual)
2025-12-24 09:11:02,589 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (564992 virtual)
2025-12-24 09:11:02,603 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (568990 virtual)
2025-12-24 09:11:02,604 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (574796 virtual)
2025-12-24 09:11:02,616 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (581815 virtual)
2025-12-24 09:11:02,627 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (585873 virtual)
2025-12-24 09:11:02,632 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (591771 virtual)
2025-12-24 09:11:02,640 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (597046 virtual)
2025-12-24 09:11:02,640 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (601849 virtual)
2025-12-24 09:11:02,648 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (608725 virtual)
2025-12-24 09:11:02,660 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (615021 virtual)
2025-12-24 09:11:02,666 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (621265 virtual)
2025-12-24 09:11:02,680 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (627724 virtual)
2025-12-24 09:11:02,703 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (631679 virtual)
2025-12-24 09:11:02,723 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (635736 virtual)
2025-12-24 09:11:02,725 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (643713 virtual)
2025-12-24 09:11:02,726 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (649874 virtual)
2025-12-24 09:11:02,736 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (658090 virtual)
2025-12-24 09:11:02,759 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (661695 virtual)
2025-12-24 09:11:02,764 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (669808 virtual)
2025-12-24 09:11:02,765 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (673686 virtual)
2025-12-24 09:11:02,766 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (678673 virtual)
2025-12-24 09:11:02,772 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (683677 virtual)
2025-12-24 09:11:02,779 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (687618 virtual)
2025-12-24 09:11:02,792 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (698272 virtual)
2025-12-24 09:11:02,804 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (703739 virtual)
2025-12-24 09:11:02,805 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (712044 virtual)
2025-12-24 09:11:02,820 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (719942 virtual)
2025-12-24 09:11:02,834 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (730593 virtual)
2025-12-24 09:11:02,840 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (738698 virtual)
2025-12-24 09:11:02,841 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (744918 virtual)
2025-12-24 09:11:02,855 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (748587 virtual)
2025-12-24 09:11:02,859 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (754283 virtual)
2025-12-24 09:11:02,867 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (759411 virtual)
2025-12-24 09:11:02,875 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (763074 virtual)
2025-12-24 09:11:02,876 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (769146 virtual)
2025-12-24 09:11:02,890 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (778550 virtual)
2025-12-24 09:11:02,895 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (782687 virtual)
2025-12-24 09:11:02,896 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (787370 virtual)
2025-12-24 09:11:02,897 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (792747 virtual)
2025-12-24 09:11:02,897 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (795834 virtual)
2025-12-24 09:11:02,898 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (799472 virtual)
2025-12-24 09:11:02,903 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (804251 virtual)
2025-12-24 09:11:02,915 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (818289 virtual)
2025-12-24 09:11:02,931 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (823742 virtual)
2025-12-24 09:11:02,942 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (829121 virtual)
2025-12-24 09:11:02,949 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (839918 virtual)
2025-12-24 09:11:02,963 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (843357 virtual)
2025-12-24 09:11:02,979 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (847194 virtual)
2025-12-24 09:11:02,983 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (850480 virtual)
2025-12-24 09:11:02,987 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (854672 virtual)
2025-12-24 09:11:02,992 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (863027 virtual)
2025-12-24 09:11:02,993 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (865772 virtual)
2025-12-24 09:11:03,008 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (884808 virtual)
2025-12-24 09:11:03,027 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (889004 virtual)
2025-12-24 09:11:03,044 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (894985 virtual)
2025-12-24 09:11:03,047 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (898703 virtual)
2025-12-24 09:11:03,063 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (901982 virtual)
2025-12-24 09:11:03,072 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (906587 virtual)
2025-12-24 09:11:03,079 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (910463 virtual)
2025-12-24 09:11:03,086 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (931102 virtual)
2025-12-24 09:11:03,112 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (942622 virtual)
2025-12-24 09:11:03,124 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (951129 virtual)
2025-12-24 09:11:03,131 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (955276 virtual)
2025-12-24 09:11:03,135 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (959539 virtual)
2025-12-24 09:11:03,148 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (965946 virtual)
2025-12-24 09:11:03,168 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (970853 virtual)
2025-12-24 09:11:03,168 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (973829 virtual)
2025-12-24 09:11:03,170 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (989097 virtual)
2025-12-24 09:11:03,192 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (994234 virtual)
2025-12-24 09:11:03,199 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (997762 virtual)
2025-12-24 09:11:03,203 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (1000793 virtual)
2025-12-24 09:11:03,204 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (1006413 virtual)
2025-12-24 09:11:03,205 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (1011923 virtual)
2025-12-24 09:11:03,215 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (1012355 virtual)
2025-12-24 09:11:03,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,238 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,276 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,300 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,524 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:03,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:03,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:04,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:04,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:11:04,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:11:06,428 INFO gensim.topic_coherence.text_analysis: 47 accumulators retrieved from output queue
2025-12-24 09:11:06,450 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1015742 virtual documents
2025-12-24 09:11:06,749 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 9738 docs
Training CobwebTree:   0%|          | 0/9738 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 24/9738 [00:00<00:40, 238.54it/s]Training CobwebTree:   0%|          | 48/9738 [00:00<00:58, 164.56it/s]Training CobwebTree:   1%|          | 66/9738 [00:00<01:07, 144.31it/s]Training CobwebTree:   1%|          | 82/9738 [00:00<01:11, 135.54it/s]Training CobwebTree:   1%|          | 96/9738 [00:00<01:13, 131.38it/s]Training CobwebTree:   1%|          | 110/9738 [00:00<01:18, 122.09it/s]Training CobwebTree:   1%|         | 123/9738 [00:00<01:22, 117.24it/s]Training CobwebTree:   1%|         | 135/9738 [00:01<01:25, 112.45it/s]Training CobwebTree:   2%|         | 148/9738 [00:01<01:23, 114.78it/s]Training CobwebTree:   2%|         | 160/9738 [00:01<01:28, 108.16it/s]Training CobwebTree:   2%|         | 171/9738 [00:01<01:30, 106.07it/s]Training CobwebTree:   2%|         | 182/9738 [00:01<01:32, 103.28it/s]Training CobwebTree:   2%|         | 193/9738 [00:01<01:32, 103.24it/s]Training CobwebTree:   2%|         | 204/9738 [00:01<01:36, 99.19it/s] Training CobwebTree:   2%|         | 214/9738 [00:01<01:37, 98.18it/s]Training CobwebTree:   2%|         | 224/9738 [00:01<01:41, 93.42it/s]Training CobwebTree:   2%|         | 234/9738 [00:02<01:42, 92.83it/s]Training CobwebTree:   3%|         | 244/9738 [00:02<01:43, 91.73it/s]Training CobwebTree:   3%|         | 254/9738 [00:02<01:42, 92.78it/s]Training CobwebTree:   3%|         | 264/9738 [00:02<01:40, 94.50it/s]Training CobwebTree:   3%|         | 274/9738 [00:02<01:40, 94.00it/s]Training CobwebTree:   3%|         | 284/9738 [00:02<01:42, 92.19it/s]Training CobwebTree:   3%|         | 294/9738 [00:02<01:42, 92.48it/s]Training CobwebTree:   3%|         | 304/9738 [00:02<01:45, 89.14it/s]Training CobwebTree:   3%|         | 313/9738 [00:02<01:45, 88.99it/s]Training CobwebTree:   3%|         | 323/9738 [00:03<01:46, 88.50it/s]Training CobwebTree:   3%|         | 332/9738 [00:03<01:48, 86.82it/s]Training CobwebTree:   4%|         | 341/9738 [00:03<01:51, 84.45it/s]Training CobwebTree:   4%|         | 350/9738 [00:03<01:51, 84.36it/s]Training CobwebTree:   4%|         | 359/9738 [00:03<01:54, 82.10it/s]Training CobwebTree:   4%|         | 368/9738 [00:03<01:53, 82.69it/s]Training CobwebTree:   4%|         | 377/9738 [00:03<01:53, 82.55it/s]Training CobwebTree:   4%|         | 386/9738 [00:03<01:53, 82.04it/s]Training CobwebTree:   4%|         | 395/9738 [00:03<01:57, 79.47it/s]Training CobwebTree:   4%|         | 405/9738 [00:04<01:53, 82.20it/s]Training CobwebTree:   4%|         | 414/9738 [00:04<01:51, 83.81it/s]Training CobwebTree:   4%|         | 423/9738 [00:04<01:49, 85.21it/s]Training CobwebTree:   4%|         | 433/9738 [00:04<01:46, 87.47it/s]Training CobwebTree:   5%|         | 443/9738 [00:04<01:46, 87.40it/s]Training CobwebTree:   5%|         | 452/9738 [00:04<01:48, 85.22it/s]Training CobwebTree:   5%|         | 461/9738 [00:04<01:50, 84.22it/s]Training CobwebTree:   5%|         | 470/9738 [00:04<01:53, 81.72it/s]Training CobwebTree:   5%|         | 479/9738 [00:04<01:51, 83.31it/s]Training CobwebTree:   5%|         | 488/9738 [00:05<01:48, 85.11it/s]Training CobwebTree:   5%|         | 497/9738 [00:05<01:48, 85.16it/s]Training CobwebTree:   5%|         | 506/9738 [00:05<01:50, 83.61it/s]Training CobwebTree:   5%|         | 515/9738 [00:05<01:48, 84.87it/s]Training CobwebTree:   5%|         | 524/9738 [00:05<01:53, 81.10it/s]Training CobwebTree:   5%|         | 533/9738 [00:05<01:55, 79.74it/s]Training CobwebTree:   6%|         | 542/9738 [00:05<01:56, 79.20it/s]Training CobwebTree:   6%|         | 551/9738 [00:05<01:54, 80.07it/s]Training CobwebTree:   6%|         | 560/9738 [00:05<01:54, 80.08it/s]Training CobwebTree:   6%|         | 569/9738 [00:06<01:55, 79.52it/s]Training CobwebTree:   6%|         | 577/9738 [00:06<01:58, 77.44it/s]Training CobwebTree:   6%|         | 586/9738 [00:06<01:55, 79.47it/s]Training CobwebTree:   6%|         | 595/9738 [00:06<01:53, 80.31it/s]Training CobwebTree:   6%|         | 604/9738 [00:06<01:52, 81.36it/s]Training CobwebTree:   6%|         | 613/9738 [00:06<01:53, 80.11it/s]Training CobwebTree:   6%|         | 622/9738 [00:06<01:56, 78.08it/s]Training CobwebTree:   6%|         | 630/9738 [00:06<01:56, 77.86it/s]Training CobwebTree:   7%|         | 639/9738 [00:06<01:54, 79.30it/s]Training CobwebTree:   7%|         | 647/9738 [00:07<01:54, 79.33it/s]Training CobwebTree:   7%|         | 655/9738 [00:07<01:56, 77.78it/s]Training CobwebTree:   7%|         | 663/9738 [00:07<01:59, 76.03it/s]Training CobwebTree:   7%|         | 671/9738 [00:07<01:58, 76.65it/s]Training CobwebTree:   7%|         | 679/9738 [00:07<02:00, 74.96it/s]Training CobwebTree:   7%|         | 687/9738 [00:07<01:59, 75.48it/s]Training CobwebTree:   7%|         | 695/9738 [00:07<02:00, 75.30it/s]Training CobwebTree:   7%|         | 703/9738 [00:07<02:07, 70.97it/s]Training CobwebTree:   7%|         | 711/9738 [00:07<02:06, 71.42it/s]Training CobwebTree:   7%|         | 719/9738 [00:08<02:02, 73.68it/s]Training CobwebTree:   7%|         | 727/9738 [00:08<01:59, 75.46it/s]Training CobwebTree:   8%|         | 735/9738 [00:08<01:58, 75.67it/s]Training CobwebTree:   8%|         | 743/9738 [00:08<02:01, 74.30it/s]Training CobwebTree:   8%|         | 752/9738 [00:08<01:59, 75.34it/s]Training CobwebTree:   8%|         | 760/9738 [00:08<01:58, 75.77it/s]Training CobwebTree:   8%|         | 768/9738 [00:08<02:01, 74.11it/s]Training CobwebTree:   8%|         | 777/9738 [00:08<01:58, 75.55it/s]Training CobwebTree:   8%|         | 785/9738 [00:08<02:00, 74.04it/s]Training CobwebTree:   8%|         | 793/9738 [00:09<02:01, 73.89it/s]Training CobwebTree:   8%|         | 801/9738 [00:09<02:00, 73.92it/s]Training CobwebTree:   8%|         | 809/9738 [00:09<01:59, 74.50it/s]Training CobwebTree:   8%|         | 817/9738 [00:09<01:59, 74.35it/s]Training CobwebTree:   8%|         | 825/9738 [00:09<01:59, 74.28it/s]Training CobwebTree:   9%|         | 833/9738 [00:09<01:59, 74.37it/s]Training CobwebTree:   9%|         | 841/9738 [00:09<01:59, 74.32it/s]Training CobwebTree:   9%|         | 849/9738 [00:09<01:59, 74.58it/s]Training CobwebTree:   9%|         | 857/9738 [00:09<02:07, 69.47it/s]Training CobwebTree:   9%|         | 865/9738 [00:09<02:04, 71.21it/s]Training CobwebTree:   9%|         | 873/9738 [00:10<02:01, 72.69it/s]Training CobwebTree:   9%|         | 881/9738 [00:10<02:02, 72.43it/s]Training CobwebTree:   9%|         | 889/9738 [00:10<02:00, 73.64it/s]Training CobwebTree:   9%|         | 898/9738 [00:10<01:57, 75.34it/s]Training CobwebTree:   9%|         | 906/9738 [00:10<01:55, 76.40it/s]Training CobwebTree:   9%|         | 914/9738 [00:10<01:55, 76.08it/s]Training CobwebTree:   9%|         | 922/9738 [00:10<01:54, 76.96it/s]Training CobwebTree:  10%|         | 930/9738 [00:10<01:56, 75.35it/s]Training CobwebTree:  10%|         | 938/9738 [00:10<01:59, 73.46it/s]Training CobwebTree:  10%|         | 946/9738 [00:11<02:00, 73.25it/s]Training CobwebTree:  10%|         | 954/9738 [00:11<01:57, 74.92it/s]Training CobwebTree:  10%|         | 962/9738 [00:11<01:57, 74.99it/s]Training CobwebTree:  10%|         | 970/9738 [00:11<01:56, 75.04it/s]Training CobwebTree:  10%|         | 978/9738 [00:11<01:59, 73.30it/s]Training CobwebTree:  10%|         | 986/9738 [00:11<02:02, 71.22it/s]Training CobwebTree:  10%|         | 994/9738 [00:11<02:02, 71.09it/s]Training CobwebTree:  10%|         | 1002/9738 [00:11<02:01, 72.10it/s]Training CobwebTree:  10%|         | 1010/9738 [00:11<01:59, 73.09it/s]Training CobwebTree:  10%|         | 1018/9738 [00:12<01:59, 72.70it/s]Training CobwebTree:  11%|         | 1026/9738 [00:12<02:03, 70.60it/s]Training CobwebTree:  11%|         | 1034/9738 [00:12<02:06, 68.77it/s]Training CobwebTree:  11%|         | 1043/9738 [00:12<02:01, 71.82it/s]Training CobwebTree:  11%|         | 1051/9738 [00:12<02:02, 71.03it/s]Training CobwebTree:  11%|         | 1059/9738 [00:12<02:04, 69.59it/s]Training CobwebTree:  11%|         | 1067/9738 [00:12<02:02, 71.06it/s]Training CobwebTree:  11%|         | 1075/9738 [00:12<02:00, 71.70it/s]Training CobwebTree:  11%|         | 1083/9738 [00:12<02:03, 70.30it/s]Training CobwebTree:  11%|         | 1091/9738 [00:13<02:02, 70.86it/s]Training CobwebTree:  11%|        | 1099/9738 [00:13<02:00, 71.69it/s]Training CobwebTree:  11%|        | 1107/9738 [00:13<02:01, 70.83it/s]Training CobwebTree:  11%|        | 1115/9738 [00:13<02:00, 71.52it/s]Training CobwebTree:  12%|        | 1123/9738 [00:13<01:59, 71.80it/s]Training CobwebTree:  12%|        | 1132/9738 [00:13<01:54, 74.96it/s]Training CobwebTree:  12%|        | 1140/9738 [00:13<01:59, 72.23it/s]Training CobwebTree:  12%|        | 1148/9738 [00:13<01:58, 72.21it/s]Training CobwebTree:  12%|        | 1156/9738 [00:13<01:56, 73.49it/s]Training CobwebTree:  12%|        | 1164/9738 [00:14<01:59, 71.58it/s]Training CobwebTree:  12%|        | 1172/9738 [00:14<02:05, 68.02it/s]Training CobwebTree:  12%|        | 1179/9738 [00:14<02:05, 68.36it/s]Training CobwebTree:  12%|        | 1187/9738 [00:14<02:01, 70.61it/s]Training CobwebTree:  12%|        | 1195/9738 [00:14<02:01, 70.04it/s]Training CobwebTree:  12%|        | 1203/9738 [00:14<02:07, 66.71it/s]Training CobwebTree:  12%|        | 1211/9738 [00:14<02:04, 68.63it/s]Training CobwebTree:  13%|        | 1218/9738 [00:14<02:04, 68.42it/s]Training CobwebTree:  13%|        | 1225/9738 [00:15<02:07, 66.93it/s]Training CobwebTree:  13%|        | 1232/9738 [00:15<02:06, 66.98it/s]Training CobwebTree:  13%|        | 1239/9738 [00:15<02:06, 67.04it/s]Training CobwebTree:  13%|        | 1246/9738 [00:15<02:06, 66.97it/s]Training CobwebTree:  13%|        | 1253/9738 [00:15<02:10, 64.93it/s]Training CobwebTree:  13%|        | 1261/9738 [00:15<02:08, 66.18it/s]Training CobwebTree:  13%|        | 1268/9738 [00:15<02:07, 66.46it/s]Training CobwebTree:  13%|        | 1275/9738 [00:15<02:05, 67.28it/s]Training CobwebTree:  13%|        | 1282/9738 [00:15<02:07, 66.44it/s]Training CobwebTree:  13%|        | 1289/9738 [00:15<02:05, 67.32it/s]Training CobwebTree:  13%|        | 1296/9738 [00:16<02:05, 67.31it/s]Training CobwebTree:  13%|        | 1303/9738 [00:16<02:06, 66.50it/s]Training CobwebTree:  13%|        | 1311/9738 [00:16<02:02, 68.55it/s]Training CobwebTree:  14%|        | 1318/9738 [00:16<02:02, 68.84it/s]Training CobwebTree:  14%|        | 1325/9738 [00:16<02:03, 68.30it/s]Training CobwebTree:  14%|        | 1332/9738 [00:16<02:03, 67.89it/s]Training CobwebTree:  14%|        | 1340/9738 [00:16<02:02, 68.56it/s]Training CobwebTree:  14%|        | 1348/9738 [00:16<02:00, 69.76it/s]Training CobwebTree:  14%|        | 1356/9738 [00:16<01:59, 70.11it/s]Training CobwebTree:  14%|        | 1364/9738 [00:17<02:01, 68.68it/s]Training CobwebTree:  14%|        | 1372/9738 [00:17<01:56, 71.68it/s]Training CobwebTree:  14%|        | 1380/9738 [00:17<01:59, 69.79it/s]Training CobwebTree:  14%|        | 1388/9738 [00:17<01:58, 70.32it/s]Training CobwebTree:  14%|        | 1396/9738 [00:17<02:02, 68.37it/s]Training CobwebTree:  14%|        | 1404/9738 [00:17<01:59, 69.73it/s]Training CobwebTree:  14%|        | 1411/9738 [00:17<02:03, 67.51it/s]Training CobwebTree:  15%|        | 1419/9738 [00:17<02:01, 68.68it/s]Training CobwebTree:  15%|        | 1426/9738 [00:17<02:03, 67.42it/s]Training CobwebTree:  15%|        | 1433/9738 [00:18<02:07, 65.08it/s]Training CobwebTree:  15%|        | 1440/9738 [00:18<02:09, 63.93it/s]Training CobwebTree:  15%|        | 1447/9738 [00:18<02:09, 63.82it/s]Training CobwebTree:  15%|        | 1455/9738 [00:18<02:05, 65.75it/s]Training CobwebTree:  15%|        | 1463/9738 [00:18<02:03, 67.17it/s]Training CobwebTree:  15%|        | 1470/9738 [00:18<02:06, 65.21it/s]Training CobwebTree:  15%|        | 1477/9738 [00:18<02:04, 66.47it/s]Training CobwebTree:  15%|        | 1484/9738 [00:18<02:04, 66.53it/s]Training CobwebTree:  15%|        | 1491/9738 [00:18<02:03, 66.72it/s]Training CobwebTree:  15%|        | 1499/9738 [00:19<02:01, 67.78it/s]Training CobwebTree:  15%|        | 1507/9738 [00:19<01:59, 68.70it/s]Training CobwebTree:  16%|        | 1514/9738 [00:19<02:02, 67.16it/s]Training CobwebTree:  16%|        | 1521/9738 [00:19<02:04, 65.86it/s]Training CobwebTree:  16%|        | 1528/9738 [00:19<02:02, 66.85it/s]Training CobwebTree:  16%|        | 1535/9738 [00:19<02:03, 66.34it/s]Training CobwebTree:  16%|        | 1542/9738 [00:19<02:07, 64.07it/s]Training CobwebTree:  16%|        | 1549/9738 [00:19<02:09, 63.41it/s]Training CobwebTree:  16%|        | 1557/9738 [00:19<02:02, 66.82it/s]Training CobwebTree:  16%|        | 1565/9738 [00:20<01:59, 68.13it/s]Training CobwebTree:  16%|        | 1572/9738 [00:20<02:02, 66.62it/s]Training CobwebTree:  16%|        | 1579/9738 [00:20<02:01, 67.33it/s]Training CobwebTree:  16%|        | 1587/9738 [00:20<01:56, 69.84it/s]Training CobwebTree:  16%|        | 1594/9738 [00:20<01:57, 69.27it/s]Training CobwebTree:  16%|        | 1601/9738 [00:20<01:59, 68.30it/s]Training CobwebTree:  17%|        | 1609/9738 [00:20<01:56, 69.84it/s]Training CobwebTree:  17%|        | 1617/9738 [00:20<01:53, 71.43it/s]Training CobwebTree:  17%|        | 1625/9738 [00:20<01:54, 70.82it/s]Training CobwebTree:  17%|        | 1633/9738 [00:21<01:52, 71.73it/s]Training CobwebTree:  17%|        | 1641/9738 [00:21<01:53, 71.18it/s]Training CobwebTree:  17%|        | 1649/9738 [00:21<01:54, 70.59it/s]Training CobwebTree:  17%|        | 1658/9738 [00:21<01:50, 73.17it/s]Training CobwebTree:  17%|        | 1666/9738 [00:21<01:51, 72.52it/s]Training CobwebTree:  17%|        | 1674/9738 [00:21<01:53, 71.16it/s]Training CobwebTree:  17%|        | 1682/9738 [00:21<01:53, 71.24it/s]Training CobwebTree:  17%|        | 1690/9738 [00:21<01:57, 68.58it/s]Training CobwebTree:  17%|        | 1697/9738 [00:21<02:00, 66.77it/s]Training CobwebTree:  17%|        | 1704/9738 [00:22<01:59, 67.34it/s]Training CobwebTree:  18%|        | 1712/9738 [00:22<02:00, 66.88it/s]Training CobwebTree:  18%|        | 1719/9738 [00:22<02:01, 66.25it/s]Training CobwebTree:  18%|        | 1726/9738 [00:22<02:03, 64.85it/s]Training CobwebTree:  18%|        | 1733/9738 [00:22<02:02, 65.20it/s]Training CobwebTree:  18%|        | 1741/9738 [00:22<01:59, 67.12it/s]Training CobwebTree:  18%|        | 1749/9738 [00:22<01:58, 67.42it/s]Training CobwebTree:  18%|        | 1756/9738 [00:22<01:59, 66.55it/s]Training CobwebTree:  18%|        | 1764/9738 [00:22<01:56, 68.57it/s]Training CobwebTree:  18%|        | 1772/9738 [00:23<01:54, 69.75it/s]Training CobwebTree:  18%|        | 1779/9738 [00:23<01:54, 69.72it/s]Training CobwebTree:  18%|        | 1787/9738 [00:23<01:51, 71.30it/s]Training CobwebTree:  18%|        | 1795/9738 [00:23<01:57, 67.37it/s]Training CobwebTree:  19%|        | 1802/9738 [00:23<01:57, 67.71it/s]Training CobwebTree:  19%|        | 1809/9738 [00:23<01:56, 67.86it/s]Training CobwebTree:  19%|        | 1816/9738 [00:23<01:58, 67.13it/s]Training CobwebTree:  19%|        | 1823/9738 [00:23<01:59, 66.33it/s]Training CobwebTree:  19%|        | 1830/9738 [00:23<01:58, 66.73it/s]Training CobwebTree:  19%|        | 1837/9738 [00:24<01:59, 66.06it/s]Training CobwebTree:  19%|        | 1844/9738 [00:24<01:58, 66.75it/s]Training CobwebTree:  19%|        | 1851/9738 [00:24<01:59, 65.93it/s]Training CobwebTree:  19%|        | 1858/9738 [00:24<01:59, 65.69it/s]Training CobwebTree:  19%|        | 1865/9738 [00:24<01:58, 66.20it/s]Training CobwebTree:  19%|        | 1873/9738 [00:24<01:59, 65.80it/s]Training CobwebTree:  19%|        | 1880/9738 [00:24<02:01, 64.82it/s]Training CobwebTree:  19%|        | 1887/9738 [00:24<02:01, 64.81it/s]Training CobwebTree:  19%|        | 1894/9738 [00:24<02:03, 63.29it/s]Training CobwebTree:  20%|        | 1901/9738 [00:25<02:03, 63.68it/s]Training CobwebTree:  20%|        | 1908/9738 [00:25<02:07, 61.46it/s]Training CobwebTree:  20%|        | 1915/9738 [00:25<02:02, 63.68it/s]Training CobwebTree:  20%|        | 1922/9738 [00:25<02:01, 64.30it/s]Training CobwebTree:  20%|        | 1929/9738 [00:25<01:59, 65.32it/s]Training CobwebTree:  20%|        | 1936/9738 [00:25<01:58, 65.99it/s]Training CobwebTree:  20%|        | 1943/9738 [00:25<01:59, 64.99it/s]Training CobwebTree:  20%|        | 1950/9738 [00:25<01:58, 65.63it/s]Training CobwebTree:  20%|        | 1958/9738 [00:25<01:54, 67.78it/s]Training CobwebTree:  20%|        | 1965/9738 [00:25<01:53, 68.23it/s]Training CobwebTree:  20%|        | 1973/9738 [00:26<01:50, 70.27it/s]Training CobwebTree:  20%|        | 1981/9738 [00:26<01:50, 70.49it/s]Training CobwebTree:  20%|        | 1989/9738 [00:26<01:55, 67.26it/s]Training CobwebTree:  20%|        | 1996/9738 [00:26<01:54, 67.33it/s]Training CobwebTree:  21%|        | 2003/9738 [00:26<01:58, 65.38it/s]Training CobwebTree:  21%|        | 2010/9738 [00:26<01:57, 65.85it/s]Training CobwebTree:  21%|        | 2017/9738 [00:26<02:03, 62.29it/s]Training CobwebTree:  21%|        | 2024/9738 [00:26<01:59, 64.35it/s]Training CobwebTree:  21%|        | 2032/9738 [00:26<01:53, 67.82it/s]Training CobwebTree:  21%|        | 2039/9738 [00:27<01:53, 67.66it/s]Training CobwebTree:  21%|        | 2047/9738 [00:27<01:53, 68.04it/s]Training CobwebTree:  21%|        | 2054/9738 [00:27<01:52, 68.33it/s]Training CobwebTree:  21%|        | 2061/9738 [00:27<01:52, 68.24it/s]Training CobwebTree:  21%|        | 2068/9738 [00:27<01:57, 65.11it/s]Training CobwebTree:  21%|       | 2075/9738 [00:27<01:59, 63.99it/s]Training CobwebTree:  21%|       | 2082/9738 [00:27<01:57, 65.05it/s]Training CobwebTree:  21%|       | 2089/9738 [00:27<01:55, 66.40it/s]Training CobwebTree:  22%|       | 2096/9738 [00:27<01:55, 66.23it/s]Training CobwebTree:  22%|       | 2103/9738 [00:28<01:53, 67.15it/s]Training CobwebTree:  22%|       | 2110/9738 [00:28<01:55, 65.84it/s]Training CobwebTree:  22%|       | 2117/9738 [00:28<01:56, 65.55it/s]Training CobwebTree:  22%|       | 2124/9738 [00:28<01:55, 65.86it/s]Training CobwebTree:  22%|       | 2131/9738 [00:28<01:58, 64.09it/s]Training CobwebTree:  22%|       | 2138/9738 [00:28<02:00, 63.32it/s]Training CobwebTree:  22%|       | 2146/9738 [00:28<01:55, 65.48it/s]Training CobwebTree:  22%|       | 2153/9738 [00:28<01:54, 65.98it/s]Training CobwebTree:  22%|       | 2160/9738 [00:28<02:01, 62.48it/s]Training CobwebTree:  22%|       | 2167/9738 [00:29<02:05, 60.46it/s]Training CobwebTree:  22%|       | 2174/9738 [00:29<02:01, 62.23it/s]Training CobwebTree:  22%|       | 2181/9738 [00:29<02:00, 62.92it/s]Training CobwebTree:  22%|       | 2188/9738 [00:29<02:02, 61.78it/s]Training CobwebTree:  23%|       | 2195/9738 [00:29<02:01, 61.84it/s]Training CobwebTree:  23%|       | 2202/9738 [00:29<01:59, 63.21it/s]Training CobwebTree:  23%|       | 2210/9738 [00:29<01:53, 66.08it/s]Training CobwebTree:  23%|       | 2217/9738 [00:29<01:53, 66.25it/s]Training CobwebTree:  23%|       | 2224/9738 [00:29<01:53, 65.93it/s]Training CobwebTree:  23%|       | 2231/9738 [00:30<01:54, 65.42it/s]Training CobwebTree:  23%|       | 2238/9738 [00:30<01:56, 64.28it/s]Training CobwebTree:  23%|       | 2245/9738 [00:30<01:55, 64.68it/s]Training CobwebTree:  23%|       | 2252/9738 [00:30<01:58, 63.30it/s]Training CobwebTree:  23%|       | 2259/9738 [00:30<01:56, 64.20it/s]Training CobwebTree:  23%|       | 2266/9738 [00:30<01:57, 63.49it/s]Training CobwebTree:  23%|       | 2273/9738 [00:30<01:59, 62.47it/s]Training CobwebTree:  23%|       | 2281/9738 [00:30<01:54, 65.29it/s]Training CobwebTree:  23%|       | 2288/9738 [00:30<01:57, 63.31it/s]Training CobwebTree:  24%|       | 2295/9738 [00:31<01:56, 64.08it/s]Training CobwebTree:  24%|       | 2302/9738 [00:31<01:55, 64.57it/s]Training CobwebTree:  24%|       | 2310/9738 [00:31<01:52, 66.20it/s]Training CobwebTree:  24%|       | 2317/9738 [00:31<01:56, 63.93it/s]Training CobwebTree:  24%|       | 2324/9738 [00:31<01:55, 64.14it/s]Training CobwebTree:  24%|       | 2331/9738 [00:31<01:55, 63.88it/s]Training CobwebTree:  24%|       | 2338/9738 [00:31<01:55, 63.88it/s]Training CobwebTree:  24%|       | 2345/9738 [00:31<01:59, 61.83it/s]Training CobwebTree:  24%|       | 2352/9738 [00:31<01:58, 62.35it/s]Training CobwebTree:  24%|       | 2359/9738 [00:32<01:55, 63.83it/s]Training CobwebTree:  24%|       | 2366/9738 [00:32<01:54, 64.22it/s]Training CobwebTree:  24%|       | 2373/9738 [00:32<01:55, 63.82it/s]Training CobwebTree:  24%|       | 2380/9738 [00:32<01:58, 62.13it/s]Training CobwebTree:  25%|       | 2387/9738 [00:32<01:55, 63.92it/s]Training CobwebTree:  25%|       | 2395/9738 [00:32<01:51, 66.01it/s]Training CobwebTree:  25%|       | 2402/9738 [00:32<01:51, 65.87it/s]Training CobwebTree:  25%|       | 2409/9738 [00:32<01:51, 65.56it/s]Training CobwebTree:  25%|       | 2416/9738 [00:32<01:50, 66.26it/s]Training CobwebTree:  25%|       | 2423/9738 [00:33<01:50, 66.39it/s]Training CobwebTree:  25%|       | 2430/9738 [00:33<01:48, 67.09it/s]Training CobwebTree:  25%|       | 2437/9738 [00:33<01:50, 66.06it/s]Training CobwebTree:  25%|       | 2444/9738 [00:33<01:51, 65.16it/s]Training CobwebTree:  25%|       | 2451/9738 [00:33<01:50, 65.92it/s]Training CobwebTree:  25%|       | 2458/9738 [00:33<01:57, 62.01it/s]Training CobwebTree:  25%|       | 2465/9738 [00:33<01:59, 60.78it/s]Training CobwebTree:  25%|       | 2472/9738 [00:33<01:56, 62.23it/s]Training CobwebTree:  25%|       | 2479/9738 [00:33<01:57, 61.85it/s]Training CobwebTree:  26%|       | 2486/9738 [00:34<01:59, 60.66it/s]Training CobwebTree:  26%|       | 2493/9738 [00:34<01:59, 60.84it/s]Training CobwebTree:  26%|       | 2500/9738 [00:34<01:57, 61.57it/s]Training CobwebTree:  26%|       | 2507/9738 [00:34<01:54, 63.07it/s]Training CobwebTree:  26%|       | 2514/9738 [00:34<01:53, 63.59it/s]Training CobwebTree:  26%|       | 2521/9738 [00:34<01:58, 60.71it/s]Training CobwebTree:  26%|       | 2528/9738 [00:34<02:02, 58.94it/s]Training CobwebTree:  26%|       | 2535/9738 [00:34<02:01, 59.47it/s]Training CobwebTree:  26%|       | 2541/9738 [00:34<02:00, 59.56it/s]Training CobwebTree:  26%|       | 2548/9738 [00:35<01:58, 60.73it/s]Training CobwebTree:  26%|       | 2555/9738 [00:35<01:57, 61.11it/s]Training CobwebTree:  26%|       | 2562/9738 [00:35<01:53, 63.44it/s]Training CobwebTree:  26%|       | 2569/9738 [00:35<01:53, 62.91it/s]Training CobwebTree:  26%|       | 2576/9738 [00:35<01:52, 63.39it/s]Training CobwebTree:  27%|       | 2583/9738 [00:35<01:59, 59.84it/s]Training CobwebTree:  27%|       | 2590/9738 [00:35<01:58, 60.07it/s]Training CobwebTree:  27%|       | 2597/9738 [00:35<01:58, 60.46it/s]Training CobwebTree:  27%|       | 2604/9738 [00:35<01:54, 62.08it/s]Training CobwebTree:  27%|       | 2611/9738 [00:36<01:53, 62.56it/s]Training CobwebTree:  27%|       | 2618/9738 [00:36<01:50, 64.45it/s]Training CobwebTree:  27%|       | 2625/9738 [00:36<01:50, 64.14it/s]Training CobwebTree:  27%|       | 2632/9738 [00:36<01:51, 63.83it/s]Training CobwebTree:  27%|       | 2639/9738 [00:36<01:52, 63.22it/s]Training CobwebTree:  27%|       | 2646/9738 [00:36<01:52, 62.85it/s]Training CobwebTree:  27%|       | 2653/9738 [00:36<01:56, 60.74it/s]Training CobwebTree:  27%|       | 2660/9738 [00:36<01:54, 62.07it/s]Training CobwebTree:  27%|       | 2667/9738 [00:36<01:54, 61.61it/s]Training CobwebTree:  27%|       | 2674/9738 [00:37<01:56, 60.77it/s]Training CobwebTree:  28%|       | 2681/9738 [00:37<01:57, 59.90it/s]Training CobwebTree:  28%|       | 2688/9738 [00:37<01:52, 62.54it/s]Training CobwebTree:  28%|       | 2695/9738 [00:37<01:51, 63.23it/s]Training CobwebTree:  28%|       | 2702/9738 [00:37<01:53, 61.83it/s]Training CobwebTree:  28%|       | 2709/9738 [00:37<01:52, 62.71it/s]Training CobwebTree:  28%|       | 2716/9738 [00:37<01:55, 60.94it/s]Training CobwebTree:  28%|       | 2723/9738 [00:37<01:53, 61.71it/s]Training CobwebTree:  28%|       | 2730/9738 [00:38<01:51, 63.07it/s]Training CobwebTree:  28%|       | 2737/9738 [00:38<01:54, 61.36it/s]Training CobwebTree:  28%|       | 2744/9738 [00:38<01:50, 63.56it/s]Training CobwebTree:  28%|       | 2752/9738 [00:38<01:45, 66.39it/s]Training CobwebTree:  28%|       | 2759/9738 [00:38<01:44, 66.51it/s]Training CobwebTree:  28%|       | 2766/9738 [00:38<01:44, 66.60it/s]Training CobwebTree:  28%|       | 2773/9738 [00:38<01:49, 63.86it/s]Training CobwebTree:  29%|       | 2781/9738 [00:38<01:45, 65.92it/s]Training CobwebTree:  29%|       | 2788/9738 [00:38<01:46, 65.07it/s]Training CobwebTree:  29%|       | 2795/9738 [00:38<01:44, 66.17it/s]Training CobwebTree:  29%|       | 2802/9738 [00:39<01:45, 65.46it/s]Training CobwebTree:  29%|       | 2809/9738 [00:39<01:48, 63.91it/s]Training CobwebTree:  29%|       | 2816/9738 [00:39<01:50, 62.91it/s]Training CobwebTree:  29%|       | 2823/9738 [00:39<01:48, 63.49it/s]Training CobwebTree:  29%|       | 2830/9738 [00:39<01:50, 62.26it/s]Training CobwebTree:  29%|       | 2837/9738 [00:39<01:55, 59.84it/s]Training CobwebTree:  29%|       | 2844/9738 [00:39<01:50, 62.29it/s]Training CobwebTree:  29%|       | 2851/9738 [00:39<01:51, 61.69it/s]Training CobwebTree:  29%|       | 2858/9738 [00:40<01:48, 63.25it/s]Training CobwebTree:  29%|       | 2865/9738 [00:40<01:47, 63.84it/s]Training CobwebTree:  29%|       | 2872/9738 [00:40<01:48, 63.23it/s]Training CobwebTree:  30%|       | 2879/9738 [00:40<01:47, 64.07it/s]Training CobwebTree:  30%|       | 2886/9738 [00:40<01:47, 63.45it/s]Training CobwebTree:  30%|       | 2893/9738 [00:40<01:50, 61.90it/s]Training CobwebTree:  30%|       | 2900/9738 [00:40<01:50, 61.61it/s]Training CobwebTree:  30%|       | 2907/9738 [00:40<01:51, 61.37it/s]Training CobwebTree:  30%|       | 2914/9738 [00:40<01:49, 62.32it/s]Training CobwebTree:  30%|       | 2921/9738 [00:41<01:46, 63.82it/s]Training CobwebTree:  30%|       | 2928/9738 [00:41<01:49, 62.15it/s]Training CobwebTree:  30%|       | 2935/9738 [00:41<01:49, 62.21it/s]Training CobwebTree:  30%|       | 2942/9738 [00:41<01:52, 60.66it/s]Training CobwebTree:  30%|       | 2949/9738 [00:41<01:51, 60.82it/s]Training CobwebTree:  30%|       | 2956/9738 [00:41<01:52, 60.34it/s]Training CobwebTree:  30%|       | 2963/9738 [00:41<01:52, 60.30it/s]Training CobwebTree:  31%|       | 2971/9738 [00:41<01:46, 63.25it/s]Training CobwebTree:  31%|       | 2978/9738 [00:41<01:47, 62.87it/s]Training CobwebTree:  31%|       | 2985/9738 [00:42<01:45, 64.26it/s]Training CobwebTree:  31%|       | 2992/9738 [00:42<01:45, 63.83it/s]Training CobwebTree:  31%|       | 2999/9738 [00:42<01:46, 63.36it/s]Training CobwebTree:  31%|       | 3006/9738 [00:42<01:43, 64.83it/s]Training CobwebTree:  31%|       | 3013/9738 [00:42<01:49, 61.19it/s]Training CobwebTree:  31%|       | 3020/9738 [00:42<01:48, 62.04it/s]Training CobwebTree:  31%|       | 3027/9738 [00:42<01:48, 61.94it/s]Training CobwebTree:  31%|       | 3034/9738 [00:42<01:44, 63.87it/s]Training CobwebTree:  31%|       | 3041/9738 [00:42<01:43, 64.52it/s]Training CobwebTree:  31%|      | 3048/9738 [00:43<01:44, 64.06it/s]Training CobwebTree:  31%|      | 3055/9738 [00:43<01:48, 61.40it/s]Training CobwebTree:  31%|      | 3062/9738 [00:43<01:49, 61.24it/s]Training CobwebTree:  32%|      | 3069/9738 [00:43<01:46, 62.90it/s]Training CobwebTree:  32%|      | 3076/9738 [00:43<01:48, 61.13it/s]Training CobwebTree:  32%|      | 3083/9738 [00:43<01:48, 61.35it/s]Training CobwebTree:  32%|      | 3090/9738 [00:43<01:48, 61.37it/s]Training CobwebTree:  32%|      | 3097/9738 [00:43<01:46, 62.64it/s]Training CobwebTree:  32%|      | 3104/9738 [00:43<01:45, 62.82it/s]Training CobwebTree:  32%|      | 3111/9738 [00:44<01:50, 60.18it/s]Training CobwebTree:  32%|      | 3118/9738 [00:44<01:47, 61.58it/s]Training CobwebTree:  32%|      | 3125/9738 [00:44<01:48, 60.88it/s]Training CobwebTree:  32%|      | 3132/9738 [00:44<01:47, 61.23it/s]Training CobwebTree:  32%|      | 3139/9738 [00:44<01:51, 59.16it/s]Training CobwebTree:  32%|      | 3146/9738 [00:44<01:47, 61.59it/s]Training CobwebTree:  32%|      | 3153/9738 [00:44<01:46, 61.67it/s]Training CobwebTree:  32%|      | 3160/9738 [00:44<01:44, 62.89it/s]Training CobwebTree:  33%|      | 3167/9738 [00:44<01:46, 61.96it/s]Training CobwebTree:  33%|      | 3174/9738 [00:45<01:43, 63.25it/s]Training CobwebTree:  33%|      | 3181/9738 [00:45<01:44, 62.61it/s]Training CobwebTree:  33%|      | 3188/9738 [00:45<01:47, 60.93it/s]Training CobwebTree:  33%|      | 3195/9738 [00:45<01:48, 60.57it/s]Training CobwebTree:  33%|      | 3202/9738 [00:45<01:48, 60.04it/s]Training CobwebTree:  33%|      | 3209/9738 [00:45<01:45, 62.13it/s]Training CobwebTree:  33%|      | 3216/9738 [00:45<01:46, 61.21it/s]Training CobwebTree:  33%|      | 3223/9738 [00:45<01:50, 58.72it/s]Training CobwebTree:  33%|      | 3230/9738 [00:46<01:47, 60.67it/s]Training CobwebTree:  33%|      | 3237/9738 [00:46<01:45, 61.56it/s]Training CobwebTree:  33%|      | 3244/9738 [00:46<01:46, 61.11it/s]Training CobwebTree:  33%|      | 3251/9738 [00:46<01:47, 60.22it/s]Training CobwebTree:  33%|      | 3258/9738 [00:46<01:46, 60.89it/s]Training CobwebTree:  34%|      | 3265/9738 [00:46<01:50, 58.67it/s]Training CobwebTree:  34%|      | 3272/9738 [00:46<01:46, 60.69it/s]Training CobwebTree:  34%|      | 3279/9738 [00:46<01:45, 61.10it/s]Training CobwebTree:  34%|      | 3286/9738 [00:46<01:47, 60.10it/s]Training CobwebTree:  34%|      | 3293/9738 [00:47<01:49, 58.66it/s]Training CobwebTree:  34%|      | 3299/9738 [00:47<01:53, 56.87it/s]Training CobwebTree:  34%|      | 3305/9738 [00:47<01:55, 55.89it/s]Training CobwebTree:  34%|      | 3312/9738 [00:47<01:49, 58.45it/s]Training CobwebTree:  34%|      | 3318/9738 [00:47<01:50, 57.86it/s]Training CobwebTree:  34%|      | 3325/9738 [00:47<01:49, 58.80it/s]Training CobwebTree:  34%|      | 3332/9738 [00:47<01:47, 59.43it/s]Training CobwebTree:  34%|      | 3339/9738 [00:47<01:45, 60.47it/s]Training CobwebTree:  34%|      | 3346/9738 [00:47<01:44, 61.06it/s]Training CobwebTree:  34%|      | 3353/9738 [00:48<01:43, 61.51it/s]Training CobwebTree:  35%|      | 3360/9738 [00:48<01:43, 61.89it/s]Training CobwebTree:  35%|      | 3367/9738 [00:48<01:42, 62.38it/s]Training CobwebTree:  35%|      | 3374/9738 [00:48<01:42, 62.26it/s]Training CobwebTree:  35%|      | 3382/9738 [00:48<01:38, 64.45it/s]Training CobwebTree:  35%|      | 3389/9738 [00:48<01:39, 63.71it/s]Training CobwebTree:  35%|      | 3396/9738 [00:48<01:38, 64.57it/s]Training CobwebTree:  35%|      | 3403/9738 [00:48<01:39, 63.93it/s]Training CobwebTree:  35%|      | 3410/9738 [00:48<01:38, 64.17it/s]Training CobwebTree:  35%|      | 3417/9738 [00:49<01:38, 64.11it/s]Training CobwebTree:  35%|      | 3424/9738 [00:49<01:37, 64.45it/s]Training CobwebTree:  35%|      | 3431/9738 [00:49<01:40, 62.98it/s]Training CobwebTree:  35%|      | 3438/9738 [00:49<01:43, 60.60it/s]Training CobwebTree:  35%|      | 3445/9738 [00:49<01:40, 62.32it/s]Training CobwebTree:  35%|      | 3452/9738 [00:49<01:39, 63.46it/s]Training CobwebTree:  36%|      | 3459/9738 [00:49<01:40, 62.49it/s]Training CobwebTree:  36%|      | 3466/9738 [00:49<01:40, 62.33it/s]Training CobwebTree:  36%|      | 3473/9738 [00:49<01:43, 60.33it/s]Training CobwebTree:  36%|      | 3480/9738 [00:50<01:42, 61.05it/s]Training CobwebTree:  36%|      | 3487/9738 [00:50<01:46, 58.85it/s]Training CobwebTree:  36%|      | 3494/9738 [00:50<01:45, 59.43it/s]Training CobwebTree:  36%|      | 3501/9738 [00:50<01:44, 59.68it/s]Training CobwebTree:  36%|      | 3507/9738 [00:50<01:44, 59.70it/s]Training CobwebTree:  36%|      | 3514/9738 [00:50<01:39, 62.39it/s]Training CobwebTree:  36%|      | 3521/9738 [00:50<01:40, 62.14it/s]Training CobwebTree:  36%|      | 3528/9738 [00:50<01:40, 61.52it/s]Training CobwebTree:  36%|      | 3535/9738 [00:51<01:39, 62.59it/s]Training CobwebTree:  36%|      | 3542/9738 [00:51<01:40, 61.64it/s]Training CobwebTree:  36%|      | 3549/9738 [00:51<01:41, 61.03it/s]Training CobwebTree:  37%|      | 3556/9738 [00:51<01:39, 62.36it/s]Training CobwebTree:  37%|      | 3563/9738 [00:51<01:41, 60.54it/s]Training CobwebTree:  37%|      | 3570/9738 [00:51<01:47, 57.39it/s]Training CobwebTree:  37%|      | 3576/9738 [00:51<01:46, 57.75it/s]Training CobwebTree:  37%|      | 3582/9738 [00:51<01:46, 57.67it/s]Training CobwebTree:  37%|      | 3588/9738 [00:51<01:46, 57.97it/s]Training CobwebTree:  37%|      | 3595/9738 [00:52<01:41, 60.26it/s]Training CobwebTree:  37%|      | 3602/9738 [00:52<01:40, 60.78it/s]Training CobwebTree:  37%|      | 3609/9738 [00:52<01:38, 62.40it/s]Training CobwebTree:  37%|      | 3616/9738 [00:52<01:38, 62.14it/s]Training CobwebTree:  37%|      | 3623/9738 [00:52<01:43, 59.35it/s]Training CobwebTree:  37%|      | 3630/9738 [00:52<01:43, 59.03it/s]Training CobwebTree:  37%|      | 3636/9738 [00:52<01:45, 57.91it/s]Training CobwebTree:  37%|      | 3642/9738 [00:52<01:47, 56.91it/s]Training CobwebTree:  37%|      | 3649/9738 [00:52<01:43, 58.78it/s]Training CobwebTree:  38%|      | 3655/9738 [00:53<01:43, 58.63it/s]Training CobwebTree:  38%|      | 3661/9738 [00:53<01:45, 57.65it/s]Training CobwebTree:  38%|      | 3667/9738 [00:53<01:47, 56.28it/s]Training CobwebTree:  38%|      | 3673/9738 [00:53<01:49, 55.62it/s]Training CobwebTree:  38%|      | 3679/9738 [00:53<01:46, 56.65it/s]Training CobwebTree:  38%|      | 3686/9738 [00:53<01:43, 58.44it/s]Training CobwebTree:  38%|      | 3693/9738 [00:53<01:39, 61.02it/s]Training CobwebTree:  38%|      | 3700/9738 [00:53<01:38, 61.03it/s]Training CobwebTree:  38%|      | 3707/9738 [00:53<01:40, 59.88it/s]Training CobwebTree:  38%|      | 3714/9738 [00:54<01:40, 59.82it/s]Training CobwebTree:  38%|      | 3721/9738 [00:54<01:39, 60.59it/s]Training CobwebTree:  38%|      | 3728/9738 [00:54<01:39, 60.32it/s]Training CobwebTree:  38%|      | 3735/9738 [00:54<01:41, 58.94it/s]Training CobwebTree:  38%|      | 3742/9738 [00:54<01:37, 61.58it/s]Training CobwebTree:  38%|      | 3749/9738 [00:54<01:40, 59.70it/s]Training CobwebTree:  39%|      | 3756/9738 [00:54<01:39, 60.29it/s]Training CobwebTree:  39%|      | 3763/9738 [00:54<01:41, 58.89it/s]Training CobwebTree:  39%|      | 3769/9738 [00:54<01:41, 58.79it/s]Training CobwebTree:  39%|      | 3775/9738 [00:55<01:42, 58.16it/s]Training CobwebTree:  39%|      | 3781/9738 [00:55<01:42, 58.29it/s]Training CobwebTree:  39%|      | 3787/9738 [00:55<01:43, 57.61it/s]Training CobwebTree:  39%|      | 3793/9738 [00:55<01:44, 56.69it/s]Training CobwebTree:  39%|      | 3800/9738 [00:55<01:40, 58.94it/s]Training CobwebTree:  39%|      | 3806/9738 [00:55<01:41, 58.46it/s]Training CobwebTree:  39%|      | 3813/9738 [00:55<01:42, 57.98it/s]Training CobwebTree:  39%|      | 3820/9738 [00:55<01:41, 58.53it/s]Training CobwebTree:  39%|      | 3826/9738 [00:55<01:41, 57.97it/s]Training CobwebTree:  39%|      | 3833/9738 [00:56<01:38, 60.24it/s]Training CobwebTree:  39%|      | 3840/9738 [00:56<01:38, 59.64it/s]Training CobwebTree:  39%|      | 3846/9738 [00:56<01:40, 58.82it/s]Training CobwebTree:  40%|      | 3853/9738 [00:56<01:38, 59.84it/s]Training CobwebTree:  40%|      | 3860/9738 [00:56<01:34, 62.05it/s]Training CobwebTree:  40%|      | 3867/9738 [00:56<01:34, 61.86it/s]Training CobwebTree:  40%|      | 3874/9738 [00:56<01:34, 61.80it/s]Training CobwebTree:  40%|      | 3881/9738 [00:56<01:40, 58.55it/s]Training CobwebTree:  40%|      | 3888/9738 [00:56<01:39, 59.05it/s]Training CobwebTree:  40%|      | 3894/9738 [00:57<01:40, 58.04it/s]Training CobwebTree:  40%|      | 3901/9738 [00:57<01:36, 60.39it/s]Training CobwebTree:  40%|      | 3908/9738 [00:57<01:36, 60.36it/s]Training CobwebTree:  40%|      | 3915/9738 [00:57<01:39, 58.47it/s]Training CobwebTree:  40%|      | 3922/9738 [00:57<01:36, 60.32it/s]Training CobwebTree:  40%|      | 3929/9738 [00:57<01:34, 61.67it/s]Training CobwebTree:  40%|      | 3936/9738 [00:57<01:37, 59.66it/s]Training CobwebTree:  40%|      | 3943/9738 [00:57<01:38, 59.02it/s]Training CobwebTree:  41%|      | 3950/9738 [00:58<01:36, 60.24it/s]Training CobwebTree:  41%|      | 3957/9738 [00:58<01:40, 57.72it/s]Training CobwebTree:  41%|      | 3964/9738 [00:58<01:36, 59.58it/s]Training CobwebTree:  41%|      | 3971/9738 [00:58<01:37, 58.90it/s]Training CobwebTree:  41%|      | 3977/9738 [00:58<01:39, 57.97it/s]Training CobwebTree:  41%|      | 3984/9738 [00:58<01:36, 59.78it/s]Training CobwebTree:  41%|      | 3991/9738 [00:58<01:33, 61.15it/s]Training CobwebTree:  41%|      | 3998/9738 [00:58<01:32, 62.29it/s]Training CobwebTree:  41%|      | 4005/9738 [00:58<01:31, 62.64it/s]Training CobwebTree:  41%|      | 4012/9738 [00:59<01:32, 62.07it/s]Training CobwebTree:  41%|     | 4019/9738 [00:59<01:30, 63.11it/s]Training CobwebTree:  41%|     | 4026/9738 [00:59<01:31, 62.45it/s]Training CobwebTree:  41%|     | 4033/9738 [00:59<01:36, 59.03it/s]Training CobwebTree:  41%|     | 4040/9738 [00:59<01:35, 59.43it/s]Training CobwebTree:  42%|     | 4047/9738 [00:59<01:34, 59.95it/s]Training CobwebTree:  42%|     | 4054/9738 [00:59<01:36, 58.72it/s]Training CobwebTree:  42%|     | 4060/9738 [00:59<01:37, 57.94it/s]Training CobwebTree:  42%|     | 4067/9738 [00:59<01:33, 60.88it/s]Training CobwebTree:  42%|     | 4074/9738 [01:00<01:35, 59.41it/s]Training CobwebTree:  42%|     | 4081/9738 [01:00<01:33, 60.45it/s]Training CobwebTree:  42%|     | 4088/9738 [01:00<01:34, 59.91it/s]Training CobwebTree:  42%|     | 4095/9738 [01:00<01:32, 61.06it/s]Training CobwebTree:  42%|     | 4102/9738 [01:00<01:31, 61.65it/s]Training CobwebTree:  42%|     | 4109/9738 [01:00<01:33, 60.47it/s]Training CobwebTree:  42%|     | 4116/9738 [01:00<01:31, 61.43it/s]Training CobwebTree:  42%|     | 4123/9738 [01:00<01:30, 61.85it/s]Training CobwebTree:  42%|     | 4130/9738 [01:00<01:33, 60.01it/s]Training CobwebTree:  42%|     | 4137/9738 [01:01<01:34, 59.10it/s]Training CobwebTree:  43%|     | 4144/9738 [01:01<01:31, 61.03it/s]Training CobwebTree:  43%|     | 4151/9738 [01:01<01:32, 60.44it/s]Training CobwebTree:  43%|     | 4158/9738 [01:01<01:31, 61.07it/s]Training CobwebTree:  43%|     | 4165/9738 [01:01<01:29, 62.29it/s]Training CobwebTree:  43%|     | 4172/9738 [01:01<01:30, 61.45it/s]Training CobwebTree:  43%|     | 4179/9738 [01:01<01:34, 59.11it/s]Training CobwebTree:  43%|     | 4186/9738 [01:01<01:32, 59.88it/s]Training CobwebTree:  43%|     | 4193/9738 [01:02<01:32, 59.64it/s]Training CobwebTree:  43%|     | 4199/9738 [01:02<01:33, 59.17it/s]Training CobwebTree:  43%|     | 4206/9738 [01:02<01:32, 59.95it/s]Training CobwebTree:  43%|     | 4213/9738 [01:02<01:33, 59.14it/s]Training CobwebTree:  43%|     | 4219/9738 [01:02<01:34, 58.35it/s]Training CobwebTree:  43%|     | 4226/9738 [01:02<01:33, 58.80it/s]Training CobwebTree:  43%|     | 4232/9738 [01:02<01:34, 58.57it/s]Training CobwebTree:  44%|     | 4238/9738 [01:02<01:34, 58.37it/s]Training CobwebTree:  44%|     | 4245/9738 [01:02<01:31, 59.95it/s]Training CobwebTree:  44%|     | 4252/9738 [01:03<01:30, 60.92it/s]Training CobwebTree:  44%|     | 4259/9738 [01:03<01:31, 60.08it/s]Training CobwebTree:  44%|     | 4266/9738 [01:03<01:29, 61.30it/s]Training CobwebTree:  44%|     | 4273/9738 [01:03<01:33, 58.54it/s]Training CobwebTree:  44%|     | 4279/9738 [01:03<01:33, 58.36it/s]Training CobwebTree:  44%|     | 4286/9738 [01:03<01:30, 60.17it/s]Training CobwebTree:  44%|     | 4293/9738 [01:03<01:30, 60.14it/s]Training CobwebTree:  44%|     | 4300/9738 [01:03<01:33, 57.90it/s]Training CobwebTree:  44%|     | 4307/9738 [01:03<01:34, 57.77it/s]Training CobwebTree:  44%|     | 4313/9738 [01:04<01:34, 57.52it/s]Training CobwebTree:  44%|     | 4319/9738 [01:04<01:33, 58.13it/s]Training CobwebTree:  44%|     | 4326/9738 [01:04<01:31, 58.90it/s]Training CobwebTree:  44%|     | 4333/9738 [01:04<01:30, 59.82it/s]Training CobwebTree:  45%|     | 4340/9738 [01:04<01:28, 61.05it/s]Training CobwebTree:  45%|     | 4347/9738 [01:04<01:28, 60.94it/s]Training CobwebTree:  45%|     | 4354/9738 [01:04<01:28, 60.61it/s]Training CobwebTree:  45%|     | 4361/9738 [01:04<01:30, 59.41it/s]Training CobwebTree:  45%|     | 4368/9738 [01:04<01:30, 59.19it/s]Training CobwebTree:  45%|     | 4375/9738 [01:05<01:29, 60.14it/s]Training CobwebTree:  45%|     | 4382/9738 [01:05<01:29, 59.99it/s]Training CobwebTree:  45%|     | 4389/9738 [01:05<01:28, 60.36it/s]Training CobwebTree:  45%|     | 4396/9738 [01:05<01:26, 61.97it/s]Training CobwebTree:  45%|     | 4403/9738 [01:05<01:27, 61.04it/s]Training CobwebTree:  45%|     | 4410/9738 [01:05<01:29, 59.49it/s]Training CobwebTree:  45%|     | 4417/9738 [01:05<01:26, 61.22it/s]Training CobwebTree:  45%|     | 4424/9738 [01:05<01:25, 61.81it/s]Training CobwebTree:  46%|     | 4431/9738 [01:06<01:30, 58.73it/s]Training CobwebTree:  46%|     | 4438/9738 [01:06<01:26, 60.93it/s]Training CobwebTree:  46%|     | 4445/9738 [01:06<01:29, 59.21it/s]Training CobwebTree:  46%|     | 4451/9738 [01:06<01:29, 59.14it/s]Training CobwebTree:  46%|     | 4457/9738 [01:06<01:30, 58.60it/s]Training CobwebTree:  46%|     | 4463/9738 [01:06<01:31, 57.43it/s]Training CobwebTree:  46%|     | 4469/9738 [01:06<01:33, 56.43it/s]Training CobwebTree:  46%|     | 4475/9738 [01:06<01:31, 57.42it/s]Training CobwebTree:  46%|     | 4481/9738 [01:06<01:33, 56.24it/s]Training CobwebTree:  46%|     | 4487/9738 [01:06<01:32, 56.91it/s]Training CobwebTree:  46%|     | 4493/9738 [01:07<01:32, 56.81it/s]Training CobwebTree:  46%|     | 4499/9738 [01:07<01:33, 56.04it/s]Training CobwebTree:  46%|     | 4505/9738 [01:07<01:33, 55.78it/s]Training CobwebTree:  46%|     | 4511/9738 [01:07<01:32, 56.81it/s]Training CobwebTree:  46%|     | 4517/9738 [01:07<01:33, 55.92it/s]Training CobwebTree:  46%|     | 4523/9738 [01:07<01:33, 55.68it/s]Training CobwebTree:  47%|     | 4529/9738 [01:07<01:32, 56.60it/s]Training CobwebTree:  47%|     | 4535/9738 [01:07<01:32, 56.33it/s]Training CobwebTree:  47%|     | 4541/9738 [01:07<01:32, 56.45it/s]Training CobwebTree:  47%|     | 4547/9738 [01:08<01:31, 56.45it/s]Training CobwebTree:  47%|     | 4554/9738 [01:08<01:27, 59.13it/s]Training CobwebTree:  47%|     | 4560/9738 [01:08<01:30, 57.23it/s]Training CobwebTree:  47%|     | 4566/9738 [01:08<01:29, 57.93it/s]Training CobwebTree:  47%|     | 4572/9738 [01:08<01:29, 57.62it/s]Training CobwebTree:  47%|     | 4578/9738 [01:08<01:29, 57.87it/s]Training CobwebTree:  47%|     | 4585/9738 [01:08<01:27, 58.82it/s]Training CobwebTree:  47%|     | 4592/9738 [01:08<01:24, 60.93it/s]Training CobwebTree:  47%|     | 4599/9738 [01:08<01:25, 60.23it/s]Training CobwebTree:  47%|     | 4606/9738 [01:09<01:27, 58.95it/s]Training CobwebTree:  47%|     | 4612/9738 [01:09<01:30, 56.81it/s]Training CobwebTree:  47%|     | 4618/9738 [01:09<01:29, 57.14it/s]Training CobwebTree:  47%|     | 4624/9738 [01:09<01:30, 56.29it/s]Training CobwebTree:  48%|     | 4630/9738 [01:09<01:31, 55.91it/s]Training CobwebTree:  48%|     | 4636/9738 [01:09<01:30, 56.46it/s]Training CobwebTree:  48%|     | 4642/9738 [01:09<01:30, 56.22it/s]Training CobwebTree:  48%|     | 4649/9738 [01:09<01:25, 59.40it/s]Training CobwebTree:  48%|     | 4655/9738 [01:09<01:25, 59.29it/s]Training CobwebTree:  48%|     | 4661/9738 [01:10<01:26, 58.85it/s]Training CobwebTree:  48%|     | 4668/9738 [01:10<01:23, 60.51it/s]Training CobwebTree:  48%|     | 4675/9738 [01:10<01:23, 60.94it/s]Training CobwebTree:  48%|     | 4682/9738 [01:10<01:27, 57.90it/s]Training CobwebTree:  48%|     | 4688/9738 [01:10<01:26, 58.24it/s]Training CobwebTree:  48%|     | 4694/9738 [01:10<01:27, 57.69it/s]Training CobwebTree:  48%|     | 4701/9738 [01:10<01:25, 58.80it/s]Training CobwebTree:  48%|     | 4708/9738 [01:10<01:24, 59.53it/s]Training CobwebTree:  48%|     | 4714/9738 [01:10<01:26, 58.30it/s]Training CobwebTree:  48%|     | 4721/9738 [01:11<01:25, 58.92it/s]Training CobwebTree:  49%|     | 4728/9738 [01:11<01:23, 60.04it/s]Training CobwebTree:  49%|     | 4735/9738 [01:11<01:21, 61.53it/s]Training CobwebTree:  49%|     | 4742/9738 [01:11<01:20, 61.79it/s]Training CobwebTree:  49%|     | 4749/9738 [01:11<01:24, 59.21it/s]Training CobwebTree:  49%|     | 4755/9738 [01:11<01:24, 58.87it/s]Training CobwebTree:  49%|     | 4761/9738 [01:11<01:24, 58.58it/s]Training CobwebTree:  49%|     | 4768/9738 [01:11<01:23, 59.60it/s]Training CobwebTree:  49%|     | 4775/9738 [01:11<01:22, 60.01it/s]Training CobwebTree:  49%|     | 4782/9738 [01:12<01:26, 57.56it/s]Training CobwebTree:  49%|     | 4788/9738 [01:12<01:27, 56.35it/s]Training CobwebTree:  49%|     | 4795/9738 [01:12<01:25, 57.82it/s]Training CobwebTree:  49%|     | 4801/9738 [01:12<01:24, 58.25it/s]Training CobwebTree:  49%|     | 4808/9738 [01:12<01:21, 60.20it/s]Training CobwebTree:  49%|     | 4815/9738 [01:12<01:20, 61.06it/s]Training CobwebTree:  50%|     | 4822/9738 [01:12<01:22, 59.70it/s]Training CobwebTree:  50%|     | 4828/9738 [01:12<01:22, 59.48it/s]Training CobwebTree:  50%|     | 4835/9738 [01:12<01:20, 60.64it/s]Training CobwebTree:  50%|     | 4842/9738 [01:13<01:24, 57.97it/s]Training CobwebTree:  50%|     | 4848/9738 [01:13<01:23, 58.30it/s]Training CobwebTree:  50%|     | 4854/9738 [01:13<01:23, 58.35it/s]Training CobwebTree:  50%|     | 4860/9738 [01:13<01:23, 58.17it/s]Training CobwebTree:  50%|     | 4866/9738 [01:13<01:26, 56.11it/s]Training CobwebTree:  50%|     | 4873/9738 [01:13<01:21, 59.50it/s]Training CobwebTree:  50%|     | 4879/9738 [01:13<01:24, 57.50it/s]Training CobwebTree:  50%|     | 4885/9738 [01:13<01:24, 57.74it/s]Training CobwebTree:  50%|     | 4891/9738 [01:13<01:24, 57.66it/s]Training CobwebTree:  50%|     | 4898/9738 [01:14<01:20, 60.12it/s]Training CobwebTree:  50%|     | 4905/9738 [01:14<01:20, 59.77it/s]Training CobwebTree:  50%|     | 4911/9738 [01:14<01:21, 59.34it/s]Training CobwebTree:  50%|     | 4917/9738 [01:14<01:24, 57.22it/s]Training CobwebTree:  51%|     | 4923/9738 [01:14<01:25, 56.44it/s]Training CobwebTree:  51%|     | 4929/9738 [01:14<01:25, 55.99it/s]Training CobwebTree:  51%|     | 4936/9738 [01:14<01:23, 57.83it/s]Training CobwebTree:  51%|     | 4943/9738 [01:14<01:22, 58.23it/s]Training CobwebTree:  51%|     | 4950/9738 [01:14<01:19, 60.02it/s]Training CobwebTree:  51%|     | 4957/9738 [01:15<01:21, 58.52it/s]Training CobwebTree:  51%|     | 4964/9738 [01:15<01:20, 59.66it/s]Training CobwebTree:  51%|     | 4970/9738 [01:15<01:21, 58.69it/s]Training CobwebTree:  51%|     | 4977/9738 [01:15<01:20, 59.14it/s]Training CobwebTree:  51%|     | 4983/9738 [01:15<01:23, 56.65it/s]Training CobwebTree:  51%|     | 4989/9738 [01:15<01:26, 54.69it/s]Training CobwebTree:  51%|    | 4995/9738 [01:15<01:26, 54.56it/s]Training CobwebTree:  51%|    | 5001/9738 [01:15<01:27, 54.30it/s]Training CobwebTree:  51%|    | 5008/9738 [01:15<01:23, 56.40it/s]Training CobwebTree:  51%|    | 5014/9738 [01:16<01:23, 56.63it/s]Training CobwebTree:  52%|    | 5020/9738 [01:16<01:25, 55.48it/s]Training CobwebTree:  52%|    | 5026/9738 [01:16<01:23, 56.10it/s]Training CobwebTree:  52%|    | 5032/9738 [01:16<01:22, 57.12it/s]Training CobwebTree:  52%|    | 5039/9738 [01:16<01:18, 59.55it/s]Training CobwebTree:  52%|    | 5045/9738 [01:16<01:20, 58.53it/s]Training CobwebTree:  52%|    | 5052/9738 [01:16<01:19, 58.78it/s]Training CobwebTree:  52%|    | 5059/9738 [01:16<01:16, 61.17it/s]Training CobwebTree:  52%|    | 5066/9738 [01:16<01:17, 60.25it/s]Training CobwebTree:  52%|    | 5073/9738 [01:17<01:17, 60.41it/s]Training CobwebTree:  52%|    | 5080/9738 [01:17<01:18, 59.35it/s]Training CobwebTree:  52%|    | 5087/9738 [01:17<01:16, 60.44it/s]Training CobwebTree:  52%|    | 5094/9738 [01:17<01:16, 61.09it/s]Training CobwebTree:  52%|    | 5101/9738 [01:17<01:18, 58.87it/s]Training CobwebTree:  52%|    | 5108/9738 [01:17<01:18, 59.32it/s]Training CobwebTree:  53%|    | 5114/9738 [01:17<01:18, 58.68it/s]Training CobwebTree:  53%|    | 5120/9738 [01:17<01:20, 57.48it/s]Training CobwebTree:  53%|    | 5126/9738 [01:17<01:21, 56.74it/s]Training CobwebTree:  53%|    | 5133/9738 [01:18<01:18, 58.57it/s]Training CobwebTree:  53%|    | 5139/9738 [01:18<01:22, 56.04it/s]Training CobwebTree:  53%|    | 5146/9738 [01:18<01:18, 58.23it/s]Training CobwebTree:  53%|    | 5152/9738 [01:18<01:19, 57.33it/s]Training CobwebTree:  53%|    | 5158/9738 [01:18<01:20, 57.05it/s]Training CobwebTree:  53%|    | 5164/9738 [01:18<01:22, 55.29it/s]Training CobwebTree:  53%|    | 5170/9738 [01:18<01:23, 54.95it/s]Training CobwebTree:  53%|    | 5176/9738 [01:18<01:21, 55.99it/s]Training CobwebTree:  53%|    | 5183/9738 [01:18<01:18, 57.86it/s]Training CobwebTree:  53%|    | 5190/9738 [01:19<01:17, 58.67it/s]Training CobwebTree:  53%|    | 5197/9738 [01:19<01:16, 59.68it/s]Training CobwebTree:  53%|    | 5203/9738 [01:19<01:16, 59.56it/s]Training CobwebTree:  53%|    | 5209/9738 [01:19<01:16, 59.56it/s]Training CobwebTree:  54%|    | 5215/9738 [01:19<01:18, 57.95it/s]Training CobwebTree:  54%|    | 5222/9738 [01:19<01:15, 59.76it/s]Training CobwebTree:  54%|    | 5228/9738 [01:19<01:16, 59.28it/s]Training CobwebTree:  54%|    | 5234/9738 [01:19<01:16, 58.66it/s]Training CobwebTree:  54%|    | 5240/9738 [01:19<01:18, 57.61it/s]Training CobwebTree:  54%|    | 5246/9738 [01:20<01:18, 57.33it/s]Training CobwebTree:  54%|    | 5252/9738 [01:20<01:17, 57.80it/s]Training CobwebTree:  54%|    | 5258/9738 [01:20<01:18, 57.16it/s]Training CobwebTree:  54%|    | 5264/9738 [01:20<01:18, 57.21it/s]Training CobwebTree:  54%|    | 5270/9738 [01:20<01:19, 55.95it/s]Training CobwebTree:  54%|    | 5276/9738 [01:20<01:18, 56.71it/s]Training CobwebTree:  54%|    | 5283/9738 [01:20<01:16, 58.52it/s]Training CobwebTree:  54%|    | 5289/9738 [01:20<01:16, 57.85it/s]Training CobwebTree:  54%|    | 5296/9738 [01:20<01:14, 59.79it/s]Training CobwebTree:  54%|    | 5303/9738 [01:21<01:13, 59.99it/s]Training CobwebTree:  55%|    | 5309/9738 [01:21<01:17, 57.31it/s]Training CobwebTree:  55%|    | 5315/9738 [01:21<01:16, 57.93it/s]Training CobwebTree:  55%|    | 5321/9738 [01:21<01:16, 57.64it/s]Training CobwebTree:  55%|    | 5328/9738 [01:21<01:15, 58.79it/s]Training CobwebTree:  55%|    | 5335/9738 [01:21<01:14, 58.79it/s]Training CobwebTree:  55%|    | 5341/9738 [01:21<01:16, 57.83it/s]Training CobwebTree:  55%|    | 5348/9738 [01:21<01:13, 59.33it/s]Training CobwebTree:  55%|    | 5355/9738 [01:21<01:12, 60.37it/s]Training CobwebTree:  55%|    | 5362/9738 [01:22<01:13, 59.19it/s]Training CobwebTree:  55%|    | 5368/9738 [01:22<01:15, 57.72it/s]Training CobwebTree:  55%|    | 5374/9738 [01:22<01:18, 55.64it/s]Training CobwebTree:  55%|    | 5380/9738 [01:22<01:18, 55.66it/s]Training CobwebTree:  55%|    | 5387/9738 [01:22<01:16, 56.98it/s]Training CobwebTree:  55%|    | 5393/9738 [01:22<01:17, 56.19it/s]Training CobwebTree:  55%|    | 5400/9738 [01:22<01:14, 57.92it/s]Training CobwebTree:  56%|    | 5406/9738 [01:22<01:15, 57.19it/s]Training CobwebTree:  56%|    | 5412/9738 [01:22<01:15, 57.18it/s]Training CobwebTree:  56%|    | 5418/9738 [01:23<01:15, 57.14it/s]Training CobwebTree:  56%|    | 5424/9738 [01:23<01:14, 57.92it/s]Training CobwebTree:  56%|    | 5431/9738 [01:23<01:12, 59.01it/s]Training CobwebTree:  56%|    | 5437/9738 [01:23<01:13, 58.64it/s]Training CobwebTree:  56%|    | 5444/9738 [01:23<01:11, 60.17it/s]Training CobwebTree:  56%|    | 5451/9738 [01:23<01:14, 57.84it/s]Training CobwebTree:  56%|    | 5457/9738 [01:23<01:13, 58.26it/s]Training CobwebTree:  56%|    | 5464/9738 [01:23<01:11, 59.96it/s]Training CobwebTree:  56%|    | 5471/9738 [01:23<01:11, 60.01it/s]Training CobwebTree:  56%|    | 5478/9738 [01:24<01:13, 57.80it/s]Training CobwebTree:  56%|    | 5484/9738 [01:24<01:14, 56.76it/s]Training CobwebTree:  56%|    | 5491/9738 [01:24<01:13, 58.02it/s]Training CobwebTree:  56%|    | 5498/9738 [01:24<01:12, 58.84it/s]Training CobwebTree:  57%|    | 5505/9738 [01:24<01:11, 59.50it/s]Training CobwebTree:  57%|    | 5511/9738 [01:24<01:12, 58.42it/s]Training CobwebTree:  57%|    | 5518/9738 [01:24<01:10, 60.15it/s]Training CobwebTree:  57%|    | 5525/9738 [01:24<01:11, 59.09it/s]Training CobwebTree:  57%|    | 5531/9738 [01:24<01:11, 58.75it/s]Training CobwebTree:  57%|    | 5538/9738 [01:25<01:10, 59.94it/s]Training CobwebTree:  57%|    | 5544/9738 [01:25<01:10, 59.73it/s]Training CobwebTree:  57%|    | 5550/9738 [01:25<01:10, 59.44it/s]Training CobwebTree:  57%|    | 5556/9738 [01:25<01:11, 58.88it/s]Training CobwebTree:  57%|    | 5562/9738 [01:25<01:11, 58.71it/s]Training CobwebTree:  57%|    | 5568/9738 [01:25<01:12, 57.70it/s]Training CobwebTree:  57%|    | 5575/9738 [01:25<01:11, 58.14it/s]Training CobwebTree:  57%|    | 5581/9738 [01:25<01:10, 58.63it/s]Training CobwebTree:  57%|    | 5587/9738 [01:25<01:11, 58.10it/s]Training CobwebTree:  57%|    | 5593/9738 [01:25<01:10, 58.57it/s]Training CobwebTree:  58%|    | 5600/9738 [01:26<01:11, 58.02it/s]Training CobwebTree:  58%|    | 5606/9738 [01:26<01:11, 58.00it/s]Training CobwebTree:  58%|    | 5612/9738 [01:26<01:14, 55.32it/s]Training CobwebTree:  58%|    | 5618/9738 [01:26<01:15, 54.51it/s]Training CobwebTree:  58%|    | 5624/9738 [01:26<01:14, 55.36it/s]Training CobwebTree:  58%|    | 5631/9738 [01:26<01:10, 58.25it/s]Training CobwebTree:  58%|    | 5638/9738 [01:26<01:07, 60.43it/s]Training CobwebTree:  58%|    | 5645/9738 [01:26<01:08, 60.05it/s]Training CobwebTree:  58%|    | 5652/9738 [01:27<01:10, 57.62it/s]Training CobwebTree:  58%|    | 5659/9738 [01:27<01:10, 57.85it/s]Training CobwebTree:  58%|    | 5665/9738 [01:27<01:13, 55.69it/s]Training CobwebTree:  58%|    | 5671/9738 [01:27<01:14, 54.30it/s]Training CobwebTree:  58%|    | 5677/9738 [01:27<01:13, 54.95it/s]Training CobwebTree:  58%|    | 5683/9738 [01:27<01:14, 54.31it/s]Training CobwebTree:  58%|    | 5689/9738 [01:27<01:13, 55.12it/s]Training CobwebTree:  58%|    | 5695/9738 [01:27<01:14, 54.62it/s]Training CobwebTree:  59%|    | 5702/9738 [01:27<01:11, 56.41it/s]Training CobwebTree:  59%|    | 5708/9738 [01:28<01:12, 55.52it/s]Training CobwebTree:  59%|    | 5715/9738 [01:28<01:09, 58.22it/s]Training CobwebTree:  59%|    | 5721/9738 [01:28<01:09, 58.21it/s]Training CobwebTree:  59%|    | 5727/9738 [01:28<01:10, 57.18it/s]Training CobwebTree:  59%|    | 5734/9738 [01:28<01:08, 58.21it/s]Training CobwebTree:  59%|    | 5740/9738 [01:28<01:11, 55.81it/s]Training CobwebTree:  59%|    | 5746/9738 [01:28<01:10, 56.74it/s]Training CobwebTree:  59%|    | 5752/9738 [01:28<01:10, 56.91it/s]Training CobwebTree:  59%|    | 5759/9738 [01:28<01:08, 58.44it/s]Training CobwebTree:  59%|    | 5765/9738 [01:29<01:08, 58.01it/s]Training CobwebTree:  59%|    | 5771/9738 [01:29<01:08, 57.68it/s]Training CobwebTree:  59%|    | 5778/9738 [01:29<01:07, 58.39it/s]Training CobwebTree:  59%|    | 5784/9738 [01:29<01:08, 57.49it/s]Training CobwebTree:  59%|    | 5790/9738 [01:29<01:08, 57.83it/s]Training CobwebTree:  60%|    | 5796/9738 [01:29<01:08, 57.79it/s]Training CobwebTree:  60%|    | 5802/9738 [01:29<01:10, 56.12it/s]Training CobwebTree:  60%|    | 5808/9738 [01:29<01:10, 56.00it/s]Training CobwebTree:  60%|    | 5814/9738 [01:29<01:08, 57.06it/s]Training CobwebTree:  60%|    | 5820/9738 [01:29<01:09, 56.24it/s]Training CobwebTree:  60%|    | 5827/9738 [01:30<01:07, 58.27it/s]Training CobwebTree:  60%|    | 5834/9738 [01:30<01:06, 59.11it/s]Training CobwebTree:  60%|    | 5841/9738 [01:30<01:06, 58.84it/s]Training CobwebTree:  60%|    | 5847/9738 [01:30<01:06, 58.52it/s]Training CobwebTree:  60%|    | 5853/9738 [01:30<01:06, 58.80it/s]Training CobwebTree:  60%|    | 5859/9738 [01:30<01:06, 57.92it/s]Training CobwebTree:  60%|    | 5866/9738 [01:30<01:05, 58.93it/s]Training CobwebTree:  60%|    | 5872/9738 [01:30<01:06, 58.40it/s]Training CobwebTree:  60%|    | 5878/9738 [01:30<01:07, 57.54it/s]Training CobwebTree:  60%|    | 5884/9738 [01:31<01:06, 57.72it/s]Training CobwebTree:  60%|    | 5890/9738 [01:31<01:08, 56.20it/s]Training CobwebTree:  61%|    | 5896/9738 [01:31<01:09, 55.06it/s]Training CobwebTree:  61%|    | 5903/9738 [01:31<01:08, 55.73it/s]Training CobwebTree:  61%|    | 5909/9738 [01:31<01:07, 56.72it/s]Training CobwebTree:  61%|    | 5915/9738 [01:31<01:06, 57.15it/s]Training CobwebTree:  61%|    | 5921/9738 [01:31<01:08, 55.47it/s]Training CobwebTree:  61%|    | 5927/9738 [01:31<01:08, 55.81it/s]Training CobwebTree:  61%|    | 5933/9738 [01:31<01:08, 55.73it/s]Training CobwebTree:  61%|    | 5939/9738 [01:32<01:09, 54.93it/s]Training CobwebTree:  61%|    | 5945/9738 [01:32<01:07, 56.02it/s]Training CobwebTree:  61%|    | 5951/9738 [01:32<01:06, 56.67it/s]Training CobwebTree:  61%|    | 5957/9738 [01:32<01:06, 57.03it/s]Training CobwebTree:  61%|    | 5963/9738 [01:32<01:06, 57.16it/s]Training CobwebTree:  61%|   | 5969/9738 [01:32<01:07, 56.07it/s]Training CobwebTree:  61%|   | 5975/9738 [01:32<01:06, 56.68it/s]Training CobwebTree:  61%|   | 5981/9738 [01:32<01:06, 56.13it/s]Training CobwebTree:  61%|   | 5987/9738 [01:32<01:07, 55.55it/s]Training CobwebTree:  62%|   | 5994/9738 [01:33<01:04, 57.78it/s]Training CobwebTree:  62%|   | 6000/9738 [01:33<01:05, 56.95it/s]Training CobwebTree:  62%|   | 6006/9738 [01:33<01:05, 56.70it/s]Training CobwebTree:  62%|   | 6012/9738 [01:33<01:04, 57.38it/s]Training CobwebTree:  62%|   | 6019/9738 [01:33<01:03, 58.38it/s]Training CobwebTree:  62%|   | 6025/9738 [01:33<01:03, 58.45it/s]Training CobwebTree:  62%|   | 6032/9738 [01:33<01:02, 59.45it/s]Training CobwebTree:  62%|   | 6038/9738 [01:33<01:04, 57.44it/s]Training CobwebTree:  62%|   | 6044/9738 [01:33<01:06, 55.26it/s]Training CobwebTree:  62%|   | 6050/9738 [01:34<01:08, 54.17it/s]Training CobwebTree:  62%|   | 6056/9738 [01:34<01:06, 55.36it/s]Training CobwebTree:  62%|   | 6062/9738 [01:34<01:06, 55.38it/s]Training CobwebTree:  62%|   | 6068/9738 [01:34<01:06, 55.45it/s]Training CobwebTree:  62%|   | 6074/9738 [01:34<01:05, 56.32it/s]Training CobwebTree:  62%|   | 6081/9738 [01:34<01:03, 57.70it/s]Training CobwebTree:  63%|   | 6088/9738 [01:34<01:01, 59.39it/s]Training CobwebTree:  63%|   | 6094/9738 [01:34<01:01, 59.32it/s]Training CobwebTree:  63%|   | 6100/9738 [01:34<01:01, 59.39it/s]Training CobwebTree:  63%|   | 6106/9738 [01:34<01:01, 59.53it/s]Training CobwebTree:  63%|   | 6112/9738 [01:35<01:02, 57.98it/s]Training CobwebTree:  63%|   | 6119/9738 [01:35<00:59, 60.70it/s]Training CobwebTree:  63%|   | 6126/9738 [01:35<01:02, 57.36it/s]Training CobwebTree:  63%|   | 6132/9738 [01:35<01:03, 56.86it/s]Training CobwebTree:  63%|   | 6138/9738 [01:35<01:03, 56.31it/s]Training CobwebTree:  63%|   | 6144/9738 [01:35<01:05, 55.12it/s]Training CobwebTree:  63%|   | 6150/9738 [01:35<01:05, 54.92it/s]Training CobwebTree:  63%|   | 6156/9738 [01:35<01:03, 56.10it/s]Training CobwebTree:  63%|   | 6162/9738 [01:35<01:02, 56.87it/s]Training CobwebTree:  63%|   | 6168/9738 [01:36<01:02, 57.04it/s]Training CobwebTree:  63%|   | 6174/9738 [01:36<01:01, 57.61it/s]Training CobwebTree:  63%|   | 6180/9738 [01:36<01:02, 57.36it/s]Training CobwebTree:  64%|   | 6186/9738 [01:36<01:01, 57.43it/s]Training CobwebTree:  64%|   | 6192/9738 [01:36<01:02, 57.18it/s]Training CobwebTree:  64%|   | 6199/9738 [01:36<00:59, 59.39it/s]Training CobwebTree:  64%|   | 6205/9738 [01:36<00:59, 59.26it/s]Training CobwebTree:  64%|   | 6211/9738 [01:36<01:01, 57.77it/s]Training CobwebTree:  64%|   | 6217/9738 [01:36<01:01, 57.16it/s]Training CobwebTree:  64%|   | 6223/9738 [01:37<01:01, 57.24it/s]Training CobwebTree:  64%|   | 6229/9738 [01:37<01:01, 57.48it/s]Training CobwebTree:  64%|   | 6235/9738 [01:37<01:02, 56.32it/s]Training CobwebTree:  64%|   | 6241/9738 [01:37<01:03, 55.45it/s]Training CobwebTree:  64%|   | 6247/9738 [01:37<01:02, 56.07it/s]Training CobwebTree:  64%|   | 6253/9738 [01:37<01:02, 55.71it/s]Training CobwebTree:  64%|   | 6260/9738 [01:37<00:59, 58.36it/s]Training CobwebTree:  64%|   | 6266/9738 [01:37<00:59, 58.40it/s]Training CobwebTree:  64%|   | 6272/9738 [01:37<00:59, 57.91it/s]Training CobwebTree:  64%|   | 6278/9738 [01:38<01:00, 56.83it/s]Training CobwebTree:  65%|   | 6284/9738 [01:38<01:00, 57.04it/s]Training CobwebTree:  65%|   | 6290/9738 [01:38<01:01, 56.32it/s]Training CobwebTree:  65%|   | 6296/9738 [01:38<01:00, 56.71it/s]Training CobwebTree:  65%|   | 6302/9738 [01:38<01:00, 57.22it/s]Training CobwebTree:  65%|   | 6308/9738 [01:38<00:59, 58.01it/s]Training CobwebTree:  65%|   | 6314/9738 [01:38<00:59, 57.96it/s]Training CobwebTree:  65%|   | 6320/9738 [01:38<00:59, 57.42it/s]Training CobwebTree:  65%|   | 6326/9738 [01:38<00:59, 57.41it/s]Training CobwebTree:  65%|   | 6333/9738 [01:38<00:58, 58.52it/s]Training CobwebTree:  65%|   | 6339/9738 [01:39<00:57, 58.76it/s]Training CobwebTree:  65%|   | 6345/9738 [01:39<00:58, 57.73it/s]Training CobwebTree:  65%|   | 6352/9738 [01:39<00:57, 58.56it/s]Training CobwebTree:  65%|   | 6359/9738 [01:39<00:56, 60.17it/s]Training CobwebTree:  65%|   | 6366/9738 [01:39<00:57, 58.15it/s]Training CobwebTree:  65%|   | 6372/9738 [01:39<01:00, 55.91it/s]Training CobwebTree:  65%|   | 6378/9738 [01:39<01:00, 55.82it/s]Training CobwebTree:  66%|   | 6384/9738 [01:39<01:00, 55.05it/s]Training CobwebTree:  66%|   | 6391/9738 [01:39<00:59, 56.33it/s]Training CobwebTree:  66%|   | 6397/9738 [01:40<00:59, 56.02it/s]Training CobwebTree:  66%|   | 6403/9738 [01:40<00:59, 56.26it/s]Training CobwebTree:  66%|   | 6410/9738 [01:40<00:57, 57.75it/s]Training CobwebTree:  66%|   | 6416/9738 [01:40<00:57, 57.83it/s]Training CobwebTree:  66%|   | 6422/9738 [01:40<00:57, 57.77it/s]Training CobwebTree:  66%|   | 6428/9738 [01:40<00:58, 56.11it/s]Training CobwebTree:  66%|   | 6434/9738 [01:40<01:00, 54.88it/s]Training CobwebTree:  66%|   | 6440/9738 [01:40<01:00, 54.19it/s]Training CobwebTree:  66%|   | 6446/9738 [01:40<01:01, 53.49it/s]Training CobwebTree:  66%|   | 6452/9738 [01:41<01:00, 53.98it/s]Training CobwebTree:  66%|   | 6458/9738 [01:41<01:00, 54.56it/s]Training CobwebTree:  66%|   | 6464/9738 [01:41<01:00, 54.03it/s]Training CobwebTree:  66%|   | 6471/9738 [01:41<00:58, 55.99it/s]Training CobwebTree:  67%|   | 6477/9738 [01:41<00:57, 56.24it/s]Training CobwebTree:  67%|   | 6483/9738 [01:41<00:57, 56.12it/s]Training CobwebTree:  67%|   | 6489/9738 [01:41<00:57, 56.37it/s]Training CobwebTree:  67%|   | 6495/9738 [01:41<00:59, 54.28it/s]Training CobwebTree:  67%|   | 6501/9738 [01:41<00:58, 55.31it/s]Training CobwebTree:  67%|   | 6507/9738 [01:42<00:58, 55.58it/s]Training CobwebTree:  67%|   | 6513/9738 [01:42<00:58, 55.06it/s]Training CobwebTree:  67%|   | 6520/9738 [01:42<00:56, 56.63it/s]Training CobwebTree:  67%|   | 6526/9738 [01:42<00:56, 56.63it/s]Training CobwebTree:  67%|   | 6532/9738 [01:42<00:56, 56.90it/s]Training CobwebTree:  67%|   | 6538/9738 [01:42<00:57, 55.72it/s]Training CobwebTree:  67%|   | 6544/9738 [01:42<00:56, 56.08it/s]Training CobwebTree:  67%|   | 6550/9738 [01:42<00:56, 56.01it/s]Training CobwebTree:  67%|   | 6556/9738 [01:42<00:57, 55.56it/s]Training CobwebTree:  67%|   | 6562/9738 [01:43<00:56, 56.11it/s]Training CobwebTree:  67%|   | 6568/9738 [01:43<00:57, 54.69it/s]Training CobwebTree:  68%|   | 6575/9738 [01:43<00:54, 58.37it/s]Training CobwebTree:  68%|   | 6581/9738 [01:43<00:54, 58.14it/s]Training CobwebTree:  68%|   | 6587/9738 [01:43<00:54, 57.88it/s]Training CobwebTree:  68%|   | 6593/9738 [01:43<00:55, 56.29it/s]Training CobwebTree:  68%|   | 6599/9738 [01:43<00:55, 56.38it/s]Training CobwebTree:  68%|   | 6605/9738 [01:43<00:55, 56.44it/s]Training CobwebTree:  68%|   | 6611/9738 [01:43<00:55, 56.60it/s]Training CobwebTree:  68%|   | 6617/9738 [01:44<00:56, 55.27it/s]Training CobwebTree:  68%|   | 6623/9738 [01:44<00:56, 55.24it/s]Training CobwebTree:  68%|   | 6629/9738 [01:44<00:55, 56.33it/s]Training CobwebTree:  68%|   | 6635/9738 [01:44<00:56, 55.31it/s]Training CobwebTree:  68%|   | 6641/9738 [01:44<00:57, 54.29it/s]Training CobwebTree:  68%|   | 6647/9738 [01:44<00:57, 53.75it/s]Training CobwebTree:  68%|   | 6653/9738 [01:44<00:58, 53.01it/s]Training CobwebTree:  68%|   | 6659/9738 [01:44<00:58, 52.94it/s]Training CobwebTree:  68%|   | 6665/9738 [01:44<00:56, 54.38it/s]Training CobwebTree:  69%|   | 6671/9738 [01:45<00:56, 53.93it/s]Training CobwebTree:  69%|   | 6677/9738 [01:45<00:56, 53.92it/s]Training CobwebTree:  69%|   | 6683/9738 [01:45<00:55, 54.76it/s]Training CobwebTree:  69%|   | 6689/9738 [01:45<00:54, 55.53it/s]Training CobwebTree:  69%|   | 6695/9738 [01:45<00:53, 56.71it/s]Training CobwebTree:  69%|   | 6701/9738 [01:45<00:53, 56.84it/s]Training CobwebTree:  69%|   | 6707/9738 [01:45<00:55, 54.72it/s]Training CobwebTree:  69%|   | 6713/9738 [01:45<00:54, 55.17it/s]Training CobwebTree:  69%|   | 6719/9738 [01:45<00:54, 55.88it/s]Training CobwebTree:  69%|   | 6725/9738 [01:45<00:55, 54.48it/s]Training CobwebTree:  69%|   | 6731/9738 [01:46<00:53, 55.85it/s]Training CobwebTree:  69%|   | 6737/9738 [01:46<00:53, 55.61it/s]Training CobwebTree:  69%|   | 6743/9738 [01:46<00:54, 55.35it/s]Training CobwebTree:  69%|   | 6749/9738 [01:46<00:55, 54.01it/s]Training CobwebTree:  69%|   | 6755/9738 [01:46<00:56, 52.92it/s]Training CobwebTree:  69%|   | 6761/9738 [01:46<00:55, 53.71it/s]Training CobwebTree:  69%|   | 6767/9738 [01:46<00:57, 52.07it/s]Training CobwebTree:  70%|   | 6773/9738 [01:46<00:57, 51.80it/s]Training CobwebTree:  70%|   | 6779/9738 [01:47<00:55, 53.48it/s]Training CobwebTree:  70%|   | 6785/9738 [01:47<00:54, 54.40it/s]Training CobwebTree:  70%|   | 6791/9738 [01:47<00:54, 54.32it/s]Training CobwebTree:  70%|   | 6797/9738 [01:47<00:53, 54.65it/s]Training CobwebTree:  70%|   | 6804/9738 [01:47<00:51, 56.44it/s]Training CobwebTree:  70%|   | 6810/9738 [01:47<00:52, 55.45it/s]Training CobwebTree:  70%|   | 6817/9738 [01:47<00:51, 56.56it/s]Training CobwebTree:  70%|   | 6824/9738 [01:47<00:50, 57.46it/s]Training CobwebTree:  70%|   | 6830/9738 [01:47<00:51, 56.25it/s]Training CobwebTree:  70%|   | 6837/9738 [01:48<00:50, 57.75it/s]Training CobwebTree:  70%|   | 6843/9738 [01:48<00:49, 57.97it/s]Training CobwebTree:  70%|   | 6849/9738 [01:48<00:50, 57.46it/s]Training CobwebTree:  70%|   | 6855/9738 [01:48<00:49, 58.12it/s]Training CobwebTree:  70%|   | 6862/9738 [01:48<00:49, 58.11it/s]Training CobwebTree:  71%|   | 6868/9738 [01:48<00:50, 57.26it/s]Training CobwebTree:  71%|   | 6874/9738 [01:48<00:49, 57.33it/s]Training CobwebTree:  71%|   | 6880/9738 [01:48<00:51, 55.60it/s]Training CobwebTree:  71%|   | 6887/9738 [01:48<00:49, 57.06it/s]Training CobwebTree:  71%|   | 6893/9738 [01:48<00:49, 57.12it/s]Training CobwebTree:  71%|   | 6900/9738 [01:49<00:48, 58.34it/s]Training CobwebTree:  71%|   | 6906/9738 [01:49<00:48, 57.89it/s]Training CobwebTree:  71%|   | 6912/9738 [01:49<00:49, 56.62it/s]Training CobwebTree:  71%|   | 6918/9738 [01:49<00:49, 56.65it/s]Training CobwebTree:  71%|   | 6924/9738 [01:49<00:50, 55.92it/s]Training CobwebTree:  71%|   | 6930/9738 [01:49<00:50, 55.11it/s]Training CobwebTree:  71%|   | 6936/9738 [01:49<00:51, 54.80it/s]Training CobwebTree:  71%|  | 6942/9738 [01:49<00:50, 55.44it/s]Training CobwebTree:  71%|  | 6948/9738 [01:49<00:51, 53.79it/s]Training CobwebTree:  71%|  | 6954/9738 [01:50<00:52, 53.18it/s]Training CobwebTree:  71%|  | 6960/9738 [01:50<00:51, 54.19it/s]Training CobwebTree:  72%|  | 6967/9738 [01:50<00:50, 54.92it/s]Training CobwebTree:  72%|  | 6973/9738 [01:50<00:49, 55.94it/s]Training CobwebTree:  72%|  | 6979/9738 [01:50<00:48, 56.97it/s]Training CobwebTree:  72%|  | 6986/9738 [01:50<00:47, 58.05it/s]Training CobwebTree:  72%|  | 6992/9738 [01:50<00:47, 57.35it/s]Training CobwebTree:  72%|  | 6998/9738 [01:50<00:47, 57.23it/s]Training CobwebTree:  72%|  | 7004/9738 [01:50<00:47, 57.29it/s]Training CobwebTree:  72%|  | 7010/9738 [01:51<00:48, 56.57it/s]Training CobwebTree:  72%|  | 7016/9738 [01:51<00:48, 55.74it/s]Training CobwebTree:  72%|  | 7022/9738 [01:51<00:48, 55.82it/s]Training CobwebTree:  72%|  | 7028/9738 [01:51<00:48, 55.85it/s]Training CobwebTree:  72%|  | 7034/9738 [01:51<00:48, 55.69it/s]Training CobwebTree:  72%|  | 7040/9738 [01:51<00:48, 56.19it/s]Training CobwebTree:  72%|  | 7046/9738 [01:51<00:47, 57.00it/s]Training CobwebTree:  72%|  | 7052/9738 [01:51<00:47, 56.72it/s]Training CobwebTree:  72%|  | 7059/9738 [01:51<00:46, 57.68it/s]Training CobwebTree:  73%|  | 7065/9738 [01:52<00:47, 55.80it/s]Training CobwebTree:  73%|  | 7071/9738 [01:52<00:47, 55.93it/s]Training CobwebTree:  73%|  | 7077/9738 [01:52<00:46, 56.69it/s]Training CobwebTree:  73%|  | 7083/9738 [01:52<00:48, 54.34it/s]Training CobwebTree:  73%|  | 7089/9738 [01:52<00:48, 54.58it/s]Training CobwebTree:  73%|  | 7096/9738 [01:52<00:46, 57.20it/s]Training CobwebTree:  73%|  | 7102/9738 [01:52<00:48, 54.23it/s]Training CobwebTree:  73%|  | 7108/9738 [01:52<00:47, 55.12it/s]Training CobwebTree:  73%|  | 7114/9738 [01:52<00:47, 54.95it/s]Training CobwebTree:  73%|  | 7120/9738 [01:53<00:46, 55.82it/s]Training CobwebTree:  73%|  | 7126/9738 [01:53<00:48, 54.06it/s]Training CobwebTree:  73%|  | 7132/9738 [01:53<00:49, 52.65it/s]Training CobwebTree:  73%|  | 7138/9738 [01:53<00:48, 53.63it/s]Training CobwebTree:  73%|  | 7144/9738 [01:53<00:47, 54.30it/s]Training CobwebTree:  73%|  | 7150/9738 [01:53<00:46, 55.28it/s]Training CobwebTree:  73%|  | 7156/9738 [01:53<00:48, 53.65it/s]Training CobwebTree:  74%|  | 7162/9738 [01:53<00:47, 54.06it/s]Training CobwebTree:  74%|  | 7168/9738 [01:53<00:46, 55.66it/s]Training CobwebTree:  74%|  | 7175/9738 [01:54<00:44, 57.16it/s]Training CobwebTree:  74%|  | 7181/9738 [01:54<00:44, 56.85it/s]Training CobwebTree:  74%|  | 7187/9738 [01:54<00:44, 57.44it/s]Training CobwebTree:  74%|  | 7194/9738 [01:54<00:42, 59.67it/s]Training CobwebTree:  74%|  | 7200/9738 [01:54<00:43, 58.78it/s]Training CobwebTree:  74%|  | 7207/9738 [01:54<00:44, 57.24it/s]Training CobwebTree:  74%|  | 7213/9738 [01:54<00:45, 55.57it/s]Training CobwebTree:  74%|  | 7220/9738 [01:54<00:43, 57.39it/s]Training CobwebTree:  74%|  | 7226/9738 [01:54<00:43, 57.27it/s]Training CobwebTree:  74%|  | 7232/9738 [01:55<00:45, 54.66it/s]Training CobwebTree:  74%|  | 7239/9738 [01:55<00:43, 57.07it/s]Training CobwebTree:  74%|  | 7246/9738 [01:55<00:42, 58.71it/s]Training CobwebTree:  74%|  | 7252/9738 [01:55<00:43, 57.32it/s]Training CobwebTree:  75%|  | 7258/9738 [01:55<00:43, 56.89it/s]Training CobwebTree:  75%|  | 7264/9738 [01:55<00:42, 57.75it/s]Training CobwebTree:  75%|  | 7270/9738 [01:55<00:43, 56.94it/s]Training CobwebTree:  75%|  | 7276/9738 [01:55<00:43, 56.66it/s]Training CobwebTree:  75%|  | 7283/9738 [01:55<00:42, 58.27it/s]Training CobwebTree:  75%|  | 7289/9738 [01:56<00:43, 55.95it/s]Training CobwebTree:  75%|  | 7295/9738 [01:56<00:43, 55.60it/s]Training CobwebTree:  75%|  | 7301/9738 [01:56<00:43, 56.10it/s]Training CobwebTree:  75%|  | 7307/9738 [01:56<00:43, 55.93it/s]Training CobwebTree:  75%|  | 7313/9738 [01:56<00:43, 55.59it/s]Training CobwebTree:  75%|  | 7319/9738 [01:56<00:44, 54.97it/s]Training CobwebTree:  75%|  | 7326/9738 [01:56<00:42, 56.67it/s]Training CobwebTree:  75%|  | 7333/9738 [01:56<00:41, 58.08it/s]Training CobwebTree:  75%|  | 7339/9738 [01:56<00:42, 56.58it/s]Training CobwebTree:  75%|  | 7345/9738 [01:57<00:42, 55.87it/s]Training CobwebTree:  75%|  | 7352/9738 [01:57<00:40, 58.33it/s]Training CobwebTree:  76%|  | 7358/9738 [01:57<00:40, 58.47it/s]Training CobwebTree:  76%|  | 7364/9738 [01:57<00:41, 56.85it/s]Training CobwebTree:  76%|  | 7370/9738 [01:57<00:41, 56.72it/s]Training CobwebTree:  76%|  | 7376/9738 [01:57<00:42, 55.96it/s]Training CobwebTree:  76%|  | 7382/9738 [01:57<00:44, 53.37it/s]Training CobwebTree:  76%|  | 7389/9738 [01:57<00:42, 55.43it/s]Training CobwebTree:  76%|  | 7395/9738 [01:57<00:42, 55.26it/s]Training CobwebTree:  76%|  | 7401/9738 [01:58<00:42, 55.22it/s]Training CobwebTree:  76%|  | 7408/9738 [01:58<00:40, 57.04it/s]Training CobwebTree:  76%|  | 7414/9738 [01:58<00:41, 56.16it/s]Training CobwebTree:  76%|  | 7420/9738 [01:58<00:41, 56.52it/s]Training CobwebTree:  76%|  | 7427/9738 [01:58<00:39, 58.83it/s]Training CobwebTree:  76%|  | 7433/9738 [01:58<00:40, 57.20it/s]Training CobwebTree:  76%|  | 7439/9738 [01:58<00:42, 54.57it/s]Training CobwebTree:  76%|  | 7445/9738 [01:58<00:41, 55.44it/s]Training CobwebTree:  77%|  | 7452/9738 [01:58<00:39, 57.66it/s]Training CobwebTree:  77%|  | 7458/9738 [01:59<00:39, 57.56it/s]Training CobwebTree:  77%|  | 7464/9738 [01:59<00:39, 58.15it/s]Training CobwebTree:  77%|  | 7470/9738 [01:59<00:39, 57.90it/s]Training CobwebTree:  77%|  | 7476/9738 [01:59<00:38, 58.05it/s]Training CobwebTree:  77%|  | 7482/9738 [01:59<00:39, 57.60it/s]Training CobwebTree:  77%|  | 7488/9738 [01:59<00:40, 56.19it/s]Training CobwebTree:  77%|  | 7494/9738 [01:59<00:40, 56.05it/s]Training CobwebTree:  77%|  | 7500/9738 [01:59<00:40, 55.47it/s]Training CobwebTree:  77%|  | 7506/9738 [01:59<00:39, 56.41it/s]Training CobwebTree:  77%|  | 7512/9738 [02:00<00:39, 56.00it/s]Training CobwebTree:  77%|  | 7518/9738 [02:00<00:39, 56.28it/s]Training CobwebTree:  77%|  | 7524/9738 [02:00<00:39, 55.69it/s]Training CobwebTree:  77%|  | 7530/9738 [02:00<00:40, 55.06it/s]Training CobwebTree:  77%|  | 7536/9738 [02:00<00:41, 53.11it/s]Training CobwebTree:  77%|  | 7542/9738 [02:00<00:41, 53.31it/s]Training CobwebTree:  78%|  | 7549/9738 [02:00<00:39, 55.08it/s]Training CobwebTree:  78%|  | 7556/9738 [02:00<00:38, 56.67it/s]Training CobwebTree:  78%|  | 7562/9738 [02:00<00:38, 56.03it/s]Training CobwebTree:  78%|  | 7568/9738 [02:01<00:38, 56.59it/s]Training CobwebTree:  78%|  | 7574/9738 [02:01<00:38, 55.86it/s]Training CobwebTree:  78%|  | 7580/9738 [02:01<00:39, 55.16it/s]Training CobwebTree:  78%|  | 7586/9738 [02:01<00:38, 55.83it/s]Training CobwebTree:  78%|  | 7593/9738 [02:01<00:37, 57.80it/s]Training CobwebTree:  78%|  | 7599/9738 [02:01<00:36, 58.18it/s]Training CobwebTree:  78%|  | 7605/9738 [02:01<00:37, 57.19it/s]Training CobwebTree:  78%|  | 7612/9738 [02:01<00:36, 58.66it/s]Training CobwebTree:  78%|  | 7619/9738 [02:01<00:35, 59.33it/s]Training CobwebTree:  78%|  | 7625/9738 [02:02<00:37, 56.99it/s]Training CobwebTree:  78%|  | 7631/9738 [02:02<00:36, 57.18it/s]Training CobwebTree:  78%|  | 7637/9738 [02:02<00:37, 55.68it/s]Training CobwebTree:  78%|  | 7643/9738 [02:02<00:37, 55.36it/s]Training CobwebTree:  79%|  | 7650/9738 [02:02<00:36, 57.09it/s]Training CobwebTree:  79%|  | 7657/9738 [02:02<00:35, 58.95it/s]Training CobwebTree:  79%|  | 7663/9738 [02:02<00:35, 57.83it/s]Training CobwebTree:  79%|  | 7669/9738 [02:02<00:36, 57.34it/s]Training CobwebTree:  79%|  | 7675/9738 [02:02<00:37, 55.37it/s]Training CobwebTree:  79%|  | 7681/9738 [02:03<00:37, 55.51it/s]Training CobwebTree:  79%|  | 7687/9738 [02:03<00:36, 56.41it/s]Training CobwebTree:  79%|  | 7693/9738 [02:03<00:36, 56.71it/s]Training CobwebTree:  79%|  | 7700/9738 [02:03<00:35, 57.85it/s]Training CobwebTree:  79%|  | 7706/9738 [02:03<00:34, 58.08it/s]Training CobwebTree:  79%|  | 7712/9738 [02:03<00:36, 55.98it/s]Training CobwebTree:  79%|  | 7718/9738 [02:03<00:35, 56.64it/s]Training CobwebTree:  79%|  | 7724/9738 [02:03<00:36, 54.92it/s]Training CobwebTree:  79%|  | 7730/9738 [02:03<00:36, 54.89it/s]Training CobwebTree:  79%|  | 7736/9738 [02:03<00:36, 54.75it/s]Training CobwebTree:  80%|  | 7742/9738 [02:04<00:36, 55.23it/s]Training CobwebTree:  80%|  | 7748/9738 [02:04<00:36, 54.29it/s]Training CobwebTree:  80%|  | 7754/9738 [02:04<00:36, 54.42it/s]Training CobwebTree:  80%|  | 7760/9738 [02:04<00:35, 55.06it/s]Training CobwebTree:  80%|  | 7766/9738 [02:04<00:36, 53.61it/s]Training CobwebTree:  80%|  | 7772/9738 [02:04<00:36, 53.86it/s]Training CobwebTree:  80%|  | 7778/9738 [02:04<00:37, 52.81it/s]Training CobwebTree:  80%|  | 7784/9738 [02:04<00:36, 54.27it/s]Training CobwebTree:  80%|  | 7790/9738 [02:04<00:36, 53.67it/s]Training CobwebTree:  80%|  | 7796/9738 [02:05<00:37, 51.54it/s]Training CobwebTree:  80%|  | 7802/9738 [02:05<00:36, 52.66it/s]Training CobwebTree:  80%|  | 7808/9738 [02:05<00:37, 51.94it/s]Training CobwebTree:  80%|  | 7814/9738 [02:05<00:37, 50.96it/s]Training CobwebTree:  80%|  | 7820/9738 [02:05<00:36, 53.08it/s]Training CobwebTree:  80%|  | 7826/9738 [02:05<00:35, 54.41it/s]Training CobwebTree:  80%|  | 7832/9738 [02:05<00:35, 53.93it/s]Training CobwebTree:  80%|  | 7838/9738 [02:05<00:35, 53.97it/s]Training CobwebTree:  81%|  | 7844/9738 [02:06<00:35, 54.00it/s]Training CobwebTree:  81%|  | 7850/9738 [02:06<00:36, 52.32it/s]Training CobwebTree:  81%|  | 7857/9738 [02:06<00:33, 55.80it/s]Training CobwebTree:  81%|  | 7863/9738 [02:06<00:33, 55.48it/s]Training CobwebTree:  81%|  | 7869/9738 [02:06<00:33, 55.58it/s]Training CobwebTree:  81%|  | 7875/9738 [02:06<00:33, 55.96it/s]Training CobwebTree:  81%|  | 7881/9738 [02:06<00:32, 56.89it/s]Training CobwebTree:  81%|  | 7888/9738 [02:06<00:31, 58.31it/s]Training CobwebTree:  81%|  | 7895/9738 [02:06<00:30, 60.18it/s]Training CobwebTree:  81%|  | 7902/9738 [02:07<00:31, 58.51it/s]Training CobwebTree:  81%|  | 7908/9738 [02:07<00:32, 57.01it/s]Training CobwebTree:  81%| | 7914/9738 [02:07<00:31, 57.47it/s]Training CobwebTree:  81%| | 7920/9738 [02:07<00:31, 57.91it/s]Training CobwebTree:  81%| | 7927/9738 [02:07<00:31, 58.11it/s]Training CobwebTree:  81%| | 7933/9738 [02:07<00:31, 56.48it/s]Training CobwebTree:  82%| | 7940/9738 [02:07<00:31, 56.93it/s]Training CobwebTree:  82%| | 7946/9738 [02:07<00:31, 56.79it/s]Training CobwebTree:  82%| | 7952/9738 [02:07<00:32, 55.61it/s]Training CobwebTree:  82%| | 7959/9738 [02:08<00:31, 56.75it/s]Training CobwebTree:  82%| | 7965/9738 [02:08<00:31, 55.63it/s]Training CobwebTree:  82%| | 7972/9738 [02:08<00:30, 57.11it/s]Training CobwebTree:  82%| | 7978/9738 [02:08<00:31, 55.28it/s]Training CobwebTree:  82%| | 7984/9738 [02:08<00:32, 53.67it/s]Training CobwebTree:  82%| | 7991/9738 [02:08<00:30, 56.40it/s]Training CobwebTree:  82%| | 7997/9738 [02:08<00:31, 54.91it/s]Training CobwebTree:  82%| | 8003/9738 [02:08<00:31, 55.60it/s]Training CobwebTree:  82%| | 8009/9738 [02:08<00:31, 55.70it/s]Training CobwebTree:  82%| | 8015/9738 [02:09<00:31, 55.48it/s]Training CobwebTree:  82%| | 8021/9738 [02:09<00:31, 53.69it/s]Training CobwebTree:  82%| | 8027/9738 [02:09<00:32, 53.08it/s]Training CobwebTree:  82%| | 8033/9738 [02:09<00:31, 53.99it/s]Training CobwebTree:  83%| | 8039/9738 [02:09<00:31, 53.84it/s]Training CobwebTree:  83%| | 8046/9738 [02:09<00:30, 55.79it/s]Training CobwebTree:  83%| | 8052/9738 [02:09<00:29, 56.37it/s]Training CobwebTree:  83%| | 8058/9738 [02:09<00:29, 56.88it/s]Training CobwebTree:  83%| | 8064/9738 [02:09<00:30, 55.65it/s]Training CobwebTree:  83%| | 8070/9738 [02:10<00:29, 55.97it/s]Training CobwebTree:  83%| | 8076/9738 [02:10<00:29, 55.86it/s]Training CobwebTree:  83%| | 8082/9738 [02:10<00:30, 53.88it/s]Training CobwebTree:  83%| | 8088/9738 [02:10<01:07, 24.47it/s]Training CobwebTree:  83%| | 8095/9738 [02:10<00:53, 30.68it/s]Training CobwebTree:  83%| | 8101/9738 [02:11<00:45, 35.60it/s]Training CobwebTree:  83%| | 8107/9738 [02:11<00:41, 39.00it/s]Training CobwebTree:  83%| | 8113/9738 [02:11<00:38, 41.95it/s]Training CobwebTree:  83%| | 8119/9738 [02:11<00:35, 45.55it/s]Training CobwebTree:  83%| | 8125/9738 [02:11<00:34, 46.12it/s]Training CobwebTree:  83%| | 8131/9738 [02:11<00:33, 47.45it/s]Training CobwebTree:  84%| | 8137/9738 [02:11<00:32, 49.79it/s]Training CobwebTree:  84%| | 8143/9738 [02:11<00:30, 51.91it/s]Training CobwebTree:  84%| | 8149/9738 [02:11<00:30, 52.36it/s]Training CobwebTree:  84%| | 8156/9738 [02:12<00:28, 55.23it/s]Training CobwebTree:  84%| | 8162/9738 [02:12<00:28, 55.65it/s]Training CobwebTree:  84%| | 8168/9738 [02:12<00:28, 55.78it/s]Training CobwebTree:  84%| | 8174/9738 [02:12<00:28, 55.05it/s]Training CobwebTree:  84%| | 8180/9738 [02:12<00:27, 55.97it/s]Training CobwebTree:  84%| | 8186/9738 [02:12<00:29, 53.39it/s]Training CobwebTree:  84%| | 8192/9738 [02:12<00:28, 54.69it/s]Training CobwebTree:  84%| | 8198/9738 [02:12<00:28, 53.63it/s]Training CobwebTree:  84%| | 8204/9738 [02:12<00:28, 53.74it/s]Training CobwebTree:  84%| | 8210/9738 [02:13<00:28, 54.33it/s]Training CobwebTree:  84%| | 8216/9738 [02:13<00:27, 54.49it/s]Training CobwebTree:  84%| | 8222/9738 [02:13<00:27, 54.59it/s]Training CobwebTree:  85%| | 8229/9738 [02:13<00:26, 56.16it/s]Training CobwebTree:  85%| | 8235/9738 [02:13<00:27, 55.28it/s]Training CobwebTree:  85%| | 8241/9738 [02:13<00:26, 56.13it/s]Training CobwebTree:  85%| | 8247/9738 [02:13<00:26, 55.30it/s]Training CobwebTree:  85%| | 8253/9738 [02:13<00:27, 54.76it/s]Training CobwebTree:  85%| | 8259/9738 [02:13<00:26, 54.89it/s]Training CobwebTree:  85%| | 8265/9738 [02:14<00:27, 53.28it/s]Training CobwebTree:  85%| | 8271/9738 [02:14<00:27, 53.22it/s]Training CobwebTree:  85%| | 8277/9738 [02:14<00:26, 54.64it/s]Training CobwebTree:  85%| | 8283/9738 [02:14<00:27, 52.19it/s]Training CobwebTree:  85%| | 8289/9738 [02:14<00:26, 53.73it/s]Training CobwebTree:  85%| | 8295/9738 [02:14<00:26, 54.48it/s]Training CobwebTree:  85%| | 8301/9738 [02:14<00:26, 54.64it/s]Training CobwebTree:  85%| | 8308/9738 [02:14<00:25, 56.54it/s]Training CobwebTree:  85%| | 8314/9738 [02:14<00:25, 56.36it/s]Training CobwebTree:  85%| | 8320/9738 [02:15<00:25, 56.61it/s]Training CobwebTree:  86%| | 8326/9738 [02:15<00:24, 56.55it/s]Training CobwebTree:  86%| | 8333/9738 [02:15<00:24, 57.71it/s]Training CobwebTree:  86%| | 8339/9738 [02:15<00:24, 57.20it/s]Training CobwebTree:  86%| | 8345/9738 [02:15<00:24, 56.75it/s]Training CobwebTree:  86%| | 8351/9738 [02:15<00:24, 56.61it/s]Training CobwebTree:  86%| | 8357/9738 [02:15<00:25, 54.41it/s]Training CobwebTree:  86%| | 8363/9738 [02:15<00:24, 55.33it/s]Training CobwebTree:  86%| | 8369/9738 [02:15<00:25, 53.48it/s]Training CobwebTree:  86%| | 8375/9738 [02:16<00:25, 54.07it/s]Training CobwebTree:  86%| | 8381/9738 [02:16<00:24, 55.13it/s]Training CobwebTree:  86%| | 8387/9738 [02:16<00:24, 54.20it/s]Training CobwebTree:  86%| | 8393/9738 [02:16<00:24, 54.10it/s]Training CobwebTree:  86%| | 8400/9738 [02:16<00:24, 55.68it/s]Training CobwebTree:  86%| | 8406/9738 [02:16<00:23, 55.59it/s]Training CobwebTree:  86%| | 8412/9738 [02:16<00:23, 56.81it/s]Training CobwebTree:  86%| | 8418/9738 [02:16<00:23, 57.06it/s]Training CobwebTree:  87%| | 8424/9738 [02:16<00:23, 55.54it/s]Training CobwebTree:  87%| | 8430/9738 [02:17<00:24, 54.48it/s]Training CobwebTree:  87%| | 8436/9738 [02:17<00:24, 53.14it/s]Training CobwebTree:  87%| | 8442/9738 [02:17<00:24, 52.73it/s]Training CobwebTree:  87%| | 8448/9738 [02:17<00:23, 53.99it/s]Training CobwebTree:  87%| | 8454/9738 [02:17<00:23, 55.04it/s]Training CobwebTree:  87%| | 8460/9738 [02:17<00:22, 56.41it/s]Training CobwebTree:  87%| | 8466/9738 [02:17<00:22, 56.60it/s]Training CobwebTree:  87%| | 8472/9738 [02:17<00:22, 56.12it/s]Training CobwebTree:  87%| | 8478/9738 [02:17<00:22, 55.82it/s]Training CobwebTree:  87%| | 8484/9738 [02:18<00:22, 56.05it/s]Training CobwebTree:  87%| | 8490/9738 [02:18<00:22, 54.33it/s]Training CobwebTree:  87%| | 8496/9738 [02:18<00:22, 54.14it/s]Training CobwebTree:  87%| | 8502/9738 [02:18<00:22, 54.88it/s]Training CobwebTree:  87%| | 8509/9738 [02:18<00:21, 57.05it/s]Training CobwebTree:  87%| | 8515/9738 [02:18<00:22, 55.51it/s]Training CobwebTree:  88%| | 8521/9738 [02:18<00:22, 55.17it/s]Training CobwebTree:  88%| | 8527/9738 [02:18<00:22, 54.09it/s]Training CobwebTree:  88%| | 8533/9738 [02:18<00:22, 54.30it/s]Training CobwebTree:  88%| | 8539/9738 [02:19<00:22, 54.36it/s]Training CobwebTree:  88%| | 8545/9738 [02:19<00:22, 54.06it/s]Training CobwebTree:  88%| | 8551/9738 [02:19<00:21, 54.00it/s]Training CobwebTree:  88%| | 8557/9738 [02:19<00:22, 52.90it/s]Training CobwebTree:  88%| | 8563/9738 [02:19<00:21, 54.42it/s]Training CobwebTree:  88%| | 8569/9738 [02:19<00:21, 55.65it/s]Training CobwebTree:  88%| | 8576/9738 [02:19<00:20, 57.29it/s]Training CobwebTree:  88%| | 8582/9738 [02:19<00:20, 55.36it/s]Training CobwebTree:  88%| | 8588/9738 [02:19<00:21, 54.50it/s]Training CobwebTree:  88%| | 8594/9738 [02:20<00:20, 54.56it/s]Training CobwebTree:  88%| | 8600/9738 [02:20<00:20, 55.43it/s]Training CobwebTree:  88%| | 8606/9738 [02:20<00:20, 55.99it/s]Training CobwebTree:  88%| | 8612/9738 [02:20<00:20, 56.05it/s]Training CobwebTree:  88%| | 8618/9738 [02:20<00:19, 56.19it/s]Training CobwebTree:  89%| | 8624/9738 [02:20<00:20, 55.10it/s]Training CobwebTree:  89%| | 8630/9738 [02:20<00:20, 53.94it/s]Training CobwebTree:  89%| | 8636/9738 [02:20<00:20, 54.72it/s]Training CobwebTree:  89%| | 8642/9738 [02:20<00:20, 53.86it/s]Training CobwebTree:  89%| | 8648/9738 [02:20<00:19, 55.05it/s]Training CobwebTree:  89%| | 8654/9738 [02:21<00:19, 54.57it/s]Training CobwebTree:  89%| | 8661/9738 [02:21<00:19, 55.28it/s]Training CobwebTree:  89%| | 8667/9738 [02:21<00:20, 53.35it/s]Training CobwebTree:  89%| | 8673/9738 [02:21<00:19, 54.20it/s]Training CobwebTree:  89%| | 8679/9738 [02:21<00:19, 54.35it/s]Training CobwebTree:  89%| | 8685/9738 [02:21<00:18, 55.77it/s]Training CobwebTree:  89%| | 8691/9738 [02:21<00:19, 53.15it/s]Training CobwebTree:  89%| | 8697/9738 [02:21<00:19, 52.79it/s]Training CobwebTree:  89%| | 8703/9738 [02:22<00:19, 54.24it/s]Training CobwebTree:  89%| | 8709/9738 [02:22<00:18, 54.85it/s]Training CobwebTree:  89%| | 8715/9738 [02:22<00:18, 54.36it/s]Training CobwebTree:  90%| | 8721/9738 [02:22<00:18, 53.75it/s]Training CobwebTree:  90%| | 8727/9738 [02:22<00:19, 52.21it/s]Training CobwebTree:  90%| | 8733/9738 [02:22<00:18, 54.04it/s]Training CobwebTree:  90%| | 8739/9738 [02:22<00:19, 51.94it/s]Training CobwebTree:  90%| | 8745/9738 [02:22<00:19, 52.08it/s]Training CobwebTree:  90%| | 8751/9738 [02:22<00:18, 52.71it/s]Training CobwebTree:  90%| | 8757/9738 [02:23<00:18, 52.26it/s]Training CobwebTree:  90%| | 8763/9738 [02:23<00:18, 52.00it/s]Training CobwebTree:  90%| | 8769/9738 [02:23<00:18, 53.09it/s]Training CobwebTree:  90%| | 8775/9738 [02:23<00:18, 53.07it/s]Training CobwebTree:  90%| | 8781/9738 [02:23<00:17, 54.61it/s]Training CobwebTree:  90%| | 8787/9738 [02:23<00:17, 54.21it/s]Training CobwebTree:  90%| | 8793/9738 [02:23<00:17, 53.17it/s]Training CobwebTree:  90%| | 8799/9738 [02:23<00:17, 53.62it/s]Training CobwebTree:  90%| | 8805/9738 [02:23<00:17, 52.45it/s]Training CobwebTree:  90%| | 8811/9738 [02:24<00:17, 53.71it/s]Training CobwebTree:  91%| | 8817/9738 [02:24<00:17, 52.51it/s]Training CobwebTree:  91%| | 8823/9738 [02:24<00:17, 52.42it/s]Training CobwebTree:  91%| | 8829/9738 [02:24<00:17, 53.00it/s]Training CobwebTree:  91%| | 8835/9738 [02:24<00:17, 51.83it/s]Training CobwebTree:  91%| | 8841/9738 [02:24<00:17, 52.55it/s]Training CobwebTree:  91%| | 8847/9738 [02:24<00:16, 53.27it/s]Training CobwebTree:  91%| | 8853/9738 [02:24<00:16, 53.16it/s]Training CobwebTree:  91%| | 8859/9738 [02:24<00:16, 52.76it/s]Training CobwebTree:  91%| | 8865/9738 [02:25<00:16, 54.36it/s]Training CobwebTree:  91%| | 8871/9738 [02:25<00:16, 53.41it/s]Training CobwebTree:  91%| | 8877/9738 [02:25<00:15, 55.10it/s]Training CobwebTree:  91%| | 8883/9738 [02:25<00:15, 54.03it/s]Training CobwebTree:  91%|| 8889/9738 [02:25<00:15, 54.94it/s]Training CobwebTree:  91%|| 8895/9738 [02:25<00:15, 55.74it/s]Training CobwebTree:  91%|| 8901/9738 [02:25<00:15, 55.06it/s]Training CobwebTree:  91%|| 8907/9738 [02:25<00:15, 54.88it/s]Training CobwebTree:  92%|| 8913/9738 [02:25<00:14, 55.47it/s]Training CobwebTree:  92%|| 8919/9738 [02:26<00:14, 55.20it/s]Training CobwebTree:  92%|| 8925/9738 [02:26<00:14, 55.45it/s]Training CobwebTree:  92%|| 8931/9738 [02:26<00:14, 56.03it/s]Training CobwebTree:  92%|| 8937/9738 [02:26<00:14, 56.28it/s]Training CobwebTree:  92%|| 8943/9738 [02:26<00:14, 56.61it/s]Training CobwebTree:  92%|| 8950/9738 [02:26<00:13, 58.53it/s]Training CobwebTree:  92%|| 8956/9738 [02:26<00:13, 57.60it/s]Training CobwebTree:  92%|| 8962/9738 [02:26<00:14, 54.77it/s]Training CobwebTree:  92%|| 8968/9738 [02:26<00:13, 55.89it/s]Training CobwebTree:  92%|| 8974/9738 [02:27<00:13, 55.78it/s]Training CobwebTree:  92%|| 8980/9738 [02:27<00:14, 53.09it/s]Training CobwebTree:  92%|| 8986/9738 [02:27<00:14, 52.02it/s]Training CobwebTree:  92%|| 8992/9738 [02:27<00:14, 52.78it/s]Training CobwebTree:  92%|| 8998/9738 [02:27<00:14, 52.57it/s]Training CobwebTree:  92%|| 9004/9738 [02:27<00:13, 54.17it/s]Training CobwebTree:  93%|| 9010/9738 [02:27<00:14, 51.91it/s]Training CobwebTree:  93%|| 9016/9738 [02:27<00:13, 53.96it/s]Training CobwebTree:  93%|| 9022/9738 [02:27<00:13, 53.44it/s]Training CobwebTree:  93%|| 9028/9738 [02:28<00:13, 52.07it/s]Training CobwebTree:  93%|| 9034/9738 [02:28<00:13, 53.20it/s]Training CobwebTree:  93%|| 9041/9738 [02:28<00:12, 54.68it/s]Training CobwebTree:  93%|| 9047/9738 [02:28<00:12, 55.18it/s]Training CobwebTree:  93%|| 9053/9738 [02:28<00:12, 55.61it/s]Training CobwebTree:  93%|| 9059/9738 [02:28<00:12, 54.95it/s]Training CobwebTree:  93%|| 9066/9738 [02:28<00:11, 56.39it/s]Training CobwebTree:  93%|| 9072/9738 [02:28<00:11, 56.65it/s]Training CobwebTree:  93%|| 9078/9738 [02:28<00:11, 55.87it/s]Training CobwebTree:  93%|| 9084/9738 [02:29<00:11, 56.76it/s]Training CobwebTree:  93%|| 9090/9738 [02:29<00:11, 54.04it/s]Training CobwebTree:  93%|| 9096/9738 [02:29<00:12, 52.34it/s]Training CobwebTree:  93%|| 9102/9738 [02:29<00:11, 53.18it/s]Training CobwebTree:  94%|| 9108/9738 [02:29<00:12, 51.97it/s]Training CobwebTree:  94%|| 9114/9738 [02:29<00:11, 52.44it/s]Training CobwebTree:  94%|| 9121/9738 [02:29<00:11, 54.67it/s]Training CobwebTree:  94%|| 9127/9738 [02:29<00:11, 54.95it/s]Training CobwebTree:  94%|| 9133/9738 [02:29<00:10, 55.24it/s]Training CobwebTree:  94%|| 9139/9738 [02:30<00:10, 56.47it/s]Training CobwebTree:  94%|| 9145/9738 [02:30<00:10, 56.32it/s]Training CobwebTree:  94%|| 9151/9738 [02:30<00:10, 55.00it/s]Training CobwebTree:  94%|| 9157/9738 [02:30<00:10, 54.79it/s]Training CobwebTree:  94%|| 9163/9738 [02:30<00:10, 55.73it/s]Training CobwebTree:  94%|| 9169/9738 [02:30<00:10, 55.05it/s]Training CobwebTree:  94%|| 9175/9738 [02:30<00:10, 53.38it/s]Training CobwebTree:  94%|| 9181/9738 [02:30<00:10, 53.25it/s]Training CobwebTree:  94%|| 9187/9738 [02:30<00:10, 53.60it/s]Training CobwebTree:  94%|| 9193/9738 [02:31<00:10, 52.66it/s]Training CobwebTree:  94%|| 9199/9738 [02:31<00:09, 54.37it/s]Training CobwebTree:  95%|| 9205/9738 [02:31<00:09, 54.39it/s]Training CobwebTree:  95%|| 9211/9738 [02:31<00:09, 53.82it/s]Training CobwebTree:  95%|| 9217/9738 [02:31<00:09, 55.03it/s]Training CobwebTree:  95%|| 9223/9738 [02:31<00:09, 54.07it/s]Training CobwebTree:  95%|| 9229/9738 [02:31<00:09, 52.77it/s]Training CobwebTree:  95%|| 9235/9738 [02:31<00:09, 51.63it/s]Training CobwebTree:  95%|| 9241/9738 [02:31<00:09, 52.85it/s]Training CobwebTree:  95%|| 9247/9738 [02:32<00:09, 53.04it/s]Training CobwebTree:  95%|| 9253/9738 [02:32<00:09, 53.39it/s]Training CobwebTree:  95%|| 9259/9738 [02:32<00:08, 54.21it/s]Training CobwebTree:  95%|| 9265/9738 [02:32<00:08, 53.61it/s]Training CobwebTree:  95%|| 9271/9738 [02:32<00:08, 52.47it/s]Training CobwebTree:  95%|| 9277/9738 [02:32<00:08, 53.91it/s]Training CobwebTree:  95%|| 9283/9738 [02:32<00:08, 53.74it/s]Training CobwebTree:  95%|| 9289/9738 [02:32<00:08, 53.31it/s]Training CobwebTree:  95%|| 9295/9738 [02:32<00:08, 54.29it/s]Training CobwebTree:  96%|| 9301/9738 [02:33<00:08, 52.75it/s]Training CobwebTree:  96%|| 9307/9738 [02:33<00:08, 52.03it/s]Training CobwebTree:  96%|| 9313/9738 [02:33<00:07, 53.21it/s]Training CobwebTree:  96%|| 9319/9738 [02:33<00:07, 54.31it/s]Training CobwebTree:  96%|| 9326/9738 [02:33<00:07, 55.77it/s]Training CobwebTree:  96%|| 9332/9738 [02:33<00:07, 55.67it/s]Training CobwebTree:  96%|| 9338/9738 [02:33<00:07, 54.71it/s]Training CobwebTree:  96%|| 9344/9738 [02:33<00:07, 55.35it/s]Training CobwebTree:  96%|| 9350/9738 [02:34<00:07, 52.65it/s]Training CobwebTree:  96%|| 9356/9738 [02:34<00:07, 54.38it/s]Training CobwebTree:  96%|| 9362/9738 [02:34<00:06, 54.61it/s]Training CobwebTree:  96%|| 9369/9738 [02:34<00:06, 57.81it/s]Training CobwebTree:  96%|| 9376/9738 [02:34<00:06, 57.86it/s]Training CobwebTree:  96%|| 9382/9738 [02:34<00:06, 54.05it/s]Training CobwebTree:  96%|| 9388/9738 [02:34<00:06, 53.62it/s]Training CobwebTree:  96%|| 9394/9738 [02:34<00:06, 54.35it/s]Training CobwebTree:  97%|| 9400/9738 [02:34<00:06, 52.97it/s]Training CobwebTree:  97%|| 9406/9738 [02:35<00:06, 52.44it/s]Training CobwebTree:  97%|| 9412/9738 [02:35<00:06, 53.80it/s]Training CobwebTree:  97%|| 9418/9738 [02:35<00:05, 54.02it/s]Training CobwebTree:  97%|| 9424/9738 [02:35<00:05, 55.23it/s]Training CobwebTree:  97%|| 9430/9738 [02:35<00:05, 52.94it/s]Training CobwebTree:  97%|| 9436/9738 [02:35<00:05, 53.72it/s]Training CobwebTree:  97%|| 9442/9738 [02:35<00:05, 54.89it/s]Training CobwebTree:  97%|| 9448/9738 [02:35<00:05, 54.74it/s]Training CobwebTree:  97%|| 9454/9738 [02:35<00:05, 54.36it/s]Training CobwebTree:  97%|| 9460/9738 [02:36<00:05, 55.18it/s]Training CobwebTree:  97%|| 9466/9738 [02:36<00:04, 55.94it/s]Training CobwebTree:  97%|| 9472/9738 [02:36<00:04, 56.02it/s]Training CobwebTree:  97%|| 9478/9738 [02:36<00:04, 54.88it/s]Training CobwebTree:  97%|| 9484/9738 [02:36<00:04, 53.58it/s]Training CobwebTree:  97%|| 9490/9738 [02:36<00:04, 54.05it/s]Training CobwebTree:  98%|| 9496/9738 [02:36<00:04, 53.66it/s]Training CobwebTree:  98%|| 9502/9738 [02:36<00:04, 53.93it/s]Training CobwebTree:  98%|| 9508/9738 [02:36<00:04, 54.13it/s]Training CobwebTree:  98%|| 9514/9738 [02:37<00:04, 54.87it/s]Training CobwebTree:  98%|| 9520/9738 [02:37<00:04, 51.82it/s]Training CobwebTree:  98%|| 9526/9738 [02:37<00:04, 51.62it/s]Training CobwebTree:  98%|| 9532/9738 [02:37<00:03, 53.73it/s]Training CobwebTree:  98%|| 9538/9738 [02:37<00:03, 52.28it/s]Training CobwebTree:  98%|| 9544/9738 [02:37<00:03, 52.99it/s]Training CobwebTree:  98%|| 9550/9738 [02:37<00:03, 54.66it/s]Training CobwebTree:  98%|| 9556/9738 [02:37<00:03, 55.16it/s]Training CobwebTree:  98%|| 9562/9738 [02:37<00:03, 55.18it/s]Training CobwebTree:  98%|| 9568/9738 [02:38<00:03, 55.82it/s]Training CobwebTree:  98%|| 9574/9738 [02:38<00:02, 56.21it/s]Training CobwebTree:  98%|| 9580/9738 [02:38<00:02, 54.49it/s]Training CobwebTree:  98%|| 9586/9738 [02:38<00:02, 55.05it/s]Training CobwebTree:  99%|| 9592/9738 [02:38<00:02, 55.48it/s]Training CobwebTree:  99%|| 9598/9738 [02:38<00:02, 52.42it/s]Training CobwebTree:  99%|| 9604/9738 [02:38<00:02, 53.06it/s]Training CobwebTree:  99%|| 9610/9738 [02:38<00:02, 52.81it/s]Training CobwebTree:  99%|| 9616/9738 [02:38<00:02, 54.00it/s]Training CobwebTree:  99%|| 9622/9738 [02:39<00:02, 52.91it/s]Training CobwebTree:  99%|| 9628/9738 [02:39<00:02, 54.53it/s]Training CobwebTree:  99%|| 9634/9738 [02:39<00:01, 53.85it/s]Training CobwebTree:  99%|| 9640/9738 [02:39<00:01, 54.92it/s]Training CobwebTree:  99%|| 9646/9738 [02:39<00:01, 54.18it/s]Training CobwebTree:  99%|| 9652/9738 [02:39<00:01, 54.85it/s]Training CobwebTree:  99%|| 9658/9738 [02:39<00:01, 54.57it/s]Training CobwebTree:  99%|| 9664/9738 [02:39<00:01, 52.43it/s]Training CobwebTree:  99%|| 9670/9738 [02:39<00:01, 53.78it/s]Training CobwebTree:  99%|| 9676/9738 [02:40<00:01, 52.79it/s]Training CobwebTree:  99%|| 9682/9738 [02:40<00:01, 52.39it/s]Training CobwebTree:  99%|| 9688/9738 [02:40<00:00, 52.94it/s]Training CobwebTree: 100%|| 9694/9738 [02:40<00:00, 53.03it/s]Training CobwebTree: 100%|| 9700/9738 [02:40<00:00, 54.78it/s]Training CobwebTree: 100%|| 9706/9738 [02:40<00:00, 55.48it/s]Training CobwebTree: 100%|| 9712/9738 [02:40<00:00, 54.32it/s]Training CobwebTree: 100%|| 9718/9738 [02:40<00:00, 54.94it/s]Training CobwebTree: 100%|| 9724/9738 [02:40<00:00, 54.86it/s]Training CobwebTree: 100%|| 9730/9738 [02:41<00:00, 54.25it/s]Training CobwebTree: 100%|| 9736/9738 [02:41<00:00, 52.89it/s]Training CobwebTree: 100%|| 9738/9738 [02:41<00:00, 60.42it/s]
2025-12-24 09:15:33,834 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=47, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-24 09:15:35,240 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (7232 virtual)
2025-12-24 09:15:35,246 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (3127 virtual)
2025-12-24 09:15:35,275 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (1323 virtual)
2025-12-24 09:15:35,278 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (713 virtual)
2025-12-24 09:15:35,281 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-5372 virtual)
2025-12-24 09:15:35,284 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (-3523 virtual)
2025-12-24 09:15:35,303 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (12146 virtual)
2025-12-24 09:15:35,306 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (22148 virtual)
2025-12-24 09:15:35,321 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (19719 virtual)
2025-12-24 09:15:35,328 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (5184 virtual)
2025-12-24 09:15:35,330 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (6705 virtual)
2025-12-24 09:15:35,354 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (-5029 virtual)
2025-12-24 09:15:35,359 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (-3476 virtual)
2025-12-24 09:15:35,376 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (-2950 virtual)
2025-12-24 09:15:35,379 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (2109 virtual)
2025-12-24 09:15:35,404 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (2483 virtual)
2025-12-24 09:15:35,406 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (6024 virtual)
2025-12-24 09:15:35,407 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (8719 virtual)
2025-12-24 09:15:35,410 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (10782 virtual)
2025-12-24 09:15:35,412 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (12662 virtual)
2025-12-24 09:15:35,423 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (14417 virtual)
2025-12-24 09:15:35,436 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (16318 virtual)
2025-12-24 09:15:35,438 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (21460 virtual)
2025-12-24 09:15:35,448 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (36852 virtual)
2025-12-24 09:15:35,525 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (39766 virtual)
2025-12-24 09:15:35,540 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (41441 virtual)
2025-12-24 09:15:35,558 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (51322 virtual)
2025-12-24 09:15:35,576 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (52687 virtual)
2025-12-24 09:15:35,588 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (52913 virtual)
2025-12-24 09:15:35,596 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (55693 virtual)
2025-12-24 09:15:35,622 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (52992 virtual)
2025-12-24 09:15:35,642 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (50615 virtual)
2025-12-24 09:15:35,688 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (45525 virtual)
2025-12-24 09:15:35,691 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (45324 virtual)
2025-12-24 09:15:35,725 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (42113 virtual)
2025-12-24 09:15:35,753 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (43690 virtual)
2025-12-24 09:15:35,768 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (42608 virtual)
2025-12-24 09:15:35,816 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (39072 virtual)
2025-12-24 09:15:35,837 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (40044 virtual)
2025-12-24 09:15:35,860 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (41542 virtual)
2025-12-24 09:15:35,868 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (45793 virtual)
2025-12-24 09:15:35,876 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (47498 virtual)
2025-12-24 09:15:35,923 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (42550 virtual)
2025-12-24 09:15:35,997 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (37489 virtual)
2025-12-24 09:15:36,026 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (39918 virtual)
2025-12-24 09:15:36,055 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (31027 virtual)
2025-12-24 09:15:36,098 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (40008 virtual)
2025-12-24 09:15:36,187 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (41502 virtual)
2025-12-24 09:15:36,221 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (46622 virtual)
2025-12-24 09:15:36,248 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (48729 virtual)
2025-12-24 09:15:36,262 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (44346 virtual)
2025-12-24 09:15:36,278 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (48297 virtual)
2025-12-24 09:15:36,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,456 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,465 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,472 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,478 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,526 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,680 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,686 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,734 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,760 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,863 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,920 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:36,961 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:36,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,590 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,784 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,857 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:37,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:37,958 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:40,137 INFO gensim.topic_coherence.text_analysis: 47 accumulators retrieved from output queue
2025-12-24 09:15:40,174 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 604327 virtual documents
2025-12-24 09:15:40,574 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=47, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-24 09:15:42,128 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5593 virtual)
2025-12-24 09:15:42,131 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (10727 virtual)
2025-12-24 09:15:42,133 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (26432 virtual)
2025-12-24 09:15:42,134 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (30159 virtual)
2025-12-24 09:15:42,135 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (32725 virtual)
2025-12-24 09:15:42,136 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (36278 virtual)
2025-12-24 09:15:42,137 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (39220 virtual)
2025-12-24 09:15:42,139 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (54327 virtual)
2025-12-24 09:15:42,140 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (60577 virtual)
2025-12-24 09:15:42,141 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (63971 virtual)
2025-12-24 09:15:42,142 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (68747 virtual)
2025-12-24 09:15:42,143 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (78123 virtual)
2025-12-24 09:15:42,144 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (82521 virtual)
2025-12-24 09:15:42,145 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (90313 virtual)
2025-12-24 09:15:42,146 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (95725 virtual)
2025-12-24 09:15:42,147 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (100032 virtual)
2025-12-24 09:15:42,148 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (103002 virtual)
2025-12-24 09:15:42,149 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (109828 virtual)
2025-12-24 09:15:42,150 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (115400 virtual)
2025-12-24 09:15:42,151 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (124477 virtual)
2025-12-24 09:15:42,152 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (128588 virtual)
2025-12-24 09:15:42,153 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (131939 virtual)
2025-12-24 09:15:42,156 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (159346 virtual)
2025-12-24 09:15:42,158 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (175748 virtual)
2025-12-24 09:15:42,159 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (179483 virtual)
2025-12-24 09:15:42,160 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (186119 virtual)
2025-12-24 09:15:42,161 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (189134 virtual)
2025-12-24 09:15:42,162 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (194011 virtual)
2025-12-24 09:15:42,163 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (198338 virtual)
2025-12-24 09:15:42,164 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (201418 virtual)
2025-12-24 09:15:42,164 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (205336 virtual)
2025-12-24 09:15:42,165 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (210865 virtual)
2025-12-24 09:15:42,166 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (215487 virtual)
2025-12-24 09:15:42,167 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (222784 virtual)
2025-12-24 09:15:42,168 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (227542 virtual)
2025-12-24 09:15:42,170 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (237105 virtual)
2025-12-24 09:15:42,171 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (242098 virtual)
2025-12-24 09:15:42,171 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (246462 virtual)
2025-12-24 09:15:42,172 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (250096 virtual)
2025-12-24 09:15:42,173 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (255323 virtual)
2025-12-24 09:15:42,174 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (258628 virtual)
2025-12-24 09:15:42,175 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (263349 virtual)
2025-12-24 09:15:42,175 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (268233 virtual)
2025-12-24 09:15:42,176 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (274132 virtual)
2025-12-24 09:15:42,178 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (282971 virtual)
2025-12-24 09:15:42,179 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (286673 virtual)
2025-12-24 09:15:42,179 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (290264 virtual)
2025-12-24 09:15:42,180 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (296037 virtual)
2025-12-24 09:15:42,183 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (310124 virtual)
2025-12-24 09:15:42,184 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (317050 virtual)
2025-12-24 09:15:42,231 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (320598 virtual)
2025-12-24 09:15:42,233 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (334909 virtual)
2025-12-24 09:15:42,234 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (341683 virtual)
2025-12-24 09:15:42,235 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (351624 virtual)
2025-12-24 09:15:42,236 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (360719 virtual)
2025-12-24 09:15:42,237 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (365651 virtual)
2025-12-24 09:15:42,259 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (375582 virtual)
2025-12-24 09:15:42,263 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (380043 virtual)
2025-12-24 09:15:42,264 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (390262 virtual)
2025-12-24 09:15:42,279 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (394060 virtual)
2025-12-24 09:15:42,280 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (398804 virtual)
2025-12-24 09:15:42,288 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (402970 virtual)
2025-12-24 09:15:42,296 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (417617 virtual)
2025-12-24 09:15:42,297 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (425918 virtual)
2025-12-24 09:15:42,299 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (437460 virtual)
2025-12-24 09:15:42,299 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (441365 virtual)
2025-12-24 09:15:42,323 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (446589 virtual)
2025-12-24 09:15:42,334 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (450244 virtual)
2025-12-24 09:15:42,346 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (478452 virtual)
2025-12-24 09:15:42,351 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (482978 virtual)
2025-12-24 09:15:42,360 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (494166 virtual)
2025-12-24 09:15:42,361 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (502241 virtual)
2025-12-24 09:15:42,363 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (518522 virtual)
2025-12-24 09:15:42,383 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (526287 virtual)
2025-12-24 09:15:42,403 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (532913 virtual)
2025-12-24 09:15:42,427 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (536619 virtual)
2025-12-24 09:15:42,429 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (548493 virtual)
2025-12-24 09:15:42,430 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (551958 virtual)
2025-12-24 09:15:42,431 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (555379 virtual)
2025-12-24 09:15:42,443 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (564992 virtual)
2025-12-24 09:15:42,450 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (568990 virtual)
2025-12-24 09:15:42,451 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (574796 virtual)
2025-12-24 09:15:42,452 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (581815 virtual)
2025-12-24 09:15:42,464 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (585873 virtual)
2025-12-24 09:15:42,465 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (591771 virtual)
2025-12-24 09:15:42,466 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (597046 virtual)
2025-12-24 09:15:42,466 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (601849 virtual)
2025-12-24 09:15:42,467 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (608725 virtual)
2025-12-24 09:15:42,469 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (615021 virtual)
2025-12-24 09:15:42,470 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (621265 virtual)
2025-12-24 09:15:42,475 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (627724 virtual)
2025-12-24 09:15:42,476 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (631679 virtual)
2025-12-24 09:15:42,477 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (635736 virtual)
2025-12-24 09:15:42,479 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (643713 virtual)
2025-12-24 09:15:42,483 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (649874 virtual)
2025-12-24 09:15:42,484 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (658090 virtual)
2025-12-24 09:15:42,485 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (661695 virtual)
2025-12-24 09:15:42,492 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (669808 virtual)
2025-12-24 09:15:42,493 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (673686 virtual)
2025-12-24 09:15:42,495 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (678673 virtual)
2025-12-24 09:15:42,496 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (683677 virtual)
2025-12-24 09:15:42,500 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (687618 virtual)
2025-12-24 09:15:42,501 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (698272 virtual)
2025-12-24 09:15:42,502 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (703739 virtual)
2025-12-24 09:15:42,512 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (712044 virtual)
2025-12-24 09:15:42,513 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (719942 virtual)
2025-12-24 09:15:42,515 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (730593 virtual)
2025-12-24 09:15:42,524 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (738698 virtual)
2025-12-24 09:15:42,525 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (744918 virtual)
2025-12-24 09:15:42,526 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (748587 virtual)
2025-12-24 09:15:42,527 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (754283 virtual)
2025-12-24 09:15:42,616 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (759411 virtual)
2025-12-24 09:15:42,659 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (763074 virtual)
2025-12-24 09:15:42,661 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (769146 virtual)
2025-12-24 09:15:42,662 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (778550 virtual)
2025-12-24 09:15:42,672 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (782687 virtual)
2025-12-24 09:15:42,704 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (787370 virtual)
2025-12-24 09:15:42,705 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (792747 virtual)
2025-12-24 09:15:42,705 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (795834 virtual)
2025-12-24 09:15:42,706 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (799472 virtual)
2025-12-24 09:15:42,712 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (804251 virtual)
2025-12-24 09:15:42,721 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (818289 virtual)
2025-12-24 09:15:42,748 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (823742 virtual)
2025-12-24 09:15:42,756 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (829121 virtual)
2025-12-24 09:15:42,757 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (839918 virtual)
2025-12-24 09:15:42,787 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (843357 virtual)
2025-12-24 09:15:42,791 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (847194 virtual)
2025-12-24 09:15:42,792 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (850480 virtual)
2025-12-24 09:15:42,793 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (854672 virtual)
2025-12-24 09:15:42,794 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (863027 virtual)
2025-12-24 09:15:42,823 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (865772 virtual)
2025-12-24 09:15:42,829 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (884808 virtual)
2025-12-24 09:15:42,840 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (889004 virtual)
2025-12-24 09:15:42,868 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (894985 virtual)
2025-12-24 09:15:42,875 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (898703 virtual)
2025-12-24 09:15:42,879 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (901982 virtual)
2025-12-24 09:15:42,880 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (906587 virtual)
2025-12-24 09:15:42,895 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (910463 virtual)
2025-12-24 09:15:42,902 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (931102 virtual)
2025-12-24 09:15:42,932 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (942622 virtual)
2025-12-24 09:15:42,952 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (951129 virtual)
2025-12-24 09:15:42,975 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (955276 virtual)
2025-12-24 09:15:42,976 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (959539 virtual)
2025-12-24 09:15:42,977 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (965946 virtual)
2025-12-24 09:15:42,988 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (970853 virtual)
2025-12-24 09:15:42,991 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (973829 virtual)
2025-12-24 09:15:43,001 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (989097 virtual)
2025-12-24 09:15:43,020 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (994234 virtual)
2025-12-24 09:15:43,027 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (997762 virtual)
2025-12-24 09:15:43,035 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (1000793 virtual)
2025-12-24 09:15:43,044 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (1006413 virtual)
2025-12-24 09:15:43,052 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (1011923 virtual)
2025-12-24 09:15:43,055 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (1012355 virtual)
2025-12-24 09:15:43,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,317 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,330 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,350 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,366 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,402 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,408 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,412 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,460 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,478 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,524 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,589 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,670 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,693 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,753 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,853 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:43,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:43,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:44,070 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:44,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:44,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:44,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:44,242 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:44,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:44,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:44,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-24 09:15:44,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-24 09:15:46,650 INFO gensim.topic_coherence.text_analysis: 47 accumulators retrieved from output queue
2025-12-24 09:15:46,676 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1015742 virtual documents
2025-12-24 09:15:47,008 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.5590825043491555, 'coherence_npmi': 0.06027015256044103, 'topic_diversity': 0.833203125, 'inter_topic_similarity': 0.12766794860363007}
2025-12-24 09:15:47,008 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.7388619919915985, 'coherence_npmi': 0.12774898259435136, 'topic_diversity': 0.865, 'inter_topic_similarity': 0.32903560996055603}
2025-12-24 09:15:47,008 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.7023097835053274, 'coherence_npmi': 0.12505548232011496, 'topic_diversity': 0.8518518518518519, 'inter_topic_similarity': 0.28600919246673584}
