2025-12-21 09:13:30,153 INFO __main__: Starting benchmark for dataset=20newsgroups
2025-12-21 09:13:31,403 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-21 09:13:31,935 INFO gensim.corpora.dictionary: adding document #10000 to Dictionary<92950 unique tokens: ['60s', '70s', 'addition', 'body', 'bricklin']...>
2025-12-21 09:13:31,991 INFO gensim.corpora.dictionary: built Dictionary<101322 unique tokens: ['60s', '70s', 'addition', 'body', 'bricklin']...> from 11014 documents (total 1223092 corpus positions)
2025-12-21 09:13:31,995 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<101322 unique tokens: ['60s', '70s', 'addition', 'body', 'bricklin']...> from 11014 documents (total 1223092 corpus positions)", 'datetime': '2025-12-21T09:13:31.991523', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-153-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-21 09:13:34,064 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-21 09:13:34,064 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-21 09:13:39,479 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 11014 docs
2025-12-21 09:15:23,685 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 09:15:26,342 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (7232 virtual)
2025-12-21 09:15:26,345 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (3127 virtual)
2025-12-21 09:15:26,348 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (1323 virtual)
2025-12-21 09:15:26,350 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (713 virtual)
2025-12-21 09:15:26,354 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-5372 virtual)
2025-12-21 09:15:26,357 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (-3523 virtual)
2025-12-21 09:15:26,363 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (12146 virtual)
2025-12-21 09:15:26,365 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (22148 virtual)
2025-12-21 09:15:26,367 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (19719 virtual)
2025-12-21 09:15:26,375 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (5184 virtual)
2025-12-21 09:15:26,378 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (6705 virtual)
2025-12-21 09:15:26,387 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (-5029 virtual)
2025-12-21 09:15:26,392 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (-3476 virtual)
2025-12-21 09:15:26,394 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (-2950 virtual)
2025-12-21 09:15:26,397 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (2109 virtual)
2025-12-21 09:15:26,398 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (2483 virtual)
2025-12-21 09:15:26,400 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (6024 virtual)
2025-12-21 09:15:26,402 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (8719 virtual)
2025-12-21 09:15:26,405 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (10782 virtual)
2025-12-21 09:15:26,407 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (12662 virtual)
2025-12-21 09:15:26,413 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (14417 virtual)
2025-12-21 09:15:26,414 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (16318 virtual)
2025-12-21 09:15:26,416 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (21460 virtual)
2025-12-21 09:15:26,440 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (36852 virtual)
2025-12-21 09:15:26,443 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (39766 virtual)
2025-12-21 09:15:26,444 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (41441 virtual)
2025-12-21 09:15:26,529 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (51322 virtual)
2025-12-21 09:15:26,584 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (52687 virtual)
2025-12-21 09:15:26,585 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (52913 virtual)
2025-12-21 09:15:26,588 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (55693 virtual)
2025-12-21 09:15:26,591 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (52992 virtual)
2025-12-21 09:15:26,620 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (50615 virtual)
2025-12-21 09:15:26,684 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (45525 virtual)
2025-12-21 09:15:26,687 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (45324 virtual)
2025-12-21 09:15:26,700 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (42113 virtual)
2025-12-21 09:15:26,781 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (43690 virtual)
2025-12-21 09:15:26,784 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (42608 virtual)
2025-12-21 09:15:26,914 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (39072 virtual)
2025-12-21 09:15:26,941 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (40044 virtual)
2025-12-21 09:15:26,943 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (41542 virtual)
2025-12-21 09:15:26,945 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (45793 virtual)
2025-12-21 09:15:26,960 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (47498 virtual)
2025-12-21 09:15:27,195 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (42550 virtual)
2025-12-21 09:15:27,252 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (37489 virtual)
2025-12-21 09:15:27,363 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (39918 virtual)
2025-12-21 09:15:27,483 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (31027 virtual)
2025-12-21 09:15:27,557 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (40008 virtual)
2025-12-21 09:15:27,633 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (41502 virtual)
2025-12-21 09:15:27,650 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (46622 virtual)
2025-12-21 09:15:27,679 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (48729 virtual)
2025-12-21 09:15:27,682 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (44346 virtual)
2025-12-21 09:15:27,687 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (48297 virtual)
2025-12-21 09:15:27,810 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (35558 virtual)
2025-12-21 09:15:27,860 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (37592 virtual)
2025-12-21 09:15:27,957 INFO gensim.topic_coherence.text_analysis: 170 batches submitted to accumulate stats from 10880 documents (24474 virtual)
2025-12-21 09:15:27,964 INFO gensim.topic_coherence.text_analysis: 171 batches submitted to accumulate stats from 10944 documents (24761 virtual)
2025-12-21 09:15:28,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,017 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,017 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,017 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,017 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,029 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,029 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,054 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,082 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,157 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,162 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,482 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:28,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:28,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:29,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:29,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:29,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:29,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:29,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:29,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:29,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:29,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:29,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:30,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:30,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:31,055 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:31,089 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:31,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:31,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:31,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:31,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:35,753 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 09:15:35,778 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 663359 virtual documents
2025-12-21 09:15:36,156 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 09:15:41,318 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5593 virtual)
2025-12-21 09:15:41,320 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (10727 virtual)
2025-12-21 09:15:41,322 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (26432 virtual)
2025-12-21 09:15:41,323 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (30159 virtual)
2025-12-21 09:15:41,323 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (32725 virtual)
2025-12-21 09:15:41,324 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (36278 virtual)
2025-12-21 09:15:41,325 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (39220 virtual)
2025-12-21 09:15:41,328 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (54327 virtual)
2025-12-21 09:15:41,329 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (60577 virtual)
2025-12-21 09:15:41,330 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (63971 virtual)
2025-12-21 09:15:41,331 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (68747 virtual)
2025-12-21 09:15:41,333 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (78123 virtual)
2025-12-21 09:15:41,334 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (82521 virtual)
2025-12-21 09:15:41,335 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (90313 virtual)
2025-12-21 09:15:41,336 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (95725 virtual)
2025-12-21 09:15:41,337 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (100032 virtual)
2025-12-21 09:15:41,338 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (103002 virtual)
2025-12-21 09:15:41,339 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (109828 virtual)
2025-12-21 09:15:41,341 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (115400 virtual)
2025-12-21 09:15:41,342 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (124477 virtual)
2025-12-21 09:15:41,343 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (128588 virtual)
2025-12-21 09:15:41,344 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (131939 virtual)
2025-12-21 09:15:41,349 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (159346 virtual)
2025-12-21 09:15:41,352 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (175748 virtual)
2025-12-21 09:15:41,353 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (179483 virtual)
2025-12-21 09:15:41,354 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (186119 virtual)
2025-12-21 09:15:41,355 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (189134 virtual)
2025-12-21 09:15:41,356 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (194011 virtual)
2025-12-21 09:15:41,357 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (198338 virtual)
2025-12-21 09:15:41,358 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (201418 virtual)
2025-12-21 09:15:41,359 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (205336 virtual)
2025-12-21 09:15:41,360 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (210865 virtual)
2025-12-21 09:15:41,361 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (215487 virtual)
2025-12-21 09:15:41,363 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (222784 virtual)
2025-12-21 09:15:41,364 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (227542 virtual)
2025-12-21 09:15:41,366 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (237105 virtual)
2025-12-21 09:15:41,367 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (242098 virtual)
2025-12-21 09:15:41,368 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (246462 virtual)
2025-12-21 09:15:41,369 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (250096 virtual)
2025-12-21 09:15:41,370 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (255323 virtual)
2025-12-21 09:15:41,371 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (258628 virtual)
2025-12-21 09:15:41,372 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (263349 virtual)
2025-12-21 09:15:41,564 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (268233 virtual)
2025-12-21 09:15:41,588 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (274132 virtual)
2025-12-21 09:15:41,613 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (282971 virtual)
2025-12-21 09:15:41,636 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (286673 virtual)
2025-12-21 09:15:41,637 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (290264 virtual)
2025-12-21 09:15:41,638 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (296037 virtual)
2025-12-21 09:15:41,641 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (310124 virtual)
2025-12-21 09:15:41,643 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (317050 virtual)
2025-12-21 09:15:41,644 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (320598 virtual)
2025-12-21 09:15:41,647 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (334909 virtual)
2025-12-21 09:15:41,655 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (341683 virtual)
2025-12-21 09:15:41,667 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (351624 virtual)
2025-12-21 09:15:41,684 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (360719 virtual)
2025-12-21 09:15:41,700 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (365651 virtual)
2025-12-21 09:15:41,717 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (375582 virtual)
2025-12-21 09:15:41,731 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (380043 virtual)
2025-12-21 09:15:41,780 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (390262 virtual)
2025-12-21 09:15:41,792 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (394060 virtual)
2025-12-21 09:15:41,793 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (398804 virtual)
2025-12-21 09:15:41,812 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (402970 virtual)
2025-12-21 09:15:41,865 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (417617 virtual)
2025-12-21 09:15:41,866 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (425918 virtual)
2025-12-21 09:15:41,881 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (437460 virtual)
2025-12-21 09:15:41,917 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (441365 virtual)
2025-12-21 09:15:41,928 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (446589 virtual)
2025-12-21 09:15:41,949 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (450244 virtual)
2025-12-21 09:15:41,976 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (478452 virtual)
2025-12-21 09:15:41,978 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (482978 virtual)
2025-12-21 09:15:42,000 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (494166 virtual)
2025-12-21 09:15:42,007 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (502241 virtual)
2025-12-21 09:15:42,042 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (518522 virtual)
2025-12-21 09:15:42,049 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (526287 virtual)
2025-12-21 09:15:42,074 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (532913 virtual)
2025-12-21 09:15:42,093 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (536619 virtual)
2025-12-21 09:15:42,095 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (548493 virtual)
2025-12-21 09:15:42,101 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (551958 virtual)
2025-12-21 09:15:42,111 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (555379 virtual)
2025-12-21 09:15:42,128 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (564992 virtual)
2025-12-21 09:15:42,144 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (568990 virtual)
2025-12-21 09:15:42,155 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (574796 virtual)
2025-12-21 09:15:42,162 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (581815 virtual)
2025-12-21 09:15:42,181 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (585873 virtual)
2025-12-21 09:15:42,196 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (591771 virtual)
2025-12-21 09:15:42,217 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (597046 virtual)
2025-12-21 09:15:42,232 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (601849 virtual)
2025-12-21 09:15:42,248 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (608725 virtual)
2025-12-21 09:15:42,273 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (615021 virtual)
2025-12-21 09:15:42,293 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (621265 virtual)
2025-12-21 09:15:42,308 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (627724 virtual)
2025-12-21 09:15:42,329 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (631679 virtual)
2025-12-21 09:15:42,330 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (635736 virtual)
2025-12-21 09:15:42,337 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (643713 virtual)
2025-12-21 09:15:42,356 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (649874 virtual)
2025-12-21 09:15:42,380 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (658090 virtual)
2025-12-21 09:15:42,396 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (661695 virtual)
2025-12-21 09:15:42,408 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (669808 virtual)
2025-12-21 09:15:42,409 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (673686 virtual)
2025-12-21 09:15:42,424 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (678673 virtual)
2025-12-21 09:15:42,440 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (683677 virtual)
2025-12-21 09:15:42,441 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (687618 virtual)
2025-12-21 09:15:42,493 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (698272 virtual)
2025-12-21 09:15:42,495 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (703739 virtual)
2025-12-21 09:15:42,496 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (712044 virtual)
2025-12-21 09:15:42,498 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (719942 virtual)
2025-12-21 09:15:42,531 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (730593 virtual)
2025-12-21 09:15:42,548 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (738698 virtual)
2025-12-21 09:15:42,560 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (744918 virtual)
2025-12-21 09:15:42,575 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (748587 virtual)
2025-12-21 09:15:42,610 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (754283 virtual)
2025-12-21 09:15:42,635 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (759411 virtual)
2025-12-21 09:15:42,636 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (763074 virtual)
2025-12-21 09:15:42,638 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (769146 virtual)
2025-12-21 09:15:42,639 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (778550 virtual)
2025-12-21 09:15:42,640 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (782687 virtual)
2025-12-21 09:15:42,641 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (787370 virtual)
2025-12-21 09:15:42,642 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (792747 virtual)
2025-12-21 09:15:42,643 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (795834 virtual)
2025-12-21 09:15:42,645 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (799472 virtual)
2025-12-21 09:15:42,646 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (804251 virtual)
2025-12-21 09:15:42,649 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (818289 virtual)
2025-12-21 09:15:42,650 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (823742 virtual)
2025-12-21 09:15:42,662 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (829121 virtual)
2025-12-21 09:15:42,665 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (839918 virtual)
2025-12-21 09:15:42,666 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (843357 virtual)
2025-12-21 09:15:42,667 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (847194 virtual)
2025-12-21 09:15:42,668 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (850480 virtual)
2025-12-21 09:15:42,669 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (854672 virtual)
2025-12-21 09:15:42,670 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (863027 virtual)
2025-12-21 09:15:42,671 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (865772 virtual)
2025-12-21 09:15:42,676 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (884808 virtual)
2025-12-21 09:15:42,680 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (889004 virtual)
2025-12-21 09:15:42,682 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (894985 virtual)
2025-12-21 09:15:42,743 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (898703 virtual)
2025-12-21 09:15:42,751 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (901982 virtual)
2025-12-21 09:15:42,768 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (906587 virtual)
2025-12-21 09:15:42,784 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (910463 virtual)
2025-12-21 09:15:42,831 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (931102 virtual)
2025-12-21 09:15:42,845 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (942622 virtual)
2025-12-21 09:15:42,851 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (951129 virtual)
2025-12-21 09:15:42,931 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (955276 virtual)
2025-12-21 09:15:42,932 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (959539 virtual)
2025-12-21 09:15:42,933 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (965946 virtual)
2025-12-21 09:15:42,935 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (970853 virtual)
2025-12-21 09:15:42,936 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (973829 virtual)
2025-12-21 09:15:42,939 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (989097 virtual)
2025-12-21 09:15:42,940 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (994234 virtual)
2025-12-21 09:15:42,958 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (997762 virtual)
2025-12-21 09:15:42,982 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (1000793 virtual)
2025-12-21 09:15:43,001 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (1006413 virtual)
2025-12-21 09:15:43,007 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (1011923 virtual)
2025-12-21 09:15:43,014 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (1014946 virtual)
2025-12-21 09:15:43,015 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (1018740 virtual)
2025-12-21 09:15:43,021 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (1022376 virtual)
2025-12-21 09:15:43,027 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (1026825 virtual)
2025-12-21 09:15:43,039 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (1040358 virtual)
2025-12-21 09:15:43,056 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (1044737 virtual)
2025-12-21 09:15:43,065 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (1050861 virtual)
2025-12-21 09:15:43,072 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (1056370 virtual)
2025-12-21 09:15:43,088 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (1062228 virtual)
2025-12-21 09:15:43,110 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (1074392 virtual)
2025-12-21 09:15:43,112 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (1078963 virtual)
2025-12-21 09:15:43,118 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (1082443 virtual)
2025-12-21 09:15:43,119 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (1088508 virtual)
2025-12-21 09:15:43,121 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (1093083 virtual)
2025-12-21 09:15:43,132 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (1096984 virtual)
2025-12-21 09:15:43,138 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (1101614 virtual)
2025-12-21 09:15:43,145 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (1105041 virtual)
2025-12-21 09:15:43,146 INFO gensim.topic_coherence.text_analysis: 170 batches submitted to accumulate stats from 10880 documents (1112474 virtual)
2025-12-21 09:15:43,164 INFO gensim.topic_coherence.text_analysis: 171 batches submitted to accumulate stats from 10944 documents (1119161 virtual)
2025-12-21 09:15:43,165 INFO gensim.topic_coherence.text_analysis: 172 batches submitted to accumulate stats from 11008 documents (1123702 virtual)
2025-12-21 09:15:43,166 INFO gensim.topic_coherence.text_analysis: 173 batches submitted to accumulate stats from 11072 documents (1123966 virtual)
2025-12-21 09:15:43,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,202 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,202 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,204 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,204 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,204 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,206 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,220 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,293 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,293 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,319 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,322 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:43,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:43,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:44,080 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:44,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:44,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:44,134 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:44,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:44,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:44,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:44,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:44,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:44,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:44,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:44,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:44,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:45,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:45,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:45,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:45,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:46,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:15:46,584 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:15:52,590 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 09:15:52,612 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1127780 virtual documents
2025-12-21 09:15:53,007 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 11014 docs
2025-12-21 09:17:21,369 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 09:17:27,419 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (7232 virtual)
2025-12-21 09:17:27,424 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (3127 virtual)
2025-12-21 09:17:27,427 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (1323 virtual)
2025-12-21 09:17:27,430 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (713 virtual)
2025-12-21 09:17:27,435 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-5372 virtual)
2025-12-21 09:17:27,438 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (-3523 virtual)
2025-12-21 09:17:27,446 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (12146 virtual)
2025-12-21 09:17:27,449 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (22148 virtual)
2025-12-21 09:17:27,452 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (19719 virtual)
2025-12-21 09:17:27,461 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (5184 virtual)
2025-12-21 09:17:27,464 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (6705 virtual)
2025-12-21 09:17:27,489 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (-5029 virtual)
2025-12-21 09:17:27,505 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (-3476 virtual)
2025-12-21 09:17:27,507 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (-2950 virtual)
2025-12-21 09:17:27,511 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (2109 virtual)
2025-12-21 09:17:27,524 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (2483 virtual)
2025-12-21 09:17:27,526 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (6024 virtual)
2025-12-21 09:17:27,528 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (8719 virtual)
2025-12-21 09:17:27,532 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (10782 virtual)
2025-12-21 09:17:27,546 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (12662 virtual)
2025-12-21 09:17:27,552 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (14417 virtual)
2025-12-21 09:17:27,564 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (16318 virtual)
2025-12-21 09:17:27,577 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (21460 virtual)
2025-12-21 09:17:27,749 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (36852 virtual)
2025-12-21 09:17:27,753 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (39766 virtual)
2025-12-21 09:17:27,764 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (41441 virtual)
2025-12-21 09:17:27,919 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (51322 virtual)
2025-12-21 09:17:27,933 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (52687 virtual)
2025-12-21 09:17:28,034 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (52913 virtual)
2025-12-21 09:17:28,050 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (55693 virtual)
2025-12-21 09:17:28,149 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (52992 virtual)
2025-12-21 09:17:28,159 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (50615 virtual)
2025-12-21 09:17:28,435 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (45525 virtual)
2025-12-21 09:17:28,440 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (45324 virtual)
2025-12-21 09:17:28,520 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (42113 virtual)
2025-12-21 09:17:28,635 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (43690 virtual)
2025-12-21 09:17:28,696 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (42608 virtual)
2025-12-21 09:17:28,769 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (39072 virtual)
2025-12-21 09:17:28,879 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (40044 virtual)
2025-12-21 09:17:28,954 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (41542 virtual)
2025-12-21 09:17:29,011 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (45793 virtual)
2025-12-21 09:17:29,013 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (47498 virtual)
2025-12-21 09:17:29,116 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (42550 virtual)
2025-12-21 09:17:29,258 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (37489 virtual)
2025-12-21 09:17:29,389 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (39918 virtual)
2025-12-21 09:17:29,462 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (31027 virtual)
2025-12-21 09:17:29,671 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (40008 virtual)
2025-12-21 09:17:29,789 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (41502 virtual)
2025-12-21 09:17:29,934 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (46622 virtual)
2025-12-21 09:17:29,952 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (48729 virtual)
2025-12-21 09:17:30,006 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (44346 virtual)
2025-12-21 09:17:30,011 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (48297 virtual)
2025-12-21 09:17:30,319 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (35558 virtual)
2025-12-21 09:17:30,419 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (37592 virtual)
2025-12-21 09:17:30,651 INFO gensim.topic_coherence.text_analysis: 170 batches submitted to accumulate stats from 10880 documents (24474 virtual)
2025-12-21 09:17:30,653 INFO gensim.topic_coherence.text_analysis: 171 batches submitted to accumulate stats from 10944 documents (24761 virtual)
2025-12-21 09:17:30,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,672 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,672 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,672 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,672 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,678 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,678 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,679 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,679 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,680 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,680 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,680 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,681 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,681 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,686 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,691 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:30,957 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:30,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,314 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,326 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,794 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:31,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:31,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,061 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:32,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:32,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:32,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:32,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:32,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:32,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:32,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:32,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:33,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:33,317 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:33,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:33,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:33,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:33,436 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:33,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:33,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:33,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:33,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:33,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:33,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:34,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:34,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:34,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:34,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:34,950 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:34,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:35,079 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:35,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:35,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:35,200 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:35,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:35,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:35,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:35,468 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:35,585 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:35,608 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:36,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:36,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:37,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:37,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:43,084 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 09:17:43,281 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 663359 virtual documents
2025-12-21 09:17:44,080 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 09:17:50,724 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5593 virtual)
2025-12-21 09:17:50,726 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (10727 virtual)
2025-12-21 09:17:50,729 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (26432 virtual)
2025-12-21 09:17:50,731 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (30159 virtual)
2025-12-21 09:17:50,733 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (32725 virtual)
2025-12-21 09:17:50,734 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (36278 virtual)
2025-12-21 09:17:50,735 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (39220 virtual)
2025-12-21 09:17:50,738 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (54327 virtual)
2025-12-21 09:17:50,740 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (60577 virtual)
2025-12-21 09:17:50,741 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (63971 virtual)
2025-12-21 09:17:50,742 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (68747 virtual)
2025-12-21 09:17:50,744 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (78123 virtual)
2025-12-21 09:17:50,746 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (82521 virtual)
2025-12-21 09:17:50,748 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (90313 virtual)
2025-12-21 09:17:50,749 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (95725 virtual)
2025-12-21 09:17:50,750 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (100032 virtual)
2025-12-21 09:17:50,751 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (103002 virtual)
2025-12-21 09:17:50,753 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (109828 virtual)
2025-12-21 09:17:50,754 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (115400 virtual)
2025-12-21 09:17:50,756 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (124477 virtual)
2025-12-21 09:17:50,757 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (128588 virtual)
2025-12-21 09:17:50,758 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (131939 virtual)
2025-12-21 09:17:50,765 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (159346 virtual)
2025-12-21 09:17:50,768 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (175748 virtual)
2025-12-21 09:17:50,769 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (179483 virtual)
2025-12-21 09:17:50,771 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (186119 virtual)
2025-12-21 09:17:50,772 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (189134 virtual)
2025-12-21 09:17:50,773 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (194011 virtual)
2025-12-21 09:17:50,774 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (198338 virtual)
2025-12-21 09:17:50,775 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (201418 virtual)
2025-12-21 09:17:50,778 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (205336 virtual)
2025-12-21 09:17:50,792 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (210865 virtual)
2025-12-21 09:17:50,794 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (215487 virtual)
2025-12-21 09:17:50,795 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (222784 virtual)
2025-12-21 09:17:50,817 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (227542 virtual)
2025-12-21 09:17:50,819 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (237105 virtual)
2025-12-21 09:17:50,832 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (242098 virtual)
2025-12-21 09:17:50,844 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (246462 virtual)
2025-12-21 09:17:50,845 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (250096 virtual)
2025-12-21 09:17:50,846 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (255323 virtual)
2025-12-21 09:17:50,847 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (258628 virtual)
2025-12-21 09:17:50,849 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (263349 virtual)
2025-12-21 09:17:50,850 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (268233 virtual)
2025-12-21 09:17:50,851 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (274132 virtual)
2025-12-21 09:17:50,854 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (282971 virtual)
2025-12-21 09:17:50,860 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (286673 virtual)
2025-12-21 09:17:50,861 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (290264 virtual)
2025-12-21 09:17:50,862 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (296037 virtual)
2025-12-21 09:17:50,878 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (310124 virtual)
2025-12-21 09:17:51,097 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (317050 virtual)
2025-12-21 09:17:51,098 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (320598 virtual)
2025-12-21 09:17:51,149 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (334909 virtual)
2025-12-21 09:17:51,150 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (341683 virtual)
2025-12-21 09:17:51,213 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (351624 virtual)
2025-12-21 09:17:51,219 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (360719 virtual)
2025-12-21 09:17:51,311 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (365651 virtual)
2025-12-21 09:17:51,313 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (375582 virtual)
2025-12-21 09:17:51,328 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (380043 virtual)
2025-12-21 09:17:51,395 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (390262 virtual)
2025-12-21 09:17:51,396 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (394060 virtual)
2025-12-21 09:17:51,398 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (398804 virtual)
2025-12-21 09:17:51,399 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (402970 virtual)
2025-12-21 09:17:51,463 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (417617 virtual)
2025-12-21 09:17:51,477 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (425918 virtual)
2025-12-21 09:17:51,567 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (437460 virtual)
2025-12-21 09:17:51,572 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (441365 virtual)
2025-12-21 09:17:51,575 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (446589 virtual)
2025-12-21 09:17:51,576 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (450244 virtual)
2025-12-21 09:17:51,651 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (478452 virtual)
2025-12-21 09:17:51,664 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (482978 virtual)
2025-12-21 09:17:51,711 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (494166 virtual)
2025-12-21 09:17:51,725 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (502241 virtual)
2025-12-21 09:17:51,779 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (518522 virtual)
2025-12-21 09:17:51,781 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (526287 virtual)
2025-12-21 09:17:51,796 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (532913 virtual)
2025-12-21 09:17:51,867 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (536619 virtual)
2025-12-21 09:17:51,869 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (548493 virtual)
2025-12-21 09:17:51,871 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (551958 virtual)
2025-12-21 09:17:51,872 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (555379 virtual)
2025-12-21 09:17:51,920 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (564992 virtual)
2025-12-21 09:17:51,932 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (568990 virtual)
2025-12-21 09:17:51,933 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (574796 virtual)
2025-12-21 09:17:51,939 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (581815 virtual)
2025-12-21 09:17:51,976 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (585873 virtual)
2025-12-21 09:17:51,988 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (591771 virtual)
2025-12-21 09:17:51,990 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (597046 virtual)
2025-12-21 09:17:51,996 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (601849 virtual)
2025-12-21 09:17:52,054 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (608725 virtual)
2025-12-21 09:17:52,079 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (615021 virtual)
2025-12-21 09:17:52,092 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (621265 virtual)
2025-12-21 09:17:52,094 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (627724 virtual)
2025-12-21 09:17:52,141 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (631679 virtual)
2025-12-21 09:17:52,152 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (635736 virtual)
2025-12-21 09:17:52,161 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (643713 virtual)
2025-12-21 09:17:52,167 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (649874 virtual)
2025-12-21 09:17:52,241 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (658090 virtual)
2025-12-21 09:17:52,252 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (661695 virtual)
2025-12-21 09:17:52,259 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (669808 virtual)
2025-12-21 09:17:52,260 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (673686 virtual)
2025-12-21 09:17:52,327 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (678673 virtual)
2025-12-21 09:17:52,329 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (683677 virtual)
2025-12-21 09:17:52,330 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (687618 virtual)
2025-12-21 09:17:52,333 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (698272 virtual)
2025-12-21 09:17:52,405 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (703739 virtual)
2025-12-21 09:17:52,408 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (712044 virtual)
2025-12-21 09:17:52,410 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (719942 virtual)
2025-12-21 09:17:52,478 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (730593 virtual)
2025-12-21 09:17:52,493 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (738698 virtual)
2025-12-21 09:17:52,494 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (744918 virtual)
2025-12-21 09:17:52,545 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (748587 virtual)
2025-12-21 09:17:52,556 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (754283 virtual)
2025-12-21 09:17:52,603 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (759411 virtual)
2025-12-21 09:17:52,616 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (763074 virtual)
2025-12-21 09:17:52,659 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (769146 virtual)
2025-12-21 09:17:52,661 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (778550 virtual)
2025-12-21 09:17:52,676 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (782687 virtual)
2025-12-21 09:17:52,680 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (787370 virtual)
2025-12-21 09:17:52,760 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (792747 virtual)
2025-12-21 09:17:52,772 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (795834 virtual)
2025-12-21 09:17:52,834 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (799472 virtual)
2025-12-21 09:17:52,836 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (804251 virtual)
2025-12-21 09:17:52,839 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (818289 virtual)
2025-12-21 09:17:52,891 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (823742 virtual)
2025-12-21 09:17:52,892 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (829121 virtual)
2025-12-21 09:17:52,895 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (839918 virtual)
2025-12-21 09:17:52,947 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (843357 virtual)
2025-12-21 09:17:52,948 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (847194 virtual)
2025-12-21 09:17:52,949 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (850480 virtual)
2025-12-21 09:17:52,956 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (854672 virtual)
2025-12-21 09:17:53,019 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (863027 virtual)
2025-12-21 09:17:53,020 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (865772 virtual)
2025-12-21 09:17:53,079 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (884808 virtual)
2025-12-21 09:17:53,092 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (889004 virtual)
2025-12-21 09:17:53,155 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (894985 virtual)
2025-12-21 09:17:53,168 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (898703 virtual)
2025-12-21 09:17:53,169 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (901982 virtual)
2025-12-21 09:17:53,171 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (906587 virtual)
2025-12-21 09:17:53,207 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (910463 virtual)
2025-12-21 09:17:53,260 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (931102 virtual)
2025-12-21 09:17:53,323 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (942622 virtual)
2025-12-21 09:17:53,328 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (951129 virtual)
2025-12-21 09:17:53,336 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (955276 virtual)
2025-12-21 09:17:53,348 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (959539 virtual)
2025-12-21 09:17:53,364 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (965946 virtual)
2025-12-21 09:17:53,366 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (970853 virtual)
2025-12-21 09:17:53,380 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (973829 virtual)
2025-12-21 09:17:53,443 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (989097 virtual)
2025-12-21 09:17:53,445 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (994234 virtual)
2025-12-21 09:17:53,446 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (997762 virtual)
2025-12-21 09:17:53,447 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (1000793 virtual)
2025-12-21 09:17:53,488 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (1006413 virtual)
2025-12-21 09:17:53,490 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (1011923 virtual)
2025-12-21 09:17:53,491 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (1014946 virtual)
2025-12-21 09:17:53,533 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (1018740 virtual)
2025-12-21 09:17:53,534 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (1022376 virtual)
2025-12-21 09:17:53,536 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (1026825 virtual)
2025-12-21 09:17:53,539 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (1040358 virtual)
2025-12-21 09:17:53,579 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (1044737 virtual)
2025-12-21 09:17:53,592 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (1050861 virtual)
2025-12-21 09:17:53,594 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (1056370 virtual)
2025-12-21 09:17:53,635 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (1062228 virtual)
2025-12-21 09:17:53,637 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (1074392 virtual)
2025-12-21 09:17:53,652 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (1078963 virtual)
2025-12-21 09:17:53,708 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (1082443 virtual)
2025-12-21 09:17:53,720 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (1088508 virtual)
2025-12-21 09:17:53,727 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (1093083 virtual)
2025-12-21 09:17:53,728 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (1096984 virtual)
2025-12-21 09:17:53,779 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (1101614 virtual)
2025-12-21 09:17:53,780 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (1105041 virtual)
2025-12-21 09:17:53,782 INFO gensim.topic_coherence.text_analysis: 170 batches submitted to accumulate stats from 10880 documents (1112474 virtual)
2025-12-21 09:17:53,784 INFO gensim.topic_coherence.text_analysis: 171 batches submitted to accumulate stats from 10944 documents (1119161 virtual)
2025-12-21 09:17:53,851 INFO gensim.topic_coherence.text_analysis: 172 batches submitted to accumulate stats from 11008 documents (1123702 virtual)
2025-12-21 09:17:53,863 INFO gensim.topic_coherence.text_analysis: 173 batches submitted to accumulate stats from 11072 documents (1123966 virtual)
2025-12-21 09:17:53,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:53,917 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,989 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:53,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,138 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,698 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,770 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,860 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,922 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:54,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:54,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,044 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,035 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,244 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,291 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,408 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,697 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:55,786 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:55,944 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:56,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:56,050 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:56,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:56,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:56,124 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:56,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:56,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:57,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:57,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:57,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:57,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:57,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:57,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:57,841 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:57,842 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:17:58,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:17:58,213 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:18:04,340 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 09:18:04,821 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1127780 virtual documents
2025-12-21 09:18:05,388 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 11014 docs
Training CobwebTree:   0%|          | 0/11014 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 23/11014 [00:00<00:48, 228.95it/s]Training CobwebTree:   0%|          | 46/11014 [00:00<01:05, 166.52it/s]Training CobwebTree:   1%|          | 64/11014 [00:00<01:14, 147.22it/s]Training CobwebTree:   1%|          | 80/11014 [00:00<01:21, 133.61it/s]Training CobwebTree:   1%|          | 94/11014 [00:00<01:26, 126.30it/s]Training CobwebTree:   1%|          | 107/11014 [00:00<01:28, 123.63it/s]Training CobwebTree:   1%|          | 120/11014 [00:00<01:32, 117.99it/s]Training CobwebTree:   1%|          | 132/11014 [00:01<01:41, 106.75it/s]Training CobwebTree:   1%|         | 144/11014 [00:01<01:40, 108.24it/s]Training CobwebTree:   1%|         | 155/11014 [00:01<01:44, 103.78it/s]Training CobwebTree:   2%|         | 166/11014 [00:01<01:50, 98.16it/s] Training CobwebTree:   2%|         | 177/11014 [00:01<01:49, 98.86it/s]Training CobwebTree:   2%|         | 187/11014 [00:01<01:53, 95.52it/s]Training CobwebTree:   2%|         | 198/11014 [00:01<01:52, 96.11it/s]Training CobwebTree:   2%|         | 208/11014 [00:01<01:52, 96.10it/s]Training CobwebTree:   2%|         | 218/11014 [00:01<01:56, 92.58it/s]Training CobwebTree:   2%|         | 228/11014 [00:02<01:58, 90.95it/s]Training CobwebTree:   2%|         | 238/11014 [00:02<01:59, 89.91it/s]Training CobwebTree:   2%|         | 248/11014 [00:02<01:59, 90.27it/s]Training CobwebTree:   2%|         | 258/11014 [00:02<01:59, 89.95it/s]Training CobwebTree:   2%|         | 268/11014 [00:02<01:59, 89.69it/s]Training CobwebTree:   3%|         | 278/11014 [00:02<01:58, 90.57it/s]Training CobwebTree:   3%|         | 288/11014 [00:02<02:01, 88.09it/s]Training CobwebTree:   3%|         | 297/11014 [00:02<02:08, 83.60it/s]Training CobwebTree:   3%|         | 306/11014 [00:02<02:09, 82.77it/s]Training CobwebTree:   3%|         | 315/11014 [00:03<02:06, 84.33it/s]Training CobwebTree:   3%|         | 324/11014 [00:03<02:09, 82.27it/s]Training CobwebTree:   3%|         | 333/11014 [00:03<02:13, 80.01it/s]Training CobwebTree:   3%|         | 342/11014 [00:03<02:09, 82.14it/s]Training CobwebTree:   3%|         | 351/11014 [00:03<02:12, 80.62it/s]Training CobwebTree:   3%|         | 360/11014 [00:03<02:12, 80.53it/s]Training CobwebTree:   3%|         | 370/11014 [00:03<02:06, 84.22it/s]Training CobwebTree:   3%|         | 379/11014 [00:03<02:09, 82.05it/s]Training CobwebTree:   4%|         | 388/11014 [00:03<02:06, 83.85it/s]Training CobwebTree:   4%|         | 397/11014 [00:04<02:07, 83.15it/s]Training CobwebTree:   4%|         | 406/11014 [00:04<02:07, 83.21it/s]Training CobwebTree:   4%|         | 415/11014 [00:04<02:09, 81.95it/s]Training CobwebTree:   4%|         | 425/11014 [00:04<02:05, 84.05it/s]Training CobwebTree:   4%|         | 434/11014 [00:04<02:10, 80.92it/s]Training CobwebTree:   4%|         | 443/11014 [00:04<02:08, 81.98it/s]Training CobwebTree:   4%|         | 452/11014 [00:04<02:09, 81.44it/s]Training CobwebTree:   4%|         | 461/11014 [00:04<02:07, 82.89it/s]Training CobwebTree:   4%|         | 470/11014 [00:04<02:11, 79.95it/s]Training CobwebTree:   4%|         | 479/11014 [00:05<02:11, 79.92it/s]Training CobwebTree:   4%|         | 488/11014 [00:05<02:11, 80.08it/s]Training CobwebTree:   5%|         | 497/11014 [00:05<02:08, 81.65it/s]Training CobwebTree:   5%|         | 506/11014 [00:05<02:10, 80.35it/s]Training CobwebTree:   5%|         | 516/11014 [00:05<02:06, 83.25it/s]Training CobwebTree:   5%|         | 525/11014 [00:05<02:10, 80.58it/s]Training CobwebTree:   5%|         | 534/11014 [00:05<02:11, 79.45it/s]Training CobwebTree:   5%|         | 542/11014 [00:05<02:14, 77.76it/s]Training CobwebTree:   5%|         | 550/11014 [00:05<02:13, 78.26it/s]Training CobwebTree:   5%|         | 558/11014 [00:06<02:14, 77.71it/s]Training CobwebTree:   5%|         | 566/11014 [00:06<02:16, 76.60it/s]Training CobwebTree:   5%|         | 575/11014 [00:06<02:13, 78.04it/s]Training CobwebTree:   5%|         | 584/11014 [00:06<02:12, 78.77it/s]Training CobwebTree:   5%|         | 593/11014 [00:06<02:09, 80.45it/s]Training CobwebTree:   5%|         | 602/11014 [00:06<02:10, 79.82it/s]Training CobwebTree:   6%|         | 610/11014 [00:06<02:14, 77.50it/s]Training CobwebTree:   6%|         | 618/11014 [00:06<02:15, 76.83it/s]Training CobwebTree:   6%|         | 626/11014 [00:06<02:17, 75.74it/s]Training CobwebTree:   6%|         | 634/11014 [00:07<02:23, 72.19it/s]Training CobwebTree:   6%|         | 642/11014 [00:07<02:21, 73.44it/s]Training CobwebTree:   6%|         | 650/11014 [00:07<02:22, 72.77it/s]Training CobwebTree:   6%|         | 658/11014 [00:07<02:22, 72.65it/s]Training CobwebTree:   6%|         | 666/11014 [00:07<02:19, 74.01it/s]Training CobwebTree:   6%|         | 674/11014 [00:07<02:20, 73.39it/s]Training CobwebTree:   6%|         | 682/11014 [00:07<02:19, 73.89it/s]Training CobwebTree:   6%|         | 690/11014 [00:07<02:21, 72.79it/s]Training CobwebTree:   6%|         | 698/11014 [00:07<02:27, 69.91it/s]Training CobwebTree:   6%|         | 706/11014 [00:08<02:29, 68.94it/s]Training CobwebTree:   6%|         | 713/11014 [00:08<02:31, 68.16it/s]Training CobwebTree:   7%|         | 721/11014 [00:08<02:28, 69.25it/s]Training CobwebTree:   7%|         | 728/11014 [00:08<02:28, 69.43it/s]Training CobwebTree:   7%|         | 736/11014 [00:08<02:26, 70.37it/s]Training CobwebTree:   7%|         | 744/11014 [00:08<02:22, 72.14it/s]Training CobwebTree:   7%|         | 752/11014 [00:08<02:19, 73.68it/s]Training CobwebTree:   7%|         | 760/11014 [00:08<02:16, 75.39it/s]Training CobwebTree:   7%|         | 768/11014 [00:08<02:17, 74.65it/s]Training CobwebTree:   7%|         | 776/11014 [00:09<02:17, 74.48it/s]Training CobwebTree:   7%|         | 784/11014 [00:09<02:23, 71.49it/s]Training CobwebTree:   7%|         | 792/11014 [00:09<02:27, 69.08it/s]Training CobwebTree:   7%|         | 799/11014 [00:09<02:28, 68.91it/s]Training CobwebTree:   7%|         | 806/11014 [00:09<02:30, 67.83it/s]Training CobwebTree:   7%|         | 814/11014 [00:09<02:26, 69.52it/s]Training CobwebTree:   7%|         | 822/11014 [00:09<02:23, 71.08it/s]Training CobwebTree:   8%|         | 830/11014 [00:09<02:24, 70.43it/s]Training CobwebTree:   8%|         | 838/11014 [00:09<02:23, 71.13it/s]Training CobwebTree:   8%|         | 846/11014 [00:10<02:21, 71.99it/s]Training CobwebTree:   8%|         | 854/11014 [00:10<02:22, 71.49it/s]Training CobwebTree:   8%|         | 863/11014 [00:10<02:14, 75.64it/s]Training CobwebTree:   8%|         | 871/11014 [00:10<02:19, 72.54it/s]Training CobwebTree:   8%|         | 879/11014 [00:10<02:19, 72.84it/s]Training CobwebTree:   8%|         | 887/11014 [00:10<02:25, 69.44it/s]Training CobwebTree:   8%|         | 894/11014 [00:10<02:26, 69.24it/s]Training CobwebTree:   8%|         | 902/11014 [00:10<02:25, 69.67it/s]Training CobwebTree:   8%|         | 910/11014 [00:10<02:20, 71.99it/s]Training CobwebTree:   8%|         | 918/11014 [00:11<02:25, 69.55it/s]Training CobwebTree:   8%|         | 926/11014 [00:11<02:22, 70.89it/s]Training CobwebTree:   8%|         | 934/11014 [00:11<02:20, 71.52it/s]Training CobwebTree:   9%|         | 942/11014 [00:11<02:24, 69.66it/s]Training CobwebTree:   9%|         | 950/11014 [00:11<02:23, 70.26it/s]Training CobwebTree:   9%|         | 958/11014 [00:11<02:25, 69.29it/s]Training CobwebTree:   9%|         | 966/11014 [00:11<02:24, 69.51it/s]Training CobwebTree:   9%|         | 973/11014 [00:11<02:26, 68.55it/s]Training CobwebTree:   9%|         | 980/11014 [00:11<02:32, 65.67it/s]Training CobwebTree:   9%|         | 987/11014 [00:12<02:33, 65.49it/s]Training CobwebTree:   9%|         | 994/11014 [00:12<02:32, 65.53it/s]Training CobwebTree:   9%|         | 1002/11014 [00:12<02:27, 67.80it/s]Training CobwebTree:   9%|         | 1010/11014 [00:12<02:20, 71.03it/s]Training CobwebTree:   9%|         | 1018/11014 [00:12<02:21, 70.41it/s]Training CobwebTree:   9%|         | 1026/11014 [00:12<02:23, 69.60it/s]Training CobwebTree:   9%|         | 1033/11014 [00:12<02:26, 68.21it/s]Training CobwebTree:   9%|         | 1041/11014 [00:12<02:25, 68.49it/s]Training CobwebTree:  10%|         | 1048/11014 [00:12<02:27, 67.74it/s]Training CobwebTree:  10%|         | 1056/11014 [00:13<02:23, 69.32it/s]Training CobwebTree:  10%|         | 1063/11014 [00:13<02:23, 69.45it/s]Training CobwebTree:  10%|         | 1070/11014 [00:13<02:26, 68.09it/s]Training CobwebTree:  10%|         | 1078/11014 [00:13<02:22, 69.60it/s]Training CobwebTree:  10%|         | 1085/11014 [00:13<02:26, 67.61it/s]Training CobwebTree:  10%|         | 1092/11014 [00:13<02:26, 67.73it/s]Training CobwebTree:  10%|         | 1099/11014 [00:13<02:28, 66.96it/s]Training CobwebTree:  10%|         | 1106/11014 [00:13<02:31, 65.30it/s]Training CobwebTree:  10%|         | 1114/11014 [00:13<02:25, 67.97it/s]Training CobwebTree:  10%|         | 1121/11014 [00:14<02:25, 68.12it/s]Training CobwebTree:  10%|         | 1129/11014 [00:14<02:21, 69.83it/s]Training CobwebTree:  10%|         | 1136/11014 [00:14<02:21, 69.69it/s]Training CobwebTree:  10%|         | 1143/11014 [00:14<02:23, 68.77it/s]Training CobwebTree:  10%|         | 1150/11014 [00:14<02:27, 67.08it/s]Training CobwebTree:  11%|         | 1157/11014 [00:14<02:25, 67.64it/s]Training CobwebTree:  11%|         | 1164/11014 [00:14<02:32, 64.77it/s]Training CobwebTree:  11%|         | 1171/11014 [00:14<02:34, 63.86it/s]Training CobwebTree:  11%|         | 1178/11014 [00:14<02:32, 64.41it/s]Training CobwebTree:  11%|         | 1186/11014 [00:15<02:27, 66.85it/s]Training CobwebTree:  11%|         | 1193/11014 [00:15<02:30, 65.43it/s]Training CobwebTree:  11%|         | 1200/11014 [00:15<02:30, 65.15it/s]Training CobwebTree:  11%|         | 1208/11014 [00:15<02:28, 65.83it/s]Training CobwebTree:  11%|         | 1215/11014 [00:15<02:27, 66.60it/s]Training CobwebTree:  11%|         | 1222/11014 [00:15<02:28, 65.77it/s]Training CobwebTree:  11%|         | 1229/11014 [00:15<02:29, 65.45it/s]Training CobwebTree:  11%|         | 1236/11014 [00:15<02:29, 65.43it/s]Training CobwebTree:  11%|        | 1243/11014 [00:15<02:29, 65.35it/s]Training CobwebTree:  11%|        | 1251/11014 [00:16<02:22, 68.35it/s]Training CobwebTree:  11%|        | 1258/11014 [00:16<02:21, 68.72it/s]Training CobwebTree:  11%|        | 1265/11014 [00:16<02:23, 68.03it/s]Training CobwebTree:  12%|        | 1272/11014 [00:16<02:25, 67.15it/s]Training CobwebTree:  12%|        | 1280/11014 [00:16<02:25, 66.69it/s]Training CobwebTree:  12%|        | 1287/11014 [00:16<02:25, 66.85it/s]Training CobwebTree:  12%|        | 1294/11014 [00:16<02:27, 66.04it/s]Training CobwebTree:  12%|        | 1301/11014 [00:16<02:31, 64.24it/s]Training CobwebTree:  12%|        | 1309/11014 [00:16<02:24, 67.29it/s]Training CobwebTree:  12%|        | 1316/11014 [00:16<02:24, 67.03it/s]Training CobwebTree:  12%|        | 1324/11014 [00:17<02:23, 67.41it/s]Training CobwebTree:  12%|        | 1331/11014 [00:17<02:25, 66.41it/s]Training CobwebTree:  12%|        | 1338/11014 [00:17<02:23, 67.25it/s]Training CobwebTree:  12%|        | 1346/11014 [00:17<02:18, 69.79it/s]Training CobwebTree:  12%|        | 1353/11014 [00:17<02:26, 66.17it/s]Training CobwebTree:  12%|        | 1360/11014 [00:17<02:23, 67.08it/s]Training CobwebTree:  12%|        | 1367/11014 [00:17<02:29, 64.64it/s]Training CobwebTree:  12%|        | 1375/11014 [00:17<02:25, 66.43it/s]Training CobwebTree:  13%|        | 1382/11014 [00:17<02:25, 66.39it/s]Training CobwebTree:  13%|        | 1389/11014 [00:18<02:24, 66.59it/s]Training CobwebTree:  13%|        | 1396/11014 [00:18<02:23, 67.13it/s]Training CobwebTree:  13%|        | 1403/11014 [00:18<02:23, 66.78it/s]Training CobwebTree:  13%|        | 1410/11014 [00:18<02:24, 66.40it/s]Training CobwebTree:  13%|        | 1417/11014 [00:18<02:23, 66.91it/s]Training CobwebTree:  13%|        | 1424/11014 [00:18<02:28, 64.61it/s]Training CobwebTree:  13%|        | 1431/11014 [00:18<02:28, 64.71it/s]Training CobwebTree:  13%|        | 1438/11014 [00:18<02:26, 65.50it/s]Training CobwebTree:  13%|        | 1445/11014 [00:18<02:23, 66.66it/s]Training CobwebTree:  13%|        | 1452/11014 [00:19<02:22, 67.15it/s]Training CobwebTree:  13%|        | 1460/11014 [00:19<02:20, 68.11it/s]Training CobwebTree:  13%|        | 1467/11014 [00:19<02:22, 67.04it/s]Training CobwebTree:  13%|        | 1474/11014 [00:19<02:27, 64.65it/s]Training CobwebTree:  13%|        | 1482/11014 [00:19<02:25, 65.53it/s]Training CobwebTree:  14%|        | 1489/11014 [00:19<02:26, 65.00it/s]Training CobwebTree:  14%|        | 1496/11014 [00:19<02:34, 61.68it/s]Training CobwebTree:  14%|        | 1504/11014 [00:19<02:26, 64.74it/s]Training CobwebTree:  14%|        | 1511/11014 [00:19<02:24, 65.55it/s]Training CobwebTree:  14%|        | 1518/11014 [00:20<02:31, 62.86it/s]Training CobwebTree:  14%|        | 1525/11014 [00:20<02:29, 63.36it/s]Training CobwebTree:  14%|        | 1532/11014 [00:20<02:26, 64.92it/s]Training CobwebTree:  14%|        | 1539/11014 [00:20<02:23, 66.24it/s]Training CobwebTree:  14%|        | 1546/11014 [00:20<02:29, 63.54it/s]Training CobwebTree:  14%|        | 1553/11014 [00:20<02:27, 64.35it/s]Training CobwebTree:  14%|        | 1560/11014 [00:20<02:26, 64.32it/s]Training CobwebTree:  14%|        | 1567/11014 [00:20<02:28, 63.56it/s]Training CobwebTree:  14%|        | 1574/11014 [00:20<02:26, 64.32it/s]Training CobwebTree:  14%|        | 1582/11014 [00:21<02:22, 66.24it/s]Training CobwebTree:  14%|        | 1590/11014 [00:21<02:19, 67.39it/s]Training CobwebTree:  14%|        | 1597/11014 [00:21<02:19, 67.27it/s]Training CobwebTree:  15%|        | 1605/11014 [00:21<02:16, 69.18it/s]Training CobwebTree:  15%|        | 1612/11014 [00:21<02:18, 68.10it/s]Training CobwebTree:  15%|        | 1620/11014 [00:21<02:14, 70.08it/s]Training CobwebTree:  15%|        | 1628/11014 [00:21<02:13, 70.21it/s]Training CobwebTree:  15%|        | 1636/11014 [00:21<02:13, 70.17it/s]Training CobwebTree:  15%|        | 1644/11014 [00:21<02:13, 69.95it/s]Training CobwebTree:  15%|        | 1651/11014 [00:22<02:17, 68.27it/s]Training CobwebTree:  15%|        | 1658/11014 [00:22<02:18, 67.73it/s]Training CobwebTree:  15%|        | 1665/11014 [00:22<02:20, 66.53it/s]Training CobwebTree:  15%|        | 1672/11014 [00:22<02:20, 66.42it/s]Training CobwebTree:  15%|        | 1679/11014 [00:22<02:21, 65.81it/s]Training CobwebTree:  15%|        | 1686/11014 [00:22<02:21, 65.75it/s]Training CobwebTree:  15%|        | 1694/11014 [00:22<02:18, 67.47it/s]Training CobwebTree:  15%|        | 1701/11014 [00:22<02:17, 67.85it/s]Training CobwebTree:  16%|        | 1708/11014 [00:22<02:19, 66.48it/s]Training CobwebTree:  16%|        | 1715/11014 [00:23<02:21, 65.78it/s]Training CobwebTree:  16%|        | 1723/11014 [00:23<02:18, 67.12it/s]Training CobwebTree:  16%|        | 1730/11014 [00:23<02:23, 64.73it/s]Training CobwebTree:  16%|        | 1737/11014 [00:23<02:26, 63.17it/s]Training CobwebTree:  16%|        | 1744/11014 [00:23<02:23, 64.72it/s]Training CobwebTree:  16%|        | 1751/11014 [00:23<02:29, 62.12it/s]Training CobwebTree:  16%|        | 1758/11014 [00:23<02:29, 61.97it/s]Training CobwebTree:  16%|        | 1765/11014 [00:23<02:24, 63.88it/s]Training CobwebTree:  16%|        | 1772/11014 [00:23<02:22, 64.74it/s]Training CobwebTree:  16%|        | 1779/11014 [00:24<02:25, 63.48it/s]Training CobwebTree:  16%|        | 1786/11014 [00:24<02:23, 64.22it/s]Training CobwebTree:  16%|        | 1793/11014 [00:24<02:23, 64.23it/s]Training CobwebTree:  16%|        | 1801/11014 [00:24<02:18, 66.30it/s]Training CobwebTree:  16%|        | 1808/11014 [00:24<02:22, 64.69it/s]Training CobwebTree:  16%|        | 1815/11014 [00:24<02:19, 65.85it/s]Training CobwebTree:  17%|        | 1823/11014 [00:24<02:15, 67.98it/s]Training CobwebTree:  17%|        | 1830/11014 [00:24<02:16, 67.36it/s]Training CobwebTree:  17%|        | 1837/11014 [00:24<02:19, 65.94it/s]Training CobwebTree:  17%|        | 1844/11014 [00:24<02:20, 65.11it/s]Training CobwebTree:  17%|        | 1851/11014 [00:25<02:27, 62.02it/s]Training CobwebTree:  17%|        | 1858/11014 [00:25<02:24, 63.35it/s]Training CobwebTree:  17%|        | 1865/11014 [00:25<02:25, 63.08it/s]Training CobwebTree:  17%|        | 1873/11014 [00:25<02:21, 64.43it/s]Training CobwebTree:  17%|        | 1880/11014 [00:25<02:21, 64.33it/s]Training CobwebTree:  17%|        | 1887/11014 [00:25<02:23, 63.54it/s]Training CobwebTree:  17%|        | 1894/11014 [00:25<02:22, 64.12it/s]Training CobwebTree:  17%|        | 1901/11014 [00:25<02:24, 63.24it/s]Training CobwebTree:  17%|        | 1908/11014 [00:26<02:20, 64.80it/s]Training CobwebTree:  17%|        | 1916/11014 [00:26<02:17, 66.25it/s]Training CobwebTree:  17%|        | 1923/11014 [00:26<02:19, 65.38it/s]Training CobwebTree:  18%|        | 1930/11014 [00:26<02:18, 65.50it/s]Training CobwebTree:  18%|        | 1937/11014 [00:26<02:18, 65.75it/s]Training CobwebTree:  18%|        | 1944/11014 [00:26<02:18, 65.43it/s]Training CobwebTree:  18%|        | 1951/11014 [00:26<02:19, 65.20it/s]Training CobwebTree:  18%|        | 1958/11014 [00:26<02:16, 66.41it/s]Training CobwebTree:  18%|        | 1965/11014 [00:26<02:22, 63.40it/s]Training CobwebTree:  18%|        | 1973/11014 [00:26<02:17, 65.67it/s]Training CobwebTree:  18%|        | 1980/11014 [00:27<02:19, 64.77it/s]Training CobwebTree:  18%|        | 1987/11014 [00:27<02:20, 64.42it/s]Training CobwebTree:  18%|        | 1994/11014 [00:27<02:21, 63.90it/s]Training CobwebTree:  18%|        | 2001/11014 [00:27<02:24, 62.31it/s]Training CobwebTree:  18%|        | 2008/11014 [00:27<02:22, 63.12it/s]Training CobwebTree:  18%|        | 2015/11014 [00:27<02:31, 59.52it/s]Training CobwebTree:  18%|        | 2022/11014 [00:27<02:31, 59.49it/s]Training CobwebTree:  18%|        | 2030/11014 [00:27<02:25, 61.77it/s]Training CobwebTree:  18%|        | 2037/11014 [00:28<02:29, 60.00it/s]Training CobwebTree:  19%|        | 2044/11014 [00:28<02:28, 60.27it/s]Training CobwebTree:  19%|        | 2051/11014 [00:28<02:23, 62.38it/s]Training CobwebTree:  19%|        | 2058/11014 [00:28<02:23, 62.56it/s]Training CobwebTree:  19%|        | 2065/11014 [00:28<02:24, 61.96it/s]Training CobwebTree:  19%|        | 2072/11014 [00:28<02:25, 61.41it/s]Training CobwebTree:  19%|        | 2079/11014 [00:28<02:21, 63.11it/s]Training CobwebTree:  19%|        | 2087/11014 [00:28<02:14, 66.25it/s]Training CobwebTree:  19%|        | 2094/11014 [00:28<02:16, 65.28it/s]Training CobwebTree:  19%|        | 2101/11014 [00:29<02:16, 65.47it/s]Training CobwebTree:  19%|        | 2109/11014 [00:29<02:13, 66.78it/s]Training CobwebTree:  19%|        | 2116/11014 [00:29<02:16, 65.24it/s]Training CobwebTree:  19%|        | 2123/11014 [00:29<02:19, 63.91it/s]Training CobwebTree:  19%|        | 2130/11014 [00:29<02:20, 63.27it/s]Training CobwebTree:  19%|        | 2137/11014 [00:29<02:26, 60.52it/s]Training CobwebTree:  19%|        | 2144/11014 [00:29<02:21, 62.50it/s]Training CobwebTree:  20%|        | 2151/11014 [00:29<02:19, 63.41it/s]Training CobwebTree:  20%|        | 2158/11014 [00:29<02:17, 64.45it/s]Training CobwebTree:  20%|        | 2165/11014 [00:30<02:26, 60.37it/s]Training CobwebTree:  20%|        | 2172/11014 [00:30<02:22, 62.25it/s]Training CobwebTree:  20%|        | 2179/11014 [00:30<02:20, 63.04it/s]Training CobwebTree:  20%|        | 2186/11014 [00:30<02:21, 62.37it/s]Training CobwebTree:  20%|        | 2193/11014 [00:30<02:20, 62.81it/s]Training CobwebTree:  20%|        | 2200/11014 [00:30<02:19, 63.30it/s]Training CobwebTree:  20%|        | 2207/11014 [00:30<02:24, 60.95it/s]Training CobwebTree:  20%|        | 2214/11014 [00:30<02:22, 61.71it/s]Training CobwebTree:  20%|        | 2221/11014 [00:30<02:21, 61.94it/s]Training CobwebTree:  20%|        | 2228/11014 [00:31<02:25, 60.42it/s]Training CobwebTree:  20%|        | 2235/11014 [00:31<02:22, 61.72it/s]Training CobwebTree:  20%|        | 2242/11014 [00:31<02:18, 63.28it/s]Training CobwebTree:  20%|        | 2249/11014 [00:31<02:15, 64.58it/s]Training CobwebTree:  20%|        | 2256/11014 [00:31<02:15, 64.51it/s]Training CobwebTree:  21%|        | 2263/11014 [00:31<02:16, 64.26it/s]Training CobwebTree:  21%|        | 2270/11014 [00:31<02:16, 64.22it/s]Training CobwebTree:  21%|        | 2277/11014 [00:31<02:14, 65.17it/s]Training CobwebTree:  21%|        | 2285/11014 [00:31<02:14, 65.00it/s]Training CobwebTree:  21%|        | 2292/11014 [00:32<02:14, 65.02it/s]Training CobwebTree:  21%|        | 2299/11014 [00:32<02:16, 63.62it/s]Training CobwebTree:  21%|        | 2306/11014 [00:32<02:15, 64.09it/s]Training CobwebTree:  21%|        | 2313/11014 [00:32<02:15, 64.29it/s]Training CobwebTree:  21%|        | 2320/11014 [00:32<02:20, 61.77it/s]Training CobwebTree:  21%|        | 2327/11014 [00:32<02:21, 61.29it/s]Training CobwebTree:  21%|        | 2334/11014 [00:32<02:20, 61.69it/s]Training CobwebTree:  21%|       | 2341/11014 [00:32<02:23, 60.65it/s]Training CobwebTree:  21%|       | 2348/11014 [00:32<02:18, 62.44it/s]Training CobwebTree:  21%|       | 2355/11014 [00:33<02:25, 59.50it/s]Training CobwebTree:  21%|       | 2363/11014 [00:33<02:18, 62.46it/s]Training CobwebTree:  22%|       | 2370/11014 [00:33<02:21, 61.29it/s]Training CobwebTree:  22%|       | 2377/11014 [00:33<02:23, 60.05it/s]Training CobwebTree:  22%|       | 2385/11014 [00:33<02:14, 63.99it/s]Training CobwebTree:  22%|       | 2392/11014 [00:33<02:16, 63.25it/s]Training CobwebTree:  22%|       | 2399/11014 [00:33<02:19, 61.78it/s]Training CobwebTree:  22%|       | 2406/11014 [00:33<02:20, 61.35it/s]Training CobwebTree:  22%|       | 2413/11014 [00:34<02:16, 62.85it/s]Training CobwebTree:  22%|       | 2420/11014 [00:34<02:18, 62.11it/s]Training CobwebTree:  22%|       | 2427/11014 [00:34<02:15, 63.16it/s]Training CobwebTree:  22%|       | 2434/11014 [00:34<02:16, 63.01it/s]Training CobwebTree:  22%|       | 2441/11014 [00:34<02:17, 62.50it/s]Training CobwebTree:  22%|       | 2448/11014 [00:34<02:14, 63.75it/s]Training CobwebTree:  22%|       | 2455/11014 [00:34<02:14, 63.73it/s]Training CobwebTree:  22%|       | 2462/11014 [00:34<02:15, 62.92it/s]Training CobwebTree:  22%|       | 2469/11014 [00:34<02:13, 64.22it/s]Training CobwebTree:  22%|       | 2476/11014 [00:35<02:14, 63.49it/s]Training CobwebTree:  23%|       | 2483/11014 [00:35<02:16, 62.44it/s]Training CobwebTree:  23%|       | 2490/11014 [00:35<02:20, 60.50it/s]Training CobwebTree:  23%|       | 2498/11014 [00:35<02:14, 63.28it/s]Training CobwebTree:  23%|       | 2505/11014 [00:35<02:12, 64.35it/s]Training CobwebTree:  23%|       | 2512/11014 [00:35<02:14, 63.41it/s]Training CobwebTree:  23%|       | 2519/11014 [00:35<02:12, 64.01it/s]Training CobwebTree:  23%|       | 2526/11014 [00:35<02:14, 63.09it/s]Training CobwebTree:  23%|       | 2533/11014 [00:35<02:13, 63.37it/s]Training CobwebTree:  23%|       | 2540/11014 [00:36<02:12, 63.81it/s]Training CobwebTree:  23%|       | 2547/11014 [00:36<02:14, 63.14it/s]Training CobwebTree:  23%|       | 2554/11014 [00:36<02:14, 62.68it/s]Training CobwebTree:  23%|       | 2561/11014 [00:36<02:15, 62.30it/s]Training CobwebTree:  23%|       | 2568/11014 [00:36<02:15, 62.46it/s]Training CobwebTree:  23%|       | 2575/11014 [00:36<02:19, 60.71it/s]Training CobwebTree:  23%|       | 2582/11014 [00:36<02:19, 60.46it/s]Training CobwebTree:  24%|       | 2589/11014 [00:36<02:23, 58.77it/s]Training CobwebTree:  24%|       | 2595/11014 [00:36<02:23, 58.54it/s]Training CobwebTree:  24%|       | 2602/11014 [00:37<02:21, 59.31it/s]Training CobwebTree:  24%|       | 2609/11014 [00:37<02:19, 60.11it/s]Training CobwebTree:  24%|       | 2616/11014 [00:37<02:16, 61.70it/s]Training CobwebTree:  24%|       | 2623/11014 [00:37<02:19, 60.28it/s]Training CobwebTree:  24%|       | 2630/11014 [00:37<02:14, 62.52it/s]Training CobwebTree:  24%|       | 2637/11014 [00:37<02:12, 63.37it/s]Training CobwebTree:  24%|       | 2644/11014 [00:37<02:09, 64.85it/s]Training CobwebTree:  24%|       | 2651/11014 [00:37<02:12, 63.21it/s]Training CobwebTree:  24%|       | 2658/11014 [00:37<02:12, 62.91it/s]Training CobwebTree:  24%|       | 2665/11014 [00:38<02:14, 62.14it/s]Training CobwebTree:  24%|       | 2672/11014 [00:38<02:16, 61.30it/s]Training CobwebTree:  24%|       | 2679/11014 [00:38<02:16, 61.24it/s]Training CobwebTree:  24%|       | 2686/11014 [00:38<02:14, 62.00it/s]Training CobwebTree:  24%|       | 2693/11014 [00:38<02:17, 60.54it/s]Training CobwebTree:  25%|       | 2700/11014 [00:38<02:18, 59.88it/s]Training CobwebTree:  25%|       | 2706/11014 [00:38<02:19, 59.58it/s]Training CobwebTree:  25%|       | 2713/11014 [00:38<02:16, 60.64it/s]Training CobwebTree:  25%|       | 2720/11014 [00:38<02:12, 62.82it/s]Training CobwebTree:  25%|       | 2727/11014 [00:39<02:14, 61.40it/s]Training CobwebTree:  25%|       | 2734/11014 [00:39<02:12, 62.27it/s]Training CobwebTree:  25%|       | 2741/11014 [00:39<02:12, 62.41it/s]Training CobwebTree:  25%|       | 2748/11014 [00:39<02:13, 61.84it/s]Training CobwebTree:  25%|       | 2755/11014 [00:39<02:12, 62.25it/s]Training CobwebTree:  25%|       | 2762/11014 [00:39<02:17, 59.80it/s]Training CobwebTree:  25%|       | 2769/11014 [00:39<02:14, 61.27it/s]Training CobwebTree:  25%|       | 2776/11014 [00:39<02:12, 62.12it/s]Training CobwebTree:  25%|       | 2783/11014 [00:39<02:10, 63.00it/s]Training CobwebTree:  25%|       | 2790/11014 [00:40<02:12, 62.24it/s]Training CobwebTree:  25%|       | 2797/11014 [00:40<02:14, 61.04it/s]Training CobwebTree:  25%|       | 2804/11014 [00:40<02:15, 60.64it/s]Training CobwebTree:  26%|       | 2811/11014 [00:40<02:15, 60.65it/s]Training CobwebTree:  26%|       | 2818/11014 [00:40<02:16, 60.12it/s]Training CobwebTree:  26%|       | 2825/11014 [00:40<02:15, 60.61it/s]Training CobwebTree:  26%|       | 2832/11014 [00:40<02:13, 61.15it/s]Training CobwebTree:  26%|       | 2839/11014 [00:40<02:15, 60.15it/s]Training CobwebTree:  26%|       | 2846/11014 [00:41<02:20, 58.26it/s]Training CobwebTree:  26%|       | 2852/11014 [00:41<02:20, 57.95it/s]Training CobwebTree:  26%|       | 2859/11014 [00:41<02:18, 58.85it/s]Training CobwebTree:  26%|       | 2866/11014 [00:41<02:14, 60.77it/s]Training CobwebTree:  26%|       | 2873/11014 [00:41<02:13, 60.90it/s]Training CobwebTree:  26%|       | 2880/11014 [00:41<02:16, 59.65it/s]Training CobwebTree:  26%|       | 2887/11014 [00:41<02:17, 59.26it/s]Training CobwebTree:  26%|       | 2893/11014 [00:41<02:19, 58.11it/s]Training CobwebTree:  26%|       | 2900/11014 [00:41<02:16, 59.42it/s]Training CobwebTree:  26%|       | 2907/11014 [00:42<02:14, 60.09it/s]Training CobwebTree:  26%|       | 2914/11014 [00:42<02:14, 60.10it/s]Training CobwebTree:  27%|       | 2921/11014 [00:42<02:12, 61.04it/s]Training CobwebTree:  27%|       | 2928/11014 [00:42<02:13, 60.64it/s]Training CobwebTree:  27%|       | 2935/11014 [00:42<02:09, 62.18it/s]Training CobwebTree:  27%|       | 2942/11014 [00:42<02:10, 61.69it/s]Training CobwebTree:  27%|       | 2949/11014 [00:42<02:13, 60.47it/s]Training CobwebTree:  27%|       | 2956/11014 [00:42<02:16, 59.19it/s]Training CobwebTree:  27%|       | 2963/11014 [00:42<02:13, 60.22it/s]Training CobwebTree:  27%|       | 2970/11014 [00:43<02:10, 61.69it/s]Training CobwebTree:  27%|       | 2977/11014 [00:43<02:12, 60.51it/s]Training CobwebTree:  27%|       | 2984/11014 [00:43<02:09, 62.20it/s]Training CobwebTree:  27%|       | 2991/11014 [00:43<02:07, 62.99it/s]Training CobwebTree:  27%|       | 2998/11014 [00:43<02:09, 61.98it/s]Training CobwebTree:  27%|       | 3005/11014 [00:43<02:12, 60.38it/s]Training CobwebTree:  27%|       | 3012/11014 [00:43<02:12, 60.34it/s]Training CobwebTree:  27%|       | 3019/11014 [00:43<02:12, 60.43it/s]Training CobwebTree:  27%|       | 3026/11014 [00:44<02:16, 58.66it/s]Training CobwebTree:  28%|       | 3033/11014 [00:44<02:13, 59.70it/s]Training CobwebTree:  28%|       | 3040/11014 [00:44<02:09, 61.43it/s]Training CobwebTree:  28%|       | 3047/11014 [00:44<02:08, 62.00it/s]Training CobwebTree:  28%|       | 3054/11014 [00:44<02:14, 59.40it/s]Training CobwebTree:  28%|       | 3061/11014 [00:44<02:10, 61.12it/s]Training CobwebTree:  28%|       | 3068/11014 [00:44<02:06, 62.57it/s]Training CobwebTree:  28%|       | 3075/11014 [00:44<02:09, 61.14it/s]Training CobwebTree:  28%|       | 3082/11014 [00:44<02:11, 60.28it/s]Training CobwebTree:  28%|       | 3089/11014 [00:45<02:11, 60.39it/s]Training CobwebTree:  28%|       | 3096/11014 [00:45<02:07, 61.89it/s]Training CobwebTree:  28%|       | 3103/11014 [00:45<02:10, 60.58it/s]Training CobwebTree:  28%|       | 3110/11014 [00:45<02:13, 59.15it/s]Training CobwebTree:  28%|       | 3116/11014 [00:45<02:14, 58.67it/s]Training CobwebTree:  28%|       | 3123/11014 [00:45<02:11, 59.90it/s]Training CobwebTree:  28%|       | 3130/11014 [00:45<02:16, 57.83it/s]Training CobwebTree:  28%|       | 3136/11014 [00:45<02:15, 57.93it/s]Training CobwebTree:  29%|       | 3142/11014 [00:45<02:17, 57.10it/s]Training CobwebTree:  29%|       | 3149/11014 [00:46<02:12, 59.31it/s]Training CobwebTree:  29%|       | 3156/11014 [00:46<02:07, 61.77it/s]Training CobwebTree:  29%|       | 3163/11014 [00:46<02:08, 61.09it/s]Training CobwebTree:  29%|       | 3170/11014 [00:46<02:08, 60.95it/s]Training CobwebTree:  29%|       | 3177/11014 [00:46<02:05, 62.57it/s]Training CobwebTree:  29%|       | 3184/11014 [00:46<02:04, 62.65it/s]Training CobwebTree:  29%|       | 3191/11014 [00:46<02:06, 61.85it/s]Training CobwebTree:  29%|       | 3198/11014 [00:46<02:07, 61.23it/s]Training CobwebTree:  29%|       | 3205/11014 [00:46<02:08, 60.63it/s]Training CobwebTree:  29%|       | 3212/11014 [00:47<02:06, 61.49it/s]Training CobwebTree:  29%|       | 3219/11014 [00:47<02:06, 61.48it/s]Training CobwebTree:  29%|       | 3226/11014 [00:47<02:10, 59.47it/s]Training CobwebTree:  29%|       | 3232/11014 [00:47<02:11, 59.34it/s]Training CobwebTree:  29%|       | 3238/11014 [00:47<02:13, 58.20it/s]Training CobwebTree:  29%|       | 3245/11014 [00:47<02:09, 59.88it/s]Training CobwebTree:  30%|       | 3251/11014 [00:47<02:09, 59.78it/s]Training CobwebTree:  30%|       | 3257/11014 [00:47<02:11, 59.09it/s]Training CobwebTree:  30%|       | 3264/11014 [00:47<02:08, 60.27it/s]Training CobwebTree:  30%|       | 3271/11014 [00:48<02:09, 59.72it/s]Training CobwebTree:  30%|       | 3278/11014 [00:48<02:10, 59.45it/s]Training CobwebTree:  30%|       | 3285/11014 [00:48<02:06, 60.95it/s]Training CobwebTree:  30%|       | 3292/11014 [00:48<02:11, 58.94it/s]Training CobwebTree:  30%|       | 3298/11014 [00:48<02:17, 55.96it/s]Training CobwebTree:  30%|       | 3304/11014 [00:48<02:17, 56.01it/s]Training CobwebTree:  30%|       | 3310/11014 [00:48<02:17, 55.94it/s]Training CobwebTree:  30%|       | 3316/11014 [00:48<02:17, 56.12it/s]Training CobwebTree:  30%|       | 3322/11014 [00:48<02:17, 56.06it/s]Training CobwebTree:  30%|       | 3328/11014 [00:49<02:17, 55.72it/s]Training CobwebTree:  30%|       | 3335/11014 [00:49<02:12, 58.01it/s]Training CobwebTree:  30%|       | 3341/11014 [00:49<02:13, 57.51it/s]Training CobwebTree:  30%|       | 3348/11014 [00:49<02:09, 59.11it/s]Training CobwebTree:  30%|       | 3354/11014 [00:49<02:12, 57.81it/s]Training CobwebTree:  31%|       | 3360/11014 [00:49<02:11, 58.40it/s]Training CobwebTree:  31%|       | 3366/11014 [00:49<02:10, 58.56it/s]Training CobwebTree:  31%|       | 3372/11014 [00:49<02:11, 58.15it/s]Training CobwebTree:  31%|       | 3378/11014 [00:49<02:10, 58.37it/s]Training CobwebTree:  31%|       | 3385/11014 [00:50<02:06, 60.31it/s]Training CobwebTree:  31%|       | 3392/11014 [00:50<02:09, 59.08it/s]Training CobwebTree:  31%|       | 3399/11014 [00:50<02:06, 60.25it/s]Training CobwebTree:  31%|       | 3406/11014 [00:50<02:02, 62.11it/s]Training CobwebTree:  31%|       | 3413/11014 [00:50<02:02, 62.09it/s]Training CobwebTree:  31%|       | 3420/11014 [00:50<02:05, 60.57it/s]Training CobwebTree:  31%|       | 3427/11014 [00:50<02:05, 60.28it/s]Training CobwebTree:  31%|       | 3434/11014 [00:50<02:08, 58.78it/s]Training CobwebTree:  31%|       | 3440/11014 [00:50<02:11, 57.68it/s]Training CobwebTree:  31%|      | 3447/11014 [00:51<02:06, 59.79it/s]Training CobwebTree:  31%|      | 3454/11014 [00:51<02:05, 60.12it/s]Training CobwebTree:  31%|      | 3461/11014 [00:51<02:08, 58.98it/s]Training CobwebTree:  31%|      | 3467/11014 [00:51<02:09, 58.49it/s]Training CobwebTree:  32%|      | 3473/11014 [00:51<02:08, 58.47it/s]Training CobwebTree:  32%|      | 3479/11014 [00:51<02:09, 58.37it/s]Training CobwebTree:  32%|      | 3485/11014 [00:51<02:10, 57.74it/s]Training CobwebTree:  32%|      | 3492/11014 [00:51<02:05, 59.70it/s]Training CobwebTree:  32%|      | 3498/11014 [00:51<02:06, 59.39it/s]Training CobwebTree:  32%|      | 3504/11014 [00:52<02:08, 58.35it/s]Training CobwebTree:  32%|      | 3511/11014 [00:52<02:08, 58.17it/s]Training CobwebTree:  32%|      | 3518/11014 [00:52<02:04, 60.06it/s]Training CobwebTree:  32%|      | 3525/11014 [00:52<02:01, 61.67it/s]Training CobwebTree:  32%|      | 3532/11014 [00:52<01:58, 63.24it/s]Training CobwebTree:  32%|      | 3539/11014 [00:52<01:59, 62.48it/s]Training CobwebTree:  32%|      | 3546/11014 [00:52<01:59, 62.66it/s]Training CobwebTree:  32%|      | 3553/11014 [00:52<01:59, 62.42it/s]Training CobwebTree:  32%|      | 3560/11014 [00:52<01:59, 62.60it/s]Training CobwebTree:  32%|      | 3567/11014 [00:53<02:01, 61.50it/s]Training CobwebTree:  32%|      | 3574/11014 [00:53<02:04, 59.96it/s]Training CobwebTree:  33%|      | 3581/11014 [00:53<02:05, 59.29it/s]Training CobwebTree:  33%|      | 3587/11014 [00:53<02:09, 57.32it/s]Training CobwebTree:  33%|      | 3593/11014 [00:53<02:09, 57.18it/s]Training CobwebTree:  33%|      | 3600/11014 [00:53<02:07, 58.31it/s]Training CobwebTree:  33%|      | 3607/11014 [00:53<02:05, 59.01it/s]Training CobwebTree:  33%|      | 3613/11014 [00:53<02:05, 59.03it/s]Training CobwebTree:  33%|      | 3619/11014 [00:53<02:10, 56.67it/s]Training CobwebTree:  33%|      | 3625/11014 [00:54<02:11, 56.05it/s]Training CobwebTree:  33%|      | 3632/11014 [00:54<02:10, 56.67it/s]Training CobwebTree:  33%|      | 3638/11014 [00:54<02:11, 56.21it/s]Training CobwebTree:  33%|      | 3645/11014 [00:54<02:08, 57.15it/s]Training CobwebTree:  33%|      | 3651/11014 [00:54<02:09, 56.69it/s]Training CobwebTree:  33%|      | 3658/11014 [00:54<02:06, 58.03it/s]Training CobwebTree:  33%|      | 3664/11014 [00:54<02:07, 57.78it/s]Training CobwebTree:  33%|      | 3671/11014 [00:54<02:04, 59.13it/s]Training CobwebTree:  33%|      | 3678/11014 [00:54<02:03, 59.33it/s]Training CobwebTree:  33%|      | 3685/11014 [00:55<02:00, 60.85it/s]Training CobwebTree:  34%|      | 3692/11014 [00:55<01:58, 61.83it/s]Training CobwebTree:  34%|      | 3699/11014 [00:55<01:57, 62.10it/s]Training CobwebTree:  34%|      | 3706/11014 [00:55<02:00, 60.82it/s]Training CobwebTree:  34%|      | 3713/11014 [00:55<02:01, 60.03it/s]Training CobwebTree:  34%|      | 3720/11014 [00:55<01:58, 61.69it/s]Training CobwebTree:  34%|      | 3727/11014 [00:55<02:02, 59.54it/s]Training CobwebTree:  34%|      | 3733/11014 [00:55<02:04, 58.57it/s]Training CobwebTree:  34%|      | 3740/11014 [00:56<02:01, 59.81it/s]Training CobwebTree:  34%|      | 3747/11014 [00:56<02:02, 59.28it/s]Training CobwebTree:  34%|      | 3753/11014 [00:56<02:04, 58.33it/s]Training CobwebTree:  34%|      | 3759/11014 [00:56<02:04, 58.18it/s]Training CobwebTree:  34%|      | 3766/11014 [00:56<02:02, 59.08it/s]Training CobwebTree:  34%|      | 3773/11014 [00:56<02:01, 59.56it/s]Training CobwebTree:  34%|      | 3779/11014 [00:56<02:02, 58.88it/s]Training CobwebTree:  34%|      | 3785/11014 [00:56<02:05, 57.80it/s]Training CobwebTree:  34%|      | 3791/11014 [00:56<02:06, 57.13it/s]Training CobwebTree:  34%|      | 3798/11014 [00:57<02:01, 59.28it/s]Training CobwebTree:  35%|      | 3804/11014 [00:57<02:03, 58.40it/s]Training CobwebTree:  35%|      | 3810/11014 [00:57<02:04, 57.69it/s]Training CobwebTree:  35%|      | 3816/11014 [00:57<02:06, 56.99it/s]Training CobwebTree:  35%|      | 3822/11014 [00:57<02:05, 57.15it/s]Training CobwebTree:  35%|      | 3828/11014 [00:57<02:06, 57.02it/s]Training CobwebTree:  35%|      | 3834/11014 [00:57<02:04, 57.46it/s]Training CobwebTree:  35%|      | 3841/11014 [00:57<02:00, 59.69it/s]Training CobwebTree:  35%|      | 3848/11014 [00:57<01:59, 59.88it/s]Training CobwebTree:  35%|      | 3855/11014 [00:57<01:59, 59.80it/s]Training CobwebTree:  35%|      | 3861/11014 [00:58<01:59, 59.79it/s]Training CobwebTree:  35%|      | 3867/11014 [00:58<02:00, 59.25it/s]Training CobwebTree:  35%|      | 3873/11014 [00:58<02:02, 58.36it/s]Training CobwebTree:  35%|      | 3879/11014 [00:58<02:01, 58.83it/s]Training CobwebTree:  35%|      | 3886/11014 [00:58<01:58, 60.13it/s]Training CobwebTree:  35%|      | 3893/11014 [00:58<02:03, 57.86it/s]Training CobwebTree:  35%|      | 3900/11014 [00:58<01:59, 59.67it/s]Training CobwebTree:  35%|      | 3906/11014 [00:58<02:00, 59.22it/s]Training CobwebTree:  36%|      | 3912/11014 [00:58<02:00, 58.95it/s]Training CobwebTree:  36%|      | 3918/11014 [00:59<02:03, 57.40it/s]Training CobwebTree:  36%|      | 3925/11014 [00:59<02:00, 59.02it/s]Training CobwebTree:  36%|      | 3931/11014 [00:59<02:00, 58.98it/s]Training CobwebTree:  36%|      | 3937/11014 [00:59<02:00, 58.51it/s]Training CobwebTree:  36%|      | 3944/11014 [00:59<01:57, 59.96it/s]Training CobwebTree:  36%|      | 3950/11014 [00:59<01:58, 59.46it/s]Training CobwebTree:  36%|      | 3956/11014 [00:59<02:02, 57.65it/s]Training CobwebTree:  36%|      | 3963/11014 [00:59<01:58, 59.74it/s]Training CobwebTree:  36%|      | 3970/11014 [00:59<01:57, 59.77it/s]Training CobwebTree:  36%|      | 3977/11014 [01:00<01:55, 61.09it/s]Training CobwebTree:  36%|      | 3984/11014 [01:00<01:59, 58.62it/s]Training CobwebTree:  36%|      | 3991/11014 [01:00<01:58, 59.25it/s]Training CobwebTree:  36%|      | 3997/11014 [01:00<02:00, 58.45it/s]Training CobwebTree:  36%|      | 4003/11014 [01:00<02:01, 57.93it/s]Training CobwebTree:  36%|      | 4009/11014 [01:00<02:04, 56.45it/s]Training CobwebTree:  36%|      | 4015/11014 [01:00<02:04, 56.17it/s]Training CobwebTree:  37%|      | 4022/11014 [01:00<02:01, 57.69it/s]Training CobwebTree:  37%|      | 4028/11014 [01:00<02:01, 57.62it/s]Training CobwebTree:  37%|      | 4034/11014 [01:01<02:03, 56.64it/s]Training CobwebTree:  37%|      | 4040/11014 [01:01<02:07, 54.77it/s]Training CobwebTree:  37%|      | 4047/11014 [01:01<02:03, 56.53it/s]Training CobwebTree:  37%|      | 4054/11014 [01:01<01:58, 58.84it/s]Training CobwebTree:  37%|      | 4060/11014 [01:01<02:03, 56.36it/s]Training CobwebTree:  37%|      | 4067/11014 [01:01<01:58, 58.50it/s]Training CobwebTree:  37%|      | 4074/11014 [01:01<01:56, 59.39it/s]Training CobwebTree:  37%|      | 4081/11014 [01:01<01:54, 60.80it/s]Training CobwebTree:  37%|      | 4088/11014 [01:01<01:51, 62.10it/s]Training CobwebTree:  37%|      | 4095/11014 [01:02<01:54, 60.40it/s]Training CobwebTree:  37%|      | 4102/11014 [01:02<01:57, 58.81it/s]Training CobwebTree:  37%|      | 4108/11014 [01:02<01:58, 58.18it/s]Training CobwebTree:  37%|      | 4115/11014 [01:02<01:56, 59.07it/s]Training CobwebTree:  37%|      | 4121/11014 [01:02<01:57, 58.43it/s]Training CobwebTree:  37%|      | 4127/11014 [01:02<01:57, 58.65it/s]Training CobwebTree:  38%|      | 4133/11014 [01:02<01:57, 58.59it/s]Training CobwebTree:  38%|      | 4140/11014 [01:02<01:53, 60.78it/s]Training CobwebTree:  38%|      | 4147/11014 [01:02<01:51, 61.66it/s]Training CobwebTree:  38%|      | 4154/11014 [01:03<01:49, 62.59it/s]Training CobwebTree:  38%|      | 4161/11014 [01:03<01:49, 62.64it/s]Training CobwebTree:  38%|      | 4168/11014 [01:03<01:53, 60.07it/s]Training CobwebTree:  38%|      | 4175/11014 [01:03<01:57, 58.15it/s]Training CobwebTree:  38%|      | 4181/11014 [01:03<01:58, 57.83it/s]Training CobwebTree:  38%|      | 4187/11014 [01:03<01:57, 58.22it/s]Training CobwebTree:  38%|      | 4193/11014 [01:03<01:58, 57.64it/s]Training CobwebTree:  38%|      | 4199/11014 [01:03<01:57, 57.82it/s]Training CobwebTree:  38%|      | 4206/11014 [01:03<01:56, 58.23it/s]Training CobwebTree:  38%|      | 4212/11014 [01:04<01:57, 57.96it/s]Training CobwebTree:  38%|      | 4218/11014 [01:04<01:57, 58.00it/s]Training CobwebTree:  38%|      | 4224/11014 [01:04<01:58, 57.18it/s]Training CobwebTree:  38%|      | 4230/11014 [01:04<02:00, 56.43it/s]Training CobwebTree:  38%|      | 4236/11014 [01:04<02:01, 55.77it/s]Training CobwebTree:  39%|      | 4243/11014 [01:04<01:56, 58.09it/s]Training CobwebTree:  39%|      | 4250/11014 [01:04<01:54, 58.98it/s]Training CobwebTree:  39%|      | 4256/11014 [01:04<01:58, 57.22it/s]Training CobwebTree:  39%|      | 4262/11014 [01:04<01:59, 56.40it/s]Training CobwebTree:  39%|      | 4269/11014 [01:05<01:56, 57.91it/s]Training CobwebTree:  39%|      | 4275/11014 [01:05<01:59, 56.44it/s]Training CobwebTree:  39%|      | 4281/11014 [01:05<02:04, 54.11it/s]Training CobwebTree:  39%|      | 4288/11014 [01:05<01:58, 56.76it/s]Training CobwebTree:  39%|      | 4294/11014 [01:05<02:00, 55.68it/s]Training CobwebTree:  39%|      | 4300/11014 [01:05<02:02, 54.71it/s]Training CobwebTree:  39%|      | 4306/11014 [01:05<02:03, 54.18it/s]Training CobwebTree:  39%|      | 4312/11014 [01:05<02:01, 55.38it/s]Training CobwebTree:  39%|      | 4318/11014 [01:05<01:59, 56.06it/s]Training CobwebTree:  39%|      | 4324/11014 [01:06<01:57, 57.09it/s]Training CobwebTree:  39%|      | 4330/11014 [01:06<01:55, 57.78it/s]Training CobwebTree:  39%|      | 4337/11014 [01:06<01:56, 57.48it/s]Training CobwebTree:  39%|      | 4343/11014 [01:06<01:57, 56.69it/s]Training CobwebTree:  39%|      | 4350/11014 [01:06<01:55, 57.92it/s]Training CobwebTree:  40%|      | 4357/11014 [01:06<01:53, 58.80it/s]Training CobwebTree:  40%|      | 4364/11014 [01:06<01:50, 59.97it/s]Training CobwebTree:  40%|      | 4370/11014 [01:06<01:51, 59.74it/s]Training CobwebTree:  40%|      | 4376/11014 [01:06<01:52, 59.11it/s]Training CobwebTree:  40%|      | 4382/11014 [01:07<01:54, 58.12it/s]Training CobwebTree:  40%|      | 4388/11014 [01:07<01:54, 57.73it/s]Training CobwebTree:  40%|      | 4395/11014 [01:07<01:51, 59.27it/s]Training CobwebTree:  40%|      | 4401/11014 [01:07<01:52, 58.88it/s]Training CobwebTree:  40%|      | 4407/11014 [01:07<01:54, 57.65it/s]Training CobwebTree:  40%|      | 4413/11014 [01:07<01:58, 55.89it/s]Training CobwebTree:  40%|      | 4420/11014 [01:07<01:51, 59.34it/s]Training CobwebTree:  40%|      | 4426/11014 [01:07<01:50, 59.45it/s]Training CobwebTree:  40%|      | 4432/11014 [01:07<01:51, 59.01it/s]Training CobwebTree:  40%|      | 4439/11014 [01:08<01:51, 58.95it/s]Training CobwebTree:  40%|      | 4446/11014 [01:08<01:48, 60.38it/s]Training CobwebTree:  40%|      | 4453/11014 [01:08<01:50, 59.33it/s]Training CobwebTree:  40%|      | 4459/11014 [01:08<01:54, 57.12it/s]Training CobwebTree:  41%|      | 4466/11014 [01:08<01:54, 57.24it/s]Training CobwebTree:  41%|      | 4472/11014 [01:08<01:57, 55.86it/s]Training CobwebTree:  41%|      | 4478/11014 [01:08<01:55, 56.37it/s]Training CobwebTree:  41%|      | 4485/11014 [01:08<01:53, 57.62it/s]Training CobwebTree:  41%|      | 4492/11014 [01:08<01:52, 57.75it/s]Training CobwebTree:  41%|      | 4498/11014 [01:09<01:53, 57.53it/s]Training CobwebTree:  41%|      | 4504/11014 [01:09<01:54, 56.72it/s]Training CobwebTree:  41%|      | 4511/11014 [01:09<01:51, 58.50it/s]Training CobwebTree:  41%|      | 4517/11014 [01:09<01:51, 58.52it/s]Training CobwebTree:  41%|      | 4523/11014 [01:09<01:53, 57.14it/s]Training CobwebTree:  41%|      | 4530/11014 [01:09<01:51, 58.07it/s]Training CobwebTree:  41%|      | 4536/11014 [01:09<01:53, 56.90it/s]Training CobwebTree:  41%|      | 4542/11014 [01:09<01:52, 57.33it/s]Training CobwebTree:  41%|     | 4548/11014 [01:09<01:52, 57.36it/s]Training CobwebTree:  41%|     | 4555/11014 [01:10<01:50, 58.40it/s]Training CobwebTree:  41%|     | 4561/11014 [01:10<01:50, 58.16it/s]Training CobwebTree:  41%|     | 4567/11014 [01:10<01:50, 58.19it/s]Training CobwebTree:  42%|     | 4573/11014 [01:10<01:52, 57.10it/s]Training CobwebTree:  42%|     | 4579/11014 [01:10<01:51, 57.86it/s]Training CobwebTree:  42%|     | 4585/11014 [01:10<01:53, 56.41it/s]Training CobwebTree:  42%|     | 4591/11014 [01:10<01:52, 57.32it/s]Training CobwebTree:  42%|     | 4598/11014 [01:10<01:47, 59.77it/s]Training CobwebTree:  42%|     | 4604/11014 [01:10<01:52, 57.09it/s]Training CobwebTree:  42%|     | 4610/11014 [01:10<01:54, 56.03it/s]Training CobwebTree:  42%|     | 4616/11014 [01:11<01:52, 56.64it/s]Training CobwebTree:  42%|     | 4623/11014 [01:11<01:49, 58.36it/s]Training CobwebTree:  42%|     | 4629/11014 [01:11<01:52, 56.68it/s]Training CobwebTree:  42%|     | 4635/11014 [01:11<01:53, 56.18it/s]Training CobwebTree:  42%|     | 4641/11014 [01:11<01:52, 56.42it/s]Training CobwebTree:  42%|     | 4648/11014 [01:11<01:49, 57.92it/s]Training CobwebTree:  42%|     | 4654/11014 [01:11<01:48, 58.38it/s]Training CobwebTree:  42%|     | 4660/11014 [01:11<01:49, 57.95it/s]Training CobwebTree:  42%|     | 4667/11014 [01:11<01:47, 59.23it/s]Training CobwebTree:  42%|     | 4673/11014 [01:12<01:51, 56.89it/s]Training CobwebTree:  42%|     | 4679/11014 [01:12<01:51, 57.02it/s]Training CobwebTree:  43%|     | 4685/11014 [01:12<01:51, 56.86it/s]Training CobwebTree:  43%|     | 4691/11014 [01:12<01:51, 56.58it/s]Training CobwebTree:  43%|     | 4697/11014 [01:12<01:54, 55.07it/s]Training CobwebTree:  43%|     | 4704/11014 [01:12<01:50, 57.24it/s]Training CobwebTree:  43%|     | 4710/11014 [01:12<01:53, 55.71it/s]Training CobwebTree:  43%|     | 4716/11014 [01:12<01:57, 53.75it/s]Training CobwebTree:  43%|     | 4722/11014 [01:12<01:54, 55.06it/s]Training CobwebTree:  43%|     | 4728/11014 [01:13<01:51, 56.42it/s]Training CobwebTree:  43%|     | 4734/11014 [01:13<01:50, 56.85it/s]Training CobwebTree:  43%|     | 4740/11014 [01:13<01:51, 56.05it/s]Training CobwebTree:  43%|     | 4746/11014 [01:13<01:52, 55.56it/s]Training CobwebTree:  43%|     | 4752/11014 [01:13<01:56, 53.65it/s]Training CobwebTree:  43%|     | 4758/11014 [01:13<01:57, 53.45it/s]Training CobwebTree:  43%|     | 4765/11014 [01:13<01:53, 55.05it/s]Training CobwebTree:  43%|     | 4771/11014 [01:13<01:52, 55.56it/s]Training CobwebTree:  43%|     | 4777/11014 [01:13<01:50, 56.53it/s]Training CobwebTree:  43%|     | 4783/11014 [01:14<01:49, 56.76it/s]Training CobwebTree:  43%|     | 4790/11014 [01:14<01:47, 57.83it/s]Training CobwebTree:  44%|     | 4797/11014 [01:14<01:45, 58.66it/s]Training CobwebTree:  44%|     | 4804/11014 [01:14<01:45, 59.08it/s]Training CobwebTree:  44%|     | 4811/11014 [01:14<01:43, 60.18it/s]Training CobwebTree:  44%|     | 4818/11014 [01:14<01:45, 58.85it/s]Training CobwebTree:  44%|     | 4824/11014 [01:14<01:44, 59.03it/s]Training CobwebTree:  44%|     | 4830/11014 [01:14<01:45, 58.66it/s]Training CobwebTree:  44%|     | 4837/11014 [01:14<01:43, 59.80it/s]Training CobwebTree:  44%|     | 4843/11014 [01:15<01:44, 59.22it/s]Training CobwebTree:  44%|     | 4849/11014 [01:15<01:46, 58.13it/s]Training CobwebTree:  44%|     | 4856/11014 [01:15<01:45, 58.17it/s]Training CobwebTree:  44%|     | 4862/11014 [01:15<01:48, 56.64it/s]Training CobwebTree:  44%|     | 4868/11014 [01:15<01:47, 56.91it/s]Training CobwebTree:  44%|     | 4875/11014 [01:15<01:44, 58.59it/s]Training CobwebTree:  44%|     | 4881/11014 [01:15<01:46, 57.52it/s]Training CobwebTree:  44%|     | 4887/11014 [01:15<01:47, 56.99it/s]Training CobwebTree:  44%|     | 4893/11014 [01:15<01:47, 57.08it/s]Training CobwebTree:  44%|     | 4899/11014 [01:16<01:46, 57.26it/s]Training CobwebTree:  45%|     | 4905/11014 [01:16<01:48, 56.05it/s]Training CobwebTree:  45%|     | 4911/11014 [01:16<01:50, 55.46it/s]Training CobwebTree:  45%|     | 4917/11014 [01:16<01:54, 53.12it/s]Training CobwebTree:  45%|     | 4923/11014 [01:16<01:53, 53.74it/s]Training CobwebTree:  45%|     | 4929/11014 [01:16<01:52, 53.92it/s]Training CobwebTree:  45%|     | 4935/11014 [01:16<01:52, 54.06it/s]Training CobwebTree:  45%|     | 4941/11014 [01:16<01:52, 53.92it/s]Training CobwebTree:  45%|     | 4948/11014 [01:16<01:47, 56.47it/s]Training CobwebTree:  45%|     | 4954/11014 [01:17<01:45, 57.19it/s]Training CobwebTree:  45%|     | 4960/11014 [01:17<01:47, 56.41it/s]Training CobwebTree:  45%|     | 4966/11014 [01:17<01:45, 57.14it/s]Training CobwebTree:  45%|     | 4972/11014 [01:17<01:45, 57.35it/s]Training CobwebTree:  45%|     | 4978/11014 [01:17<01:44, 57.53it/s]Training CobwebTree:  45%|     | 4984/11014 [01:17<01:47, 55.95it/s]Training CobwebTree:  45%|     | 4990/11014 [01:17<01:51, 53.83it/s]Training CobwebTree:  45%|     | 4996/11014 [01:17<01:51, 53.80it/s]Training CobwebTree:  45%|     | 5002/11014 [01:17<01:52, 53.29it/s]Training CobwebTree:  45%|     | 5008/11014 [01:18<01:52, 53.17it/s]Training CobwebTree:  46%|     | 5014/11014 [01:18<01:53, 52.78it/s]Training CobwebTree:  46%|     | 5020/11014 [01:18<01:50, 54.28it/s]Training CobwebTree:  46%|     | 5026/11014 [01:18<01:47, 55.52it/s]Training CobwebTree:  46%|     | 5033/11014 [01:18<01:44, 57.40it/s]Training CobwebTree:  46%|     | 5040/11014 [01:18<01:40, 59.48it/s]Training CobwebTree:  46%|     | 5046/11014 [01:18<01:41, 58.57it/s]Training CobwebTree:  46%|     | 5053/11014 [01:18<01:39, 59.92it/s]Training CobwebTree:  46%|     | 5060/11014 [01:18<01:38, 60.43it/s]Training CobwebTree:  46%|     | 5067/11014 [01:19<01:38, 60.47it/s]Training CobwebTree:  46%|     | 5074/11014 [01:19<01:36, 61.37it/s]Training CobwebTree:  46%|     | 5081/11014 [01:19<01:37, 60.57it/s]Training CobwebTree:  46%|     | 5088/11014 [01:19<01:38, 60.22it/s]Training CobwebTree:  46%|     | 5095/11014 [01:19<01:35, 62.09it/s]Training CobwebTree:  46%|     | 5102/11014 [01:19<01:35, 61.60it/s]Training CobwebTree:  46%|     | 5109/11014 [01:19<01:39, 59.23it/s]Training CobwebTree:  46%|     | 5115/11014 [01:19<01:42, 57.43it/s]Training CobwebTree:  46%|     | 5121/11014 [01:19<01:42, 57.69it/s]Training CobwebTree:  47%|     | 5127/11014 [01:20<01:43, 57.00it/s]Training CobwebTree:  47%|     | 5133/11014 [01:20<01:44, 56.33it/s]Training CobwebTree:  47%|     | 5139/11014 [01:20<01:43, 56.92it/s]Training CobwebTree:  47%|     | 5146/11014 [01:20<01:40, 58.41it/s]Training CobwebTree:  47%|     | 5152/11014 [01:20<01:44, 56.16it/s]Training CobwebTree:  47%|     | 5158/11014 [01:20<01:43, 56.58it/s]Training CobwebTree:  47%|     | 5164/11014 [01:20<01:45, 55.65it/s]Training CobwebTree:  47%|     | 5170/11014 [01:20<01:43, 56.62it/s]Training CobwebTree:  47%|     | 5176/11014 [01:20<01:42, 57.16it/s]Training CobwebTree:  47%|     | 5183/11014 [01:21<01:35, 60.79it/s]Training CobwebTree:  47%|     | 5190/11014 [01:21<01:37, 59.46it/s]Training CobwebTree:  47%|     | 5196/11014 [01:21<01:39, 58.52it/s]Training CobwebTree:  47%|     | 5202/11014 [01:21<01:40, 57.65it/s]Training CobwebTree:  47%|     | 5209/11014 [01:21<01:39, 58.41it/s]Training CobwebTree:  47%|     | 5215/11014 [01:21<01:40, 57.50it/s]Training CobwebTree:  47%|     | 5221/11014 [01:21<01:40, 57.39it/s]Training CobwebTree:  47%|     | 5227/11014 [01:21<01:41, 57.11it/s]Training CobwebTree:  48%|     | 5233/11014 [01:21<01:42, 56.30it/s]Training CobwebTree:  48%|     | 5239/11014 [01:22<01:42, 56.48it/s]Training CobwebTree:  48%|     | 5245/11014 [01:22<01:42, 56.08it/s]Training CobwebTree:  48%|     | 5251/11014 [01:22<01:43, 55.55it/s]Training CobwebTree:  48%|     | 5257/11014 [01:22<01:42, 56.16it/s]Training CobwebTree:  48%|     | 5263/11014 [01:22<01:41, 56.76it/s]Training CobwebTree:  48%|     | 5269/11014 [01:22<01:39, 57.54it/s]Training CobwebTree:  48%|     | 5275/11014 [01:22<01:43, 55.31it/s]Training CobwebTree:  48%|     | 5281/11014 [01:22<01:41, 56.36it/s]Training CobwebTree:  48%|     | 5287/11014 [01:22<01:41, 56.50it/s]Training CobwebTree:  48%|     | 5294/11014 [01:22<01:39, 57.60it/s]Training CobwebTree:  48%|     | 5300/11014 [01:23<01:39, 57.70it/s]Training CobwebTree:  48%|     | 5307/11014 [01:23<01:35, 59.58it/s]Training CobwebTree:  48%|     | 5314/11014 [01:23<01:36, 59.34it/s]Training CobwebTree:  48%|     | 5321/11014 [01:23<01:34, 60.29it/s]Training CobwebTree:  48%|     | 5328/11014 [01:23<01:35, 59.37it/s]Training CobwebTree:  48%|     | 5335/11014 [01:23<01:34, 60.32it/s]Training CobwebTree:  49%|     | 5342/11014 [01:23<01:37, 58.19it/s]Training CobwebTree:  49%|     | 5348/11014 [01:23<01:39, 56.95it/s]Training CobwebTree:  49%|     | 5354/11014 [01:24<01:40, 56.57it/s]Training CobwebTree:  49%|     | 5360/11014 [01:24<01:42, 55.27it/s]Training CobwebTree:  49%|     | 5366/11014 [01:24<01:41, 55.49it/s]Training CobwebTree:  49%|     | 5372/11014 [01:24<01:41, 55.77it/s]Training CobwebTree:  49%|     | 5378/11014 [01:24<01:42, 54.96it/s]Training CobwebTree:  49%|     | 5384/11014 [01:24<01:41, 55.48it/s]Training CobwebTree:  49%|     | 5390/11014 [01:24<01:40, 56.12it/s]Training CobwebTree:  49%|     | 5396/11014 [01:24<01:40, 55.75it/s]Training CobwebTree:  49%|     | 5402/11014 [01:24<01:40, 55.67it/s]Training CobwebTree:  49%|     | 5409/11014 [01:24<01:37, 57.39it/s]Training CobwebTree:  49%|     | 5415/11014 [01:25<01:39, 56.23it/s]Training CobwebTree:  49%|     | 5421/11014 [01:25<01:41, 54.90it/s]Training CobwebTree:  49%|     | 5427/11014 [01:25<01:41, 54.93it/s]Training CobwebTree:  49%|     | 5433/11014 [01:25<01:39, 55.89it/s]Training CobwebTree:  49%|     | 5439/11014 [01:25<01:41, 54.80it/s]Training CobwebTree:  49%|     | 5446/11014 [01:25<01:38, 56.45it/s]Training CobwebTree:  50%|     | 5452/11014 [01:25<01:40, 55.33it/s]Training CobwebTree:  50%|     | 5458/11014 [01:25<01:38, 56.50it/s]Training CobwebTree:  50%|     | 5464/11014 [01:25<01:38, 56.26it/s]Training CobwebTree:  50%|     | 5470/11014 [01:26<01:37, 57.08it/s]Training CobwebTree:  50%|     | 5476/11014 [01:26<01:40, 55.22it/s]Training CobwebTree:  50%|     | 5482/11014 [01:26<01:40, 54.96it/s]Training CobwebTree:  50%|     | 5488/11014 [01:26<01:39, 55.41it/s]Training CobwebTree:  50%|     | 5494/11014 [01:26<01:41, 54.64it/s]Training CobwebTree:  50%|     | 5500/11014 [01:26<01:41, 54.28it/s]Training CobwebTree:  50%|     | 5506/11014 [01:26<01:38, 55.70it/s]Training CobwebTree:  50%|     | 5512/11014 [01:26<01:39, 55.36it/s]Training CobwebTree:  50%|     | 5519/11014 [01:26<01:34, 58.14it/s]Training CobwebTree:  50%|     | 5525/11014 [01:27<01:33, 58.42it/s]Training CobwebTree:  50%|     | 5531/11014 [01:27<01:34, 58.22it/s]Training CobwebTree:  50%|     | 5538/11014 [01:27<01:32, 59.49it/s]Training CobwebTree:  50%|     | 5544/11014 [01:27<01:32, 59.40it/s]Training CobwebTree:  50%|     | 5550/11014 [01:27<01:35, 57.28it/s]Training CobwebTree:  50%|     | 5556/11014 [01:27<01:34, 57.59it/s]Training CobwebTree:  50%|     | 5562/11014 [01:27<01:36, 56.32it/s]Training CobwebTree:  51%|     | 5568/11014 [01:27<01:38, 55.42it/s]Training CobwebTree:  51%|     | 5575/11014 [01:27<01:35, 56.79it/s]Training CobwebTree:  51%|     | 5581/11014 [01:28<01:34, 57.47it/s]Training CobwebTree:  51%|     | 5587/11014 [01:28<01:37, 55.83it/s]Training CobwebTree:  51%|     | 5593/11014 [01:28<01:36, 56.16it/s]Training CobwebTree:  51%|     | 5599/11014 [01:28<01:40, 54.04it/s]Training CobwebTree:  51%|     | 5605/11014 [01:28<01:40, 53.87it/s]Training CobwebTree:  51%|     | 5611/11014 [01:28<01:39, 54.11it/s]Training CobwebTree:  51%|     | 5617/11014 [01:28<01:39, 54.27it/s]Training CobwebTree:  51%|     | 5623/11014 [01:28<01:38, 54.82it/s]Training CobwebTree:  51%|     | 5629/11014 [01:28<01:37, 55.00it/s]Training CobwebTree:  51%|     | 5635/11014 [01:29<01:37, 55.03it/s]Training CobwebTree:  51%|     | 5642/11014 [01:29<01:33, 57.53it/s]Training CobwebTree:  51%|    | 5648/11014 [01:29<01:36, 55.84it/s]Training CobwebTree:  51%|    | 5654/11014 [01:29<01:36, 55.28it/s]Training CobwebTree:  51%|    | 5660/11014 [01:29<01:36, 55.61it/s]Training CobwebTree:  51%|    | 5666/11014 [01:29<01:34, 56.39it/s]Training CobwebTree:  51%|    | 5672/11014 [01:29<01:41, 52.66it/s]Training CobwebTree:  52%|    | 5678/11014 [01:29<01:42, 52.03it/s]Training CobwebTree:  52%|    | 5684/11014 [01:29<01:41, 52.34it/s]Training CobwebTree:  52%|    | 5690/11014 [01:30<01:39, 53.29it/s]Training CobwebTree:  52%|    | 5696/11014 [01:30<01:41, 52.53it/s]Training CobwebTree:  52%|    | 5702/11014 [01:30<01:38, 54.00it/s]Training CobwebTree:  52%|    | 5708/11014 [01:30<01:40, 53.03it/s]Training CobwebTree:  52%|    | 5714/11014 [01:30<01:38, 53.74it/s]Training CobwebTree:  52%|    | 5720/11014 [01:30<01:37, 54.09it/s]Training CobwebTree:  52%|    | 5726/11014 [01:30<01:36, 54.81it/s]Training CobwebTree:  52%|    | 5732/11014 [01:30<01:35, 55.31it/s]Training CobwebTree:  52%|    | 5738/11014 [01:30<01:35, 55.17it/s]Training CobwebTree:  52%|    | 5744/11014 [01:31<01:35, 55.27it/s]Training CobwebTree:  52%|    | 5750/11014 [01:31<01:34, 56.00it/s]Training CobwebTree:  52%|    | 5756/11014 [01:31<01:32, 56.57it/s]Training CobwebTree:  52%|    | 5762/11014 [01:31<01:32, 57.00it/s]Training CobwebTree:  52%|    | 5769/11014 [01:31<01:27, 59.64it/s]Training CobwebTree:  52%|    | 5775/11014 [01:31<01:32, 56.91it/s]Training CobwebTree:  52%|    | 5781/11014 [01:31<01:31, 57.42it/s]Training CobwebTree:  53%|    | 5787/11014 [01:31<01:31, 57.17it/s]Training CobwebTree:  53%|    | 5793/11014 [01:31<01:31, 57.21it/s]Training CobwebTree:  53%|    | 5799/11014 [01:32<01:35, 54.48it/s]Training CobwebTree:  53%|    | 5805/11014 [01:32<01:33, 55.85it/s]Training CobwebTree:  53%|    | 5811/11014 [01:32<01:33, 55.90it/s]Training CobwebTree:  53%|    | 5818/11014 [01:32<01:30, 57.59it/s]Training CobwebTree:  53%|    | 5824/11014 [01:32<01:30, 57.18it/s]Training CobwebTree:  53%|    | 5830/11014 [01:32<01:32, 56.27it/s]Training CobwebTree:  53%|    | 5836/11014 [01:32<01:34, 54.97it/s]Training CobwebTree:  53%|    | 5842/11014 [01:32<01:32, 55.78it/s]Training CobwebTree:  53%|    | 5849/11014 [01:32<01:29, 58.02it/s]Training CobwebTree:  53%|    | 5855/11014 [01:32<01:29, 57.86it/s]Training CobwebTree:  53%|    | 5861/11014 [01:33<01:31, 56.17it/s]Training CobwebTree:  53%|    | 5867/11014 [01:33<01:32, 55.52it/s]Training CobwebTree:  53%|    | 5873/11014 [01:33<01:31, 56.12it/s]Training CobwebTree:  53%|    | 5879/11014 [01:33<01:31, 55.85it/s]Training CobwebTree:  53%|    | 5885/11014 [01:33<01:30, 56.69it/s]Training CobwebTree:  53%|    | 5891/11014 [01:33<01:31, 56.02it/s]Training CobwebTree:  54%|    | 5897/11014 [01:33<01:32, 55.57it/s]Training CobwebTree:  54%|    | 5903/11014 [01:33<01:32, 55.38it/s]Training CobwebTree:  54%|    | 5909/11014 [01:33<01:35, 53.62it/s]Training CobwebTree:  54%|    | 5915/11014 [01:34<01:33, 54.66it/s]Training CobwebTree:  54%|    | 5921/11014 [01:34<01:32, 55.26it/s]Training CobwebTree:  54%|    | 5927/11014 [01:34<01:35, 53.33it/s]Training CobwebTree:  54%|    | 5933/11014 [01:34<01:32, 54.91it/s]Training CobwebTree:  54%|    | 5939/11014 [01:34<01:33, 54.22it/s]Training CobwebTree:  54%|    | 5945/11014 [01:34<01:35, 53.27it/s]Training CobwebTree:  54%|    | 5951/11014 [01:34<01:38, 51.56it/s]Training CobwebTree:  54%|    | 5957/11014 [01:34<01:36, 52.40it/s]Training CobwebTree:  54%|    | 5963/11014 [01:34<01:35, 52.88it/s]Training CobwebTree:  54%|    | 5969/11014 [01:35<01:39, 50.94it/s]Training CobwebTree:  54%|    | 5975/11014 [01:35<01:35, 52.56it/s]Training CobwebTree:  54%|    | 5981/11014 [01:35<01:37, 51.78it/s]Training CobwebTree:  54%|    | 5987/11014 [01:35<01:34, 53.10it/s]Training CobwebTree:  54%|    | 5994/11014 [01:35<01:28, 56.71it/s]Training CobwebTree:  54%|    | 6000/11014 [01:35<01:30, 55.48it/s]Training CobwebTree:  55%|    | 6006/11014 [01:35<01:30, 55.31it/s]Training CobwebTree:  55%|    | 6012/11014 [01:35<01:31, 54.41it/s]Training CobwebTree:  55%|    | 6019/11014 [01:36<01:27, 57.25it/s]Training CobwebTree:  55%|    | 6026/11014 [01:36<01:24, 59.20it/s]Training CobwebTree:  55%|    | 6032/11014 [01:36<01:23, 59.35it/s]Training CobwebTree:  55%|    | 6038/11014 [01:36<01:29, 55.31it/s]Training CobwebTree:  55%|    | 6044/11014 [01:36<01:33, 53.24it/s]Training CobwebTree:  55%|    | 6050/11014 [01:36<01:33, 53.11it/s]Training CobwebTree:  55%|    | 6056/11014 [01:36<01:33, 52.81it/s]Training CobwebTree:  55%|    | 6062/11014 [01:36<01:34, 52.42it/s]Training CobwebTree:  55%|    | 6068/11014 [01:36<01:34, 52.59it/s]Training CobwebTree:  55%|    | 6074/11014 [01:37<01:36, 51.14it/s]Training CobwebTree:  55%|    | 6080/11014 [01:37<01:35, 51.64it/s]Training CobwebTree:  55%|    | 6086/11014 [01:37<01:35, 51.61it/s]Training CobwebTree:  55%|    | 6093/11014 [01:37<01:29, 54.80it/s]Training CobwebTree:  55%|    | 6099/11014 [01:37<01:27, 56.00it/s]Training CobwebTree:  55%|    | 6105/11014 [01:37<01:28, 55.23it/s]Training CobwebTree:  55%|    | 6111/11014 [01:37<01:31, 53.66it/s]Training CobwebTree:  56%|    | 6117/11014 [01:37<01:28, 55.35it/s]Training CobwebTree:  56%|    | 6123/11014 [01:37<01:29, 54.78it/s]Training CobwebTree:  56%|    | 6129/11014 [01:38<01:30, 54.20it/s]Training CobwebTree:  56%|    | 6135/11014 [01:38<01:31, 53.33it/s]Training CobwebTree:  56%|    | 6141/11014 [01:38<01:32, 52.74it/s]Training CobwebTree:  56%|    | 6147/11014 [01:38<01:34, 51.64it/s]Training CobwebTree:  56%|    | 6153/11014 [01:38<01:32, 52.84it/s]Training CobwebTree:  56%|    | 6159/11014 [01:38<01:31, 52.80it/s]Training CobwebTree:  56%|    | 6166/11014 [01:38<01:27, 55.48it/s]Training CobwebTree:  56%|    | 6172/11014 [01:38<01:27, 55.22it/s]Training CobwebTree:  56%|    | 6178/11014 [01:38<01:29, 53.89it/s]Training CobwebTree:  56%|    | 6184/11014 [01:39<01:27, 54.89it/s]Training CobwebTree:  56%|    | 6190/11014 [01:39<01:25, 56.23it/s]Training CobwebTree:  56%|    | 6197/11014 [01:39<01:22, 58.71it/s]Training CobwebTree:  56%|    | 6203/11014 [01:39<01:22, 57.98it/s]Training CobwebTree:  56%|    | 6209/11014 [01:39<01:23, 57.74it/s]Training CobwebTree:  56%|    | 6215/11014 [01:39<01:25, 55.97it/s]Training CobwebTree:  56%|    | 6221/11014 [01:39<01:25, 56.31it/s]Training CobwebTree:  57%|    | 6228/11014 [01:39<01:22, 57.91it/s]Training CobwebTree:  57%|    | 6234/11014 [01:39<01:25, 55.87it/s]Training CobwebTree:  57%|    | 6240/11014 [01:40<01:24, 56.41it/s]Training CobwebTree:  57%|    | 6246/11014 [01:40<01:27, 54.71it/s]Training CobwebTree:  57%|    | 6252/11014 [01:40<01:27, 54.50it/s]Training CobwebTree:  57%|    | 6259/11014 [01:40<01:24, 56.56it/s]Training CobwebTree:  57%|    | 6266/11014 [01:40<01:22, 57.45it/s]Training CobwebTree:  57%|    | 6272/11014 [01:40<01:22, 57.20it/s]Training CobwebTree:  57%|    | 6278/11014 [01:40<01:22, 57.33it/s]Training CobwebTree:  57%|    | 6285/11014 [01:40<01:21, 58.14it/s]Training CobwebTree:  57%|    | 6291/11014 [01:40<01:23, 56.87it/s]Training CobwebTree:  57%|    | 6297/11014 [01:41<01:23, 56.17it/s]Training CobwebTree:  57%|    | 6303/11014 [01:41<01:23, 56.16it/s]Training CobwebTree:  57%|    | 6310/11014 [01:41<01:18, 59.70it/s]Training CobwebTree:  57%|    | 6316/11014 [01:41<01:22, 56.91it/s]Training CobwebTree:  57%|    | 6322/11014 [01:41<01:23, 56.03it/s]Training CobwebTree:  57%|    | 6328/11014 [01:41<01:23, 56.09it/s]Training CobwebTree:  58%|    | 6334/11014 [01:41<01:25, 55.03it/s]Training CobwebTree:  58%|    | 6340/11014 [01:41<01:22, 56.40it/s]Training CobwebTree:  58%|    | 6346/11014 [01:41<01:25, 54.90it/s]Training CobwebTree:  58%|    | 6353/11014 [01:42<01:22, 56.63it/s]Training CobwebTree:  58%|    | 6359/11014 [01:42<01:22, 56.18it/s]Training CobwebTree:  58%|    | 6365/11014 [01:42<01:25, 54.40it/s]Training CobwebTree:  58%|    | 6371/11014 [01:42<01:26, 53.89it/s]Training CobwebTree:  58%|    | 6377/11014 [01:42<01:23, 55.40it/s]Training CobwebTree:  58%|    | 6383/11014 [01:42<01:22, 56.02it/s]Training CobwebTree:  58%|    | 6389/11014 [01:42<01:22, 56.38it/s]Training CobwebTree:  58%|    | 6395/11014 [01:42<01:21, 56.45it/s]Training CobwebTree:  58%|    | 6401/11014 [01:42<01:22, 56.02it/s]Training CobwebTree:  58%|    | 6407/11014 [01:43<01:20, 56.98it/s]Training CobwebTree:  58%|    | 6413/11014 [01:43<01:22, 55.97it/s]Training CobwebTree:  58%|    | 6419/11014 [01:43<01:22, 55.65it/s]Training CobwebTree:  58%|    | 6425/11014 [01:43<01:23, 54.73it/s]Training CobwebTree:  58%|    | 6431/11014 [01:43<01:26, 52.98it/s]Training CobwebTree:  58%|    | 6437/11014 [01:43<01:24, 54.12it/s]Training CobwebTree:  58%|    | 6443/11014 [01:43<01:27, 52.27it/s]Training CobwebTree:  59%|    | 6449/11014 [01:43<01:28, 51.46it/s]Training CobwebTree:  59%|    | 6455/11014 [01:43<01:29, 50.71it/s]Training CobwebTree:  59%|    | 6461/11014 [01:44<01:27, 52.30it/s]Training CobwebTree:  59%|    | 6467/11014 [01:44<01:26, 52.66it/s]Training CobwebTree:  59%|    | 6473/11014 [01:44<01:27, 51.91it/s]Training CobwebTree:  59%|    | 6479/11014 [01:44<01:26, 52.63it/s]Training CobwebTree:  59%|    | 6485/11014 [01:44<01:23, 54.00it/s]Training CobwebTree:  59%|    | 6491/11014 [01:44<01:24, 53.59it/s]Training CobwebTree:  59%|    | 6497/11014 [01:44<01:22, 54.72it/s]Training CobwebTree:  59%|    | 6503/11014 [01:44<01:24, 53.64it/s]Training CobwebTree:  59%|    | 6509/11014 [01:44<01:22, 54.74it/s]Training CobwebTree:  59%|    | 6516/11014 [01:45<01:19, 56.48it/s]Training CobwebTree:  59%|    | 6523/11014 [01:45<01:17, 58.27it/s]Training CobwebTree:  59%|    | 6529/11014 [01:45<01:18, 57.28it/s]Training CobwebTree:  59%|    | 6535/11014 [01:45<01:18, 57.08it/s]Training CobwebTree:  59%|    | 6541/11014 [01:45<01:18, 56.67it/s]Training CobwebTree:  59%|    | 6548/11014 [01:45<01:18, 56.58it/s]Training CobwebTree:  60%|    | 6554/11014 [01:45<01:18, 56.83it/s]Training CobwebTree:  60%|    | 6560/11014 [01:45<01:22, 54.15it/s]Training CobwebTree:  60%|    | 6566/11014 [01:45<01:21, 54.34it/s]Training CobwebTree:  60%|    | 6572/11014 [01:46<01:24, 52.76it/s]Training CobwebTree:  60%|    | 6578/11014 [01:46<01:23, 53.32it/s]Training CobwebTree:  60%|    | 6584/11014 [01:46<01:26, 51.00it/s]Training CobwebTree:  60%|    | 6590/11014 [01:46<01:25, 51.52it/s]Training CobwebTree:  60%|    | 6596/11014 [01:46<01:24, 52.48it/s]Training CobwebTree:  60%|    | 6602/11014 [01:46<01:21, 54.39it/s]Training CobwebTree:  60%|    | 6608/11014 [01:46<01:19, 55.33it/s]Training CobwebTree:  60%|    | 6614/11014 [01:46<01:22, 53.06it/s]Training CobwebTree:  60%|    | 6620/11014 [01:46<01:20, 54.40it/s]Training CobwebTree:  60%|    | 6626/11014 [01:47<01:21, 53.65it/s]Training CobwebTree:  60%|    | 6632/11014 [01:47<01:20, 54.51it/s]Training CobwebTree:  60%|    | 6638/11014 [01:47<01:20, 54.58it/s]Training CobwebTree:  60%|    | 6644/11014 [01:47<01:18, 55.65it/s]Training CobwebTree:  60%|    | 6650/11014 [01:47<01:20, 54.41it/s]Training CobwebTree:  60%|    | 6656/11014 [01:47<01:21, 53.46it/s]Training CobwebTree:  60%|    | 6662/11014 [01:47<01:20, 54.00it/s]Training CobwebTree:  61%|    | 6668/11014 [01:47<01:22, 52.84it/s]Training CobwebTree:  61%|    | 6674/11014 [01:47<01:20, 53.62it/s]Training CobwebTree:  61%|    | 6680/11014 [01:48<01:20, 54.15it/s]Training CobwebTree:  61%|    | 6686/11014 [01:48<01:17, 55.77it/s]Training CobwebTree:  61%|    | 6692/11014 [01:48<01:16, 56.76it/s]Training CobwebTree:  61%|    | 6698/11014 [01:48<01:15, 57.45it/s]Training CobwebTree:  61%|    | 6704/11014 [01:48<01:17, 55.38it/s]Training CobwebTree:  61%|    | 6710/11014 [01:48<01:19, 54.37it/s]Training CobwebTree:  61%|    | 6716/11014 [01:48<01:18, 54.45it/s]Training CobwebTree:  61%|    | 6722/11014 [01:48<01:17, 55.46it/s]Training CobwebTree:  61%|    | 6728/11014 [01:48<01:19, 53.84it/s]Training CobwebTree:  61%|    | 6734/11014 [01:49<01:20, 53.42it/s]Training CobwebTree:  61%|    | 6740/11014 [01:49<01:18, 54.20it/s]Training CobwebTree:  61%|    | 6746/11014 [01:49<01:19, 53.42it/s]Training CobwebTree:  61%|   | 6752/11014 [01:49<01:19, 53.38it/s]Training CobwebTree:  61%|   | 6758/11014 [01:49<01:19, 53.71it/s]Training CobwebTree:  61%|   | 6764/11014 [01:49<01:18, 54.04it/s]Training CobwebTree:  61%|   | 6770/11014 [01:49<01:18, 54.33it/s]Training CobwebTree:  62%|   | 6776/11014 [01:49<01:17, 54.73it/s]Training CobwebTree:  62%|   | 6782/11014 [01:49<01:17, 54.28it/s]Training CobwebTree:  62%|   | 6789/11014 [01:50<01:13, 57.13it/s]Training CobwebTree:  62%|   | 6795/11014 [01:50<01:16, 55.42it/s]Training CobwebTree:  62%|   | 6801/11014 [01:50<01:16, 54.82it/s]Training CobwebTree:  62%|   | 6807/11014 [01:50<01:15, 55.93it/s]Training CobwebTree:  62%|   | 6813/11014 [01:50<01:15, 55.34it/s]Training CobwebTree:  62%|   | 6819/11014 [01:50<01:18, 53.20it/s]Training CobwebTree:  62%|   | 6825/11014 [01:50<01:17, 53.90it/s]Training CobwebTree:  62%|   | 6831/11014 [01:50<01:17, 54.06it/s]Training CobwebTree:  62%|   | 6837/11014 [01:50<01:18, 53.50it/s]Training CobwebTree:  62%|   | 6843/11014 [01:51<01:17, 53.68it/s]Training CobwebTree:  62%|   | 6849/11014 [01:51<01:19, 52.38it/s]Training CobwebTree:  62%|   | 6855/11014 [01:51<01:17, 53.45it/s]Training CobwebTree:  62%|   | 6861/11014 [01:51<01:16, 54.57it/s]Training CobwebTree:  62%|   | 6867/11014 [01:51<01:15, 55.10it/s]Training CobwebTree:  62%|   | 6873/11014 [01:51<01:15, 54.91it/s]Training CobwebTree:  62%|   | 6880/11014 [01:51<01:11, 57.94it/s]Training CobwebTree:  63%|   | 6886/11014 [01:51<01:10, 58.40it/s]Training CobwebTree:  63%|   | 6893/11014 [01:51<01:10, 58.79it/s]Training CobwebTree:  63%|   | 6899/11014 [01:52<01:10, 58.08it/s]Training CobwebTree:  63%|   | 6905/11014 [01:52<01:11, 57.41it/s]Training CobwebTree:  63%|   | 6911/11014 [01:52<01:14, 55.03it/s]Training CobwebTree:  63%|   | 6918/11014 [01:52<01:12, 56.59it/s]Training CobwebTree:  63%|   | 6924/11014 [01:52<01:13, 55.46it/s]Training CobwebTree:  63%|   | 6930/11014 [01:52<01:14, 55.13it/s]Training CobwebTree:  63%|   | 6936/11014 [01:52<01:15, 53.90it/s]Training CobwebTree:  63%|   | 6942/11014 [01:52<01:14, 54.39it/s]Training CobwebTree:  63%|   | 6948/11014 [01:52<01:16, 52.89it/s]Training CobwebTree:  63%|   | 6954/11014 [01:53<01:20, 50.31it/s]Training CobwebTree:  63%|   | 6960/11014 [01:53<01:16, 52.71it/s]Training CobwebTree:  63%|   | 6966/11014 [01:53<01:18, 51.83it/s]Training CobwebTree:  63%|   | 6972/11014 [01:53<01:17, 52.10it/s]Training CobwebTree:  63%|   | 6978/11014 [01:53<01:16, 52.64it/s]Training CobwebTree:  63%|   | 6984/11014 [01:53<01:16, 52.83it/s]Training CobwebTree:  63%|   | 6990/11014 [01:53<01:15, 53.07it/s]Training CobwebTree:  64%|   | 6996/11014 [01:53<01:14, 54.24it/s]Training CobwebTree:  64%|   | 7002/11014 [01:53<01:12, 55.24it/s]Training CobwebTree:  64%|   | 7008/11014 [01:54<01:13, 54.82it/s]Training CobwebTree:  64%|   | 7014/11014 [01:54<01:13, 54.72it/s]Training CobwebTree:  64%|   | 7020/11014 [01:54<01:12, 55.31it/s]Training CobwebTree:  64%|   | 7026/11014 [01:54<01:13, 54.02it/s]Training CobwebTree:  64%|   | 7032/11014 [01:54<01:14, 53.54it/s]Training CobwebTree:  64%|   | 7039/11014 [01:54<01:12, 54.79it/s]Training CobwebTree:  64%|   | 7045/11014 [01:54<01:12, 54.38it/s]Training CobwebTree:  64%|   | 7051/11014 [01:54<01:14, 53.37it/s]Training CobwebTree:  64%|   | 7058/11014 [01:54<01:10, 55.77it/s]Training CobwebTree:  64%|   | 7064/11014 [01:55<01:10, 55.75it/s]Training CobwebTree:  64%|   | 7070/11014 [01:55<01:11, 55.25it/s]Training CobwebTree:  64%|   | 7077/11014 [01:55<01:09, 56.39it/s]Training CobwebTree:  64%|   | 7083/11014 [01:55<01:12, 54.16it/s]Training CobwebTree:  64%|   | 7089/11014 [01:55<01:11, 55.17it/s]Training CobwebTree:  64%|   | 7095/11014 [01:55<01:12, 54.09it/s]Training CobwebTree:  64%|   | 7101/11014 [01:55<01:11, 54.88it/s]Training CobwebTree:  65%|   | 7107/11014 [01:55<01:10, 55.74it/s]Training CobwebTree:  65%|   | 7113/11014 [01:55<01:08, 56.92it/s]Training CobwebTree:  65%|   | 7119/11014 [01:56<01:09, 56.24it/s]Training CobwebTree:  65%|   | 7125/11014 [01:56<01:09, 55.85it/s]Training CobwebTree:  65%|   | 7131/11014 [01:56<01:09, 55.90it/s]Training CobwebTree:  65%|   | 7137/11014 [01:56<01:14, 52.31it/s]Training CobwebTree:  65%|   | 7143/11014 [01:56<01:14, 51.98it/s]Training CobwebTree:  65%|   | 7149/11014 [01:56<01:13, 52.76it/s]Training CobwebTree:  65%|   | 7155/11014 [01:56<01:13, 52.34it/s]Training CobwebTree:  65%|   | 7161/11014 [01:56<01:12, 53.09it/s]Training CobwebTree:  65%|   | 7167/11014 [01:56<01:10, 54.42it/s]Training CobwebTree:  65%|   | 7173/11014 [01:57<01:09, 55.06it/s]Training CobwebTree:  65%|   | 7179/11014 [01:57<01:12, 53.19it/s]Training CobwebTree:  65%|   | 7185/11014 [01:57<01:11, 53.26it/s]Training CobwebTree:  65%|   | 7191/11014 [01:57<01:10, 54.54it/s]Training CobwebTree:  65%|   | 7197/11014 [01:57<01:09, 54.73it/s]Training CobwebTree:  65%|   | 7203/11014 [01:57<01:10, 54.07it/s]Training CobwebTree:  65%|   | 7209/11014 [01:57<01:10, 53.62it/s]Training CobwebTree:  66%|   | 7215/11014 [01:57<01:11, 53.00it/s]Training CobwebTree:  66%|   | 7222/11014 [01:58<01:08, 55.58it/s]Training CobwebTree:  66%|   | 7228/11014 [01:58<01:08, 55.39it/s]Training CobwebTree:  66%|   | 7234/11014 [01:58<01:08, 55.09it/s]Training CobwebTree:  66%|   | 7240/11014 [01:58<01:07, 55.89it/s]Training CobwebTree:  66%|   | 7246/11014 [01:58<01:06, 56.42it/s]Training CobwebTree:  66%|   | 7252/11014 [01:58<01:07, 55.46it/s]Training CobwebTree:  66%|   | 7259/11014 [01:58<01:05, 57.67it/s]Training CobwebTree:  66%|   | 7265/11014 [01:58<01:08, 55.02it/s]Training CobwebTree:  66%|   | 7271/11014 [01:58<01:08, 54.38it/s]Training CobwebTree:  66%|   | 7277/11014 [01:59<01:11, 52.53it/s]Training CobwebTree:  66%|   | 7283/11014 [01:59<01:08, 54.13it/s]Training CobwebTree:  66%|   | 7289/11014 [01:59<01:09, 53.87it/s]Training CobwebTree:  66%|   | 7295/11014 [01:59<01:08, 54.13it/s]Training CobwebTree:  66%|   | 7301/11014 [01:59<01:09, 53.61it/s]Training CobwebTree:  66%|   | 7307/11014 [01:59<01:09, 53.52it/s]Training CobwebTree:  66%|   | 7313/11014 [01:59<01:08, 54.37it/s]Training CobwebTree:  66%|   | 7319/11014 [01:59<01:08, 53.63it/s]Training CobwebTree:  67%|   | 7325/11014 [01:59<01:09, 53.21it/s]Training CobwebTree:  67%|   | 7331/11014 [02:00<01:08, 53.71it/s]Training CobwebTree:  67%|   | 7337/11014 [02:00<01:07, 54.23it/s]Training CobwebTree:  67%|   | 7343/11014 [02:00<01:08, 53.83it/s]Training CobwebTree:  67%|   | 7350/11014 [02:00<01:05, 56.26it/s]Training CobwebTree:  67%|   | 7356/11014 [02:00<01:04, 56.52it/s]Training CobwebTree:  67%|   | 7362/11014 [02:00<01:05, 55.40it/s]Training CobwebTree:  67%|   | 7368/11014 [02:00<01:06, 54.57it/s]Training CobwebTree:  67%|   | 7374/11014 [02:00<01:06, 54.62it/s]Training CobwebTree:  67%|   | 7380/11014 [02:00<01:09, 52.54it/s]Training CobwebTree:  67%|   | 7386/11014 [02:01<01:09, 51.95it/s]Training CobwebTree:  67%|   | 7392/11014 [02:01<01:07, 53.63it/s]Training CobwebTree:  67%|   | 7398/11014 [02:01<01:09, 52.31it/s]Training CobwebTree:  67%|   | 7405/11014 [02:01<01:05, 54.99it/s]Training CobwebTree:  67%|   | 7411/11014 [02:01<01:08, 52.46it/s]Training CobwebTree:  67%|   | 7417/11014 [02:01<01:09, 51.40it/s]Training CobwebTree:  67%|   | 7423/11014 [02:01<01:08, 52.36it/s]Training CobwebTree:  67%|   | 7429/11014 [02:01<01:10, 50.78it/s]Training CobwebTree:  68%|   | 7435/11014 [02:01<01:09, 51.60it/s]Training CobwebTree:  68%|   | 7441/11014 [02:02<01:10, 50.89it/s]Training CobwebTree:  68%|   | 7447/11014 [02:02<01:08, 52.06it/s]Training CobwebTree:  68%|   | 7453/11014 [02:02<01:07, 52.92it/s]Training CobwebTree:  68%|   | 7459/11014 [02:02<01:07, 52.67it/s]Training CobwebTree:  68%|   | 7465/11014 [02:02<01:05, 53.98it/s]Training CobwebTree:  68%|   | 7471/11014 [02:02<01:04, 55.04it/s]Training CobwebTree:  68%|   | 7477/11014 [02:02<01:04, 54.71it/s]Training CobwebTree:  68%|   | 7483/11014 [02:02<01:06, 53.44it/s]Training CobwebTree:  68%|   | 7489/11014 [02:02<01:06, 53.31it/s]Training CobwebTree:  68%|   | 7495/11014 [02:03<01:09, 50.95it/s]Training CobwebTree:  68%|   | 7501/11014 [02:03<01:08, 51.29it/s]Training CobwebTree:  68%|   | 7507/11014 [02:03<01:05, 53.21it/s]Training CobwebTree:  68%|   | 7513/11014 [02:03<01:08, 51.28it/s]Training CobwebTree:  68%|   | 7519/11014 [02:03<01:05, 53.36it/s]Training CobwebTree:  68%|   | 7525/11014 [02:03<01:05, 53.05it/s]Training CobwebTree:  68%|   | 7531/11014 [02:03<01:03, 54.45it/s]Training CobwebTree:  68%|   | 7537/11014 [02:03<01:03, 54.70it/s]Training CobwebTree:  68%|   | 7543/11014 [02:03<01:04, 54.07it/s]Training CobwebTree:  69%|   | 7549/11014 [02:04<01:02, 55.68it/s]Training CobwebTree:  69%|   | 7555/11014 [02:04<01:01, 56.14it/s]Training CobwebTree:  69%|   | 7561/11014 [02:04<01:01, 56.10it/s]Training CobwebTree:  69%|   | 7567/11014 [02:04<01:00, 57.10it/s]Training CobwebTree:  69%|   | 7573/11014 [02:04<01:01, 56.36it/s]Training CobwebTree:  69%|   | 7579/11014 [02:04<01:01, 55.47it/s]Training CobwebTree:  69%|   | 7585/11014 [02:04<01:01, 56.12it/s]Training CobwebTree:  69%|   | 7591/11014 [02:04<01:00, 56.35it/s]Training CobwebTree:  69%|   | 7597/11014 [02:04<00:59, 57.13it/s]Training CobwebTree:  69%|   | 7603/11014 [02:05<00:59, 56.88it/s]Training CobwebTree:  69%|   | 7609/11014 [02:05<01:00, 56.18it/s]Training CobwebTree:  69%|   | 7615/11014 [02:05<01:00, 55.93it/s]Training CobwebTree:  69%|   | 7621/11014 [02:05<01:00, 55.91it/s]Training CobwebTree:  69%|   | 7627/11014 [02:05<01:03, 53.54it/s]Training CobwebTree:  69%|   | 7633/11014 [02:05<01:02, 53.74it/s]Training CobwebTree:  69%|   | 7639/11014 [02:05<01:03, 53.28it/s]Training CobwebTree:  69%|   | 7646/11014 [02:05<00:59, 56.29it/s]Training CobwebTree:  69%|   | 7652/11014 [02:05<01:00, 55.61it/s]Training CobwebTree:  70%|   | 7658/11014 [02:06<01:01, 54.58it/s]Training CobwebTree:  70%|   | 7664/11014 [02:06<01:01, 54.28it/s]Training CobwebTree:  70%|   | 7670/11014 [02:06<01:02, 53.87it/s]Training CobwebTree:  70%|   | 7676/11014 [02:06<01:02, 53.69it/s]Training CobwebTree:  70%|   | 7682/11014 [02:06<01:00, 54.71it/s]Training CobwebTree:  70%|   | 7688/11014 [02:06<01:01, 53.84it/s]Training CobwebTree:  70%|   | 7694/11014 [02:06<01:04, 51.10it/s]Training CobwebTree:  70%|   | 7700/11014 [02:06<01:04, 51.48it/s]Training CobwebTree:  70%|   | 7706/11014 [02:06<01:02, 52.73it/s]Training CobwebTree:  70%|   | 7712/11014 [02:07<01:04, 51.50it/s]Training CobwebTree:  70%|   | 7718/11014 [02:07<01:02, 52.88it/s]Training CobwebTree:  70%|   | 7724/11014 [02:07<01:01, 53.54it/s]Training CobwebTree:  70%|   | 7730/11014 [02:07<01:02, 52.21it/s]Training CobwebTree:  70%|   | 7736/11014 [02:07<01:02, 52.52it/s]Training CobwebTree:  70%|   | 7742/11014 [02:07<01:00, 54.12it/s]Training CobwebTree:  70%|   | 7748/11014 [02:07<01:02, 52.49it/s]Training CobwebTree:  70%|   | 7754/11014 [02:07<01:01, 53.18it/s]Training CobwebTree:  70%|   | 7760/11014 [02:07<00:59, 54.72it/s]Training CobwebTree:  71%|   | 7766/11014 [02:08<01:00, 53.34it/s]Training CobwebTree:  71%|   | 7772/11014 [02:08<01:01, 52.92it/s]Training CobwebTree:  71%|   | 7778/11014 [02:08<00:59, 53.98it/s]Training CobwebTree:  71%|   | 7784/11014 [02:08<01:02, 51.76it/s]Training CobwebTree:  71%|   | 7790/11014 [02:08<01:02, 51.70it/s]Training CobwebTree:  71%|   | 7796/11014 [02:08<01:03, 50.40it/s]Training CobwebTree:  71%|   | 7802/11014 [02:08<01:01, 52.33it/s]Training CobwebTree:  71%|   | 7808/11014 [02:08<01:03, 50.60it/s]Training CobwebTree:  71%|   | 7814/11014 [02:09<01:02, 51.02it/s]Training CobwebTree:  71%|   | 7820/11014 [02:09<01:00, 53.02it/s]Training CobwebTree:  71%|   | 7826/11014 [02:09<01:00, 52.31it/s]Training CobwebTree:  71%|   | 7832/11014 [02:09<01:00, 52.89it/s]Training CobwebTree:  71%|   | 7838/11014 [02:09<01:01, 51.29it/s]Training CobwebTree:  71%|   | 7844/11014 [02:09<01:01, 51.15it/s]Training CobwebTree:  71%|  | 7850/11014 [02:09<01:00, 51.92it/s]Training CobwebTree:  71%|  | 7856/11014 [02:09<01:00, 52.02it/s]Training CobwebTree:  71%|  | 7862/11014 [02:09<00:59, 52.62it/s]Training CobwebTree:  71%|  | 7868/11014 [02:10<00:58, 54.10it/s]Training CobwebTree:  71%|  | 7874/11014 [02:10<01:00, 51.87it/s]Training CobwebTree:  72%|  | 7880/11014 [02:10<01:01, 51.25it/s]Training CobwebTree:  72%|  | 7886/11014 [02:10<01:03, 49.64it/s]Training CobwebTree:  72%|  | 7892/11014 [02:10<01:01, 51.06it/s]Training CobwebTree:  72%|  | 7898/11014 [02:10<00:58, 53.07it/s]Training CobwebTree:  72%|  | 7904/11014 [02:10<01:00, 51.69it/s]Training CobwebTree:  72%|  | 7910/11014 [02:10<01:00, 51.65it/s]Training CobwebTree:  72%|  | 7916/11014 [02:10<01:00, 51.20it/s]Training CobwebTree:  72%|  | 7922/11014 [02:11<01:00, 51.26it/s]Training CobwebTree:  72%|  | 7928/11014 [02:11<01:00, 51.13it/s]Training CobwebTree:  72%|  | 7934/11014 [02:11<00:59, 51.59it/s]Training CobwebTree:  72%|  | 7940/11014 [02:11<00:57, 53.36it/s]Training CobwebTree:  72%|  | 7946/11014 [02:11<00:55, 54.89it/s]Training CobwebTree:  72%|  | 7952/11014 [02:11<00:55, 55.28it/s]Training CobwebTree:  72%|  | 7958/11014 [02:11<00:54, 56.38it/s]Training CobwebTree:  72%|  | 7964/11014 [02:11<00:53, 57.23it/s]Training CobwebTree:  72%|  | 7970/11014 [02:11<00:53, 56.95it/s]Training CobwebTree:  72%|  | 7976/11014 [02:12<00:53, 56.73it/s]Training CobwebTree:  72%|  | 7982/11014 [02:12<00:56, 53.95it/s]Training CobwebTree:  73%|  | 7989/11014 [02:12<00:53, 56.10it/s]Training CobwebTree:  73%|  | 7995/11014 [02:12<00:55, 53.95it/s]Training CobwebTree:  73%|  | 8001/11014 [02:12<00:56, 53.27it/s]Training CobwebTree:  73%|  | 8007/11014 [02:12<00:59, 50.40it/s]Training CobwebTree:  73%|  | 8013/11014 [02:12<00:59, 50.23it/s]Training CobwebTree:  73%|  | 8019/11014 [02:12<01:00, 49.25it/s]Training CobwebTree:  73%|  | 8025/11014 [02:13<00:59, 50.04it/s]Training CobwebTree:  73%|  | 8031/11014 [02:13<00:59, 50.39it/s]Training CobwebTree:  73%|  | 8037/11014 [02:13<00:58, 51.08it/s]Training CobwebTree:  73%|  | 8043/11014 [02:13<00:56, 52.55it/s]Training CobwebTree:  73%|  | 8049/11014 [02:13<00:54, 54.51it/s]Training CobwebTree:  73%|  | 8055/11014 [02:13<00:57, 51.42it/s]Training CobwebTree:  73%|  | 8061/11014 [02:13<00:57, 51.24it/s]Training CobwebTree:  73%|  | 8067/11014 [02:13<00:57, 51.32it/s]Training CobwebTree:  73%|  | 8073/11014 [02:13<00:58, 50.71it/s]Training CobwebTree:  73%|  | 8079/11014 [02:14<00:57, 51.01it/s]Training CobwebTree:  73%|  | 8085/11014 [02:14<00:55, 52.97it/s]Training CobwebTree:  73%|  | 8091/11014 [02:14<00:56, 51.71it/s]Training CobwebTree:  74%|  | 8097/11014 [02:14<00:58, 49.97it/s]Training CobwebTree:  74%|  | 8103/11014 [02:14<00:57, 50.26it/s]Training CobwebTree:  74%|  | 8109/11014 [02:14<00:55, 52.07it/s]Training CobwebTree:  74%|  | 8115/11014 [02:14<00:56, 51.75it/s]Training CobwebTree:  74%|  | 8121/11014 [02:14<00:54, 52.81it/s]Training CobwebTree:  74%|  | 8127/11014 [02:15<00:55, 52.03it/s]Training CobwebTree:  74%|  | 8133/11014 [02:15<00:57, 50.04it/s]Training CobwebTree:  74%|  | 8139/11014 [02:15<00:55, 51.87it/s]Training CobwebTree:  74%|  | 8145/11014 [02:15<00:54, 52.22it/s]Training CobwebTree:  74%|  | 8151/11014 [02:15<00:53, 53.85it/s]Training CobwebTree:  74%|  | 8157/11014 [02:15<00:52, 54.48it/s]Training CobwebTree:  74%|  | 8163/11014 [02:15<00:51, 55.75it/s]Training CobwebTree:  74%|  | 8169/11014 [02:15<00:51, 55.65it/s]Training CobwebTree:  74%|  | 8175/11014 [02:15<00:52, 54.34it/s]Training CobwebTree:  74%|  | 8181/11014 [02:15<00:50, 55.61it/s]Training CobwebTree:  74%|  | 8187/11014 [02:16<00:52, 53.62it/s]Training CobwebTree:  74%|  | 8193/11014 [02:16<00:52, 53.82it/s]Training CobwebTree:  74%|  | 8199/11014 [02:16<00:53, 53.06it/s]Training CobwebTree:  74%|  | 8205/11014 [02:16<00:52, 53.83it/s]Training CobwebTree:  75%|  | 8211/11014 [02:16<00:51, 54.73it/s]Training CobwebTree:  75%|  | 8217/11014 [02:16<00:52, 53.50it/s]Training CobwebTree:  75%|  | 8223/11014 [02:16<00:51, 53.86it/s]Training CobwebTree:  75%|  | 8229/11014 [02:16<00:50, 54.74it/s]Training CobwebTree:  75%|  | 8235/11014 [02:17<00:54, 50.78it/s]Training CobwebTree:  75%|  | 8241/11014 [02:17<00:55, 50.06it/s]Training CobwebTree:  75%|  | 8247/11014 [02:17<00:53, 51.72it/s]Training CobwebTree:  75%|  | 8253/11014 [02:17<00:53, 51.20it/s]Training CobwebTree:  75%|  | 8259/11014 [02:17<00:54, 50.30it/s]Training CobwebTree:  75%|  | 8265/11014 [02:17<00:54, 50.72it/s]Training CobwebTree:  75%|  | 8271/11014 [02:17<00:55, 49.02it/s]Training CobwebTree:  75%|  | 8277/11014 [02:17<00:53, 51.48it/s]Training CobwebTree:  75%|  | 8283/11014 [02:17<00:52, 51.59it/s]Training CobwebTree:  75%|  | 8289/11014 [02:18<00:52, 51.73it/s]Training CobwebTree:  75%|  | 8295/11014 [02:18<00:52, 52.11it/s]Training CobwebTree:  75%|  | 8301/11014 [02:18<00:50, 53.72it/s]Training CobwebTree:  75%|  | 8307/11014 [02:18<00:51, 52.88it/s]Training CobwebTree:  75%|  | 8313/11014 [02:18<00:50, 53.84it/s]Training CobwebTree:  76%|  | 8319/11014 [02:18<00:50, 53.56it/s]Training CobwebTree:  76%|  | 8325/11014 [02:18<00:51, 52.03it/s]Training CobwebTree:  76%|  | 8332/11014 [02:18<00:48, 55.10it/s]Training CobwebTree:  76%|  | 8338/11014 [02:18<00:49, 54.60it/s]Training CobwebTree:  76%|  | 8344/11014 [02:19<00:49, 53.60it/s]Training CobwebTree:  76%|  | 8350/11014 [02:19<00:48, 54.81it/s]Training CobwebTree:  76%|  | 8356/11014 [02:19<00:49, 53.49it/s]Training CobwebTree:  76%|  | 8362/11014 [02:19<00:48, 54.47it/s]Training CobwebTree:  76%|  | 8368/11014 [02:19<00:49, 53.22it/s]Training CobwebTree:  76%|  | 8374/11014 [02:19<00:49, 53.51it/s]Training CobwebTree:  76%|  | 8380/11014 [02:19<00:49, 53.03it/s]Training CobwebTree:  76%|  | 8386/11014 [02:19<00:50, 52.33it/s]Training CobwebTree:  76%|  | 8392/11014 [02:20<00:50, 52.34it/s]Training CobwebTree:  76%|  | 8398/11014 [02:20<00:49, 52.34it/s]Training CobwebTree:  76%|  | 8404/11014 [02:20<00:48, 53.88it/s]Training CobwebTree:  76%|  | 8410/11014 [02:20<00:48, 53.48it/s]Training CobwebTree:  76%|  | 8416/11014 [02:20<00:48, 53.78it/s]Training CobwebTree:  76%|  | 8422/11014 [02:20<00:47, 54.12it/s]Training CobwebTree:  77%|  | 8428/11014 [02:20<00:48, 53.33it/s]Training CobwebTree:  77%|  | 8434/11014 [02:20<00:48, 53.14it/s]Training CobwebTree:  77%|  | 8440/11014 [02:20<00:49, 51.69it/s]Training CobwebTree:  77%|  | 8446/11014 [02:21<00:49, 51.53it/s]Training CobwebTree:  77%|  | 8452/11014 [02:21<00:49, 51.90it/s]Training CobwebTree:  77%|  | 8458/11014 [02:21<00:49, 51.31it/s]Training CobwebTree:  77%|  | 8464/11014 [02:21<00:48, 52.89it/s]Training CobwebTree:  77%|  | 8470/11014 [02:21<00:48, 52.90it/s]Training CobwebTree:  77%|  | 8476/11014 [02:21<00:47, 53.32it/s]Training CobwebTree:  77%|  | 8482/11014 [02:21<00:48, 52.26it/s]Training CobwebTree:  77%|  | 8488/11014 [02:21<00:48, 52.01it/s]Training CobwebTree:  77%|  | 8494/11014 [02:21<00:48, 52.15it/s]Training CobwebTree:  77%|  | 8500/11014 [02:22<00:48, 51.42it/s]Training CobwebTree:  77%|  | 8506/11014 [02:22<00:49, 50.89it/s]Training CobwebTree:  77%|  | 8512/11014 [02:22<00:49, 50.99it/s]Training CobwebTree:  77%|  | 8519/11014 [02:22<00:45, 54.41it/s]Training CobwebTree:  77%|  | 8525/11014 [02:22<00:45, 54.62it/s]Training CobwebTree:  77%|  | 8531/11014 [02:22<00:45, 54.27it/s]Training CobwebTree:  78%|  | 8537/11014 [02:22<00:46, 53.78it/s]Training CobwebTree:  78%|  | 8543/11014 [02:22<00:47, 52.33it/s]Training CobwebTree:  78%|  | 8549/11014 [02:22<00:45, 53.79it/s]Training CobwebTree:  78%|  | 8555/11014 [02:23<00:46, 52.54it/s]Training CobwebTree:  78%|  | 8562/11014 [02:23<00:44, 55.16it/s]Training CobwebTree:  78%|  | 8568/11014 [02:23<00:47, 51.65it/s]Training CobwebTree:  78%|  | 8574/11014 [02:23<00:45, 53.16it/s]Training CobwebTree:  78%|  | 8580/11014 [02:23<00:45, 53.39it/s]Training CobwebTree:  78%|  | 8586/11014 [02:23<00:46, 52.69it/s]Training CobwebTree:  78%|  | 8592/11014 [02:23<00:45, 53.78it/s]Training CobwebTree:  78%|  | 8598/11014 [02:23<00:44, 54.19it/s]Training CobwebTree:  78%|  | 8604/11014 [02:24<00:44, 54.76it/s]Training CobwebTree:  78%|  | 8610/11014 [02:24<00:43, 54.91it/s]Training CobwebTree:  78%|  | 8616/11014 [02:24<00:42, 55.83it/s]Training CobwebTree:  78%|  | 8622/11014 [02:24<00:43, 55.58it/s]Training CobwebTree:  78%|  | 8628/11014 [02:24<00:42, 55.80it/s]Training CobwebTree:  78%|  | 8634/11014 [02:24<00:43, 55.03it/s]Training CobwebTree:  78%|  | 8640/11014 [02:24<00:44, 53.33it/s]Training CobwebTree:  79%|  | 8646/11014 [02:24<00:43, 54.83it/s]Training CobwebTree:  79%|  | 8652/11014 [02:24<00:43, 53.99it/s]Training CobwebTree:  79%|  | 8658/11014 [02:24<00:44, 53.42it/s]Training CobwebTree:  79%|  | 8664/11014 [02:25<00:43, 54.45it/s]Training CobwebTree:  79%|  | 8670/11014 [02:25<00:43, 53.75it/s]Training CobwebTree:  79%|  | 8676/11014 [02:25<00:43, 53.24it/s]Training CobwebTree:  79%|  | 8682/11014 [02:25<00:43, 54.12it/s]Training CobwebTree:  79%|  | 8688/11014 [02:25<00:43, 53.56it/s]Training CobwebTree:  79%|  | 8694/11014 [02:25<00:42, 54.56it/s]Training CobwebTree:  79%|  | 8700/11014 [02:25<00:44, 51.72it/s]Training CobwebTree:  79%|  | 8706/11014 [02:25<00:44, 52.33it/s]Training CobwebTree:  79%|  | 8712/11014 [02:26<00:43, 52.93it/s]Training CobwebTree:  79%|  | 8718/11014 [02:26<00:44, 51.89it/s]Training CobwebTree:  79%|  | 8724/11014 [02:26<00:44, 50.95it/s]Training CobwebTree:  79%|  | 8730/11014 [02:26<00:43, 52.21it/s]Training CobwebTree:  79%|  | 8736/11014 [02:26<00:43, 51.81it/s]Training CobwebTree:  79%|  | 8742/11014 [02:26<00:45, 49.97it/s]Training CobwebTree:  79%|  | 8748/11014 [02:26<00:44, 51.43it/s]Training CobwebTree:  79%|  | 8754/11014 [02:26<00:43, 52.28it/s]Training CobwebTree:  80%|  | 8760/11014 [02:26<00:44, 50.32it/s]Training CobwebTree:  80%|  | 8766/11014 [02:27<00:44, 50.83it/s]Training CobwebTree:  80%|  | 8772/11014 [02:27<00:44, 50.94it/s]Training CobwebTree:  80%|  | 8778/11014 [02:27<00:42, 52.81it/s]Training CobwebTree:  80%|  | 8784/11014 [02:27<00:43, 51.52it/s]Training CobwebTree:  80%|  | 8790/11014 [02:27<00:43, 51.17it/s]Training CobwebTree:  80%|  | 8796/11014 [02:27<00:42, 52.35it/s]Training CobwebTree:  80%|  | 8802/11014 [02:27<00:41, 53.25it/s]Training CobwebTree:  80%|  | 8808/11014 [02:27<00:41, 52.96it/s]Training CobwebTree:  80%|  | 8814/11014 [02:27<00:41, 52.72it/s]Training CobwebTree:  80%|  | 8820/11014 [02:28<00:40, 54.64it/s]Training CobwebTree:  80%|  | 8826/11014 [02:28<00:39, 55.07it/s]Training CobwebTree:  80%|  | 8832/11014 [02:28<00:40, 53.48it/s]Training CobwebTree:  80%|  | 8838/11014 [02:28<00:40, 53.18it/s]Training CobwebTree:  80%|  | 8844/11014 [02:28<00:40, 53.41it/s]Training CobwebTree:  80%|  | 8850/11014 [02:28<00:41, 52.33it/s]Training CobwebTree:  80%|  | 8856/11014 [02:28<00:40, 53.76it/s]Training CobwebTree:  80%|  | 8862/11014 [02:28<00:39, 54.51it/s]Training CobwebTree:  81%|  | 8868/11014 [02:28<00:40, 52.52it/s]Training CobwebTree:  81%|  | 8874/11014 [02:29<00:40, 53.42it/s]Training CobwebTree:  81%|  | 8880/11014 [02:29<00:39, 53.43it/s]Training CobwebTree:  81%|  | 8886/11014 [02:29<00:40, 52.73it/s]Training CobwebTree:  81%|  | 8892/11014 [02:29<00:41, 51.32it/s]Training CobwebTree:  81%|  | 8898/11014 [02:29<00:41, 50.57it/s]Training CobwebTree:  81%|  | 8904/11014 [02:29<00:41, 51.35it/s]Training CobwebTree:  81%|  | 8910/11014 [02:29<00:40, 51.95it/s]Training CobwebTree:  81%|  | 8916/11014 [02:29<00:39, 53.24it/s]Training CobwebTree:  81%|  | 8922/11014 [02:30<00:39, 52.90it/s]Training CobwebTree:  81%|  | 8928/11014 [02:30<00:38, 54.25it/s]Training CobwebTree:  81%|  | 8934/11014 [02:30<00:38, 53.87it/s]Training CobwebTree:  81%|  | 8940/11014 [02:30<00:39, 53.15it/s]Training CobwebTree:  81%|  | 8946/11014 [02:30<00:39, 52.33it/s]Training CobwebTree:  81%| | 8952/11014 [02:30<00:38, 53.66it/s]Training CobwebTree:  81%| | 8958/11014 [02:30<00:39, 52.06it/s]Training CobwebTree:  81%| | 8964/11014 [02:30<00:38, 53.52it/s]Training CobwebTree:  81%| | 8970/11014 [02:30<00:38, 52.46it/s]Training CobwebTree:  82%| | 8977/11014 [02:31<00:37, 54.71it/s]Training CobwebTree:  82%| | 8983/11014 [02:31<00:37, 53.94it/s]Training CobwebTree:  82%| | 8989/11014 [02:31<00:38, 52.65it/s]Training CobwebTree:  82%| | 8995/11014 [02:31<00:39, 51.71it/s]Training CobwebTree:  82%| | 9001/11014 [02:31<00:37, 53.55it/s]Training CobwebTree:  82%| | 9007/11014 [02:31<00:39, 51.38it/s]Training CobwebTree:  82%| | 9013/11014 [02:31<00:38, 51.45it/s]Training CobwebTree:  82%| | 9019/11014 [02:31<00:38, 52.12it/s]Training CobwebTree:  82%| | 9025/11014 [02:31<00:38, 51.57it/s]Training CobwebTree:  82%| | 9031/11014 [02:32<00:39, 50.07it/s]Training CobwebTree:  82%| | 9037/11014 [02:32<00:39, 50.46it/s]Training CobwebTree:  82%| | 9043/11014 [02:32<00:38, 50.68it/s]Training CobwebTree:  82%| | 9049/11014 [02:32<00:39, 49.93it/s]Training CobwebTree:  82%| | 9055/11014 [02:32<00:38, 51.49it/s]Training CobwebTree:  82%| | 9061/11014 [02:32<00:36, 53.16it/s]Training CobwebTree:  82%| | 9067/11014 [02:32<00:36, 53.77it/s]Training CobwebTree:  82%| | 9073/11014 [02:32<00:36, 53.04it/s]Training CobwebTree:  82%| | 9079/11014 [02:33<00:37, 51.09it/s]Training CobwebTree:  82%| | 9085/11014 [02:33<00:38, 50.43it/s]Training CobwebTree:  83%| | 9091/11014 [02:33<00:39, 48.48it/s]Training CobwebTree:  83%| | 9096/11014 [02:33<00:39, 48.57it/s]Training CobwebTree:  83%| | 9102/11014 [02:33<00:38, 49.11it/s]Training CobwebTree:  83%| | 9107/11014 [02:33<00:39, 48.38it/s]Training CobwebTree:  83%| | 9112/11014 [02:33<00:39, 48.74it/s]Training CobwebTree:  83%| | 9117/11014 [02:33<00:39, 48.33it/s]Training CobwebTree:  83%| | 9122/11014 [02:33<00:38, 48.61it/s]Training CobwebTree:  83%| | 9127/11014 [02:34<00:38, 48.47it/s]Training CobwebTree:  83%| | 9133/11014 [02:34<00:37, 49.56it/s]Training CobwebTree:  83%| | 9139/11014 [02:34<00:35, 52.31it/s]Training CobwebTree:  83%| | 9145/11014 [02:34<00:35, 52.90it/s]Training CobwebTree:  83%| | 9151/11014 [02:34<00:34, 54.26it/s]Training CobwebTree:  83%| | 9157/11014 [02:34<00:34, 53.84it/s]Training CobwebTree:  83%| | 9163/11014 [02:34<00:33, 55.01it/s]Training CobwebTree:  83%| | 9169/11014 [02:34<00:34, 53.26it/s]Training CobwebTree:  83%| | 9175/11014 [02:34<00:36, 50.22it/s]Training CobwebTree:  83%| | 9181/11014 [02:35<00:36, 50.66it/s]Training CobwebTree:  83%| | 9187/11014 [02:35<00:35, 50.85it/s]Training CobwebTree:  83%| | 9193/11014 [02:35<00:35, 51.63it/s]Training CobwebTree:  84%| | 9199/11014 [02:35<00:34, 52.40it/s]Training CobwebTree:  84%| | 9205/11014 [02:35<00:35, 50.55it/s]Training CobwebTree:  84%| | 9211/11014 [02:35<00:35, 51.41it/s]Training CobwebTree:  84%| | 9217/11014 [02:35<00:34, 52.57it/s]Training CobwebTree:  84%| | 9223/11014 [02:35<00:34, 52.33it/s]Training CobwebTree:  84%| | 9229/11014 [02:35<00:34, 51.73it/s]Training CobwebTree:  84%| | 9235/11014 [02:36<00:33, 52.63it/s]Training CobwebTree:  84%| | 9241/11014 [02:36<00:33, 53.07it/s]Training CobwebTree:  84%| | 9247/11014 [02:36<00:33, 53.05it/s]Training CobwebTree:  84%| | 9253/11014 [02:36<00:32, 53.43it/s]Training CobwebTree:  84%| | 9259/11014 [02:36<00:34, 51.58it/s]Training CobwebTree:  84%| | 9265/11014 [02:36<00:33, 52.63it/s]Training CobwebTree:  84%| | 9271/11014 [02:36<00:34, 50.73it/s]Training CobwebTree:  84%| | 9277/11014 [02:36<00:34, 49.87it/s]Training CobwebTree:  84%| | 9283/11014 [02:37<00:35, 48.78it/s]Training CobwebTree:  84%| | 9289/11014 [02:37<00:35, 48.87it/s]Training CobwebTree:  84%| | 9295/11014 [02:37<00:33, 51.42it/s]Training CobwebTree:  84%| | 9301/11014 [02:37<00:34, 49.61it/s]Training CobwebTree:  85%| | 9307/11014 [02:37<00:35, 48.17it/s]Training CobwebTree:  85%| | 9313/11014 [02:37<00:34, 49.29it/s]Training CobwebTree:  85%| | 9319/11014 [02:37<00:33, 49.95it/s]Training CobwebTree:  85%| | 9325/11014 [02:37<00:32, 52.06it/s]Training CobwebTree:  85%| | 9331/11014 [02:37<00:33, 50.52it/s]Training CobwebTree:  85%| | 9337/11014 [02:38<00:32, 51.42it/s]Training CobwebTree:  85%| | 9343/11014 [02:38<00:31, 52.39it/s]Training CobwebTree:  85%| | 9349/11014 [02:38<00:31, 52.82it/s]Training CobwebTree:  85%| | 9355/11014 [02:38<00:31, 52.97it/s]Training CobwebTree:  85%| | 9361/11014 [02:38<00:31, 53.28it/s]Training CobwebTree:  85%| | 9367/11014 [02:38<00:30, 53.97it/s]Training CobwebTree:  85%| | 9373/11014 [02:38<00:31, 52.44it/s]Training CobwebTree:  85%| | 9380/11014 [02:38<00:29, 54.89it/s]Training CobwebTree:  85%| | 9386/11014 [02:39<00:30, 53.10it/s]Training CobwebTree:  85%| | 9392/11014 [02:39<00:30, 52.92it/s]Training CobwebTree:  85%| | 9398/11014 [02:39<00:31, 51.50it/s]Training CobwebTree:  85%| | 9404/11014 [02:39<00:31, 50.55it/s]Training CobwebTree:  85%| | 9410/11014 [02:39<00:31, 50.74it/s]Training CobwebTree:  85%| | 9416/11014 [02:39<00:30, 51.65it/s]Training CobwebTree:  86%| | 9422/11014 [02:39<00:30, 52.29it/s]Training CobwebTree:  86%| | 9428/11014 [02:39<00:30, 51.86it/s]Training CobwebTree:  86%| | 9434/11014 [02:39<00:29, 53.39it/s]Training CobwebTree:  86%| | 9440/11014 [02:40<00:29, 52.87it/s]Training CobwebTree:  86%| | 9446/11014 [02:40<00:29, 52.29it/s]Training CobwebTree:  86%| | 9452/11014 [02:40<00:29, 52.39it/s]Training CobwebTree:  86%| | 9458/11014 [02:40<00:30, 50.49it/s]Training CobwebTree:  86%| | 9464/11014 [02:40<00:29, 52.97it/s]Training CobwebTree:  86%| | 9470/11014 [02:40<00:29, 52.61it/s]Training CobwebTree:  86%| | 9476/11014 [02:40<00:29, 52.22it/s]Training CobwebTree:  86%| | 9482/11014 [02:40<00:29, 52.46it/s]Training CobwebTree:  86%| | 9489/11014 [02:40<00:28, 54.29it/s]Training CobwebTree:  86%| | 9495/11014 [02:41<00:27, 54.87it/s]Training CobwebTree:  86%| | 9501/11014 [02:41<00:28, 52.69it/s]Training CobwebTree:  86%| | 9507/11014 [02:41<00:27, 54.61it/s]Training CobwebTree:  86%| | 9513/11014 [02:41<00:27, 54.86it/s]Training CobwebTree:  86%| | 9519/11014 [02:41<00:27, 54.95it/s]Training CobwebTree:  86%| | 9525/11014 [02:41<00:28, 53.05it/s]Training CobwebTree:  87%| | 9532/11014 [02:41<00:26, 55.74it/s]Training CobwebTree:  87%| | 9538/11014 [02:41<00:28, 52.51it/s]Training CobwebTree:  87%| | 9544/11014 [02:42<00:28, 52.30it/s]Training CobwebTree:  87%| | 9550/11014 [02:42<00:28, 51.56it/s]Training CobwebTree:  87%| | 9557/11014 [02:42<00:26, 54.31it/s]Training CobwebTree:  87%| | 9563/11014 [02:42<00:26, 54.88it/s]Training CobwebTree:  87%| | 9569/11014 [02:42<00:26, 54.70it/s]Training CobwebTree:  87%| | 9575/11014 [02:42<00:26, 55.20it/s]Training CobwebTree:  87%| | 9581/11014 [02:42<00:26, 53.50it/s]Training CobwebTree:  87%| | 9587/11014 [02:42<00:26, 53.96it/s]Training CobwebTree:  87%| | 9593/11014 [02:42<00:25, 54.77it/s]Training CobwebTree:  87%| | 9599/11014 [02:43<00:26, 53.23it/s]Training CobwebTree:  87%| | 9605/11014 [02:43<00:26, 53.06it/s]Training CobwebTree:  87%| | 9611/11014 [02:43<00:26, 53.01it/s]Training CobwebTree:  87%| | 9617/11014 [02:43<00:26, 52.80it/s]Training CobwebTree:  87%| | 9623/11014 [02:43<00:26, 52.85it/s]Training CobwebTree:  87%| | 9629/11014 [02:43<00:26, 52.94it/s]Training CobwebTree:  87%| | 9635/11014 [02:43<00:25, 53.77it/s]Training CobwebTree:  88%| | 9641/11014 [02:43<00:25, 54.60it/s]Training CobwebTree:  88%| | 9647/11014 [02:43<00:25, 54.49it/s]Training CobwebTree:  88%| | 9653/11014 [02:44<00:24, 55.04it/s]Training CobwebTree:  88%| | 9659/11014 [02:44<00:25, 53.05it/s]Training CobwebTree:  88%| | 9665/11014 [02:44<00:26, 50.78it/s]Training CobwebTree:  88%| | 9671/11014 [02:44<00:26, 51.29it/s]Training CobwebTree:  88%| | 9677/11014 [02:44<00:25, 51.87it/s]Training CobwebTree:  88%| | 9683/11014 [02:44<00:24, 53.60it/s]Training CobwebTree:  88%| | 9689/11014 [02:44<00:24, 53.20it/s]Training CobwebTree:  88%| | 9695/11014 [02:44<00:24, 53.52it/s]Training CobwebTree:  88%| | 9701/11014 [02:44<00:24, 52.96it/s]Training CobwebTree:  88%| | 9707/11014 [02:45<00:24, 52.53it/s]Training CobwebTree:  88%| | 9713/11014 [02:45<00:25, 51.99it/s]Training CobwebTree:  88%| | 9719/11014 [02:45<00:25, 51.46it/s]Training CobwebTree:  88%| | 9725/11014 [02:45<00:24, 51.60it/s]Training CobwebTree:  88%| | 9731/11014 [02:45<00:25, 51.11it/s]Training CobwebTree:  88%| | 9737/11014 [02:45<00:24, 51.29it/s]Training CobwebTree:  88%| | 9743/11014 [02:45<00:24, 52.15it/s]Training CobwebTree:  89%| | 9749/11014 [02:45<00:23, 54.06it/s]Training CobwebTree:  89%| | 9755/11014 [02:45<00:23, 52.89it/s]Training CobwebTree:  89%| | 9761/11014 [02:46<00:24, 51.39it/s]Training CobwebTree:  89%| | 9767/11014 [02:46<00:23, 52.38it/s]Training CobwebTree:  89%| | 9773/11014 [02:46<00:24, 51.53it/s]Training CobwebTree:  89%| | 9779/11014 [02:46<00:24, 51.00it/s]Training CobwebTree:  89%| | 9785/11014 [02:46<00:23, 52.02it/s]Training CobwebTree:  89%| | 9791/11014 [02:46<00:22, 53.36it/s]Training CobwebTree:  89%| | 9797/11014 [02:46<00:22, 52.94it/s]Training CobwebTree:  89%| | 9803/11014 [02:46<00:22, 52.86it/s]Training CobwebTree:  89%| | 9809/11014 [02:47<00:22, 53.18it/s]Training CobwebTree:  89%| | 9815/11014 [02:47<00:22, 52.59it/s]Training CobwebTree:  89%| | 9821/11014 [02:47<00:22, 53.09it/s]Training CobwebTree:  89%| | 9827/11014 [02:47<00:22, 53.65it/s]Training CobwebTree:  89%| | 9833/11014 [02:47<00:22, 52.02it/s]Training CobwebTree:  89%| | 9839/11014 [02:47<00:23, 50.72it/s]Training CobwebTree:  89%| | 9845/11014 [02:47<00:22, 52.77it/s]Training CobwebTree:  89%| | 9851/11014 [02:47<00:21, 53.42it/s]Training CobwebTree:  89%| | 9857/11014 [02:47<00:22, 52.37it/s]Training CobwebTree:  90%| | 9863/11014 [02:48<00:22, 51.94it/s]Training CobwebTree:  90%| | 9869/11014 [02:48<00:22, 52.00it/s]Training CobwebTree:  90%| | 9875/11014 [02:48<00:21, 53.78it/s]Training CobwebTree:  90%| | 9881/11014 [02:48<00:21, 52.92it/s]Training CobwebTree:  90%| | 9887/11014 [02:48<00:21, 52.68it/s]Training CobwebTree:  90%| | 9893/11014 [02:48<00:21, 53.27it/s]Training CobwebTree:  90%| | 9899/11014 [02:48<00:21, 51.35it/s]Training CobwebTree:  90%| | 9905/11014 [02:48<00:21, 51.09it/s]Training CobwebTree:  90%| | 9911/11014 [02:48<00:22, 49.39it/s]Training CobwebTree:  90%| | 9917/11014 [02:49<00:21, 50.08it/s]Training CobwebTree:  90%| | 9923/11014 [02:49<00:21, 49.91it/s]Training CobwebTree:  90%| | 9929/11014 [02:49<00:21, 49.46it/s]Training CobwebTree:  90%| | 9935/11014 [02:49<00:21, 50.32it/s]Training CobwebTree:  90%| | 9941/11014 [02:49<00:20, 51.90it/s]Training CobwebTree:  90%| | 9947/11014 [02:49<00:20, 51.27it/s]Training CobwebTree:  90%| | 9953/11014 [02:49<00:20, 51.08it/s]Training CobwebTree:  90%| | 9959/11014 [02:49<00:20, 51.84it/s]Training CobwebTree:  90%| | 9965/11014 [02:50<00:19, 52.74it/s]Training CobwebTree:  91%| | 9971/11014 [02:50<00:19, 53.04it/s]Training CobwebTree:  91%| | 9977/11014 [02:50<00:19, 53.45it/s]Training CobwebTree:  91%| | 9983/11014 [02:50<00:19, 53.99it/s]Training CobwebTree:  91%| | 9989/11014 [02:50<00:19, 53.46it/s]Training CobwebTree:  91%| | 9995/11014 [02:50<00:19, 53.05it/s]Training CobwebTree:  91%| | 10001/11014 [02:50<00:18, 54.76it/s]Training CobwebTree:  91%| | 10007/11014 [02:50<00:18, 53.66it/s]Training CobwebTree:  91%| | 10014/11014 [02:50<00:17, 57.04it/s]Training CobwebTree:  91%| | 10020/11014 [02:51<00:17, 57.29it/s]Training CobwebTree:  91%| | 10026/11014 [02:51<00:18, 53.95it/s]Training CobwebTree:  91%| | 10032/11014 [02:51<00:17, 55.24it/s]Training CobwebTree:  91%| | 10038/11014 [02:51<00:17, 54.79it/s]Training CobwebTree:  91%| | 10044/11014 [02:51<00:17, 54.07it/s]Training CobwebTree:  91%| | 10050/11014 [02:51<00:18, 51.71it/s]Training CobwebTree:  91%|| 10056/11014 [02:51<00:18, 52.76it/s]Training CobwebTree:  91%|| 10062/11014 [02:51<00:17, 53.02it/s]Training CobwebTree:  91%|| 10068/11014 [02:51<00:18, 51.53it/s]Training CobwebTree:  91%|| 10074/11014 [02:52<00:18, 51.03it/s]Training CobwebTree:  92%|| 10080/11014 [02:52<00:18, 51.38it/s]Training CobwebTree:  92%|| 10086/11014 [02:52<00:18, 51.33it/s]Training CobwebTree:  92%|| 10092/11014 [02:52<00:17, 52.90it/s]Training CobwebTree:  92%|| 10098/11014 [02:52<00:17, 53.56it/s]Training CobwebTree:  92%|| 10104/11014 [02:52<00:17, 52.96it/s]Training CobwebTree:  92%|| 10110/11014 [02:52<00:17, 52.83it/s]Training CobwebTree:  92%|| 10116/11014 [02:52<00:16, 53.31it/s]Training CobwebTree:  92%|| 10122/11014 [02:52<00:16, 53.88it/s]Training CobwebTree:  92%|| 10128/11014 [02:53<00:16, 52.14it/s]Training CobwebTree:  92%|| 10134/11014 [02:53<00:16, 53.40it/s]Training CobwebTree:  92%|| 10141/11014 [02:53<00:15, 56.02it/s]Training CobwebTree:  92%|| 10147/11014 [02:53<00:15, 56.44it/s]Training CobwebTree:  92%|| 10153/11014 [02:53<00:15, 55.23it/s]Training CobwebTree:  92%|| 10159/11014 [02:53<00:15, 53.65it/s]Training CobwebTree:  92%|| 10165/11014 [02:53<00:16, 51.09it/s]Training CobwebTree:  92%|| 10171/11014 [02:53<00:16, 50.63it/s]Training CobwebTree:  92%|| 10177/11014 [02:54<00:16, 50.60it/s]Training CobwebTree:  92%|| 10183/11014 [02:54<00:16, 51.46it/s]Training CobwebTree:  93%|| 10189/11014 [02:54<00:15, 52.45it/s]Training CobwebTree:  93%|| 10195/11014 [02:54<00:16, 50.06it/s]Training CobwebTree:  93%|| 10201/11014 [02:54<00:16, 50.00it/s]Training CobwebTree:  93%|| 10207/11014 [02:54<00:15, 50.69it/s]Training CobwebTree:  93%|| 10213/11014 [02:54<00:15, 51.68it/s]Training CobwebTree:  93%|| 10219/11014 [02:54<00:15, 52.15it/s]Training CobwebTree:  93%|| 10225/11014 [02:54<00:15, 50.49it/s]Training CobwebTree:  93%|| 10231/11014 [02:55<00:15, 50.24it/s]Training CobwebTree:  93%|| 10237/11014 [02:55<00:15, 50.90it/s]Training CobwebTree:  93%|| 10243/11014 [02:55<00:14, 52.50it/s]Training CobwebTree:  93%|| 10249/11014 [02:55<00:15, 50.67it/s]Training CobwebTree:  93%|| 10255/11014 [02:55<00:14, 51.70it/s]Training CobwebTree:  93%|| 10261/11014 [02:55<00:15, 50.15it/s]Training CobwebTree:  93%|| 10267/11014 [02:55<00:14, 51.41it/s]Training CobwebTree:  93%|| 10273/11014 [02:55<00:13, 53.57it/s]Training CobwebTree:  93%|| 10280/11014 [02:55<00:13, 55.07it/s]Training CobwebTree:  93%|| 10286/11014 [02:56<00:13, 54.43it/s]Training CobwebTree:  93%|| 10292/11014 [02:56<00:13, 53.36it/s]Training CobwebTree:  93%|| 10298/11014 [02:56<00:13, 54.69it/s]Training CobwebTree:  94%|| 10304/11014 [02:56<00:13, 53.11it/s]Training CobwebTree:  94%|| 10310/11014 [02:56<00:13, 51.68it/s]Training CobwebTree:  94%|| 10316/11014 [02:56<00:13, 52.71it/s]Training CobwebTree:  94%|| 10322/11014 [02:56<00:13, 52.87it/s]Training CobwebTree:  94%|| 10328/11014 [02:56<00:13, 52.70it/s]Training CobwebTree:  94%|| 10334/11014 [02:57<00:12, 52.55it/s]Training CobwebTree:  94%|| 10340/11014 [02:57<00:12, 54.07it/s]Training CobwebTree:  94%|| 10346/11014 [02:57<00:12, 54.65it/s]Training CobwebTree:  94%|| 10352/11014 [02:57<00:12, 54.51it/s]Training CobwebTree:  94%|| 10358/11014 [02:57<00:12, 51.19it/s]Training CobwebTree:  94%|| 10364/11014 [02:57<00:12, 52.53it/s]Training CobwebTree:  94%|| 10370/11014 [02:57<00:12, 51.91it/s]Training CobwebTree:  94%|| 10376/11014 [02:57<00:12, 52.82it/s]Training CobwebTree:  94%|| 10382/11014 [02:57<00:12, 49.46it/s]Training CobwebTree:  94%|| 10388/11014 [02:58<00:12, 51.95it/s]Training CobwebTree:  94%|| 10394/11014 [02:58<00:12, 51.42it/s]Training CobwebTree:  94%|| 10400/11014 [02:58<00:12, 50.19it/s]Training CobwebTree:  94%|| 10406/11014 [02:58<00:11, 51.33it/s]Training CobwebTree:  95%|| 10412/11014 [02:58<00:11, 52.02it/s]Training CobwebTree:  95%|| 10418/11014 [02:58<00:11, 52.01it/s]Training CobwebTree:  95%|| 10424/11014 [02:58<00:11, 53.13it/s]Training CobwebTree:  95%|| 10430/11014 [02:58<00:10, 54.94it/s]Training CobwebTree:  95%|| 10436/11014 [02:58<00:10, 54.55it/s]Training CobwebTree:  95%|| 10442/11014 [02:59<00:10, 53.88it/s]Training CobwebTree:  95%|| 10448/11014 [02:59<00:10, 52.53it/s]Training CobwebTree:  95%|| 10454/11014 [02:59<00:10, 52.26it/s]Training CobwebTree:  95%|| 10460/11014 [02:59<00:10, 53.33it/s]Training CobwebTree:  95%|| 10466/11014 [02:59<00:10, 51.64it/s]Training CobwebTree:  95%|| 10472/11014 [02:59<00:10, 51.50it/s]Training CobwebTree:  95%|| 10478/11014 [02:59<00:10, 52.43it/s]Training CobwebTree:  95%|| 10484/11014 [02:59<00:10, 51.98it/s]Training CobwebTree:  95%|| 10490/11014 [03:00<00:10, 51.10it/s]Training CobwebTree:  95%|| 10496/11014 [03:00<00:10, 50.19it/s]Training CobwebTree:  95%|| 10502/11014 [03:00<00:09, 51.25it/s]Training CobwebTree:  95%|| 10508/11014 [03:00<00:09, 51.81it/s]Training CobwebTree:  95%|| 10514/11014 [03:00<00:10, 49.35it/s]Training CobwebTree:  96%|| 10520/11014 [03:00<00:09, 50.23it/s]Training CobwebTree:  96%|| 10526/11014 [03:00<00:09, 50.35it/s]Training CobwebTree:  96%|| 10532/11014 [03:00<00:09, 50.96it/s]Training CobwebTree:  96%|| 10538/11014 [03:00<00:09, 52.44it/s]Training CobwebTree:  96%|| 10544/11014 [03:01<00:08, 53.73it/s]Training CobwebTree:  96%|| 10550/11014 [03:01<00:08, 53.32it/s]Training CobwebTree:  96%|| 10556/11014 [03:01<00:08, 52.45it/s]Training CobwebTree:  96%|| 10562/11014 [03:01<00:08, 53.21it/s]Training CobwebTree:  96%|| 10568/11014 [03:01<00:08, 50.05it/s]Training CobwebTree:  96%|| 10574/11014 [03:01<00:08, 51.01it/s]Training CobwebTree:  96%|| 10580/11014 [03:01<00:08, 50.66it/s]Training CobwebTree:  96%|| 10586/11014 [03:01<00:08, 48.91it/s]Training CobwebTree:  96%|| 10592/11014 [03:01<00:08, 51.15it/s]Training CobwebTree:  96%|| 10598/11014 [03:02<00:08, 51.25it/s]Training CobwebTree:  96%|| 10604/11014 [03:02<00:07, 52.28it/s]Training CobwebTree:  96%|| 10610/11014 [03:02<00:07, 52.53it/s]Training CobwebTree:  96%|| 10616/11014 [03:02<00:07, 53.51it/s]Training CobwebTree:  96%|| 10622/11014 [03:02<00:07, 53.91it/s]Training CobwebTree:  96%|| 10628/11014 [03:02<00:07, 52.89it/s]Training CobwebTree:  97%|| 10634/11014 [03:02<00:07, 53.25it/s]Training CobwebTree:  97%|| 10640/11014 [03:02<00:07, 50.32it/s]Training CobwebTree:  97%|| 10646/11014 [03:03<00:07, 49.67it/s]Training CobwebTree:  97%|| 10652/11014 [03:03<00:07, 51.08it/s]Training CobwebTree:  97%|| 10658/11014 [03:03<00:06, 51.89it/s]Training CobwebTree:  97%|| 10664/11014 [03:03<00:06, 52.26it/s]Training CobwebTree:  97%|| 10670/11014 [03:03<00:06, 52.02it/s]Training CobwebTree:  97%|| 10676/11014 [03:03<00:06, 52.15it/s]Training CobwebTree:  97%|| 10682/11014 [03:03<00:06, 51.59it/s]Training CobwebTree:  97%|| 10688/11014 [03:03<00:06, 50.99it/s]Training CobwebTree:  97%|| 10694/11014 [03:03<00:06, 51.02it/s]Training CobwebTree:  97%|| 10701/11014 [03:04<00:05, 53.86it/s]Training CobwebTree:  97%|| 10707/11014 [03:04<00:05, 54.07it/s]Training CobwebTree:  97%|| 10713/11014 [03:04<00:05, 52.60it/s]Training CobwebTree:  97%|| 10719/11014 [03:04<00:05, 53.39it/s]Training CobwebTree:  97%|| 10725/11014 [03:04<00:05, 53.74it/s]Training CobwebTree:  97%|| 10731/11014 [03:04<00:05, 53.52it/s]Training CobwebTree:  97%|| 10737/11014 [03:04<00:05, 54.52it/s]Training CobwebTree:  98%|| 10743/11014 [03:04<00:05, 52.71it/s]Training CobwebTree:  98%|| 10749/11014 [03:04<00:04, 53.29it/s]Training CobwebTree:  98%|| 10755/11014 [03:05<00:04, 52.11it/s]Training CobwebTree:  98%|| 10761/11014 [03:05<00:04, 51.72it/s]Training CobwebTree:  98%|| 10767/11014 [03:05<00:04, 52.61it/s]Training CobwebTree:  98%|| 10773/11014 [03:05<00:04, 51.52it/s]Training CobwebTree:  98%|| 10779/11014 [03:05<00:04, 52.36it/s]Training CobwebTree:  98%|| 10785/11014 [03:05<00:04, 53.16it/s]Training CobwebTree:  98%|| 10791/11014 [03:05<00:04, 53.37it/s]Training CobwebTree:  98%|| 10797/11014 [03:05<00:04, 53.50it/s]Training CobwebTree:  98%|| 10804/11014 [03:06<00:03, 55.69it/s]Training CobwebTree:  98%|| 10810/11014 [03:06<00:03, 56.13it/s]Training CobwebTree:  98%|| 10816/11014 [03:06<00:03, 54.02it/s]Training CobwebTree:  98%|| 10822/11014 [03:06<00:03, 50.13it/s]Training CobwebTree:  98%|| 10828/11014 [03:06<00:03, 51.24it/s]Training CobwebTree:  98%|| 10834/11014 [03:06<00:03, 50.79it/s]Training CobwebTree:  98%|| 10840/11014 [03:06<00:03, 50.30it/s]Training CobwebTree:  98%|| 10846/11014 [03:06<00:03, 50.86it/s]Training CobwebTree:  99%|| 10852/11014 [03:06<00:03, 50.79it/s]Training CobwebTree:  99%|| 10858/11014 [03:07<00:03, 50.88it/s]Training CobwebTree:  99%|| 10864/11014 [03:07<00:02, 51.96it/s]Training CobwebTree:  99%|| 10870/11014 [03:07<00:02, 51.25it/s]Training CobwebTree:  99%|| 10876/11014 [03:07<00:02, 50.35it/s]Training CobwebTree:  99%|| 10882/11014 [03:07<00:02, 50.62it/s]Training CobwebTree:  99%|| 10888/11014 [03:07<00:02, 48.34it/s]Training CobwebTree:  99%|| 10893/11014 [03:07<00:02, 48.49it/s]Training CobwebTree:  99%|| 10899/11014 [03:07<00:02, 50.64it/s]Training CobwebTree:  99%|| 10905/11014 [03:08<00:02, 50.06it/s]Training CobwebTree:  99%|| 10911/11014 [03:08<00:01, 51.64it/s]Training CobwebTree:  99%|| 10917/11014 [03:08<00:01, 50.33it/s]Training CobwebTree:  99%|| 10923/11014 [03:08<00:01, 51.15it/s]Training CobwebTree:  99%|| 10929/11014 [03:08<00:01, 51.39it/s]Training CobwebTree:  99%|| 10935/11014 [03:08<00:01, 53.04it/s]Training CobwebTree:  99%|| 10941/11014 [03:08<00:01, 51.70it/s]Training CobwebTree:  99%|| 10947/11014 [03:08<00:01, 51.64it/s]Training CobwebTree:  99%|| 10953/11014 [03:08<00:01, 52.21it/s]Training CobwebTree: 100%|| 10959/11014 [03:09<00:01, 51.44it/s]Training CobwebTree: 100%|| 10965/11014 [03:09<00:00, 52.31it/s]Training CobwebTree: 100%|| 10972/11014 [03:09<00:00, 54.15it/s]Training CobwebTree: 100%|| 10978/11014 [03:09<00:00, 55.26it/s]Training CobwebTree: 100%|| 10984/11014 [03:09<00:00, 52.72it/s]Training CobwebTree: 100%|| 10990/11014 [03:09<00:00, 52.65it/s]Training CobwebTree: 100%|| 10996/11014 [03:09<00:00, 52.35it/s]Training CobwebTree: 100%|| 11003/11014 [03:09<00:00, 54.84it/s]Training CobwebTree: 100%|| 11009/11014 [03:09<00:00, 51.49it/s]Training CobwebTree: 100%|| 11014/11014 [03:10<00:00, 57.94it/s]
2025-12-21 09:22:44,029 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 09:22:50,073 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (7232 virtual)
2025-12-21 09:22:50,078 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (3127 virtual)
2025-12-21 09:22:50,082 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (1323 virtual)
2025-12-21 09:22:50,086 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (713 virtual)
2025-12-21 09:22:50,091 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-5372 virtual)
2025-12-21 09:22:50,094 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (-3523 virtual)
2025-12-21 09:22:50,102 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (12146 virtual)
2025-12-21 09:22:50,105 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (22148 virtual)
2025-12-21 09:22:50,108 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (19719 virtual)
2025-12-21 09:22:50,118 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (5184 virtual)
2025-12-21 09:22:50,121 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (6705 virtual)
2025-12-21 09:22:50,205 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (-5029 virtual)
2025-12-21 09:22:50,253 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (-3476 virtual)
2025-12-21 09:22:50,316 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (-2950 virtual)
2025-12-21 09:22:50,320 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (2109 virtual)
2025-12-21 09:22:50,332 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (2483 virtual)
2025-12-21 09:22:50,453 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (6024 virtual)
2025-12-21 09:22:50,456 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (8719 virtual)
2025-12-21 09:22:50,459 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (10782 virtual)
2025-12-21 09:22:50,506 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (12662 virtual)
2025-12-21 09:22:50,685 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (14417 virtual)
2025-12-21 09:22:50,727 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (16318 virtual)
2025-12-21 09:22:50,741 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (21460 virtual)
2025-12-21 09:22:50,920 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (36852 virtual)
2025-12-21 09:22:50,924 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (39766 virtual)
2025-12-21 09:22:51,003 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (41441 virtual)
2025-12-21 09:22:51,063 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (51322 virtual)
2025-12-21 09:22:51,127 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (52687 virtual)
2025-12-21 09:22:51,129 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (52913 virtual)
2025-12-21 09:22:51,133 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (55693 virtual)
2025-12-21 09:22:51,237 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (52992 virtual)
2025-12-21 09:22:51,292 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (50615 virtual)
2025-12-21 09:22:51,447 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (45525 virtual)
2025-12-21 09:22:51,452 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (45324 virtual)
2025-12-21 09:22:51,541 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (42113 virtual)
2025-12-21 09:22:51,615 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (43690 virtual)
2025-12-21 09:22:51,618 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (42608 virtual)
2025-12-21 09:22:51,737 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (39072 virtual)
2025-12-21 09:22:51,879 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (40044 virtual)
2025-12-21 09:22:51,969 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (41542 virtual)
2025-12-21 09:22:51,972 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (45793 virtual)
2025-12-21 09:22:51,974 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (47498 virtual)
2025-12-21 09:22:52,166 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (42550 virtual)
2025-12-21 09:22:52,397 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (37489 virtual)
2025-12-21 09:22:52,431 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (39918 virtual)
2025-12-21 09:22:52,567 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (31027 virtual)
2025-12-21 09:22:52,573 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (40008 virtual)
2025-12-21 09:22:52,786 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (41502 virtual)
2025-12-21 09:22:52,789 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (46622 virtual)
2025-12-21 09:22:52,841 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (48729 virtual)
2025-12-21 09:22:52,858 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (44346 virtual)
2025-12-21 09:22:52,934 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (48297 virtual)
2025-12-21 09:22:53,215 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (35558 virtual)
2025-12-21 09:22:53,305 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (37592 virtual)
2025-12-21 09:22:53,392 INFO gensim.topic_coherence.text_analysis: 170 batches submitted to accumulate stats from 10880 documents (24474 virtual)
2025-12-21 09:22:53,395 INFO gensim.topic_coherence.text_analysis: 171 batches submitted to accumulate stats from 10944 documents (24761 virtual)
2025-12-21 09:22:53,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,632 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:53,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:53,990 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,134 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,126 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,284 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:54,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:54,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,115 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,573 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:55,949 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:55,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:56,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:56,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:56,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:56,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:56,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:56,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:56,700 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:56,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:56,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:57,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:57,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:57,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:57,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:57,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:57,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:57,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:57,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:57,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:57,915 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:57,916 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:57,993 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:58,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:58,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:58,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:58,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:58,340 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:22:58,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:22:58,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:05,003 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 09:23:05,216 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 663359 virtual documents
2025-12-21 09:23:06,406 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 09:23:12,832 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (5593 virtual)
2025-12-21 09:23:12,836 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (10727 virtual)
2025-12-21 09:23:12,839 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (26432 virtual)
2025-12-21 09:23:12,841 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (30159 virtual)
2025-12-21 09:23:12,842 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (32725 virtual)
2025-12-21 09:23:12,843 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (36278 virtual)
2025-12-21 09:23:12,844 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (39220 virtual)
2025-12-21 09:23:12,847 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (54327 virtual)
2025-12-21 09:23:12,849 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (60577 virtual)
2025-12-21 09:23:12,850 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (63971 virtual)
2025-12-21 09:23:12,852 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (68747 virtual)
2025-12-21 09:23:12,854 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (78123 virtual)
2025-12-21 09:23:12,855 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (82521 virtual)
2025-12-21 09:23:12,857 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (90313 virtual)
2025-12-21 09:23:12,859 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (95725 virtual)
2025-12-21 09:23:12,860 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (100032 virtual)
2025-12-21 09:23:12,861 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (103002 virtual)
2025-12-21 09:23:12,863 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (109828 virtual)
2025-12-21 09:23:12,864 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (115400 virtual)
2025-12-21 09:23:12,866 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (124477 virtual)
2025-12-21 09:23:12,868 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (128588 virtual)
2025-12-21 09:23:12,869 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (131939 virtual)
2025-12-21 09:23:12,875 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (159346 virtual)
2025-12-21 09:23:12,878 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (175748 virtual)
2025-12-21 09:23:12,896 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (179483 virtual)
2025-12-21 09:23:12,898 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (186119 virtual)
2025-12-21 09:23:12,899 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (189134 virtual)
2025-12-21 09:23:12,900 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (194011 virtual)
2025-12-21 09:23:12,902 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (198338 virtual)
2025-12-21 09:23:12,903 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (201418 virtual)
2025-12-21 09:23:12,904 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (205336 virtual)
2025-12-21 09:23:12,905 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (210865 virtual)
2025-12-21 09:23:12,906 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (215487 virtual)
2025-12-21 09:23:12,908 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (222784 virtual)
2025-12-21 09:23:12,911 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (227542 virtual)
2025-12-21 09:23:12,913 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (237105 virtual)
2025-12-21 09:23:12,914 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (242098 virtual)
2025-12-21 09:23:12,916 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (246462 virtual)
2025-12-21 09:23:12,917 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (250096 virtual)
2025-12-21 09:23:12,918 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (255323 virtual)
2025-12-21 09:23:12,920 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (258628 virtual)
2025-12-21 09:23:12,921 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (263349 virtual)
2025-12-21 09:23:12,923 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (268233 virtual)
2025-12-21 09:23:12,925 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (274132 virtual)
2025-12-21 09:23:12,927 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (282971 virtual)
2025-12-21 09:23:12,929 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (286673 virtual)
2025-12-21 09:23:12,930 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (290264 virtual)
2025-12-21 09:23:12,931 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (296037 virtual)
2025-12-21 09:23:12,936 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (310124 virtual)
2025-12-21 09:23:12,939 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (317050 virtual)
2025-12-21 09:23:12,940 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (320598 virtual)
2025-12-21 09:23:12,944 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (334909 virtual)
2025-12-21 09:23:12,946 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (341683 virtual)
2025-12-21 09:23:12,949 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (351624 virtual)
2025-12-21 09:23:12,951 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (360719 virtual)
2025-12-21 09:23:12,952 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (365651 virtual)
2025-12-21 09:23:12,955 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (375582 virtual)
2025-12-21 09:23:12,956 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (380043 virtual)
2025-12-21 09:23:12,958 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (390262 virtual)
2025-12-21 09:23:12,959 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (394060 virtual)
2025-12-21 09:23:12,961 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (398804 virtual)
2025-12-21 09:23:12,962 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (402970 virtual)
2025-12-21 09:23:12,965 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (417617 virtual)
2025-12-21 09:23:12,967 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (425918 virtual)
2025-12-21 09:23:12,970 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (437460 virtual)
2025-12-21 09:23:12,971 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (441365 virtual)
2025-12-21 09:23:12,973 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (446589 virtual)
2025-12-21 09:23:12,974 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (450244 virtual)
2025-12-21 09:23:12,980 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (478452 virtual)
2025-12-21 09:23:12,982 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (482978 virtual)
2025-12-21 09:23:12,984 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (494166 virtual)
2025-12-21 09:23:12,986 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (502241 virtual)
2025-12-21 09:23:12,990 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (518522 virtual)
2025-12-21 09:23:12,993 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (526287 virtual)
2025-12-21 09:23:12,995 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (532913 virtual)
2025-12-21 09:23:12,996 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (536619 virtual)
2025-12-21 09:23:12,999 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (548493 virtual)
2025-12-21 09:23:13,000 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (551958 virtual)
2025-12-21 09:23:13,001 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (555379 virtual)
2025-12-21 09:23:13,004 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (564992 virtual)
2025-12-21 09:23:13,006 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (568990 virtual)
2025-12-21 09:23:13,240 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (574796 virtual)
2025-12-21 09:23:13,241 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (581815 virtual)
2025-12-21 09:23:13,242 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (585873 virtual)
2025-12-21 09:23:13,312 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (591771 virtual)
2025-12-21 09:23:13,314 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (597046 virtual)
2025-12-21 09:23:13,315 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (601849 virtual)
2025-12-21 09:23:13,317 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (608725 virtual)
2025-12-21 09:23:13,340 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (615021 virtual)
2025-12-21 09:23:13,360 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (621265 virtual)
2025-12-21 09:23:13,472 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (627724 virtual)
2025-12-21 09:23:13,474 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (631679 virtual)
2025-12-21 09:23:13,475 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (635736 virtual)
2025-12-21 09:23:13,605 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (643713 virtual)
2025-12-21 09:23:13,672 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (649874 virtual)
2025-12-21 09:23:13,717 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (658090 virtual)
2025-12-21 09:23:13,788 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (661695 virtual)
2025-12-21 09:23:13,887 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (669808 virtual)
2025-12-21 09:23:13,900 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (673686 virtual)
2025-12-21 09:23:13,902 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (678673 virtual)
2025-12-21 09:23:13,999 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (683677 virtual)
2025-12-21 09:23:14,001 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (687618 virtual)
2025-12-21 09:23:14,119 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (698272 virtual)
2025-12-21 09:23:14,121 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (703739 virtual)
2025-12-21 09:23:14,123 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (712044 virtual)
2025-12-21 09:23:14,221 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (719942 virtual)
2025-12-21 09:23:14,225 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (730593 virtual)
2025-12-21 09:23:14,318 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (738698 virtual)
2025-12-21 09:23:14,321 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (744918 virtual)
2025-12-21 09:23:14,322 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (748587 virtual)
2025-12-21 09:23:14,394 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (754283 virtual)
2025-12-21 09:23:14,396 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (759411 virtual)
2025-12-21 09:23:14,479 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (763074 virtual)
2025-12-21 09:23:14,481 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (769146 virtual)
2025-12-21 09:23:14,573 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (778550 virtual)
2025-12-21 09:23:14,616 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (782687 virtual)
2025-12-21 09:23:14,617 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (787370 virtual)
2025-12-21 09:23:14,619 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (792747 virtual)
2025-12-21 09:23:14,621 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (795834 virtual)
2025-12-21 09:23:14,678 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (799472 virtual)
2025-12-21 09:23:14,680 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (804251 virtual)
2025-12-21 09:23:14,759 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (818289 virtual)
2025-12-21 09:23:14,772 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (823742 virtual)
2025-12-21 09:23:14,774 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (829121 virtual)
2025-12-21 09:23:14,871 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (839918 virtual)
2025-12-21 09:23:14,884 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (843357 virtual)
2025-12-21 09:23:14,947 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (847194 virtual)
2025-12-21 09:23:14,960 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (850480 virtual)
2025-12-21 09:23:15,011 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (854672 virtual)
2025-12-21 09:23:15,025 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (863027 virtual)
2025-12-21 09:23:15,040 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (865772 virtual)
2025-12-21 09:23:15,135 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (884808 virtual)
2025-12-21 09:23:15,136 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (889004 virtual)
2025-12-21 09:23:15,138 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (894985 virtual)
2025-12-21 09:23:15,139 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (898703 virtual)
2025-12-21 09:23:15,195 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (901982 virtual)
2025-12-21 09:23:15,208 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (906587 virtual)
2025-12-21 09:23:15,210 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (910463 virtual)
2025-12-21 09:23:15,291 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (931102 virtual)
2025-12-21 09:23:15,295 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (942622 virtual)
2025-12-21 09:23:15,367 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (951129 virtual)
2025-12-21 09:23:15,368 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (955276 virtual)
2025-12-21 09:23:15,370 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (959539 virtual)
2025-12-21 09:23:15,372 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (965946 virtual)
2025-12-21 09:23:15,449 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (970853 virtual)
2025-12-21 09:23:15,475 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (973829 virtual)
2025-12-21 09:23:15,478 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (989097 virtual)
2025-12-21 09:23:15,480 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (994234 virtual)
2025-12-21 09:23:15,541 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (997762 virtual)
2025-12-21 09:23:15,557 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (1000793 virtual)
2025-12-21 09:23:15,583 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (1006413 virtual)
2025-12-21 09:23:15,584 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (1011923 virtual)
2025-12-21 09:23:15,586 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (1014946 virtual)
2025-12-21 09:23:15,587 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (1018740 virtual)
2025-12-21 09:23:15,623 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (1022376 virtual)
2025-12-21 09:23:15,624 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (1026825 virtual)
2025-12-21 09:23:15,678 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (1040358 virtual)
2025-12-21 09:23:15,703 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (1044737 virtual)
2025-12-21 09:23:15,716 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (1050861 virtual)
2025-12-21 09:23:15,763 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (1056370 virtual)
2025-12-21 09:23:15,764 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (1062228 virtual)
2025-12-21 09:23:15,767 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (1074392 virtual)
2025-12-21 09:23:15,768 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (1078963 virtual)
2025-12-21 09:23:15,831 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (1082443 virtual)
2025-12-21 09:23:15,833 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (1088508 virtual)
2025-12-21 09:23:15,834 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (1093083 virtual)
2025-12-21 09:23:15,848 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (1096984 virtual)
2025-12-21 09:23:15,910 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (1101614 virtual)
2025-12-21 09:23:15,924 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (1105041 virtual)
2025-12-21 09:23:15,936 INFO gensim.topic_coherence.text_analysis: 170 batches submitted to accumulate stats from 10880 documents (1112474 virtual)
2025-12-21 09:23:15,983 INFO gensim.topic_coherence.text_analysis: 171 batches submitted to accumulate stats from 10944 documents (1119161 virtual)
2025-12-21 09:23:15,984 INFO gensim.topic_coherence.text_analysis: 172 batches submitted to accumulate stats from 11008 documents (1123702 virtual)
2025-12-21 09:23:15,985 INFO gensim.topic_coherence.text_analysis: 173 batches submitted to accumulate stats from 11072 documents (1123966 virtual)
2025-12-21 09:23:15,994 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,994 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:15,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,000 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,000 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,032 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,051 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,763 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:16,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:16,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,281 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:17,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:17,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:18,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:18,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:18,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:18,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:18,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:18,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:18,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:18,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:18,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:18,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:18,711 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:18,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:19,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:19,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:19,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:19,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:19,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:19,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:19,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:19,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:20,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 09:23:20,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 09:23:26,621 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 09:23:26,782 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 1127780 virtual documents
2025-12-21 09:23:27,390 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.6193012383626985, 'coherence_npmi': -0.015790994243445933, 'topic_diversity': 1.0, 'inter_topic_similarity': 0.4592117965221405}
2025-12-21 09:23:27,390 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.694239049142908, 'coherence_npmi': 0.12255744885011632, 'topic_diversity': 0.796, 'inter_topic_similarity': 0.28838300704956055}
2025-12-21 09:23:27,390 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6715260616257317, 'coherence_npmi': 0.10712287767629523, 'topic_diversity': 0.7859649122807018, 'inter_topic_similarity': 0.2618086636066437}
