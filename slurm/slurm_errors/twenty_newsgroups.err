2025-12-22 12:10:44,286 INFO __main__: Starting benchmark for dataset=20newsgroups
2025-12-22 12:10:45,510 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-22 12:10:45,999 INFO gensim.corpora.dictionary: built Dictionary<70709 unique tokens: ['88', '89', 'best', 'bonnevilles', 'book']...> from 7317 documents (total 691444 corpus positions)
2025-12-22 12:10:46,001 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<70709 unique tokens: ['88', '89', 'best', 'bonnevilles', 'book']...> from 7317 documents (total 691444 corpus positions)", 'datetime': '2025-12-22T12:10:45.999468', 'gensim': '4.4.0', 'python': '3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-187-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-22 12:10:47,404 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-22 12:10:47,404 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-22 12:10:50,318 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 7317 docs
2025-12-22 12:12:19,679 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:12:21,639 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-22 12:12:21,642 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-22 12:12:21,645 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-22 12:12:21,648 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-22 12:12:21,657 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-22 12:12:21,664 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-22 12:12:21,667 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-22 12:12:21,669 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-22 12:12:21,676 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-22 12:12:21,681 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-22 12:12:21,687 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-22 12:12:21,690 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-22 12:12:21,695 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-22 12:12:21,698 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-22 12:12:21,709 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-22 12:12:21,715 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-22 12:12:21,719 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-22 12:12:21,728 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-22 12:12:21,733 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-22 12:12:21,738 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-22 12:12:21,760 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-22 12:12:21,806 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-22 12:12:21,823 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-22 12:12:22,005 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-22 12:12:22,009 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-22 12:12:22,095 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-22 12:12:22,097 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-22 12:12:22,211 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-22 12:12:22,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,466 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,585 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,678 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,689 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,694 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,714 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,714 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,770 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,778 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,780 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,793 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,790 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,874 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,935 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,945 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,948 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:22,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,993 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:22,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,001 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,136 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,158 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,285 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,574 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,774 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:23,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:23,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:25,821 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:12:25,838 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-22 12:12:26,079 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:12:28,062 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-22 12:12:28,064 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-22 12:12:28,064 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-22 12:12:28,065 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-22 12:12:28,066 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-22 12:12:28,067 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-22 12:12:28,068 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-22 12:12:28,070 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-22 12:12:28,072 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-22 12:12:28,073 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-22 12:12:28,075 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-22 12:12:28,076 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-22 12:12:28,078 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-22 12:12:28,080 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-22 12:12:28,081 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-22 12:12:28,083 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-22 12:12:28,086 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-22 12:12:28,089 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-22 12:12:28,090 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-22 12:12:28,091 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-22 12:12:28,094 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-22 12:12:28,095 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-22 12:12:28,097 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-22 12:12:28,098 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-22 12:12:28,099 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-22 12:12:28,101 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-22 12:12:28,102 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-22 12:12:28,105 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-22 12:12:28,106 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-22 12:12:28,107 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-22 12:12:28,113 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-22 12:12:28,114 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-22 12:12:28,116 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-22 12:12:28,118 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-22 12:12:28,119 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-22 12:12:28,120 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-22 12:12:28,121 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-22 12:12:28,124 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-22 12:12:28,126 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-22 12:12:28,128 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-22 12:12:28,129 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-22 12:12:28,130 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-22 12:12:28,132 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-22 12:12:28,134 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-22 12:12:28,136 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-22 12:12:28,138 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-22 12:12:28,140 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-22 12:12:28,141 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-22 12:12:28,142 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-22 12:12:28,143 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-22 12:12:28,145 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-22 12:12:28,146 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-22 12:12:28,148 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-22 12:12:28,149 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-22 12:12:28,151 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-22 12:12:28,153 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-22 12:12:28,154 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-22 12:12:28,156 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-22 12:12:28,158 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-22 12:12:28,159 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-22 12:12:28,161 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-22 12:12:28,163 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-22 12:12:28,166 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-22 12:12:28,167 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-22 12:12:28,168 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-22 12:12:28,169 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-22 12:12:28,171 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-22 12:12:28,174 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-22 12:12:28,175 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-22 12:12:28,177 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-22 12:12:28,178 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-22 12:12:28,180 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-22 12:12:28,181 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-22 12:12:28,183 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-22 12:12:28,185 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-22 12:12:28,187 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-22 12:12:28,190 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-22 12:12:28,191 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-22 12:12:28,193 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-22 12:12:28,195 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-22 12:12:28,197 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-22 12:12:28,198 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-22 12:12:28,199 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-22 12:12:28,200 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-22 12:12:28,202 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-22 12:12:28,203 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-22 12:12:28,205 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-22 12:12:28,206 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-22 12:12:28,207 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-22 12:12:28,209 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-22 12:12:28,211 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-22 12:12:28,212 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-22 12:12:28,213 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-22 12:12:28,217 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-22 12:12:28,220 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-22 12:12:28,222 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-22 12:12:28,224 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-22 12:12:28,226 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-22 12:12:28,228 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-22 12:12:28,230 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-22 12:12:28,231 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-22 12:12:28,233 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-22 12:12:28,234 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-22 12:12:28,444 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-22 12:12:28,446 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-22 12:12:28,508 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-22 12:12:28,509 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-22 12:12:28,510 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-22 12:12:28,512 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-22 12:12:28,528 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-22 12:12:28,544 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-22 12:12:28,617 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-22 12:12:28,618 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-22 12:12:28,620 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-22 12:12:28,653 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-22 12:12:29,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,261 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,273 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,285 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,324 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,327 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,322 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,556 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,612 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,711 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,755 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,798 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,849 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:29,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:29,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:30,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:30,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:30,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:30,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:30,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:30,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:30,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:30,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:12:30,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:30,546 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:12:32,596 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:12:32,614 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-22 12:12:32,841 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 7317 docs
2025-12-22 12:13:40,434 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:13:42,547 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-22 12:13:42,552 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-22 12:13:42,555 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-22 12:13:42,559 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-22 12:13:42,564 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-22 12:13:42,568 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-22 12:13:42,572 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-22 12:13:42,574 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-22 12:13:42,582 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-22 12:13:42,587 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-22 12:13:42,594 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-22 12:13:42,598 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-22 12:13:42,604 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-22 12:13:42,609 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-22 12:13:42,622 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-22 12:13:42,629 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-22 12:13:42,634 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-22 12:13:42,643 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-22 12:13:42,650 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-22 12:13:42,656 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-22 12:13:42,676 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-22 12:13:42,767 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-22 12:13:42,772 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-22 12:13:42,847 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-22 12:13:42,853 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-22 12:13:42,953 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-22 12:13:42,956 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-22 12:13:43,231 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-22 12:13:44,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,261 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,281 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,293 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,449 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,646 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,688 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,693 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,715 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,852 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:44,925 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,961 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:44,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,078 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,105 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,256 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,284 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,327 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,357 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,550 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,767 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,783 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,820 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:45,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:45,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,125 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,548 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,593 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,786 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:46,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:46,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:47,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:47,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:48,924 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:13:49,046 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-22 12:13:49,722 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:13:52,237 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-22 12:13:52,239 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-22 12:13:52,241 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-22 12:13:52,243 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-22 12:13:52,245 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-22 12:13:52,247 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-22 12:13:52,248 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-22 12:13:52,250 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-22 12:13:52,252 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-22 12:13:52,253 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-22 12:13:52,255 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-22 12:13:52,256 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-22 12:13:52,258 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-22 12:13:52,261 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-22 12:13:52,262 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-22 12:13:52,265 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-22 12:13:52,268 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-22 12:13:52,272 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-22 12:13:52,274 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-22 12:13:52,276 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-22 12:13:52,278 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-22 12:13:52,279 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-22 12:13:52,281 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-22 12:13:52,283 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-22 12:13:52,284 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-22 12:13:52,287 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-22 12:13:52,288 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-22 12:13:52,291 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-22 12:13:52,293 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-22 12:13:52,294 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-22 12:13:52,349 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-22 12:13:52,356 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-22 12:13:52,417 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-22 12:13:52,433 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-22 12:13:52,456 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-22 12:13:52,503 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-22 12:13:52,516 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-22 12:13:52,560 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-22 12:13:52,562 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-22 12:13:52,565 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-22 12:13:52,631 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-22 12:13:52,644 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-22 12:13:52,650 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-22 12:13:52,734 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-22 12:13:52,737 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-22 12:13:52,742 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-22 12:13:52,744 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-22 12:13:52,802 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-22 12:13:52,804 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-22 12:13:52,805 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-22 12:13:52,810 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-22 12:13:52,883 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-22 12:13:52,896 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-22 12:13:52,902 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-22 12:13:52,975 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-22 12:13:52,977 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-22 12:13:52,979 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-22 12:13:53,031 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-22 12:13:53,033 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-22 12:13:53,035 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-22 12:13:53,087 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-22 12:13:53,090 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-22 12:13:53,104 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-22 12:13:53,162 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-22 12:13:53,164 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-22 12:13:53,166 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-22 12:13:53,212 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-22 12:13:53,266 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-22 12:13:53,307 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-22 12:13:53,309 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-22 12:13:53,310 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-22 12:13:53,369 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-22 12:13:53,401 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-22 12:13:53,403 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-22 12:13:53,410 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-22 12:13:53,482 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-22 12:13:53,485 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-22 12:13:53,486 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-22 12:13:53,545 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-22 12:13:53,582 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-22 12:13:53,585 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-22 12:13:53,586 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-22 12:13:53,588 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-22 12:13:53,677 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-22 12:13:53,725 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-22 12:13:53,728 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-22 12:13:53,730 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-22 12:13:53,782 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-22 12:13:53,796 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-22 12:13:53,798 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-22 12:13:53,883 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-22 12:13:53,885 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-22 12:13:53,886 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-22 12:13:53,944 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-22 12:13:53,946 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-22 12:13:54,021 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-22 12:13:54,024 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-22 12:13:54,083 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-22 12:13:54,085 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-22 12:13:54,087 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-22 12:13:54,155 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-22 12:13:54,157 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-22 12:13:54,172 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-22 12:13:54,225 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-22 12:13:54,228 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-22 12:13:54,234 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-22 12:13:54,291 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-22 12:13:54,293 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-22 12:13:54,295 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-22 12:13:54,353 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-22 12:13:54,356 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-22 12:13:54,395 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-22 12:13:54,397 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-22 12:13:54,443 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-22 12:13:54,444 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-22 12:13:54,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,472 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,526 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,852 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,919 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,922 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,938 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:54,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:54,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,060 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,326 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,561 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:55,850 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:55,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:56,022 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:56,302 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:56,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:56,763 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:13:56,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:13:58,569 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:13:58,653 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-22 12:13:59,154 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 7317 docs
Training CobwebTree:   0%|          | 0/7317 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 17/7317 [00:00<00:43, 169.28it/s]Training CobwebTree:   0%|          | 34/7317 [00:00<00:59, 121.87it/s]Training CobwebTree:   1%|          | 47/7317 [00:00<01:04, 111.87it/s]Training CobwebTree:   1%|          | 59/7317 [00:00<01:18, 92.34it/s] Training CobwebTree:   1%|          | 69/7317 [00:00<01:22, 88.26it/s]Training CobwebTree:   1%|          | 79/7317 [00:00<01:30, 80.36it/s]Training CobwebTree:   1%|          | 88/7317 [00:00<01:34, 76.62it/s]Training CobwebTree:   1%|         | 96/7317 [00:01<01:34, 76.17it/s]Training CobwebTree:   1%|         | 104/7317 [00:01<01:37, 74.24it/s]Training CobwebTree:   2%|         | 112/7317 [00:01<01:37, 73.82it/s]Training CobwebTree:   2%|         | 120/7317 [00:01<01:36, 74.24it/s]Training CobwebTree:   2%|         | 128/7317 [00:01<01:40, 71.73it/s]Training CobwebTree:   2%|         | 136/7317 [00:01<01:41, 70.65it/s]Training CobwebTree:   2%|         | 144/7317 [00:01<01:43, 69.41it/s]Training CobwebTree:   2%|         | 151/7317 [00:01<01:52, 63.53it/s]Training CobwebTree:   2%|         | 159/7317 [00:02<01:49, 65.38it/s]Training CobwebTree:   2%|         | 167/7317 [00:02<01:43, 68.90it/s]Training CobwebTree:   2%|         | 174/7317 [00:02<01:50, 64.64it/s]Training CobwebTree:   2%|         | 181/7317 [00:02<01:48, 65.84it/s]Training CobwebTree:   3%|         | 188/7317 [00:02<01:50, 64.63it/s]Training CobwebTree:   3%|         | 195/7317 [00:02<01:51, 63.65it/s]Training CobwebTree:   3%|         | 202/7317 [00:02<01:55, 61.74it/s]Training CobwebTree:   3%|         | 209/7317 [00:02<01:57, 60.72it/s]Training CobwebTree:   3%|         | 216/7317 [00:02<01:56, 61.18it/s]Training CobwebTree:   3%|         | 223/7317 [00:03<01:52, 62.87it/s]Training CobwebTree:   3%|         | 230/7317 [00:03<01:51, 63.42it/s]Training CobwebTree:   3%|         | 237/7317 [00:03<01:50, 64.23it/s]Training CobwebTree:   3%|         | 244/7317 [00:03<01:50, 63.88it/s]Training CobwebTree:   3%|         | 251/7317 [00:03<01:59, 58.94it/s]Training CobwebTree:   4%|         | 258/7317 [00:03<01:55, 61.09it/s]Training CobwebTree:   4%|         | 265/7317 [00:03<02:03, 57.19it/s]Training CobwebTree:   4%|         | 271/7317 [00:03<02:04, 56.44it/s]Training CobwebTree:   4%|         | 278/7317 [00:03<02:02, 57.60it/s]Training CobwebTree:   4%|         | 284/7317 [00:04<02:01, 57.98it/s]Training CobwebTree:   4%|         | 290/7317 [00:04<02:01, 57.75it/s]Training CobwebTree:   4%|         | 297/7317 [00:04<01:57, 59.77it/s]Training CobwebTree:   4%|         | 303/7317 [00:04<01:59, 58.63it/s]Training CobwebTree:   4%|         | 309/7317 [00:04<01:59, 58.62it/s]Training CobwebTree:   4%|         | 315/7317 [00:04<02:03, 56.58it/s]Training CobwebTree:   4%|         | 321/7317 [00:04<02:03, 56.78it/s]Training CobwebTree:   4%|         | 327/7317 [00:04<02:05, 55.67it/s]Training CobwebTree:   5%|         | 333/7317 [00:04<02:04, 56.00it/s]Training CobwebTree:   5%|         | 339/7317 [00:05<02:05, 55.40it/s]Training CobwebTree:   5%|         | 345/7317 [00:05<02:08, 54.26it/s]Training CobwebTree:   5%|         | 351/7317 [00:05<02:08, 54.16it/s]Training CobwebTree:   5%|         | 357/7317 [00:05<02:10, 53.54it/s]Training CobwebTree:   5%|         | 363/7317 [00:05<02:19, 49.76it/s]Training CobwebTree:   5%|         | 369/7317 [00:05<02:17, 50.64it/s]Training CobwebTree:   5%|         | 375/7317 [00:05<02:14, 51.46it/s]Training CobwebTree:   5%|         | 381/7317 [00:05<02:16, 50.91it/s]Training CobwebTree:   5%|         | 387/7317 [00:05<02:12, 52.34it/s]Training CobwebTree:   5%|         | 393/7317 [00:06<02:19, 49.48it/s]Training CobwebTree:   5%|         | 399/7317 [00:06<02:13, 51.87it/s]Training CobwebTree:   6%|         | 405/7317 [00:06<02:18, 50.05it/s]Training CobwebTree:   6%|         | 411/7317 [00:06<02:16, 50.46it/s]Training CobwebTree:   6%|         | 417/7317 [00:06<02:17, 50.20it/s]Training CobwebTree:   6%|         | 423/7317 [00:06<02:12, 51.99it/s]Training CobwebTree:   6%|         | 429/7317 [00:06<02:09, 53.09it/s]Training CobwebTree:   6%|         | 435/7317 [00:06<02:08, 53.60it/s]Training CobwebTree:   6%|         | 441/7317 [00:07<02:04, 55.02it/s]Training CobwebTree:   6%|         | 447/7317 [00:07<02:10, 52.49it/s]Training CobwebTree:   6%|         | 453/7317 [00:07<02:13, 51.33it/s]Training CobwebTree:   6%|         | 459/7317 [00:07<02:20, 48.90it/s]Training CobwebTree:   6%|         | 465/7317 [00:07<02:14, 50.89it/s]Training CobwebTree:   6%|         | 471/7317 [00:07<02:16, 50.00it/s]Training CobwebTree:   7%|         | 477/7317 [00:07<02:14, 50.97it/s]Training CobwebTree:   7%|         | 483/7317 [00:07<02:15, 50.41it/s]Training CobwebTree:   7%|         | 489/7317 [00:08<02:19, 48.80it/s]Training CobwebTree:   7%|         | 495/7317 [00:08<02:19, 48.81it/s]Training CobwebTree:   7%|         | 502/7317 [00:08<02:12, 51.59it/s]Training CobwebTree:   7%|         | 508/7317 [00:08<02:18, 49.17it/s]Training CobwebTree:   7%|         | 514/7317 [00:08<02:15, 50.30it/s]Training CobwebTree:   7%|         | 520/7317 [00:08<02:15, 50.28it/s]Training CobwebTree:   7%|         | 526/7317 [00:08<02:12, 51.38it/s]Training CobwebTree:   7%|         | 532/7317 [00:08<02:11, 51.46it/s]Training CobwebTree:   7%|         | 538/7317 [00:08<02:11, 51.46it/s]Training CobwebTree:   7%|         | 544/7317 [00:09<02:10, 51.77it/s]Training CobwebTree:   8%|         | 550/7317 [00:09<02:12, 51.01it/s]Training CobwebTree:   8%|         | 556/7317 [00:09<02:12, 51.00it/s]Training CobwebTree:   8%|         | 562/7317 [00:09<02:10, 51.95it/s]Training CobwebTree:   8%|         | 568/7317 [00:09<02:09, 51.94it/s]Training CobwebTree:   8%|         | 574/7317 [00:09<02:05, 53.54it/s]Training CobwebTree:   8%|         | 580/7317 [00:09<02:08, 52.58it/s]Training CobwebTree:   8%|         | 586/7317 [00:09<02:08, 52.46it/s]Training CobwebTree:   8%|         | 592/7317 [00:09<02:09, 51.96it/s]Training CobwebTree:   8%|         | 599/7317 [00:10<02:07, 52.71it/s]Training CobwebTree:   8%|         | 605/7317 [00:10<02:08, 52.18it/s]Training CobwebTree:   8%|         | 611/7317 [00:10<02:04, 53.68it/s]Training CobwebTree:   8%|         | 617/7317 [00:10<02:10, 51.21it/s]Training CobwebTree:   9%|         | 623/7317 [00:10<02:13, 50.09it/s]Training CobwebTree:   9%|         | 630/7317 [00:10<02:06, 52.81it/s]Training CobwebTree:   9%|         | 636/7317 [00:10<02:13, 49.88it/s]Training CobwebTree:   9%|         | 642/7317 [00:10<02:10, 51.16it/s]Training CobwebTree:   9%|         | 648/7317 [00:11<02:12, 50.16it/s]Training CobwebTree:   9%|         | 654/7317 [00:11<02:17, 48.38it/s]Training CobwebTree:   9%|         | 660/7317 [00:11<02:13, 49.88it/s]Training CobwebTree:   9%|         | 667/7317 [00:11<02:06, 52.74it/s]Training CobwebTree:   9%|         | 673/7317 [00:11<02:06, 52.43it/s]Training CobwebTree:   9%|         | 679/7317 [00:11<02:04, 53.19it/s]Training CobwebTree:   9%|         | 685/7317 [00:11<02:06, 52.22it/s]Training CobwebTree:   9%|         | 691/7317 [00:11<02:09, 51.23it/s]Training CobwebTree:  10%|         | 697/7317 [00:12<02:11, 50.38it/s]Training CobwebTree:  10%|         | 703/7317 [00:12<02:12, 50.05it/s]Training CobwebTree:  10%|         | 709/7317 [00:12<02:10, 50.46it/s]Training CobwebTree:  10%|         | 715/7317 [00:12<02:14, 49.01it/s]Training CobwebTree:  10%|         | 721/7317 [00:12<02:10, 50.45it/s]Training CobwebTree:  10%|         | 727/7317 [00:12<02:08, 51.14it/s]Training CobwebTree:  10%|         | 733/7317 [00:12<02:13, 49.30it/s]Training CobwebTree:  10%|         | 739/7317 [00:12<02:12, 49.78it/s]Training CobwebTree:  10%|         | 745/7317 [00:13<02:13, 49.31it/s]Training CobwebTree:  10%|         | 751/7317 [00:13<02:07, 51.33it/s]Training CobwebTree:  10%|         | 757/7317 [00:13<02:13, 49.18it/s]Training CobwebTree:  10%|         | 763/7317 [00:13<02:12, 49.32it/s]Training CobwebTree:  11%|         | 769/7317 [00:13<02:12, 49.48it/s]Training CobwebTree:  11%|         | 774/7317 [00:13<02:13, 48.98it/s]Training CobwebTree:  11%|         | 779/7317 [00:13<02:15, 48.33it/s]Training CobwebTree:  11%|         | 785/7317 [00:13<02:11, 49.70it/s]Training CobwebTree:  11%|         | 791/7317 [00:13<02:08, 50.96it/s]Training CobwebTree:  11%|         | 797/7317 [00:14<02:16, 47.94it/s]Training CobwebTree:  11%|         | 802/7317 [00:14<02:17, 47.23it/s]Training CobwebTree:  11%|         | 808/7317 [00:14<02:16, 47.75it/s]Training CobwebTree:  11%|         | 813/7317 [00:14<02:15, 47.84it/s]Training CobwebTree:  11%|         | 818/7317 [00:14<02:15, 48.09it/s]Training CobwebTree:  11%|         | 823/7317 [00:14<02:15, 47.86it/s]Training CobwebTree:  11%|        | 828/7317 [00:14<02:14, 48.14it/s]Training CobwebTree:  11%|        | 834/7317 [00:14<02:11, 49.45it/s]Training CobwebTree:  11%|        | 839/7317 [00:14<02:12, 48.82it/s]Training CobwebTree:  12%|        | 845/7317 [00:15<02:08, 50.37it/s]Training CobwebTree:  12%|        | 851/7317 [00:15<02:09, 49.99it/s]Training CobwebTree:  12%|        | 856/7317 [00:15<02:09, 49.82it/s]Training CobwebTree:  12%|        | 861/7317 [00:15<02:10, 49.50it/s]Training CobwebTree:  12%|        | 866/7317 [00:15<02:14, 47.85it/s]Training CobwebTree:  12%|        | 871/7317 [00:15<02:18, 46.46it/s]Training CobwebTree:  12%|        | 876/7317 [00:15<02:16, 47.29it/s]Training CobwebTree:  12%|        | 881/7317 [00:15<02:14, 47.88it/s]Training CobwebTree:  12%|        | 887/7317 [00:15<02:10, 49.38it/s]Training CobwebTree:  12%|        | 893/7317 [00:16<02:08, 50.04it/s]Training CobwebTree:  12%|        | 899/7317 [00:16<02:08, 49.78it/s]Training CobwebTree:  12%|        | 905/7317 [00:16<02:06, 50.53it/s]Training CobwebTree:  12%|        | 911/7317 [00:16<02:06, 50.51it/s]Training CobwebTree:  13%|        | 917/7317 [00:16<02:08, 49.90it/s]Training CobwebTree:  13%|        | 922/7317 [00:16<02:09, 49.49it/s]Training CobwebTree:  13%|        | 928/7317 [00:16<02:08, 49.62it/s]Training CobwebTree:  13%|        | 933/7317 [00:16<02:16, 46.61it/s]Training CobwebTree:  13%|        | 938/7317 [00:16<02:20, 45.52it/s]Training CobwebTree:  13%|        | 944/7317 [00:17<02:10, 48.92it/s]Training CobwebTree:  13%|        | 949/7317 [00:17<02:19, 45.80it/s]Training CobwebTree:  13%|        | 955/7317 [00:17<02:14, 47.14it/s]Training CobwebTree:  13%|        | 961/7317 [00:17<02:09, 49.03it/s]Training CobwebTree:  13%|        | 966/7317 [00:17<02:08, 49.24it/s]Training CobwebTree:  13%|        | 971/7317 [00:17<02:12, 47.90it/s]Training CobwebTree:  13%|        | 976/7317 [00:17<02:12, 47.73it/s]Training CobwebTree:  13%|        | 982/7317 [00:17<02:10, 48.70it/s]Training CobwebTree:  13%|        | 987/7317 [00:17<02:14, 47.08it/s]Training CobwebTree:  14%|        | 992/7317 [00:18<02:15, 46.66it/s]Training CobwebTree:  14%|        | 998/7317 [00:18<02:13, 47.51it/s]Training CobwebTree:  14%|        | 1003/7317 [00:18<02:13, 47.47it/s]Training CobwebTree:  14%|        | 1008/7317 [00:18<02:15, 46.66it/s]Training CobwebTree:  14%|        | 1013/7317 [00:18<02:15, 46.53it/s]Training CobwebTree:  14%|        | 1019/7317 [00:18<02:14, 46.73it/s]Training CobwebTree:  14%|        | 1024/7317 [00:18<02:16, 46.21it/s]Training CobwebTree:  14%|        | 1029/7317 [00:18<02:16, 46.17it/s]Training CobwebTree:  14%|        | 1034/7317 [00:19<02:16, 45.92it/s]Training CobwebTree:  14%|        | 1039/7317 [00:19<02:20, 44.76it/s]Training CobwebTree:  14%|        | 1044/7317 [00:19<02:17, 45.58it/s]Training CobwebTree:  14%|        | 1050/7317 [00:19<02:14, 46.54it/s]Training CobwebTree:  14%|        | 1055/7317 [00:19<02:14, 46.52it/s]Training CobwebTree:  14%|        | 1060/7317 [00:19<02:13, 46.90it/s]Training CobwebTree:  15%|        | 1065/7317 [00:19<02:13, 47.00it/s]Training CobwebTree:  15%|        | 1071/7317 [00:19<02:09, 48.25it/s]Training CobwebTree:  15%|        | 1077/7317 [00:19<02:05, 49.57it/s]Training CobwebTree:  15%|        | 1082/7317 [00:20<02:17, 45.42it/s]Training CobwebTree:  15%|        | 1087/7317 [00:20<02:15, 45.89it/s]Training CobwebTree:  15%|        | 1093/7317 [00:20<02:10, 47.67it/s]Training CobwebTree:  15%|        | 1099/7317 [00:20<02:06, 49.28it/s]Training CobwebTree:  15%|        | 1104/7317 [00:20<02:11, 47.17it/s]Training CobwebTree:  15%|        | 1110/7317 [00:20<02:08, 48.38it/s]Training CobwebTree:  15%|        | 1115/7317 [00:20<02:07, 48.51it/s]Training CobwebTree:  15%|        | 1120/7317 [00:20<02:10, 47.59it/s]Training CobwebTree:  15%|        | 1125/7317 [00:20<02:08, 48.12it/s]Training CobwebTree:  15%|        | 1130/7317 [00:21<02:10, 47.32it/s]Training CobwebTree:  16%|        | 1135/7317 [00:21<02:09, 47.77it/s]Training CobwebTree:  16%|        | 1140/7317 [00:21<02:09, 47.70it/s]Training CobwebTree:  16%|        | 1145/7317 [00:21<02:08, 48.09it/s]Training CobwebTree:  16%|        | 1150/7317 [00:21<02:11, 46.90it/s]Training CobwebTree:  16%|        | 1155/7317 [00:21<02:13, 46.08it/s]Training CobwebTree:  16%|        | 1160/7317 [00:21<02:15, 45.27it/s]Training CobwebTree:  16%|        | 1165/7317 [00:21<02:19, 44.20it/s]Training CobwebTree:  16%|        | 1170/7317 [00:21<02:20, 43.76it/s]Training CobwebTree:  16%|        | 1175/7317 [00:22<02:17, 44.66it/s]Training CobwebTree:  16%|        | 1180/7317 [00:22<02:15, 45.40it/s]Training CobwebTree:  16%|        | 1185/7317 [00:22<02:20, 43.66it/s]Training CobwebTree:  16%|        | 1190/7317 [00:22<02:20, 43.48it/s]Training CobwebTree:  16%|        | 1195/7317 [00:22<02:19, 43.89it/s]Training CobwebTree:  16%|        | 1200/7317 [00:22<02:16, 44.73it/s]Training CobwebTree:  16%|        | 1205/7317 [00:22<02:17, 44.31it/s]Training CobwebTree:  17%|        | 1210/7317 [00:22<02:16, 44.71it/s]Training CobwebTree:  17%|        | 1215/7317 [00:22<02:23, 42.59it/s]Training CobwebTree:  17%|        | 1220/7317 [00:23<02:23, 42.36it/s]Training CobwebTree:  17%|        | 1226/7317 [00:23<02:15, 44.93it/s]Training CobwebTree:  17%|        | 1231/7317 [00:23<02:18, 43.93it/s]Training CobwebTree:  17%|        | 1236/7317 [00:23<02:19, 43.47it/s]Training CobwebTree:  17%|        | 1241/7317 [00:23<02:20, 43.21it/s]Training CobwebTree:  17%|        | 1246/7317 [00:23<02:20, 43.10it/s]Training CobwebTree:  17%|        | 1251/7317 [00:23<02:16, 44.54it/s]Training CobwebTree:  17%|        | 1256/7317 [00:23<02:16, 44.55it/s]Training CobwebTree:  17%|        | 1261/7317 [00:23<02:13, 45.48it/s]Training CobwebTree:  17%|        | 1266/7317 [00:24<02:19, 43.46it/s]Training CobwebTree:  17%|        | 1271/7317 [00:24<02:15, 44.59it/s]Training CobwebTree:  17%|        | 1276/7317 [00:24<02:16, 44.20it/s]Training CobwebTree:  18%|        | 1281/7317 [00:24<02:15, 44.62it/s]Training CobwebTree:  18%|        | 1286/7317 [00:24<02:19, 43.23it/s]Training CobwebTree:  18%|        | 1291/7317 [00:24<02:17, 43.98it/s]Training CobwebTree:  18%|        | 1296/7317 [00:24<02:18, 43.47it/s]Training CobwebTree:  18%|        | 1302/7317 [00:24<02:13, 44.99it/s]Training CobwebTree:  18%|        | 1307/7317 [00:25<02:10, 45.91it/s]Training CobwebTree:  18%|        | 1312/7317 [00:25<02:07, 46.97it/s]Training CobwebTree:  18%|        | 1317/7317 [00:25<02:08, 46.68it/s]Training CobwebTree:  18%|        | 1322/7317 [00:25<02:10, 45.79it/s]Training CobwebTree:  18%|        | 1327/7317 [00:25<02:10, 45.90it/s]Training CobwebTree:  18%|        | 1332/7317 [00:25<02:09, 46.12it/s]Training CobwebTree:  18%|        | 1337/7317 [00:25<02:11, 45.57it/s]Training CobwebTree:  18%|        | 1342/7317 [00:25<02:12, 45.11it/s]Training CobwebTree:  18%|        | 1347/7317 [00:25<02:15, 43.93it/s]Training CobwebTree:  18%|        | 1352/7317 [00:26<02:15, 44.13it/s]Training CobwebTree:  19%|        | 1357/7317 [00:26<02:17, 43.33it/s]Training CobwebTree:  19%|        | 1362/7317 [00:26<02:13, 44.62it/s]Training CobwebTree:  19%|        | 1367/7317 [00:26<02:18, 42.85it/s]Training CobwebTree:  19%|        | 1372/7317 [00:26<02:19, 42.69it/s]Training CobwebTree:  19%|        | 1377/7317 [00:26<02:17, 43.16it/s]Training CobwebTree:  19%|        | 1382/7317 [00:26<02:19, 42.54it/s]Training CobwebTree:  19%|        | 1388/7317 [00:26<02:11, 45.03it/s]Training CobwebTree:  19%|        | 1393/7317 [00:26<02:20, 42.13it/s]Training CobwebTree:  19%|        | 1398/7317 [00:27<02:15, 43.64it/s]Training CobwebTree:  19%|        | 1403/7317 [00:27<02:13, 44.24it/s]Training CobwebTree:  19%|        | 1408/7317 [00:27<02:11, 44.95it/s]Training CobwebTree:  19%|        | 1413/7317 [00:27<02:16, 43.23it/s]Training CobwebTree:  19%|        | 1418/7317 [00:27<02:15, 43.64it/s]Training CobwebTree:  19%|        | 1424/7317 [00:27<02:09, 45.65it/s]Training CobwebTree:  20%|        | 1429/7317 [00:27<02:09, 45.35it/s]Training CobwebTree:  20%|        | 1434/7317 [00:27<02:10, 45.18it/s]Training CobwebTree:  20%|        | 1439/7317 [00:27<02:07, 46.19it/s]Training CobwebTree:  20%|        | 1444/7317 [00:28<02:08, 45.83it/s]Training CobwebTree:  20%|        | 1449/7317 [00:28<02:05, 46.80it/s]Training CobwebTree:  20%|        | 1455/7317 [00:28<02:01, 48.09it/s]Training CobwebTree:  20%|        | 1460/7317 [00:28<02:03, 47.50it/s]Training CobwebTree:  20%|        | 1465/7317 [00:28<02:11, 44.39it/s]Training CobwebTree:  20%|        | 1470/7317 [00:28<02:12, 44.21it/s]Training CobwebTree:  20%|        | 1475/7317 [00:28<02:08, 45.49it/s]Training CobwebTree:  20%|        | 1480/7317 [00:28<02:06, 46.19it/s]Training CobwebTree:  20%|        | 1485/7317 [00:28<02:12, 44.07it/s]Training CobwebTree:  20%|        | 1490/7317 [00:29<02:11, 44.30it/s]Training CobwebTree:  20%|        | 1495/7317 [00:29<02:10, 44.72it/s]Training CobwebTree:  21%|        | 1500/7317 [00:29<02:16, 42.59it/s]Training CobwebTree:  21%|        | 1505/7317 [00:29<02:18, 41.97it/s]Training CobwebTree:  21%|        | 1510/7317 [00:29<02:14, 43.19it/s]Training CobwebTree:  21%|        | 1515/7317 [00:29<02:14, 43.16it/s]Training CobwebTree:  21%|        | 1520/7317 [00:29<02:10, 44.41it/s]Training CobwebTree:  21%|        | 1525/7317 [00:29<02:14, 43.16it/s]Training CobwebTree:  21%|        | 1530/7317 [00:30<02:11, 44.02it/s]Training CobwebTree:  21%|        | 1535/7317 [00:30<02:10, 44.28it/s]Training CobwebTree:  21%|        | 1540/7317 [00:30<02:11, 44.07it/s]Training CobwebTree:  21%|        | 1545/7317 [00:30<02:11, 43.79it/s]Training CobwebTree:  21%|        | 1550/7317 [00:30<02:15, 42.50it/s]Training CobwebTree:  21%|       | 1555/7317 [00:30<02:12, 43.38it/s]Training CobwebTree:  21%|       | 1561/7317 [00:30<02:07, 45.28it/s]Training CobwebTree:  21%|       | 1566/7317 [00:30<02:08, 44.77it/s]Training CobwebTree:  21%|       | 1572/7317 [00:30<02:03, 46.56it/s]Training CobwebTree:  22%|       | 1577/7317 [00:31<02:05, 45.72it/s]Training CobwebTree:  22%|       | 1582/7317 [00:31<02:06, 45.32it/s]Training CobwebTree:  22%|       | 1587/7317 [00:31<02:08, 44.69it/s]Training CobwebTree:  22%|       | 1592/7317 [00:31<02:09, 44.34it/s]Training CobwebTree:  22%|       | 1597/7317 [00:31<02:10, 43.91it/s]Training CobwebTree:  22%|       | 1602/7317 [00:31<02:09, 44.17it/s]Training CobwebTree:  22%|       | 1607/7317 [00:31<02:09, 44.06it/s]Training CobwebTree:  22%|       | 1612/7317 [00:31<02:11, 43.43it/s]Training CobwebTree:  22%|       | 1617/7317 [00:31<02:09, 43.97it/s]Training CobwebTree:  22%|       | 1623/7317 [00:32<02:08, 44.46it/s]Training CobwebTree:  22%|       | 1628/7317 [00:32<02:05, 45.28it/s]Training CobwebTree:  22%|       | 1633/7317 [00:32<02:08, 44.28it/s]Training CobwebTree:  22%|       | 1638/7317 [00:32<02:07, 44.70it/s]Training CobwebTree:  22%|       | 1643/7317 [00:32<02:05, 45.31it/s]Training CobwebTree:  23%|       | 1648/7317 [00:32<02:10, 43.56it/s]Training CobwebTree:  23%|       | 1653/7317 [00:32<02:11, 43.05it/s]Training CobwebTree:  23%|       | 1658/7317 [00:32<02:10, 43.47it/s]Training CobwebTree:  23%|       | 1663/7317 [00:33<02:07, 44.47it/s]Training CobwebTree:  23%|       | 1669/7317 [00:33<01:59, 47.22it/s]Training CobwebTree:  23%|       | 1674/7317 [00:33<01:59, 47.34it/s]Training CobwebTree:  23%|       | 1679/7317 [00:33<02:04, 45.21it/s]Training CobwebTree:  23%|       | 1685/7317 [00:33<02:05, 45.03it/s]Training CobwebTree:  23%|       | 1690/7317 [00:33<02:05, 44.78it/s]Training CobwebTree:  23%|       | 1695/7317 [00:33<02:06, 44.55it/s]Training CobwebTree:  23%|       | 1700/7317 [00:33<02:06, 44.55it/s]Training CobwebTree:  23%|       | 1705/7317 [00:33<02:07, 44.06it/s]Training CobwebTree:  23%|       | 1710/7317 [00:34<02:06, 44.47it/s]Training CobwebTree:  23%|       | 1715/7317 [00:34<02:07, 43.84it/s]Training CobwebTree:  24%|       | 1720/7317 [00:34<02:05, 44.46it/s]Training CobwebTree:  24%|       | 1725/7317 [00:34<02:05, 44.49it/s]Training CobwebTree:  24%|       | 1731/7317 [00:34<01:59, 46.88it/s]Training CobwebTree:  24%|       | 1736/7317 [00:34<02:00, 46.30it/s]Training CobwebTree:  24%|       | 1741/7317 [00:34<02:00, 46.14it/s]Training CobwebTree:  24%|       | 1746/7317 [00:34<02:03, 45.17it/s]Training CobwebTree:  24%|       | 1751/7317 [00:34<02:04, 44.66it/s]Training CobwebTree:  24%|       | 1756/7317 [00:35<02:08, 43.41it/s]Training CobwebTree:  24%|       | 1761/7317 [00:35<02:16, 40.78it/s]Training CobwebTree:  24%|       | 1766/7317 [00:35<02:16, 40.72it/s]Training CobwebTree:  24%|       | 1772/7317 [00:35<02:05, 44.33it/s]Training CobwebTree:  24%|       | 1777/7317 [00:35<02:10, 42.42it/s]Training CobwebTree:  24%|       | 1782/7317 [00:35<02:10, 42.44it/s]Training CobwebTree:  24%|       | 1787/7317 [00:35<02:09, 42.63it/s]Training CobwebTree:  25%|       | 1793/7317 [00:35<02:02, 44.98it/s]Training CobwebTree:  25%|       | 1798/7317 [00:36<02:03, 44.55it/s]Training CobwebTree:  25%|       | 1803/7317 [00:36<02:01, 45.22it/s]Training CobwebTree:  25%|       | 1808/7317 [00:36<02:02, 45.06it/s]Training CobwebTree:  25%|       | 1813/7317 [00:36<02:01, 45.21it/s]Training CobwebTree:  25%|       | 1818/7317 [00:36<02:07, 43.04it/s]Training CobwebTree:  25%|       | 1823/7317 [00:36<02:10, 41.95it/s]Training CobwebTree:  25%|       | 1828/7317 [00:36<02:11, 41.86it/s]Training CobwebTree:  25%|       | 1833/7317 [00:36<02:10, 41.87it/s]Training CobwebTree:  25%|       | 1838/7317 [00:36<02:07, 42.94it/s]Training CobwebTree:  25%|       | 1843/7317 [00:37<02:03, 44.17it/s]Training CobwebTree:  25%|       | 1848/7317 [00:37<02:06, 43.13it/s]Training CobwebTree:  25%|       | 1853/7317 [00:37<02:05, 43.42it/s]Training CobwebTree:  25%|       | 1858/7317 [00:37<02:03, 44.36it/s]Training CobwebTree:  25%|       | 1863/7317 [00:37<02:00, 45.39it/s]Training CobwebTree:  26%|       | 1868/7317 [00:37<02:08, 42.28it/s]Training CobwebTree:  26%|       | 1873/7317 [00:37<02:14, 40.54it/s]Training CobwebTree:  26%|       | 1878/7317 [00:37<02:08, 42.17it/s]Training CobwebTree:  26%|       | 1883/7317 [00:38<02:06, 43.12it/s]Training CobwebTree:  26%|       | 1888/7317 [00:38<02:01, 44.52it/s]Training CobwebTree:  26%|       | 1893/7317 [00:38<02:10, 41.69it/s]Training CobwebTree:  26%|       | 1898/7317 [00:38<02:07, 42.36it/s]Training CobwebTree:  26%|       | 1903/7317 [00:38<02:04, 43.45it/s]Training CobwebTree:  26%|       | 1908/7317 [00:38<02:03, 43.77it/s]Training CobwebTree:  26%|       | 1913/7317 [00:38<02:03, 43.62it/s]Training CobwebTree:  26%|       | 1918/7317 [00:38<02:00, 44.98it/s]Training CobwebTree:  26%|       | 1923/7317 [00:38<02:02, 44.13it/s]Training CobwebTree:  26%|       | 1928/7317 [00:39<02:05, 42.91it/s]Training CobwebTree:  26%|       | 1933/7317 [00:39<02:11, 40.86it/s]Training CobwebTree:  26%|       | 1938/7317 [00:39<02:07, 42.14it/s]Training CobwebTree:  27%|       | 1943/7317 [00:39<02:09, 41.42it/s]Training CobwebTree:  27%|       | 1948/7317 [00:39<02:07, 42.05it/s]Training CobwebTree:  27%|       | 1953/7317 [00:39<02:03, 43.32it/s]Training CobwebTree:  27%|       | 1958/7317 [00:39<02:04, 43.10it/s]Training CobwebTree:  27%|       | 1963/7317 [00:39<02:06, 42.40it/s]Training CobwebTree:  27%|       | 1968/7317 [00:40<02:08, 41.49it/s]Training CobwebTree:  27%|       | 1973/7317 [00:40<02:07, 42.07it/s]Training CobwebTree:  27%|       | 1978/7317 [00:40<02:11, 40.69it/s]Training CobwebTree:  27%|       | 1983/7317 [00:40<02:10, 40.74it/s]Training CobwebTree:  27%|       | 1988/7317 [00:40<02:10, 40.72it/s]Training CobwebTree:  27%|       | 1993/7317 [00:40<02:11, 40.62it/s]Training CobwebTree:  27%|       | 1998/7317 [00:40<02:10, 40.84it/s]Training CobwebTree:  27%|       | 2003/7317 [00:40<02:07, 41.70it/s]Training CobwebTree:  27%|       | 2008/7317 [00:40<02:01, 43.61it/s]Training CobwebTree:  28%|       | 2013/7317 [00:41<02:01, 43.53it/s]Training CobwebTree:  28%|       | 2018/7317 [00:41<01:57, 45.28it/s]Training CobwebTree:  28%|       | 2023/7317 [00:41<02:01, 43.59it/s]Training CobwebTree:  28%|       | 2028/7317 [00:41<02:00, 43.86it/s]Training CobwebTree:  28%|       | 2033/7317 [00:41<01:59, 44.34it/s]Training CobwebTree:  28%|       | 2038/7317 [00:41<01:57, 45.10it/s]Training CobwebTree:  28%|       | 2043/7317 [00:41<02:01, 43.58it/s]Training CobwebTree:  28%|       | 2048/7317 [00:41<02:03, 42.50it/s]Training CobwebTree:  28%|       | 2053/7317 [00:42<02:00, 43.55it/s]Training CobwebTree:  28%|       | 2058/7317 [00:42<01:58, 44.55it/s]Training CobwebTree:  28%|       | 2063/7317 [00:42<01:59, 44.12it/s]Training CobwebTree:  28%|       | 2068/7317 [00:42<02:00, 43.52it/s]Training CobwebTree:  28%|       | 2073/7317 [00:42<02:00, 43.53it/s]Training CobwebTree:  28%|       | 2078/7317 [00:42<01:59, 43.90it/s]Training CobwebTree:  28%|       | 2083/7317 [00:42<02:00, 43.59it/s]Training CobwebTree:  29%|       | 2088/7317 [00:42<02:02, 42.57it/s]Training CobwebTree:  29%|       | 2093/7317 [00:42<02:07, 40.89it/s]Training CobwebTree:  29%|       | 2098/7317 [00:43<02:03, 42.18it/s]Training CobwebTree:  29%|       | 2103/7317 [00:43<02:09, 40.29it/s]Training CobwebTree:  29%|       | 2108/7317 [00:43<02:10, 39.93it/s]Training CobwebTree:  29%|       | 2113/7317 [00:43<02:08, 40.36it/s]Training CobwebTree:  29%|       | 2118/7317 [00:43<02:09, 40.28it/s]Training CobwebTree:  29%|       | 2123/7317 [00:43<02:02, 42.52it/s]Training CobwebTree:  29%|       | 2128/7317 [00:43<02:01, 42.75it/s]Training CobwebTree:  29%|       | 2133/7317 [00:43<02:03, 41.81it/s]Training CobwebTree:  29%|       | 2138/7317 [00:44<02:01, 42.76it/s]Training CobwebTree:  29%|       | 2143/7317 [00:44<02:01, 42.42it/s]Training CobwebTree:  29%|       | 2148/7317 [00:44<02:02, 42.15it/s]Training CobwebTree:  29%|       | 2153/7317 [00:44<01:59, 43.14it/s]Training CobwebTree:  29%|       | 2158/7317 [00:44<02:00, 42.75it/s]Training CobwebTree:  30%|       | 2163/7317 [00:44<01:57, 43.92it/s]Training CobwebTree:  30%|       | 2168/7317 [00:44<02:04, 41.49it/s]Training CobwebTree:  30%|       | 2173/7317 [00:44<01:59, 43.07it/s]Training CobwebTree:  30%|       | 2178/7317 [00:44<02:01, 42.30it/s]Training CobwebTree:  30%|       | 2183/7317 [00:45<02:09, 39.56it/s]Training CobwebTree:  30%|       | 2188/7317 [00:45<02:10, 39.22it/s]Training CobwebTree:  30%|       | 2193/7317 [00:45<02:04, 41.08it/s]Training CobwebTree:  30%|       | 2198/7317 [00:45<02:02, 41.66it/s]Training CobwebTree:  30%|       | 2203/7317 [00:45<02:02, 41.81it/s]Training CobwebTree:  30%|       | 2208/7317 [00:45<02:03, 41.48it/s]Training CobwebTree:  30%|       | 2213/7317 [00:45<02:05, 40.70it/s]Training CobwebTree:  30%|       | 2218/7317 [00:45<02:07, 39.97it/s]Training CobwebTree:  30%|       | 2223/7317 [00:46<02:01, 41.92it/s]Training CobwebTree:  30%|       | 2228/7317 [00:46<02:00, 42.37it/s]Training CobwebTree:  31%|       | 2233/7317 [00:46<01:57, 43.10it/s]Training CobwebTree:  31%|       | 2238/7317 [00:46<02:00, 42.10it/s]Training CobwebTree:  31%|       | 2243/7317 [00:46<02:02, 41.37it/s]Training CobwebTree:  31%|       | 2248/7317 [00:46<01:56, 43.38it/s]Training CobwebTree:  31%|       | 2253/7317 [00:46<02:00, 42.07it/s]Training CobwebTree:  31%|       | 2258/7317 [00:46<01:59, 42.17it/s]Training CobwebTree:  31%|       | 2263/7317 [00:47<02:05, 40.13it/s]Training CobwebTree:  31%|       | 2268/7317 [00:47<02:06, 40.05it/s]Training CobwebTree:  31%|       | 2273/7317 [00:47<02:04, 40.36it/s]Training CobwebTree:  31%|       | 2278/7317 [00:47<02:01, 41.36it/s]Training CobwebTree:  31%|       | 2283/7317 [00:47<02:00, 41.62it/s]Training CobwebTree:  31%|      | 2288/7317 [00:47<01:59, 41.96it/s]Training CobwebTree:  31%|      | 2293/7317 [00:47<02:01, 41.22it/s]Training CobwebTree:  31%|      | 2298/7317 [00:47<01:59, 42.02it/s]Training CobwebTree:  31%|      | 2303/7317 [00:47<01:57, 42.58it/s]Training CobwebTree:  32%|      | 2308/7317 [00:48<02:00, 41.62it/s]Training CobwebTree:  32%|      | 2313/7317 [00:48<02:01, 41.21it/s]Training CobwebTree:  32%|      | 2318/7317 [00:48<02:00, 41.58it/s]Training CobwebTree:  32%|      | 2323/7317 [00:48<02:00, 41.28it/s]Training CobwebTree:  32%|      | 2328/7317 [00:48<01:56, 42.84it/s]Training CobwebTree:  32%|      | 2333/7317 [00:48<01:55, 43.26it/s]Training CobwebTree:  32%|      | 2338/7317 [00:48<01:56, 42.91it/s]Training CobwebTree:  32%|      | 2343/7317 [00:48<01:59, 41.74it/s]Training CobwebTree:  32%|      | 2348/7317 [00:49<01:59, 41.67it/s]Training CobwebTree:  32%|      | 2353/7317 [00:49<02:00, 41.24it/s]Training CobwebTree:  32%|      | 2358/7317 [00:49<02:00, 41.12it/s]Training CobwebTree:  32%|      | 2363/7317 [00:49<02:04, 39.92it/s]Training CobwebTree:  32%|      | 2368/7317 [00:49<02:00, 41.18it/s]Training CobwebTree:  32%|      | 2373/7317 [00:49<01:59, 41.45it/s]Training CobwebTree:  32%|      | 2378/7317 [00:49<02:00, 41.03it/s]Training CobwebTree:  33%|      | 2383/7317 [00:49<01:58, 41.78it/s]Training CobwebTree:  33%|      | 2389/7317 [00:50<01:52, 43.66it/s]Training CobwebTree:  33%|      | 2394/7317 [00:50<01:55, 42.51it/s]Training CobwebTree:  33%|      | 2399/7317 [00:50<01:55, 42.60it/s]Training CobwebTree:  33%|      | 2404/7317 [00:50<01:53, 43.47it/s]Training CobwebTree:  33%|      | 2409/7317 [00:50<01:55, 42.32it/s]Training CobwebTree:  33%|      | 2414/7317 [00:50<01:57, 41.61it/s]Training CobwebTree:  33%|      | 2419/7317 [00:50<01:55, 42.42it/s]Training CobwebTree:  33%|      | 2424/7317 [00:50<02:02, 39.92it/s]Training CobwebTree:  33%|      | 2429/7317 [00:51<02:02, 40.03it/s]Training CobwebTree:  33%|      | 2434/7317 [00:51<01:55, 42.28it/s]Training CobwebTree:  33%|      | 2439/7317 [00:51<01:53, 42.79it/s]Training CobwebTree:  33%|      | 2444/7317 [00:51<02:04, 39.18it/s]Training CobwebTree:  33%|      | 2449/7317 [00:51<02:05, 38.87it/s]Training CobwebTree:  34%|      | 2454/7317 [00:51<01:59, 40.59it/s]Training CobwebTree:  34%|      | 2459/7317 [00:51<01:57, 41.46it/s]Training CobwebTree:  34%|      | 2464/7317 [00:51<01:53, 42.91it/s]Training CobwebTree:  34%|      | 2469/7317 [00:51<01:58, 40.90it/s]Training CobwebTree:  34%|      | 2474/7317 [00:52<01:52, 42.99it/s]Training CobwebTree:  34%|      | 2479/7317 [00:52<01:55, 41.83it/s]Training CobwebTree:  34%|      | 2484/7317 [00:52<01:54, 42.12it/s]Training CobwebTree:  34%|      | 2489/7317 [00:52<01:54, 42.06it/s]Training CobwebTree:  34%|      | 2494/7317 [00:52<01:49, 43.94it/s]Training CobwebTree:  34%|      | 2499/7317 [00:52<01:49, 44.08it/s]Training CobwebTree:  34%|      | 2504/7317 [00:52<01:49, 43.81it/s]Training CobwebTree:  34%|      | 2509/7317 [00:52<01:54, 42.03it/s]Training CobwebTree:  34%|      | 2514/7317 [00:53<02:01, 39.53it/s]Training CobwebTree:  34%|      | 2519/7317 [00:53<02:01, 39.40it/s]Training CobwebTree:  34%|      | 2523/7317 [00:53<02:03, 38.78it/s]Training CobwebTree:  35%|      | 2528/7317 [00:53<01:59, 40.23it/s]Training CobwebTree:  35%|      | 2533/7317 [00:53<01:54, 41.89it/s]Training CobwebTree:  35%|      | 2538/7317 [00:53<01:55, 41.53it/s]Training CobwebTree:  35%|      | 2543/7317 [00:53<01:54, 41.84it/s]Training CobwebTree:  35%|      | 2548/7317 [00:53<01:57, 40.43it/s]Training CobwebTree:  35%|      | 2553/7317 [00:54<01:54, 41.48it/s]Training CobwebTree:  35%|      | 2558/7317 [00:54<02:00, 39.46it/s]Training CobwebTree:  35%|      | 2562/7317 [00:54<02:04, 38.26it/s]Training CobwebTree:  35%|      | 2567/7317 [00:54<01:56, 40.81it/s]Training CobwebTree:  35%|      | 2572/7317 [00:54<01:55, 41.17it/s]Training CobwebTree:  35%|      | 2577/7317 [00:54<01:51, 42.36it/s]Training CobwebTree:  35%|      | 2582/7317 [00:54<01:48, 43.81it/s]Training CobwebTree:  35%|      | 2587/7317 [00:54<01:53, 41.55it/s]Training CobwebTree:  35%|      | 2592/7317 [00:54<01:52, 42.09it/s]Training CobwebTree:  35%|      | 2597/7317 [00:55<01:50, 42.71it/s]Training CobwebTree:  36%|      | 2602/7317 [00:55<01:49, 43.21it/s]Training CobwebTree:  36%|      | 2607/7317 [00:55<01:47, 43.89it/s]Training CobwebTree:  36%|      | 2612/7317 [00:55<01:45, 44.69it/s]Training CobwebTree:  36%|      | 2617/7317 [00:55<01:47, 43.87it/s]Training CobwebTree:  36%|      | 2622/7317 [00:55<01:50, 42.49it/s]Training CobwebTree:  36%|      | 2627/7317 [00:55<01:52, 41.70it/s]Training CobwebTree:  36%|      | 2632/7317 [00:55<01:50, 42.24it/s]Training CobwebTree:  36%|      | 2637/7317 [00:55<01:51, 42.16it/s]Training CobwebTree:  36%|      | 2642/7317 [00:56<01:55, 40.46it/s]Training CobwebTree:  36%|      | 2647/7317 [00:56<01:51, 41.98it/s]Training CobwebTree:  36%|      | 2652/7317 [00:56<01:48, 43.11it/s]Training CobwebTree:  36%|      | 2657/7317 [00:56<01:53, 40.89it/s]Training CobwebTree:  36%|      | 2662/7317 [00:56<01:54, 40.79it/s]Training CobwebTree:  36%|      | 2667/7317 [00:56<01:53, 41.09it/s]Training CobwebTree:  37%|      | 2672/7317 [00:56<01:51, 41.50it/s]Training CobwebTree:  37%|      | 2677/7317 [00:56<01:49, 42.49it/s]Training CobwebTree:  37%|      | 2682/7317 [00:57<01:46, 43.37it/s]Training CobwebTree:  37%|      | 2687/7317 [00:57<01:49, 42.12it/s]Training CobwebTree:  37%|      | 2692/7317 [00:57<01:46, 43.58it/s]Training CobwebTree:  37%|      | 2697/7317 [00:57<01:47, 43.05it/s]Training CobwebTree:  37%|      | 2702/7317 [00:57<01:53, 40.73it/s]Training CobwebTree:  37%|      | 2707/7317 [00:57<01:54, 40.29it/s]Training CobwebTree:  37%|      | 2712/7317 [00:57<01:57, 39.26it/s]Training CobwebTree:  37%|      | 2716/7317 [00:57<01:57, 39.14it/s]Training CobwebTree:  37%|      | 2720/7317 [00:58<01:59, 38.57it/s]Training CobwebTree:  37%|      | 2724/7317 [00:58<02:02, 37.57it/s]Training CobwebTree:  37%|      | 2729/7317 [00:58<01:55, 39.84it/s]Training CobwebTree:  37%|      | 2735/7317 [00:58<01:47, 42.65it/s]Training CobwebTree:  37%|      | 2740/7317 [00:58<01:45, 43.53it/s]Training CobwebTree:  38%|      | 2745/7317 [00:58<01:44, 43.78it/s]Training CobwebTree:  38%|      | 2750/7317 [00:58<01:50, 41.25it/s]Training CobwebTree:  38%|      | 2755/7317 [00:58<01:52, 40.60it/s]Training CobwebTree:  38%|      | 2760/7317 [00:58<01:49, 41.71it/s]Training CobwebTree:  38%|      | 2765/7317 [00:59<01:49, 41.64it/s]Training CobwebTree:  38%|      | 2770/7317 [00:59<01:53, 39.92it/s]Training CobwebTree:  38%|      | 2775/7317 [00:59<01:56, 39.07it/s]Training CobwebTree:  38%|      | 2779/7317 [00:59<01:57, 38.70it/s]Training CobwebTree:  38%|      | 2783/7317 [00:59<02:00, 37.54it/s]Training CobwebTree:  38%|      | 2788/7317 [00:59<01:55, 39.10it/s]Training CobwebTree:  38%|      | 2793/7317 [00:59<01:50, 41.10it/s]Training CobwebTree:  38%|      | 2798/7317 [00:59<01:48, 41.54it/s]Training CobwebTree:  38%|      | 2803/7317 [01:00<01:48, 41.59it/s]Training CobwebTree:  38%|      | 2808/7317 [01:00<01:48, 41.62it/s]Training CobwebTree:  38%|      | 2813/7317 [01:00<01:47, 42.03it/s]Training CobwebTree:  39%|      | 2818/7317 [01:00<01:51, 40.50it/s]Training CobwebTree:  39%|      | 2823/7317 [01:00<01:51, 40.40it/s]Training CobwebTree:  39%|      | 2828/7317 [01:00<01:50, 40.70it/s]Training CobwebTree:  39%|      | 2833/7317 [01:00<01:52, 39.84it/s]Training CobwebTree:  39%|      | 2838/7317 [01:00<01:48, 41.21it/s]Training CobwebTree:  39%|      | 2843/7317 [01:01<01:52, 39.88it/s]Training CobwebTree:  39%|      | 2848/7317 [01:01<01:54, 39.08it/s]Training CobwebTree:  39%|      | 2852/7317 [01:01<01:56, 38.33it/s]Training CobwebTree:  39%|      | 2857/7317 [01:01<01:53, 39.15it/s]Training CobwebTree:  39%|      | 2861/7317 [01:01<01:53, 39.18it/s]Training CobwebTree:  39%|      | 2866/7317 [01:01<01:50, 40.33it/s]Training CobwebTree:  39%|      | 2871/7317 [01:01<01:54, 38.74it/s]Training CobwebTree:  39%|      | 2876/7317 [01:01<01:49, 40.68it/s]Training CobwebTree:  39%|      | 2881/7317 [01:01<01:46, 41.58it/s]Training CobwebTree:  39%|      | 2886/7317 [01:02<01:47, 41.21it/s]Training CobwebTree:  40%|      | 2891/7317 [01:02<01:42, 43.08it/s]Training CobwebTree:  40%|      | 2896/7317 [01:02<01:46, 41.37it/s]Training CobwebTree:  40%|      | 2901/7317 [01:02<01:49, 40.17it/s]Training CobwebTree:  40%|      | 2906/7317 [01:02<01:48, 40.50it/s]Training CobwebTree:  40%|      | 2911/7317 [01:02<01:43, 42.39it/s]Training CobwebTree:  40%|      | 2916/7317 [01:02<01:40, 43.59it/s]Training CobwebTree:  40%|      | 2921/7317 [01:02<01:45, 41.79it/s]Training CobwebTree:  40%|      | 2926/7317 [01:03<01:43, 42.62it/s]Training CobwebTree:  40%|      | 2931/7317 [01:03<01:41, 43.19it/s]Training CobwebTree:  40%|      | 2936/7317 [01:03<01:44, 41.92it/s]Training CobwebTree:  40%|      | 2941/7317 [01:03<01:43, 42.11it/s]Training CobwebTree:  40%|      | 2946/7317 [01:03<01:44, 41.76it/s]Training CobwebTree:  40%|      | 2951/7317 [01:03<01:52, 38.86it/s]Training CobwebTree:  40%|      | 2955/7317 [01:03<01:58, 36.87it/s]Training CobwebTree:  40%|      | 2960/7317 [01:03<01:54, 37.96it/s]Training CobwebTree:  41%|      | 2964/7317 [01:04<01:55, 37.56it/s]Training CobwebTree:  41%|      | 2969/7317 [01:04<01:48, 40.18it/s]Training CobwebTree:  41%|      | 2974/7317 [01:04<01:46, 40.65it/s]Training CobwebTree:  41%|      | 2979/7317 [01:04<01:42, 42.16it/s]Training CobwebTree:  41%|      | 2984/7317 [01:04<01:50, 39.14it/s]Training CobwebTree:  41%|      | 2989/7317 [01:04<01:46, 40.67it/s]Training CobwebTree:  41%|      | 2994/7317 [01:04<01:50, 39.10it/s]Training CobwebTree:  41%|      | 2998/7317 [01:04<01:52, 38.28it/s]Training CobwebTree:  41%|      | 3002/7317 [01:05<01:54, 37.80it/s]Training CobwebTree:  41%|      | 3007/7317 [01:05<01:50, 39.15it/s]Training CobwebTree:  41%|      | 3011/7317 [01:05<01:50, 39.09it/s]Training CobwebTree:  41%|      | 3016/7317 [01:05<01:47, 40.04it/s]Training CobwebTree:  41%|     | 3021/7317 [01:05<01:44, 41.06it/s]Training CobwebTree:  41%|     | 3026/7317 [01:05<01:45, 40.79it/s]Training CobwebTree:  41%|     | 3031/7317 [01:05<01:44, 41.03it/s]Training CobwebTree:  41%|     | 3036/7317 [01:05<01:42, 41.85it/s]Training CobwebTree:  42%|     | 3041/7317 [01:05<01:42, 41.69it/s]Training CobwebTree:  42%|     | 3046/7317 [01:06<01:45, 40.49it/s]Training CobwebTree:  42%|     | 3051/7317 [01:06<01:48, 39.26it/s]Training CobwebTree:  42%|     | 3055/7317 [01:06<01:49, 38.80it/s]Training CobwebTree:  42%|     | 3060/7317 [01:06<01:47, 39.68it/s]Training CobwebTree:  42%|     | 3064/7317 [01:06<01:47, 39.74it/s]Training CobwebTree:  42%|     | 3069/7317 [01:06<01:47, 39.52it/s]Training CobwebTree:  42%|     | 3073/7317 [01:06<01:50, 38.34it/s]Training CobwebTree:  42%|     | 3078/7317 [01:06<01:48, 39.21it/s]Training CobwebTree:  42%|     | 3083/7317 [01:07<01:44, 40.51it/s]Training CobwebTree:  42%|     | 3088/7317 [01:07<01:42, 41.13it/s]Training CobwebTree:  42%|     | 3093/7317 [01:07<01:43, 41.00it/s]Training CobwebTree:  42%|     | 3098/7317 [01:07<01:44, 40.21it/s]Training CobwebTree:  42%|     | 3103/7317 [01:07<01:43, 40.79it/s]Training CobwebTree:  42%|     | 3108/7317 [01:07<01:44, 40.24it/s]Training CobwebTree:  43%|     | 3113/7317 [01:07<01:43, 40.50it/s]Training CobwebTree:  43%|     | 3118/7317 [01:07<01:42, 40.78it/s]Training CobwebTree:  43%|     | 3123/7317 [01:07<01:40, 41.54it/s]Training CobwebTree:  43%|     | 3128/7317 [01:08<01:40, 41.79it/s]Training CobwebTree:  43%|     | 3133/7317 [01:08<01:44, 39.87it/s]Training CobwebTree:  43%|     | 3138/7317 [01:08<01:40, 41.40it/s]Training CobwebTree:  43%|     | 3143/7317 [01:08<01:43, 40.18it/s]Training CobwebTree:  43%|     | 3148/7317 [01:08<01:41, 40.91it/s]Training CobwebTree:  43%|     | 3153/7317 [01:08<01:40, 41.25it/s]Training CobwebTree:  43%|     | 3158/7317 [01:08<01:43, 40.30it/s]Training CobwebTree:  43%|     | 3163/7317 [01:08<01:42, 40.63it/s]Training CobwebTree:  43%|     | 3168/7317 [01:09<01:42, 40.46it/s]Training CobwebTree:  43%|     | 3173/7317 [01:09<01:40, 41.40it/s]Training CobwebTree:  43%|     | 3178/7317 [01:09<01:42, 40.53it/s]Training CobwebTree:  44%|     | 3183/7317 [01:09<01:44, 39.58it/s]Training CobwebTree:  44%|     | 3188/7317 [01:09<01:41, 40.74it/s]Training CobwebTree:  44%|     | 3193/7317 [01:09<01:40, 41.19it/s]Training CobwebTree:  44%|     | 3198/7317 [01:09<01:39, 41.20it/s]Training CobwebTree:  44%|     | 3203/7317 [01:09<01:42, 39.98it/s]Training CobwebTree:  44%|     | 3208/7317 [01:10<01:44, 39.42it/s]Training CobwebTree:  44%|     | 3212/7317 [01:10<01:44, 39.23it/s]Training CobwebTree:  44%|     | 3216/7317 [01:10<01:47, 38.24it/s]Training CobwebTree:  44%|     | 3221/7317 [01:10<01:44, 39.33it/s]Training CobwebTree:  44%|     | 3225/7317 [01:10<01:44, 39.15it/s]Training CobwebTree:  44%|     | 3229/7317 [01:10<01:46, 38.46it/s]Training CobwebTree:  44%|     | 3233/7317 [01:10<01:46, 38.42it/s]Training CobwebTree:  44%|     | 3238/7317 [01:10<01:41, 40.18it/s]Training CobwebTree:  44%|     | 3243/7317 [01:10<01:45, 38.50it/s]Training CobwebTree:  44%|     | 3248/7317 [01:11<01:45, 38.63it/s]Training CobwebTree:  44%|     | 3252/7317 [01:11<01:48, 37.60it/s]Training CobwebTree:  45%|     | 3257/7317 [01:11<01:46, 38.04it/s]Training CobwebTree:  45%|     | 3261/7317 [01:11<01:45, 38.40it/s]Training CobwebTree:  45%|     | 3265/7317 [01:11<01:44, 38.67it/s]Training CobwebTree:  45%|     | 3269/7317 [01:11<01:45, 38.19it/s]Training CobwebTree:  45%|     | 3273/7317 [01:11<01:45, 38.17it/s]Training CobwebTree:  45%|     | 3277/7317 [01:11<01:46, 38.08it/s]Training CobwebTree:  45%|     | 3282/7317 [01:12<01:41, 39.78it/s]Training CobwebTree:  45%|     | 3287/7317 [01:12<01:40, 39.90it/s]Training CobwebTree:  45%|     | 3291/7317 [01:12<01:42, 39.36it/s]Training CobwebTree:  45%|     | 3296/7317 [01:12<01:40, 40.17it/s]Training CobwebTree:  45%|     | 3301/7317 [01:12<01:36, 41.41it/s]Training CobwebTree:  45%|     | 3306/7317 [01:12<01:35, 41.79it/s]Training CobwebTree:  45%|     | 3311/7317 [01:12<01:38, 40.66it/s]Training CobwebTree:  45%|     | 3316/7317 [01:12<01:41, 39.42it/s]Training CobwebTree:  45%|     | 3320/7317 [01:12<01:45, 37.94it/s]Training CobwebTree:  45%|     | 3325/7317 [01:13<01:39, 40.29it/s]Training CobwebTree:  46%|     | 3330/7317 [01:13<01:38, 40.62it/s]Training CobwebTree:  46%|     | 3335/7317 [01:13<01:41, 39.16it/s]Training CobwebTree:  46%|     | 3340/7317 [01:13<01:35, 41.47it/s]Training CobwebTree:  46%|     | 3345/7317 [01:13<01:39, 39.99it/s]Training CobwebTree:  46%|     | 3350/7317 [01:13<01:38, 40.18it/s]Training CobwebTree:  46%|     | 3355/7317 [01:13<01:40, 39.53it/s]Training CobwebTree:  46%|     | 3360/7317 [01:13<01:38, 40.10it/s]Training CobwebTree:  46%|     | 3365/7317 [01:14<01:36, 40.74it/s]Training CobwebTree:  46%|     | 3370/7317 [01:14<01:33, 42.36it/s]Training CobwebTree:  46%|     | 3375/7317 [01:14<01:35, 41.31it/s]Training CobwebTree:  46%|     | 3380/7317 [01:14<01:30, 43.44it/s]Training CobwebTree:  46%|     | 3385/7317 [01:14<01:36, 40.87it/s]Training CobwebTree:  46%|     | 3390/7317 [01:14<01:35, 41.00it/s]Training CobwebTree:  46%|     | 3395/7317 [01:14<01:35, 41.04it/s]Training CobwebTree:  46%|     | 3400/7317 [01:14<01:42, 38.32it/s]Training CobwebTree:  47%|     | 3405/7317 [01:15<01:40, 39.11it/s]Training CobwebTree:  47%|     | 3410/7317 [01:15<01:38, 39.57it/s]Training CobwebTree:  47%|     | 3414/7317 [01:15<01:39, 39.12it/s]Training CobwebTree:  47%|     | 3419/7317 [01:15<01:36, 40.28it/s]Training CobwebTree:  47%|     | 3424/7317 [01:15<01:39, 39.09it/s]Training CobwebTree:  47%|     | 3428/7317 [01:15<01:42, 37.94it/s]Training CobwebTree:  47%|     | 3432/7317 [01:15<01:42, 37.93it/s]Training CobwebTree:  47%|     | 3437/7317 [01:15<01:37, 39.81it/s]Training CobwebTree:  47%|     | 3441/7317 [01:15<01:39, 38.89it/s]Training CobwebTree:  47%|     | 3446/7317 [01:16<01:40, 38.66it/s]Training CobwebTree:  47%|     | 3450/7317 [01:16<01:43, 37.37it/s]Training CobwebTree:  47%|     | 3455/7317 [01:16<01:40, 38.29it/s]Training CobwebTree:  47%|     | 3459/7317 [01:16<01:44, 36.94it/s]Training CobwebTree:  47%|     | 3464/7317 [01:16<01:38, 39.22it/s]Training CobwebTree:  47%|     | 3469/7317 [01:16<01:36, 39.77it/s]Training CobwebTree:  47%|     | 3473/7317 [01:16<01:37, 39.53it/s]Training CobwebTree:  48%|     | 3477/7317 [01:16<01:38, 38.98it/s]Training CobwebTree:  48%|     | 3482/7317 [01:17<01:35, 40.31it/s]Training CobwebTree:  48%|     | 3487/7317 [01:17<01:36, 39.72it/s]Training CobwebTree:  48%|     | 3491/7317 [01:17<01:37, 39.37it/s]Training CobwebTree:  48%|     | 3495/7317 [01:17<01:36, 39.43it/s]Training CobwebTree:  48%|     | 3499/7317 [01:17<01:38, 38.71it/s]Training CobwebTree:  48%|     | 3503/7317 [01:17<01:39, 38.43it/s]Training CobwebTree:  48%|     | 3508/7317 [01:17<01:36, 39.46it/s]Training CobwebTree:  48%|     | 3512/7317 [01:17<01:36, 39.51it/s]Training CobwebTree:  48%|     | 3516/7317 [01:17<01:37, 39.02it/s]Training CobwebTree:  48%|     | 3521/7317 [01:18<01:33, 40.50it/s]Training CobwebTree:  48%|     | 3526/7317 [01:18<01:36, 39.16it/s]Training CobwebTree:  48%|     | 3531/7317 [01:18<01:32, 40.95it/s]Training CobwebTree:  48%|     | 3536/7317 [01:18<01:36, 39.36it/s]Training CobwebTree:  48%|     | 3541/7317 [01:18<01:32, 40.91it/s]Training CobwebTree:  48%|     | 3546/7317 [01:18<01:37, 38.74it/s]Training CobwebTree:  49%|     | 3550/7317 [01:18<01:37, 38.48it/s]Training CobwebTree:  49%|     | 3554/7317 [01:18<01:36, 38.87it/s]Training CobwebTree:  49%|     | 3559/7317 [01:18<01:34, 39.69it/s]Training CobwebTree:  49%|     | 3564/7317 [01:19<01:30, 41.24it/s]Training CobwebTree:  49%|     | 3569/7317 [01:19<01:32, 40.44it/s]Training CobwebTree:  49%|     | 3574/7317 [01:19<01:32, 40.62it/s]Training CobwebTree:  49%|     | 3579/7317 [01:19<01:32, 40.20it/s]Training CobwebTree:  49%|     | 3584/7317 [01:19<01:35, 39.29it/s]Training CobwebTree:  49%|     | 3588/7317 [01:19<01:37, 38.13it/s]Training CobwebTree:  49%|     | 3593/7317 [01:19<01:32, 40.26it/s]Training CobwebTree:  49%|     | 3598/7317 [01:19<01:35, 38.99it/s]Training CobwebTree:  49%|     | 3602/7317 [01:20<01:36, 38.68it/s]Training CobwebTree:  49%|     | 3606/7317 [01:20<01:38, 37.85it/s]Training CobwebTree:  49%|     | 3611/7317 [01:20<01:34, 39.40it/s]Training CobwebTree:  49%|     | 3616/7317 [01:20<01:32, 39.94it/s]Training CobwebTree:  49%|     | 3621/7317 [01:20<01:35, 38.83it/s]Training CobwebTree:  50%|     | 3625/7317 [01:20<01:37, 37.83it/s]Training CobwebTree:  50%|     | 3629/7317 [01:20<01:36, 38.12it/s]Training CobwebTree:  50%|     | 3633/7317 [01:20<01:35, 38.57it/s]Training CobwebTree:  50%|     | 3637/7317 [01:20<01:36, 38.12it/s]Training CobwebTree:  50%|     | 3642/7317 [01:21<01:33, 39.46it/s]Training CobwebTree:  50%|     | 3647/7317 [01:21<01:30, 40.60it/s]Training CobwebTree:  50%|     | 3652/7317 [01:21<01:29, 40.87it/s]Training CobwebTree:  50%|     | 3657/7317 [01:21<01:28, 41.13it/s]Training CobwebTree:  50%|     | 3662/7317 [01:21<01:30, 40.53it/s]Training CobwebTree:  50%|     | 3667/7317 [01:21<01:28, 41.40it/s]Training CobwebTree:  50%|     | 3672/7317 [01:21<01:29, 40.62it/s]Training CobwebTree:  50%|     | 3677/7317 [01:21<01:28, 40.94it/s]Training CobwebTree:  50%|     | 3682/7317 [01:22<01:25, 42.42it/s]Training CobwebTree:  50%|     | 3687/7317 [01:22<01:28, 40.83it/s]Training CobwebTree:  50%|     | 3692/7317 [01:22<01:29, 40.71it/s]Training CobwebTree:  51%|     | 3697/7317 [01:22<01:31, 39.53it/s]Training CobwebTree:  51%|     | 3701/7317 [01:22<01:34, 38.07it/s]Training CobwebTree:  51%|     | 3706/7317 [01:22<01:32, 38.93it/s]Training CobwebTree:  51%|     | 3711/7317 [01:22<01:30, 39.77it/s]Training CobwebTree:  51%|     | 3715/7317 [01:22<01:33, 38.54it/s]Training CobwebTree:  51%|     | 3720/7317 [01:23<01:28, 40.51it/s]Training CobwebTree:  51%|     | 3725/7317 [01:23<01:31, 39.11it/s]Training CobwebTree:  51%|     | 3730/7317 [01:23<01:31, 39.39it/s]Training CobwebTree:  51%|     | 3735/7317 [01:23<01:28, 40.54it/s]Training CobwebTree:  51%|     | 3740/7317 [01:23<01:26, 41.40it/s]Training CobwebTree:  51%|     | 3745/7317 [01:23<01:24, 42.48it/s]Training CobwebTree:  51%|    | 3750/7317 [01:23<01:24, 42.43it/s]Training CobwebTree:  51%|    | 3755/7317 [01:23<01:26, 41.13it/s]Training CobwebTree:  51%|    | 3760/7317 [01:24<01:32, 38.56it/s]Training CobwebTree:  51%|    | 3765/7317 [01:24<01:28, 39.94it/s]Training CobwebTree:  52%|    | 3770/7317 [01:24<01:26, 40.79it/s]Training CobwebTree:  52%|    | 3775/7317 [01:24<01:31, 38.85it/s]Training CobwebTree:  52%|    | 3779/7317 [01:24<01:31, 38.78it/s]Training CobwebTree:  52%|    | 3783/7317 [01:24<01:31, 38.70it/s]Training CobwebTree:  52%|    | 3787/7317 [01:24<01:33, 37.71it/s]Training CobwebTree:  52%|    | 3792/7317 [01:24<01:26, 40.87it/s]Training CobwebTree:  52%|    | 3797/7317 [01:24<01:28, 39.57it/s]Training CobwebTree:  52%|    | 3802/7317 [01:25<01:28, 39.51it/s]Training CobwebTree:  52%|    | 3807/7317 [01:25<01:28, 39.80it/s]Training CobwebTree:  52%|    | 3812/7317 [01:25<01:29, 39.28it/s]Training CobwebTree:  52%|    | 3816/7317 [01:25<01:29, 39.33it/s]Training CobwebTree:  52%|    | 3820/7317 [01:25<01:28, 39.45it/s]Training CobwebTree:  52%|    | 3824/7317 [01:25<01:31, 38.36it/s]Training CobwebTree:  52%|    | 3828/7317 [01:25<01:30, 38.70it/s]Training CobwebTree:  52%|    | 3833/7317 [01:25<01:30, 38.61it/s]Training CobwebTree:  52%|    | 3838/7317 [01:26<01:28, 39.50it/s]Training CobwebTree:  53%|    | 3842/7317 [01:26<01:27, 39.57it/s]Training CobwebTree:  53%|    | 3846/7317 [01:26<01:33, 37.07it/s]Training CobwebTree:  53%|    | 3851/7317 [01:26<01:27, 39.50it/s]Training CobwebTree:  53%|    | 3856/7317 [01:26<01:27, 39.75it/s]Training CobwebTree:  53%|    | 3860/7317 [01:26<01:29, 38.80it/s]Training CobwebTree:  53%|    | 3865/7317 [01:26<01:24, 40.65it/s]Training CobwebTree:  53%|    | 3870/7317 [01:26<01:28, 39.11it/s]Training CobwebTree:  53%|    | 3874/7317 [01:26<01:29, 38.61it/s]Training CobwebTree:  53%|    | 3878/7317 [01:27<01:31, 37.73it/s]Training CobwebTree:  53%|    | 3882/7317 [01:27<01:32, 36.98it/s]Training CobwebTree:  53%|    | 3887/7317 [01:27<01:29, 38.17it/s]Training CobwebTree:  53%|    | 3891/7317 [01:27<01:30, 37.88it/s]Training CobwebTree:  53%|    | 3896/7317 [01:27<01:24, 40.62it/s]Training CobwebTree:  53%|    | 3901/7317 [01:27<01:25, 39.97it/s]Training CobwebTree:  53%|    | 3906/7317 [01:27<01:27, 39.09it/s]Training CobwebTree:  53%|    | 3910/7317 [01:27<01:27, 38.89it/s]Training CobwebTree:  53%|    | 3914/7317 [01:27<01:28, 38.55it/s]Training CobwebTree:  54%|    | 3918/7317 [01:28<01:28, 38.47it/s]Training CobwebTree:  54%|    | 3922/7317 [01:28<01:28, 38.39it/s]Training CobwebTree:  54%|    | 3926/7317 [01:28<01:29, 38.10it/s]Training CobwebTree:  54%|    | 3932/7317 [01:28<01:21, 41.60it/s]Training CobwebTree:  54%|    | 3937/7317 [01:28<01:24, 40.23it/s]Training CobwebTree:  54%|    | 3942/7317 [01:28<01:25, 39.64it/s]Training CobwebTree:  54%|    | 3946/7317 [01:28<01:26, 39.03it/s]Training CobwebTree:  54%|    | 3950/7317 [01:28<01:30, 37.12it/s]Training CobwebTree:  54%|    | 3954/7317 [01:29<01:32, 36.55it/s]Training CobwebTree:  54%|    | 3958/7317 [01:29<01:30, 36.94it/s]Training CobwebTree:  54%|    | 3962/7317 [01:29<01:28, 37.77it/s]Training CobwebTree:  54%|    | 3966/7317 [01:29<01:30, 37.04it/s]Training CobwebTree:  54%|    | 3970/7317 [01:29<01:30, 37.02it/s]Training CobwebTree:  54%|    | 3974/7317 [01:29<01:30, 36.88it/s]Training CobwebTree:  54%|    | 3978/7317 [01:29<01:33, 35.76it/s]Training CobwebTree:  54%|    | 3982/7317 [01:29<01:30, 36.82it/s]Training CobwebTree:  54%|    | 3987/7317 [01:29<01:30, 36.93it/s]Training CobwebTree:  55%|    | 3992/7317 [01:30<01:27, 37.86it/s]Training CobwebTree:  55%|    | 3996/7317 [01:30<01:27, 38.15it/s]Training CobwebTree:  55%|    | 4001/7317 [01:30<01:24, 39.40it/s]Training CobwebTree:  55%|    | 4005/7317 [01:30<01:26, 38.10it/s]Training CobwebTree:  55%|    | 4009/7317 [01:30<01:26, 38.12it/s]Training CobwebTree:  55%|    | 4013/7317 [01:30<01:30, 36.35it/s]Training CobwebTree:  55%|    | 4017/7317 [01:30<01:28, 37.21it/s]Training CobwebTree:  55%|    | 4021/7317 [01:30<01:28, 37.13it/s]Training CobwebTree:  55%|    | 4026/7317 [01:30<01:26, 37.91it/s]Training CobwebTree:  55%|    | 4031/7317 [01:31<01:23, 39.15it/s]Training CobwebTree:  55%|    | 4035/7317 [01:31<01:25, 38.60it/s]Training CobwebTree:  55%|    | 4039/7317 [01:31<01:24, 38.77it/s]Training CobwebTree:  55%|    | 4044/7317 [01:31<01:22, 39.69it/s]Training CobwebTree:  55%|    | 4049/7317 [01:31<01:19, 41.21it/s]Training CobwebTree:  55%|    | 4054/7317 [01:31<01:19, 40.96it/s]Training CobwebTree:  55%|    | 4059/7317 [01:31<01:19, 41.20it/s]Training CobwebTree:  56%|    | 4064/7317 [01:31<01:21, 39.96it/s]Training CobwebTree:  56%|    | 4069/7317 [01:32<01:22, 39.32it/s]Training CobwebTree:  56%|    | 4073/7317 [01:32<01:26, 37.30it/s]Training CobwebTree:  56%|    | 4077/7317 [01:32<01:31, 35.24it/s]Training CobwebTree:  56%|    | 4081/7317 [01:32<01:32, 34.82it/s]Training CobwebTree:  56%|    | 4085/7317 [01:32<01:34, 34.17it/s]Training CobwebTree:  56%|    | 4090/7317 [01:32<01:31, 35.34it/s]Training CobwebTree:  56%|    | 4094/7317 [01:32<01:31, 35.29it/s]Training CobwebTree:  56%|    | 4098/7317 [01:32<01:28, 36.20it/s]Training CobwebTree:  56%|    | 4103/7317 [01:32<01:26, 37.01it/s]Training CobwebTree:  56%|    | 4108/7317 [01:33<01:24, 38.09it/s]Training CobwebTree:  56%|    | 4113/7317 [01:33<01:21, 39.46it/s]Training CobwebTree:  56%|    | 4117/7317 [01:33<01:24, 37.69it/s]Training CobwebTree:  56%|    | 4121/7317 [01:33<01:23, 38.24it/s]Training CobwebTree:  56%|    | 4126/7317 [01:33<01:20, 39.82it/s]Training CobwebTree:  56%|    | 4130/7317 [01:33<01:20, 39.70it/s]Training CobwebTree:  56%|    | 4134/7317 [01:33<01:24, 37.63it/s]Training CobwebTree:  57%|    | 4138/7317 [01:33<01:23, 37.94it/s]Training CobwebTree:  57%|    | 4143/7317 [01:34<01:23, 38.20it/s]Training CobwebTree:  57%|    | 4148/7317 [01:34<01:19, 39.80it/s]Training CobwebTree:  57%|    | 4153/7317 [01:34<01:16, 41.30it/s]Training CobwebTree:  57%|    | 4158/7317 [01:34<01:18, 40.30it/s]Training CobwebTree:  57%|    | 4163/7317 [01:34<01:20, 39.06it/s]Training CobwebTree:  57%|    | 4167/7317 [01:34<01:20, 39.29it/s]Training CobwebTree:  57%|    | 4171/7317 [01:34<01:23, 37.54it/s]Training CobwebTree:  57%|    | 4175/7317 [01:34<01:24, 37.27it/s]Training CobwebTree:  57%|    | 4180/7317 [01:34<01:17, 40.68it/s]Training CobwebTree:  57%|    | 4185/7317 [01:35<01:25, 36.63it/s]Training CobwebTree:  57%|    | 4189/7317 [01:35<01:26, 36.07it/s]Training CobwebTree:  57%|    | 4193/7317 [01:35<01:26, 36.21it/s]Training CobwebTree:  57%|    | 4198/7317 [01:35<01:21, 38.17it/s]Training CobwebTree:  57%|    | 4203/7317 [01:35<01:18, 39.70it/s]Training CobwebTree:  58%|    | 4208/7317 [01:35<01:18, 39.61it/s]Training CobwebTree:  58%|    | 4213/7317 [01:35<01:17, 40.27it/s]Training CobwebTree:  58%|    | 4218/7317 [01:35<01:17, 40.06it/s]Training CobwebTree:  58%|    | 4223/7317 [01:36<01:15, 40.77it/s]Training CobwebTree:  58%|    | 4228/7317 [01:36<01:16, 40.33it/s]Training CobwebTree:  58%|    | 4233/7317 [01:36<01:17, 39.57it/s]Training CobwebTree:  58%|    | 4237/7317 [01:36<01:18, 39.38it/s]Training CobwebTree:  58%|    | 4242/7317 [01:36<01:16, 40.13it/s]Training CobwebTree:  58%|    | 4247/7317 [01:36<01:14, 41.11it/s]Training CobwebTree:  58%|    | 4252/7317 [01:36<01:14, 41.13it/s]Training CobwebTree:  58%|    | 4257/7317 [01:36<01:17, 39.58it/s]Training CobwebTree:  58%|    | 4261/7317 [01:37<01:18, 38.80it/s]Training CobwebTree:  58%|    | 4266/7317 [01:37<01:19, 38.24it/s]Training CobwebTree:  58%|    | 4270/7317 [01:37<01:22, 36.92it/s]Training CobwebTree:  58%|    | 4274/7317 [01:37<01:26, 35.17it/s]Training CobwebTree:  58%|    | 4278/7317 [01:37<01:25, 35.35it/s]Training CobwebTree:  59%|    | 4283/7317 [01:37<01:20, 37.83it/s]Training CobwebTree:  59%|    | 4287/7317 [01:37<01:19, 38.28it/s]Training CobwebTree:  59%|    | 4291/7317 [01:37<01:20, 37.58it/s]Training CobwebTree:  59%|    | 4295/7317 [01:37<01:21, 37.20it/s]Training CobwebTree:  59%|    | 4299/7317 [01:38<01:23, 36.23it/s]Training CobwebTree:  59%|    | 4304/7317 [01:38<01:21, 37.01it/s]Training CobwebTree:  59%|    | 4309/7317 [01:38<01:16, 39.38it/s]Training CobwebTree:  59%|    | 4313/7317 [01:38<01:17, 38.89it/s]Training CobwebTree:  59%|    | 4317/7317 [01:38<01:18, 38.19it/s]Training CobwebTree:  59%|    | 4321/7317 [01:38<01:18, 37.98it/s]Training CobwebTree:  59%|    | 4325/7317 [01:38<01:19, 37.47it/s]Training CobwebTree:  59%|    | 4329/7317 [01:38<01:18, 38.17it/s]Training CobwebTree:  59%|    | 4334/7317 [01:38<01:16, 39.11it/s]Training CobwebTree:  59%|    | 4339/7317 [01:39<01:14, 39.92it/s]Training CobwebTree:  59%|    | 4343/7317 [01:39<01:15, 39.22it/s]Training CobwebTree:  59%|    | 4347/7317 [01:39<01:16, 38.71it/s]Training CobwebTree:  59%|    | 4351/7317 [01:39<01:17, 38.08it/s]Training CobwebTree:  60%|    | 4356/7317 [01:39<01:19, 37.29it/s]Training CobwebTree:  60%|    | 4361/7317 [01:39<01:16, 38.44it/s]Training CobwebTree:  60%|    | 4365/7317 [01:39<01:17, 37.97it/s]Training CobwebTree:  60%|    | 4370/7317 [01:39<01:15, 38.95it/s]Training CobwebTree:  60%|    | 4375/7317 [01:39<01:13, 39.76it/s]Training CobwebTree:  60%|    | 4379/7317 [01:40<01:13, 39.76it/s]Training CobwebTree:  60%|    | 4384/7317 [01:40<01:12, 40.40it/s]Training CobwebTree:  60%|    | 4389/7317 [01:40<01:14, 39.40it/s]Training CobwebTree:  60%|    | 4393/7317 [01:40<01:16, 38.00it/s]Training CobwebTree:  60%|    | 4397/7317 [01:40<01:16, 38.38it/s]Training CobwebTree:  60%|    | 4401/7317 [01:40<01:18, 37.22it/s]Training CobwebTree:  60%|    | 4405/7317 [01:40<01:18, 37.23it/s]Training CobwebTree:  60%|    | 4409/7317 [01:40<01:17, 37.58it/s]Training CobwebTree:  60%|    | 4413/7317 [01:40<01:16, 38.08it/s]Training CobwebTree:  60%|    | 4417/7317 [01:41<01:19, 36.68it/s]Training CobwebTree:  60%|    | 4421/7317 [01:41<01:17, 37.61it/s]Training CobwebTree:  60%|    | 4426/7317 [01:41<01:14, 38.55it/s]Training CobwebTree:  61%|    | 4431/7317 [01:41<01:11, 40.08it/s]Training CobwebTree:  61%|    | 4436/7317 [01:41<01:12, 39.73it/s]Training CobwebTree:  61%|    | 4440/7317 [01:41<01:13, 39.12it/s]Training CobwebTree:  61%|    | 4444/7317 [01:41<01:13, 39.27it/s]Training CobwebTree:  61%|    | 4449/7317 [01:41<01:12, 39.77it/s]Training CobwebTree:  61%|    | 4453/7317 [01:42<01:12, 39.56it/s]Training CobwebTree:  61%|    | 4458/7317 [01:42<01:12, 39.63it/s]Training CobwebTree:  61%|    | 4462/7317 [01:42<01:16, 37.39it/s]Training CobwebTree:  61%|    | 4466/7317 [01:42<01:15, 37.63it/s]Training CobwebTree:  61%|    | 4471/7317 [01:42<01:13, 38.73it/s]Training CobwebTree:  61%|    | 4475/7317 [01:42<01:13, 38.47it/s]Training CobwebTree:  61%|    | 4480/7317 [01:42<01:09, 40.80it/s]Training CobwebTree:  61%|   | 4485/7317 [01:42<01:07, 41.77it/s]Training CobwebTree:  61%|   | 4490/7317 [01:42<01:06, 42.76it/s]Training CobwebTree:  61%|   | 4495/7317 [01:43<01:11, 39.71it/s]Training CobwebTree:  62%|   | 4500/7317 [01:43<01:11, 39.25it/s]Training CobwebTree:  62%|   | 4505/7317 [01:43<01:09, 40.17it/s]Training CobwebTree:  62%|   | 4510/7317 [01:43<01:09, 40.44it/s]Training CobwebTree:  62%|   | 4515/7317 [01:43<01:08, 40.90it/s]Training CobwebTree:  62%|   | 4520/7317 [01:43<01:09, 40.28it/s]Training CobwebTree:  62%|   | 4525/7317 [01:43<01:09, 40.09it/s]Training CobwebTree:  62%|   | 4530/7317 [01:43<01:12, 38.37it/s]Training CobwebTree:  62%|   | 4535/7317 [01:44<01:12, 38.64it/s]Training CobwebTree:  62%|   | 4539/7317 [01:44<01:12, 38.29it/s]Training CobwebTree:  62%|   | 4543/7317 [01:44<01:13, 37.86it/s]Training CobwebTree:  62%|   | 4547/7317 [01:44<01:13, 37.62it/s]Training CobwebTree:  62%|   | 4551/7317 [01:44<01:12, 38.22it/s]Training CobwebTree:  62%|   | 4556/7317 [01:44<01:08, 40.18it/s]Training CobwebTree:  62%|   | 4561/7317 [01:44<01:06, 41.36it/s]Training CobwebTree:  62%|   | 4566/7317 [01:44<01:06, 41.08it/s]Training CobwebTree:  62%|   | 4571/7317 [01:44<01:07, 40.86it/s]Training CobwebTree:  63%|   | 4576/7317 [01:45<01:06, 41.09it/s]Training CobwebTree:  63%|   | 4581/7317 [01:45<01:07, 40.25it/s]Training CobwebTree:  63%|   | 4586/7317 [01:45<01:06, 41.26it/s]Training CobwebTree:  63%|   | 4591/7317 [01:45<01:08, 39.99it/s]Training CobwebTree:  63%|   | 4596/7317 [01:45<01:09, 39.42it/s]Training CobwebTree:  63%|   | 4600/7317 [01:45<01:10, 38.76it/s]Training CobwebTree:  63%|   | 4604/7317 [01:45<01:12, 37.26it/s]Training CobwebTree:  63%|   | 4608/7317 [01:45<01:12, 37.35it/s]Training CobwebTree:  63%|   | 4612/7317 [01:46<01:11, 37.70it/s]Training CobwebTree:  63%|   | 4616/7317 [01:46<01:10, 38.24it/s]Training CobwebTree:  63%|   | 4620/7317 [01:46<01:11, 37.87it/s]Training CobwebTree:  63%|   | 4624/7317 [01:46<01:13, 36.45it/s]Training CobwebTree:  63%|   | 4628/7317 [01:46<01:12, 37.27it/s]Training CobwebTree:  63%|   | 4632/7317 [01:46<01:13, 36.46it/s]Training CobwebTree:  63%|   | 4637/7317 [01:46<01:10, 38.16it/s]Training CobwebTree:  63%|   | 4641/7317 [01:46<01:09, 38.56it/s]Training CobwebTree:  63%|   | 4646/7317 [01:46<01:07, 39.28it/s]Training CobwebTree:  64%|   | 4651/7317 [01:47<01:07, 39.63it/s]Training CobwebTree:  64%|   | 4655/7317 [01:47<01:11, 37.21it/s]Training CobwebTree:  64%|   | 4660/7317 [01:47<01:09, 38.11it/s]Training CobwebTree:  64%|   | 4665/7317 [01:47<01:08, 38.98it/s]Training CobwebTree:  64%|   | 4669/7317 [01:47<01:12, 36.66it/s]Training CobwebTree:  64%|   | 4674/7317 [01:47<01:10, 37.71it/s]Training CobwebTree:  64%|   | 4679/7317 [01:47<01:06, 39.47it/s]Training CobwebTree:  64%|   | 4683/7317 [01:47<01:07, 39.13it/s]Training CobwebTree:  64%|   | 4687/7317 [01:48<01:07, 38.97it/s]Training CobwebTree:  64%|   | 4691/7317 [01:48<01:10, 37.00it/s]Training CobwebTree:  64%|   | 4695/7317 [01:48<01:09, 37.60it/s]Training CobwebTree:  64%|   | 4699/7317 [01:48<01:11, 36.70it/s]Training CobwebTree:  64%|   | 4703/7317 [01:48<01:10, 37.22it/s]Training CobwebTree:  64%|   | 4708/7317 [01:48<01:06, 39.14it/s]Training CobwebTree:  64%|   | 4712/7317 [01:48<01:06, 39.21it/s]Training CobwebTree:  64%|   | 4716/7317 [01:48<01:08, 38.11it/s]Training CobwebTree:  65%|   | 4720/7317 [01:48<01:10, 36.61it/s]Training CobwebTree:  65%|   | 4724/7317 [01:49<01:12, 35.89it/s]Training CobwebTree:  65%|   | 4728/7317 [01:49<01:10, 36.83it/s]Training CobwebTree:  65%|   | 4732/7317 [01:49<01:09, 37.30it/s]Training CobwebTree:  65%|   | 4736/7317 [01:49<01:07, 38.05it/s]Training CobwebTree:  65%|   | 4741/7317 [01:49<01:05, 39.37it/s]Training CobwebTree:  65%|   | 4746/7317 [01:49<01:03, 40.38it/s]Training CobwebTree:  65%|   | 4751/7317 [01:49<01:05, 39.35it/s]Training CobwebTree:  65%|   | 4755/7317 [01:49<01:05, 38.84it/s]Training CobwebTree:  65%|   | 4760/7317 [01:49<01:05, 38.94it/s]Training CobwebTree:  65%|   | 4764/7317 [01:50<01:08, 37.32it/s]Training CobwebTree:  65%|   | 4768/7317 [01:50<01:11, 35.59it/s]Training CobwebTree:  65%|   | 4772/7317 [01:50<01:09, 36.53it/s]Training CobwebTree:  65%|   | 4776/7317 [01:50<01:10, 36.03it/s]Training CobwebTree:  65%|   | 4780/7317 [01:50<01:10, 36.14it/s]Training CobwebTree:  65%|   | 4785/7317 [01:50<01:08, 37.01it/s]Training CobwebTree:  65%|   | 4789/7317 [01:50<01:08, 36.84it/s]Training CobwebTree:  66%|   | 4794/7317 [01:50<01:05, 38.78it/s]Training CobwebTree:  66%|   | 4798/7317 [01:50<01:04, 39.07it/s]Training CobwebTree:  66%|   | 4803/7317 [01:51<01:02, 40.40it/s]Training CobwebTree:  66%|   | 4808/7317 [01:51<01:04, 39.14it/s]Training CobwebTree:  66%|   | 4812/7317 [01:51<01:05, 38.13it/s]Training CobwebTree:  66%|   | 4817/7317 [01:51<01:04, 38.77it/s]Training CobwebTree:  66%|   | 4821/7317 [01:51<01:08, 36.69it/s]Training CobwebTree:  66%|   | 4825/7317 [01:51<01:09, 35.79it/s]Training CobwebTree:  66%|   | 4829/7317 [01:51<01:09, 35.77it/s]Training CobwebTree:  66%|   | 4833/7317 [01:51<01:08, 36.18it/s]Training CobwebTree:  66%|   | 4838/7317 [01:52<01:05, 37.94it/s]Training CobwebTree:  66%|   | 4843/7317 [01:52<01:01, 40.43it/s]Training CobwebTree:  66%|   | 4848/7317 [01:52<01:03, 38.71it/s]Training CobwebTree:  66%|   | 4852/7317 [01:52<01:03, 38.54it/s]Training CobwebTree:  66%|   | 4856/7317 [01:52<01:03, 38.67it/s]Training CobwebTree:  66%|   | 4860/7317 [01:52<01:03, 38.97it/s]Training CobwebTree:  66%|   | 4864/7317 [01:52<01:05, 37.51it/s]Training CobwebTree:  67%|   | 4868/7317 [01:52<01:07, 36.08it/s]Training CobwebTree:  67%|   | 4873/7317 [01:52<01:05, 37.51it/s]Training CobwebTree:  67%|   | 4878/7317 [01:53<01:02, 38.86it/s]Training CobwebTree:  67%|   | 4882/7317 [01:53<01:03, 38.30it/s]Training CobwebTree:  67%|   | 4886/7317 [01:53<01:03, 38.27it/s]Training CobwebTree:  67%|   | 4891/7317 [01:53<01:00, 40.04it/s]Training CobwebTree:  67%|   | 4896/7317 [01:53<00:58, 41.17it/s]Training CobwebTree:  67%|   | 4901/7317 [01:53<00:57, 42.17it/s]Training CobwebTree:  67%|   | 4906/7317 [01:53<01:03, 37.93it/s]Training CobwebTree:  67%|   | 4911/7317 [01:53<01:02, 38.45it/s]Training CobwebTree:  67%|   | 4916/7317 [01:54<01:00, 39.53it/s]Training CobwebTree:  67%|   | 4921/7317 [01:54<01:03, 37.77it/s]Training CobwebTree:  67%|   | 4926/7317 [01:54<01:01, 39.02it/s]Training CobwebTree:  67%|   | 4930/7317 [01:54<01:01, 38.82it/s]Training CobwebTree:  67%|   | 4934/7317 [01:54<01:01, 39.05it/s]Training CobwebTree:  67%|   | 4938/7317 [01:54<01:02, 38.35it/s]Training CobwebTree:  68%|   | 4942/7317 [01:54<01:03, 37.49it/s]Training CobwebTree:  68%|   | 4947/7317 [01:54<01:01, 38.71it/s]Training CobwebTree:  68%|   | 4951/7317 [01:54<01:02, 37.81it/s]Training CobwebTree:  68%|   | 4956/7317 [01:55<01:00, 38.74it/s]Training CobwebTree:  68%|   | 4960/7317 [01:55<01:01, 38.18it/s]Training CobwebTree:  68%|   | 4964/7317 [01:55<01:00, 38.60it/s]Training CobwebTree:  68%|   | 4969/7317 [01:55<01:00, 38.91it/s]Training CobwebTree:  68%|   | 4973/7317 [01:55<01:01, 38.41it/s]Training CobwebTree:  68%|   | 4978/7317 [01:55<00:58, 39.79it/s]Training CobwebTree:  68%|   | 4983/7317 [01:55<00:57, 40.58it/s]Training CobwebTree:  68%|   | 4988/7317 [01:55<00:59, 38.99it/s]Training CobwebTree:  68%|   | 4993/7317 [01:56<01:00, 38.67it/s]Training CobwebTree:  68%|   | 4998/7317 [01:56<00:57, 40.16it/s]Training CobwebTree:  68%|   | 5003/7317 [01:56<00:57, 39.93it/s]Training CobwebTree:  68%|   | 5008/7317 [01:56<01:01, 37.74it/s]Training CobwebTree:  69%|   | 5013/7317 [01:56<00:58, 39.07it/s]Training CobwebTree:  69%|   | 5018/7317 [01:56<00:58, 39.33it/s]Training CobwebTree:  69%|   | 5022/7317 [01:56<00:58, 39.08it/s]Training CobwebTree:  69%|   | 5026/7317 [01:56<00:58, 39.01it/s]Training CobwebTree:  69%|   | 5030/7317 [01:56<00:58, 39.12it/s]Training CobwebTree:  69%|   | 5034/7317 [01:57<01:00, 37.75it/s]Training CobwebTree:  69%|   | 5039/7317 [01:57<00:56, 40.32it/s]Training CobwebTree:  69%|   | 5044/7317 [01:57<00:56, 40.46it/s]Training CobwebTree:  69%|   | 5049/7317 [01:57<00:59, 37.87it/s]Training CobwebTree:  69%|   | 5054/7317 [01:57<00:56, 39.83it/s]Training CobwebTree:  69%|   | 5059/7317 [01:57<00:58, 38.54it/s]Training CobwebTree:  69%|   | 5063/7317 [01:57<01:01, 36.92it/s]Training CobwebTree:  69%|   | 5067/7317 [01:57<00:59, 37.63it/s]Training CobwebTree:  69%|   | 5071/7317 [01:58<01:04, 35.04it/s]Training CobwebTree:  69%|   | 5075/7317 [01:58<01:02, 36.05it/s]Training CobwebTree:  69%|   | 5079/7317 [01:58<01:00, 36.75it/s]Training CobwebTree:  69%|   | 5083/7317 [01:58<01:00, 37.17it/s]Training CobwebTree:  70%|   | 5087/7317 [01:58<01:00, 36.90it/s]Training CobwebTree:  70%|   | 5091/7317 [01:58<01:00, 36.94it/s]Training CobwebTree:  70%|   | 5096/7317 [01:58<00:56, 39.46it/s]Training CobwebTree:  70%|   | 5100/7317 [01:58<00:57, 38.76it/s]Training CobwebTree:  70%|   | 5104/7317 [01:58<00:58, 37.62it/s]Training CobwebTree:  70%|   | 5109/7317 [01:59<00:55, 39.70it/s]Training CobwebTree:  70%|   | 5114/7317 [01:59<00:54, 40.37it/s]Training CobwebTree:  70%|   | 5119/7317 [01:59<01:01, 36.02it/s]Training CobwebTree:  70%|   | 5125/7317 [01:59<00:53, 40.62it/s]Training CobwebTree:  70%|   | 5130/7317 [01:59<00:54, 40.47it/s]Training CobwebTree:  70%|   | 5135/7317 [01:59<00:55, 39.36it/s]Training CobwebTree:  70%|   | 5140/7317 [01:59<00:56, 38.68it/s]Training CobwebTree:  70%|   | 5144/7317 [01:59<00:59, 36.48it/s]Training CobwebTree:  70%|   | 5148/7317 [02:00<00:58, 37.17it/s]Training CobwebTree:  70%|   | 5153/7317 [02:00<00:58, 37.27it/s]Training CobwebTree:  70%|   | 5157/7317 [02:00<00:57, 37.71it/s]Training CobwebTree:  71%|   | 5161/7317 [02:00<00:56, 38.21it/s]Training CobwebTree:  71%|   | 5166/7317 [02:00<00:54, 39.43it/s]Training CobwebTree:  71%|   | 5170/7317 [02:00<00:55, 38.36it/s]Training CobwebTree:  71%|   | 5175/7317 [02:00<00:54, 39.25it/s]Training CobwebTree:  71%|   | 5179/7317 [02:00<00:55, 38.56it/s]Training CobwebTree:  71%|   | 5183/7317 [02:00<00:56, 37.74it/s]Training CobwebTree:  71%|   | 5188/7317 [02:01<00:55, 38.58it/s]Training CobwebTree:  71%|   | 5192/7317 [02:01<00:55, 38.46it/s]Training CobwebTree:  71%|   | 5196/7317 [02:01<00:56, 37.63it/s]Training CobwebTree:  71%|   | 5200/7317 [02:01<00:56, 37.70it/s]Training CobwebTree:  71%|   | 5204/7317 [02:01<00:56, 37.46it/s]Training CobwebTree:  71%|   | 5208/7317 [02:01<00:55, 37.73it/s]Training CobwebTree:  71%|   | 5213/7317 [02:01<00:53, 39.37it/s]Training CobwebTree:  71%|  | 5218/7317 [02:01<00:52, 39.93it/s]Training CobwebTree:  71%|  | 5222/7317 [02:01<00:56, 37.41it/s]Training CobwebTree:  71%|  | 5227/7317 [02:02<00:54, 38.48it/s]Training CobwebTree:  72%|  | 5232/7317 [02:02<00:52, 39.99it/s]Training CobwebTree:  72%|  | 5237/7317 [02:02<00:51, 40.51it/s]Training CobwebTree:  72%|  | 5242/7317 [02:02<00:52, 39.56it/s]Training CobwebTree:  72%|  | 5247/7317 [02:02<00:50, 41.11it/s]Training CobwebTree:  72%|  | 5252/7317 [02:02<00:50, 40.67it/s]Training CobwebTree:  72%|  | 5257/7317 [02:02<00:52, 39.27it/s]Training CobwebTree:  72%|  | 5262/7317 [02:02<00:51, 40.19it/s]Training CobwebTree:  72%|  | 5267/7317 [02:03<00:53, 38.25it/s]Training CobwebTree:  72%|  | 5271/7317 [02:03<00:56, 36.15it/s]Training CobwebTree:  72%|  | 5276/7317 [02:03<00:55, 36.88it/s]Training CobwebTree:  72%|  | 5280/7317 [02:03<00:54, 37.54it/s]Training CobwebTree:  72%|  | 5284/7317 [02:03<00:53, 38.12it/s]Training CobwebTree:  72%|  | 5288/7317 [02:03<00:53, 38.09it/s]Training CobwebTree:  72%|  | 5292/7317 [02:03<00:54, 37.36it/s]Training CobwebTree:  72%|  | 5296/7317 [02:03<00:54, 37.39it/s]Training CobwebTree:  72%|  | 5300/7317 [02:04<00:54, 37.34it/s]Training CobwebTree:  72%|  | 5304/7317 [02:04<00:54, 36.72it/s]Training CobwebTree:  73%|  | 5308/7317 [02:04<00:54, 36.97it/s]Training CobwebTree:  73%|  | 5313/7317 [02:04<00:52, 38.18it/s]Training CobwebTree:  73%|  | 5317/7317 [02:04<00:53, 37.35it/s]Training CobwebTree:  73%|  | 5321/7317 [02:04<00:53, 37.40it/s]Training CobwebTree:  73%|  | 5325/7317 [02:04<00:54, 36.57it/s]Training CobwebTree:  73%|  | 5329/7317 [02:04<00:53, 36.84it/s]Training CobwebTree:  73%|  | 5333/7317 [02:04<00:54, 36.11it/s]Training CobwebTree:  73%|  | 5337/7317 [02:05<00:54, 36.02it/s]Training CobwebTree:  73%|  | 5341/7317 [02:05<00:53, 36.71it/s]Training CobwebTree:  73%|  | 5345/7317 [02:05<00:56, 35.18it/s]Training CobwebTree:  73%|  | 5349/7317 [02:05<00:54, 36.18it/s]Training CobwebTree:  73%|  | 5353/7317 [02:05<00:54, 36.23it/s]Training CobwebTree:  73%|  | 5357/7317 [02:05<00:56, 34.87it/s]Training CobwebTree:  73%|  | 5361/7317 [02:05<00:54, 36.15it/s]Training CobwebTree:  73%|  | 5365/7317 [02:05<00:53, 36.16it/s]Training CobwebTree:  73%|  | 5369/7317 [02:05<00:54, 35.54it/s]Training CobwebTree:  73%|  | 5373/7317 [02:06<00:54, 35.49it/s]Training CobwebTree:  73%|  | 5377/7317 [02:06<00:53, 36.09it/s]Training CobwebTree:  74%|  | 5381/7317 [02:06<00:54, 35.61it/s]Training CobwebTree:  74%|  | 5385/7317 [02:06<00:52, 36.81it/s]Training CobwebTree:  74%|  | 5389/7317 [02:06<00:51, 37.68it/s]Training CobwebTree:  74%|  | 5393/7317 [02:06<00:52, 36.67it/s]Training CobwebTree:  74%|  | 5397/7317 [02:06<00:51, 37.39it/s]Training CobwebTree:  74%|  | 5401/7317 [02:06<00:50, 38.06it/s]Training CobwebTree:  74%|  | 5405/7317 [02:06<00:51, 37.45it/s]Training CobwebTree:  74%|  | 5409/7317 [02:06<00:50, 38.03it/s]Training CobwebTree:  74%|  | 5414/7317 [02:07<00:47, 39.78it/s]Training CobwebTree:  74%|  | 5418/7317 [02:07<00:48, 38.99it/s]Training CobwebTree:  74%|  | 5423/7317 [02:07<00:48, 39.32it/s]Training CobwebTree:  74%|  | 5427/7317 [02:07<00:48, 39.04it/s]Training CobwebTree:  74%|  | 5432/7317 [02:07<00:47, 39.30it/s]Training CobwebTree:  74%|  | 5436/7317 [02:07<00:51, 36.72it/s]Training CobwebTree:  74%|  | 5440/7317 [02:07<00:50, 36.86it/s]Training CobwebTree:  74%|  | 5444/7317 [02:07<00:50, 37.01it/s]Training CobwebTree:  74%|  | 5448/7317 [02:08<00:51, 36.02it/s]Training CobwebTree:  75%|  | 5453/7317 [02:08<00:48, 38.78it/s]Training CobwebTree:  75%|  | 5457/7317 [02:08<00:50, 37.20it/s]Training CobwebTree:  75%|  | 5461/7317 [02:08<00:52, 35.08it/s]Training CobwebTree:  75%|  | 5466/7317 [02:08<00:49, 37.09it/s]Training CobwebTree:  75%|  | 5470/7317 [02:08<00:49, 37.14it/s]Training CobwebTree:  75%|  | 5474/7317 [02:08<00:51, 36.03it/s]Training CobwebTree:  75%|  | 5478/7317 [02:08<00:51, 35.72it/s]Training CobwebTree:  75%|  | 5482/7317 [02:08<00:50, 36.29it/s]Training CobwebTree:  75%|  | 5487/7317 [02:09<00:48, 37.88it/s]Training CobwebTree:  75%|  | 5491/7317 [02:09<00:48, 37.40it/s]Training CobwebTree:  75%|  | 5495/7317 [02:09<00:48, 37.54it/s]Training CobwebTree:  75%|  | 5499/7317 [02:09<00:49, 36.43it/s]Training CobwebTree:  75%|  | 5504/7317 [02:09<00:48, 37.58it/s]Training CobwebTree:  75%|  | 5508/7317 [02:09<00:47, 38.02it/s]Training CobwebTree:  75%|  | 5513/7317 [02:09<00:46, 38.61it/s]Training CobwebTree:  75%|  | 5518/7317 [02:09<00:45, 39.15it/s]Training CobwebTree:  75%|  | 5523/7317 [02:10<00:44, 39.97it/s]Training CobwebTree:  76%|  | 5527/7317 [02:10<00:48, 36.63it/s]Training CobwebTree:  76%|  | 5532/7317 [02:10<00:48, 37.15it/s]Training CobwebTree:  76%|  | 5536/7317 [02:10<00:48, 36.47it/s]Training CobwebTree:  76%|  | 5540/7317 [02:10<00:47, 37.38it/s]Training CobwebTree:  76%|  | 5544/7317 [02:10<00:48, 36.68it/s]Training CobwebTree:  76%|  | 5549/7317 [02:10<00:46, 37.91it/s]Training CobwebTree:  76%|  | 5553/7317 [02:10<00:46, 37.88it/s]Training CobwebTree:  76%|  | 5557/7317 [02:10<00:50, 35.18it/s]Training CobwebTree:  76%|  | 5562/7317 [02:11<00:48, 36.28it/s]Training CobwebTree:  76%|  | 5566/7317 [02:11<00:47, 37.18it/s]Training CobwebTree:  76%|  | 5570/7317 [02:11<00:46, 37.46it/s]Training CobwebTree:  76%|  | 5574/7317 [02:11<00:46, 37.54it/s]Training CobwebTree:  76%|  | 5578/7317 [02:11<00:46, 37.30it/s]Training CobwebTree:  76%|  | 5582/7317 [02:11<00:45, 37.73it/s]Training CobwebTree:  76%|  | 5586/7317 [02:11<00:46, 37.54it/s]Training CobwebTree:  76%|  | 5590/7317 [02:11<00:46, 36.92it/s]Training CobwebTree:  76%|  | 5594/7317 [02:11<00:46, 37.04it/s]Training CobwebTree:  77%|  | 5598/7317 [02:12<00:45, 37.74it/s]Training CobwebTree:  77%|  | 5602/7317 [02:12<00:49, 34.92it/s]Training CobwebTree:  77%|  | 5606/7317 [02:12<00:47, 35.76it/s]Training CobwebTree:  77%|  | 5611/7317 [02:12<00:46, 36.57it/s]Training CobwebTree:  77%|  | 5615/7317 [02:12<00:45, 37.26it/s]Training CobwebTree:  77%|  | 5619/7317 [02:12<00:46, 36.78it/s]Training CobwebTree:  77%|  | 5624/7317 [02:12<00:44, 38.05it/s]Training CobwebTree:  77%|  | 5629/7317 [02:12<00:41, 40.20it/s]Training CobwebTree:  77%|  | 5634/7317 [02:12<00:43, 39.02it/s]Training CobwebTree:  77%|  | 5639/7317 [02:13<00:42, 39.36it/s]Training CobwebTree:  77%|  | 5644/7317 [02:13<00:40, 40.97it/s]Training CobwebTree:  77%|  | 5649/7317 [02:13<00:40, 41.19it/s]Training CobwebTree:  77%|  | 5654/7317 [02:13<00:41, 39.79it/s]Training CobwebTree:  77%|  | 5659/7317 [02:13<00:43, 37.76it/s]Training CobwebTree:  77%|  | 5663/7317 [02:13<00:44, 37.25it/s]Training CobwebTree:  77%|  | 5668/7317 [02:13<00:43, 38.12it/s]Training CobwebTree:  78%|  | 5673/7317 [02:14<00:43, 37.80it/s]Training CobwebTree:  78%|  | 5677/7317 [02:14<00:45, 36.35it/s]Training CobwebTree:  78%|  | 5681/7317 [02:14<00:45, 36.27it/s]Training CobwebTree:  78%|  | 5685/7317 [02:14<00:45, 35.95it/s]Training CobwebTree:  78%|  | 5689/7317 [02:14<00:44, 36.30it/s]Training CobwebTree:  78%|  | 5694/7317 [02:14<00:43, 37.27it/s]Training CobwebTree:  78%|  | 5698/7317 [02:14<00:44, 36.48it/s]Training CobwebTree:  78%|  | 5702/7317 [02:14<00:43, 37.12it/s]Training CobwebTree:  78%|  | 5706/7317 [02:14<00:43, 36.95it/s]Training CobwebTree:  78%|  | 5710/7317 [02:15<00:44, 35.87it/s]Training CobwebTree:  78%|  | 5714/7317 [02:15<00:44, 36.00it/s]Training CobwebTree:  78%|  | 5718/7317 [02:15<00:44, 36.30it/s]Training CobwebTree:  78%|  | 5722/7317 [02:15<00:44, 35.65it/s]Training CobwebTree:  78%|  | 5727/7317 [02:15<00:42, 37.04it/s]Training CobwebTree:  78%|  | 5732/7317 [02:15<00:42, 37.66it/s]Training CobwebTree:  78%|  | 5737/7317 [02:15<00:41, 38.53it/s]Training CobwebTree:  78%|  | 5741/7317 [02:15<00:43, 36.29it/s]Training CobwebTree:  79%|  | 5745/7317 [02:15<00:44, 35.73it/s]Training CobwebTree:  79%|  | 5750/7317 [02:16<00:41, 37.69it/s]Training CobwebTree:  79%|  | 5754/7317 [02:16<00:41, 37.29it/s]Training CobwebTree:  79%|  | 5758/7317 [02:16<00:43, 35.57it/s]Training CobwebTree:  79%|  | 5763/7317 [02:16<00:42, 36.43it/s]Training CobwebTree:  79%|  | 5768/7317 [02:16<00:39, 39.02it/s]Training CobwebTree:  79%|  | 5772/7317 [02:16<00:40, 38.33it/s]Training CobwebTree:  79%|  | 5777/7317 [02:16<00:39, 39.09it/s]Training CobwebTree:  79%|  | 5781/7317 [02:16<00:40, 37.70it/s]Training CobwebTree:  79%|  | 5785/7317 [02:17<00:41, 36.89it/s]Training CobwebTree:  79%|  | 5789/7317 [02:17<00:42, 35.59it/s]Training CobwebTree:  79%|  | 5793/7317 [02:17<00:42, 35.89it/s]Training CobwebTree:  79%|  | 5797/7317 [02:17<00:44, 34.29it/s]Training CobwebTree:  79%|  | 5801/7317 [02:17<00:43, 34.95it/s]Training CobwebTree:  79%|  | 5806/7317 [02:17<00:40, 37.12it/s]Training CobwebTree:  79%|  | 5810/7317 [02:17<00:39, 37.74it/s]Training CobwebTree:  79%|  | 5814/7317 [02:17<00:40, 37.25it/s]Training CobwebTree:  80%|  | 5818/7317 [02:17<00:41, 36.22it/s]Training CobwebTree:  80%|  | 5822/7317 [02:18<00:42, 34.90it/s]Training CobwebTree:  80%|  | 5826/7317 [02:18<00:41, 35.75it/s]Training CobwebTree:  80%|  | 5831/7317 [02:18<00:39, 37.59it/s]Training CobwebTree:  80%|  | 5835/7317 [02:18<00:40, 36.63it/s]Training CobwebTree:  80%|  | 5839/7317 [02:18<00:41, 35.84it/s]Training CobwebTree:  80%|  | 5843/7317 [02:18<00:41, 35.75it/s]Training CobwebTree:  80%|  | 5847/7317 [02:18<00:40, 36.39it/s]Training CobwebTree:  80%|  | 5852/7317 [02:18<00:38, 37.84it/s]Training CobwebTree:  80%|  | 5856/7317 [02:19<00:41, 35.51it/s]Training CobwebTree:  80%|  | 5860/7317 [02:19<00:41, 35.34it/s]Training CobwebTree:  80%|  | 5864/7317 [02:19<00:39, 36.34it/s]Training CobwebTree:  80%|  | 5869/7317 [02:19<00:38, 37.79it/s]Training CobwebTree:  80%|  | 5874/7317 [02:19<00:36, 39.07it/s]Training CobwebTree:  80%|  | 5878/7317 [02:19<00:37, 38.09it/s]Training CobwebTree:  80%|  | 5882/7317 [02:19<00:37, 38.09it/s]Training CobwebTree:  80%|  | 5886/7317 [02:19<00:39, 36.07it/s]Training CobwebTree:  80%|  | 5890/7317 [02:19<00:39, 36.23it/s]Training CobwebTree:  81%|  | 5894/7317 [02:20<00:39, 36.26it/s]Training CobwebTree:  81%|  | 5898/7317 [02:20<00:39, 36.29it/s]Training CobwebTree:  81%|  | 5903/7317 [02:20<00:38, 36.97it/s]Training CobwebTree:  81%|  | 5907/7317 [02:20<00:38, 37.03it/s]Training CobwebTree:  81%|  | 5911/7317 [02:20<00:37, 37.21it/s]Training CobwebTree:  81%|  | 5915/7317 [02:20<00:37, 37.04it/s]Training CobwebTree:  81%|  | 5919/7317 [02:20<00:38, 36.57it/s]Training CobwebTree:  81%|  | 5924/7317 [02:20<00:36, 38.19it/s]Training CobwebTree:  81%|  | 5928/7317 [02:20<00:36, 38.03it/s]Training CobwebTree:  81%|  | 5932/7317 [02:21<00:37, 37.29it/s]Training CobwebTree:  81%|  | 5936/7317 [02:21<00:36, 37.35it/s]Training CobwebTree:  81%|  | 5941/7317 [02:21<00:34, 39.50it/s]Training CobwebTree:  81%|  | 5945/7317 [02:21<00:35, 39.09it/s]Training CobwebTree:  81%| | 5949/7317 [02:21<00:35, 38.46it/s]Training CobwebTree:  81%| | 5953/7317 [02:21<00:35, 38.86it/s]Training CobwebTree:  81%| | 5958/7317 [02:21<00:34, 39.85it/s]Training CobwebTree:  81%| | 5962/7317 [02:21<00:34, 39.38it/s]Training CobwebTree:  82%| | 5966/7317 [02:21<00:35, 38.28it/s]Training CobwebTree:  82%| | 5970/7317 [02:22<00:35, 37.97it/s]Training CobwebTree:  82%| | 5975/7317 [02:22<00:34, 38.38it/s]Training CobwebTree:  82%| | 5979/7317 [02:22<00:34, 38.80it/s]Training CobwebTree:  82%| | 5983/7317 [02:22<00:36, 36.97it/s]Training CobwebTree:  82%| | 5987/7317 [02:22<00:36, 36.06it/s]Training CobwebTree:  82%| | 5991/7317 [02:22<00:35, 36.89it/s]Training CobwebTree:  82%| | 5995/7317 [02:22<00:36, 36.52it/s]Training CobwebTree:  82%| | 5999/7317 [02:22<00:36, 36.08it/s]Training CobwebTree:  82%| | 6003/7317 [02:22<00:36, 35.65it/s]Training CobwebTree:  82%| | 6007/7317 [02:23<00:36, 36.26it/s]Training CobwebTree:  82%| | 6012/7317 [02:23<00:35, 36.66it/s]Training CobwebTree:  82%| | 6016/7317 [02:23<00:35, 36.29it/s]Training CobwebTree:  82%| | 6021/7317 [02:23<00:33, 38.23it/s]Training CobwebTree:  82%| | 6025/7317 [02:23<00:34, 37.48it/s]Training CobwebTree:  82%| | 6029/7317 [02:23<00:34, 36.88it/s]Training CobwebTree:  82%| | 6033/7317 [02:23<00:34, 37.32it/s]Training CobwebTree:  83%| | 6037/7317 [02:23<00:35, 36.07it/s]Training CobwebTree:  83%| | 6042/7317 [02:23<00:33, 37.88it/s]Training CobwebTree:  83%| | 6046/7317 [02:24<00:34, 36.68it/s]Training CobwebTree:  83%| | 6050/7317 [02:24<00:35, 35.31it/s]Training CobwebTree:  83%| | 6054/7317 [02:24<00:34, 36.11it/s]Training CobwebTree:  83%| | 6059/7317 [02:24<00:33, 37.80it/s]Training CobwebTree:  83%| | 6063/7317 [02:24<00:33, 37.61it/s]Training CobwebTree:  83%| | 6067/7317 [02:24<00:33, 37.69it/s]Training CobwebTree:  83%| | 6071/7317 [02:24<00:34, 36.44it/s]Training CobwebTree:  83%| | 6075/7317 [02:24<00:34, 35.86it/s]Training CobwebTree:  83%| | 6079/7317 [02:25<00:35, 35.32it/s]Training CobwebTree:  83%| | 6083/7317 [02:25<00:35, 34.43it/s]Training CobwebTree:  83%| | 6087/7317 [02:25<00:35, 35.13it/s]Training CobwebTree:  83%| | 6092/7317 [02:25<00:32, 38.16it/s]Training CobwebTree:  83%| | 6096/7317 [02:25<00:32, 37.23it/s]Training CobwebTree:  83%| | 6100/7317 [02:25<00:33, 36.46it/s]Training CobwebTree:  83%| | 6104/7317 [02:25<00:34, 35.11it/s]Training CobwebTree:  83%| | 6108/7317 [02:25<00:34, 35.35it/s]Training CobwebTree:  84%| | 6112/7317 [02:25<00:33, 35.66it/s]Training CobwebTree:  84%| | 6116/7317 [02:26<00:34, 35.09it/s]Training CobwebTree:  84%| | 6120/7317 [02:26<00:33, 35.26it/s]Training CobwebTree:  84%| | 6125/7317 [02:26<00:31, 37.34it/s]Training CobwebTree:  84%| | 6130/7317 [02:26<00:30, 39.10it/s]Training CobwebTree:  84%| | 6134/7317 [02:26<00:30, 38.42it/s]Training CobwebTree:  84%| | 6138/7317 [02:26<00:31, 37.63it/s]Training CobwebTree:  84%| | 6142/7317 [02:26<00:32, 36.54it/s]Training CobwebTree:  84%| | 6146/7317 [02:26<00:32, 36.42it/s]Training CobwebTree:  84%| | 6150/7317 [02:26<00:31, 37.33it/s]Training CobwebTree:  84%| | 6154/7317 [02:27<00:30, 37.59it/s]Training CobwebTree:  84%| | 6158/7317 [02:27<00:31, 36.78it/s]Training CobwebTree:  84%| | 6162/7317 [02:27<00:33, 34.74it/s]Training CobwebTree:  84%| | 6166/7317 [02:27<00:31, 36.13it/s]Training CobwebTree:  84%| | 6170/7317 [02:27<00:33, 34.73it/s]Training CobwebTree:  84%| | 6174/7317 [02:27<00:32, 34.71it/s]Training CobwebTree:  84%| | 6179/7317 [02:27<00:31, 35.92it/s]Training CobwebTree:  85%| | 6183/7317 [02:27<00:31, 35.59it/s]Training CobwebTree:  85%| | 6187/7317 [02:27<00:31, 35.80it/s]Training CobwebTree:  85%| | 6191/7317 [02:28<00:31, 36.13it/s]Training CobwebTree:  85%| | 6195/7317 [02:28<00:30, 36.39it/s]Training CobwebTree:  85%| | 6199/7317 [02:28<00:31, 35.35it/s]Training CobwebTree:  85%| | 6204/7317 [02:28<00:29, 37.37it/s]Training CobwebTree:  85%| | 6208/7317 [02:28<00:29, 37.62it/s]Training CobwebTree:  85%| | 6212/7317 [02:28<00:29, 38.01it/s]Training CobwebTree:  85%| | 6217/7317 [02:28<00:27, 39.91it/s]Training CobwebTree:  85%| | 6222/7317 [02:28<00:27, 40.12it/s]Training CobwebTree:  85%| | 6227/7317 [02:29<00:27, 39.81it/s]Training CobwebTree:  85%| | 6231/7317 [02:29<00:27, 39.57it/s]Training CobwebTree:  85%| | 6235/7317 [02:29<00:27, 39.14it/s]Training CobwebTree:  85%| | 6239/7317 [02:29<00:28, 38.22it/s]Training CobwebTree:  85%| | 6243/7317 [02:29<00:28, 37.04it/s]Training CobwebTree:  85%| | 6247/7317 [02:29<00:28, 37.05it/s]Training CobwebTree:  85%| | 6251/7317 [02:29<00:29, 36.54it/s]Training CobwebTree:  85%| | 6255/7317 [02:29<00:32, 33.17it/s]Training CobwebTree:  86%| | 6259/7317 [02:29<00:31, 33.78it/s]Training CobwebTree:  86%| | 6263/7317 [02:30<00:31, 33.08it/s]Training CobwebTree:  86%| | 6268/7317 [02:30<00:29, 35.31it/s]Training CobwebTree:  86%| | 6273/7317 [02:30<00:28, 36.88it/s]Training CobwebTree:  86%| | 6277/7317 [02:30<00:28, 36.50it/s]Training CobwebTree:  86%| | 6281/7317 [02:30<00:28, 36.85it/s]Training CobwebTree:  86%| | 6285/7317 [02:30<00:27, 37.25it/s]Training CobwebTree:  86%| | 6289/7317 [02:30<00:27, 36.99it/s]Training CobwebTree:  86%| | 6293/7317 [02:30<00:28, 36.01it/s]Training CobwebTree:  86%| | 6297/7317 [02:30<00:28, 35.95it/s]Training CobwebTree:  86%| | 6301/7317 [02:31<00:27, 36.48it/s]Training CobwebTree:  86%| | 6306/7317 [02:31<00:26, 37.47it/s]Training CobwebTree:  86%| | 6310/7317 [02:31<00:27, 36.17it/s]Training CobwebTree:  86%| | 6314/7317 [02:31<00:27, 36.65it/s]Training CobwebTree:  86%| | 6318/7317 [02:31<00:26, 37.33it/s]Training CobwebTree:  86%| | 6322/7317 [02:31<00:26, 37.96it/s]Training CobwebTree:  86%| | 6326/7317 [02:32<00:49, 19.86it/s]Training CobwebTree:  87%| | 6331/7317 [02:32<00:41, 24.00it/s]Training CobwebTree:  87%| | 6336/7317 [02:32<00:35, 27.68it/s]Training CobwebTree:  87%| | 6340/7317 [02:32<00:32, 29.61it/s]Training CobwebTree:  87%| | 6344/7317 [02:32<00:31, 30.92it/s]Training CobwebTree:  87%| | 6349/7317 [02:32<00:28, 33.64it/s]Training CobwebTree:  87%| | 6354/7317 [02:32<00:26, 36.54it/s]Training CobwebTree:  87%| | 6358/7317 [02:32<00:25, 37.00it/s]Training CobwebTree:  87%| | 6362/7317 [02:32<00:26, 36.51it/s]Training CobwebTree:  87%| | 6366/7317 [02:33<00:26, 35.89it/s]Training CobwebTree:  87%| | 6370/7317 [02:33<00:25, 36.42it/s]Training CobwebTree:  87%| | 6374/7317 [02:33<00:25, 36.53it/s]Training CobwebTree:  87%| | 6378/7317 [02:33<00:26, 35.47it/s]Training CobwebTree:  87%| | 6382/7317 [02:33<00:25, 36.29it/s]Training CobwebTree:  87%| | 6386/7317 [02:33<00:26, 35.54it/s]Training CobwebTree:  87%| | 6390/7317 [02:33<00:25, 35.97it/s]Training CobwebTree:  87%| | 6394/7317 [02:33<00:27, 33.79it/s]Training CobwebTree:  87%| | 6399/7317 [02:34<00:25, 35.79it/s]Training CobwebTree:  88%| | 6403/7317 [02:34<00:25, 36.01it/s]Training CobwebTree:  88%| | 6407/7317 [02:34<00:25, 36.39it/s]Training CobwebTree:  88%| | 6411/7317 [02:34<00:24, 37.01it/s]Training CobwebTree:  88%| | 6415/7317 [02:34<00:25, 36.06it/s]Training CobwebTree:  88%| | 6420/7317 [02:34<00:23, 37.95it/s]Training CobwebTree:  88%| | 6424/7317 [02:34<00:24, 36.53it/s]Training CobwebTree:  88%| | 6428/7317 [02:34<00:25, 35.38it/s]Training CobwebTree:  88%| | 6432/7317 [02:34<00:24, 36.55it/s]Training CobwebTree:  88%| | 6436/7317 [02:35<00:23, 37.43it/s]Training CobwebTree:  88%| | 6440/7317 [02:35<00:23, 37.35it/s]Training CobwebTree:  88%| | 6444/7317 [02:35<00:23, 37.22it/s]Training CobwebTree:  88%| | 6448/7317 [02:35<00:23, 37.13it/s]Training CobwebTree:  88%| | 6453/7317 [02:35<00:22, 38.53it/s]Training CobwebTree:  88%| | 6457/7317 [02:35<00:22, 37.91it/s]Training CobwebTree:  88%| | 6462/7317 [02:35<00:22, 38.02it/s]Training CobwebTree:  88%| | 6466/7317 [02:35<00:22, 37.90it/s]Training CobwebTree:  88%| | 6471/7317 [02:35<00:22, 38.37it/s]Training CobwebTree:  88%| | 6475/7317 [02:36<00:21, 38.41it/s]Training CobwebTree:  89%| | 6480/7317 [02:36<00:21, 39.12it/s]Training CobwebTree:  89%| | 6484/7317 [02:36<00:21, 39.29it/s]Training CobwebTree:  89%| | 6488/7317 [02:36<00:21, 39.46it/s]Training CobwebTree:  89%| | 6493/7317 [02:36<00:20, 39.79it/s]Training CobwebTree:  89%| | 6497/7317 [02:36<00:21, 38.39it/s]Training CobwebTree:  89%| | 6501/7317 [02:36<00:21, 38.30it/s]Training CobwebTree:  89%| | 6505/7317 [02:36<00:22, 36.45it/s]Training CobwebTree:  89%| | 6509/7317 [02:36<00:22, 35.84it/s]Training CobwebTree:  89%| | 6514/7317 [02:37<00:21, 36.61it/s]Training CobwebTree:  89%| | 6519/7317 [02:37<00:20, 38.65it/s]Training CobwebTree:  89%| | 6523/7317 [02:37<00:21, 37.26it/s]Training CobwebTree:  89%| | 6528/7317 [02:37<00:20, 38.69it/s]Training CobwebTree:  89%| | 6532/7317 [02:37<00:20, 38.86it/s]Training CobwebTree:  89%| | 6536/7317 [02:37<00:20, 38.97it/s]Training CobwebTree:  89%| | 6540/7317 [02:37<00:20, 37.93it/s]Training CobwebTree:  89%| | 6544/7317 [02:37<00:20, 37.20it/s]Training CobwebTree:  89%| | 6548/7317 [02:37<00:21, 36.15it/s]Training CobwebTree:  90%| | 6552/7317 [02:38<00:21, 35.42it/s]Training CobwebTree:  90%| | 6557/7317 [02:38<00:20, 37.18it/s]Training CobwebTree:  90%| | 6561/7317 [02:38<00:20, 36.99it/s]Training CobwebTree:  90%| | 6565/7317 [02:38<00:20, 37.42it/s]Training CobwebTree:  90%| | 6570/7317 [02:38<00:19, 38.61it/s]Training CobwebTree:  90%| | 6574/7317 [02:38<00:19, 37.76it/s]Training CobwebTree:  90%| | 6579/7317 [02:38<00:19, 38.25it/s]Training CobwebTree:  90%| | 6583/7317 [02:38<00:19, 37.49it/s]Training CobwebTree:  90%| | 6587/7317 [02:39<00:19, 37.90it/s]Training CobwebTree:  90%| | 6591/7317 [02:39<00:19, 37.15it/s]Training CobwebTree:  90%| | 6595/7317 [02:39<00:19, 37.86it/s]Training CobwebTree:  90%| | 6599/7317 [02:39<00:19, 37.43it/s]Training CobwebTree:  90%| | 6603/7317 [02:39<00:19, 36.64it/s]Training CobwebTree:  90%| | 6607/7317 [02:39<00:19, 36.45it/s]Training CobwebTree:  90%| | 6612/7317 [02:39<00:18, 38.03it/s]Training CobwebTree:  90%| | 6616/7317 [02:39<00:18, 38.26it/s]Training CobwebTree:  90%| | 6620/7317 [02:39<00:18, 37.82it/s]Training CobwebTree:  91%| | 6624/7317 [02:39<00:18, 37.87it/s]Training CobwebTree:  91%| | 6628/7317 [02:40<00:18, 36.74it/s]Training CobwebTree:  91%| | 6632/7317 [02:40<00:18, 36.25it/s]Training CobwebTree:  91%| | 6636/7317 [02:40<00:18, 37.16it/s]Training CobwebTree:  91%| | 6640/7317 [02:40<00:18, 37.54it/s]Training CobwebTree:  91%| | 6644/7317 [02:40<00:17, 37.57it/s]Training CobwebTree:  91%| | 6648/7317 [02:40<00:17, 37.49it/s]Training CobwebTree:  91%| | 6652/7317 [02:40<00:17, 37.18it/s]Training CobwebTree:  91%| | 6656/7317 [02:40<00:18, 35.63it/s]Training CobwebTree:  91%| | 6661/7317 [02:40<00:17, 37.96it/s]Training CobwebTree:  91%| | 6665/7317 [02:41<00:17, 37.91it/s]Training CobwebTree:  91%| | 6669/7317 [02:41<00:16, 38.15it/s]Training CobwebTree:  91%| | 6673/7317 [02:41<00:17, 36.86it/s]Training CobwebTree:  91%|| 6677/7317 [02:41<00:17, 36.03it/s]Training CobwebTree:  91%|| 6681/7317 [02:41<00:17, 35.96it/s]Training CobwebTree:  91%|| 6685/7317 [02:41<00:19, 32.91it/s]Training CobwebTree:  91%|| 6690/7317 [02:41<00:17, 36.35it/s]Training CobwebTree:  91%|| 6694/7317 [02:41<00:17, 35.75it/s]Training CobwebTree:  92%|| 6699/7317 [02:42<00:16, 36.60it/s]Training CobwebTree:  92%|| 6703/7317 [02:42<00:16, 37.46it/s]Training CobwebTree:  92%|| 6707/7317 [02:42<00:16, 37.30it/s]Training CobwebTree:  92%|| 6712/7317 [02:42<00:15, 39.17it/s]Training CobwebTree:  92%|| 6716/7317 [02:42<00:15, 39.36it/s]Training CobwebTree:  92%|| 6720/7317 [02:42<00:15, 38.58it/s]Training CobwebTree:  92%|| 6724/7317 [02:42<00:16, 36.18it/s]Training CobwebTree:  92%|| 6728/7317 [02:42<00:16, 36.50it/s]Training CobwebTree:  92%|| 6732/7317 [02:42<00:16, 36.30it/s]Training CobwebTree:  92%|| 6736/7317 [02:43<00:16, 35.85it/s]Training CobwebTree:  92%|| 6740/7317 [02:43<00:16, 35.11it/s]Training CobwebTree:  92%|| 6744/7317 [02:43<00:16, 35.28it/s]Training CobwebTree:  92%|| 6748/7317 [02:43<00:15, 35.60it/s]Training CobwebTree:  92%|| 6752/7317 [02:43<00:15, 35.52it/s]Training CobwebTree:  92%|| 6756/7317 [02:43<00:16, 34.94it/s]Training CobwebTree:  92%|| 6760/7317 [02:43<00:15, 35.72it/s]Training CobwebTree:  92%|| 6764/7317 [02:43<00:14, 36.89it/s]Training CobwebTree:  92%|| 6768/7317 [02:43<00:14, 37.32it/s]Training CobwebTree:  93%|| 6772/7317 [02:44<00:15, 36.08it/s]Training CobwebTree:  93%|| 6776/7317 [02:44<00:15, 35.50it/s]Training CobwebTree:  93%|| 6780/7317 [02:44<00:15, 35.42it/s]Training CobwebTree:  93%|| 6784/7317 [02:44<00:15, 35.51it/s]Training CobwebTree:  93%|| 6789/7317 [02:44<00:14, 36.90it/s]Training CobwebTree:  93%|| 6793/7317 [02:44<00:14, 37.35it/s]Training CobwebTree:  93%|| 6797/7317 [02:44<00:13, 37.92it/s]Training CobwebTree:  93%|| 6801/7317 [02:44<00:13, 37.26it/s]Training CobwebTree:  93%|| 6805/7317 [02:44<00:13, 37.82it/s]Training CobwebTree:  93%|| 6809/7317 [02:45<00:13, 36.34it/s]Training CobwebTree:  93%|| 6813/7317 [02:45<00:13, 36.73it/s]Training CobwebTree:  93%|| 6817/7317 [02:45<00:13, 36.22it/s]Training CobwebTree:  93%|| 6822/7317 [02:45<00:13, 37.18it/s]Training CobwebTree:  93%|| 6827/7317 [02:45<00:12, 39.06it/s]Training CobwebTree:  93%|| 6831/7317 [02:45<00:12, 38.77it/s]Training CobwebTree:  93%|| 6835/7317 [02:45<00:12, 38.28it/s]Training CobwebTree:  93%|| 6839/7317 [02:45<00:12, 38.01it/s]Training CobwebTree:  94%|| 6843/7317 [02:45<00:12, 38.49it/s]Training CobwebTree:  94%|| 6847/7317 [02:46<00:12, 37.11it/s]Training CobwebTree:  94%|| 6851/7317 [02:46<00:12, 36.40it/s]Training CobwebTree:  94%|| 6855/7317 [02:46<00:12, 36.98it/s]Training CobwebTree:  94%|| 6859/7317 [02:46<00:12, 37.63it/s]Training CobwebTree:  94%|| 6863/7317 [02:46<00:12, 35.88it/s]Training CobwebTree:  94%|| 6867/7317 [02:46<00:12, 35.67it/s]Training CobwebTree:  94%|| 6871/7317 [02:46<00:12, 34.62it/s]Training CobwebTree:  94%|| 6875/7317 [02:46<00:13, 33.36it/s]Training CobwebTree:  94%|| 6879/7317 [02:46<00:12, 33.92it/s]Training CobwebTree:  94%|| 6883/7317 [02:47<00:12, 35.31it/s]Training CobwebTree:  94%|| 6887/7317 [02:47<00:12, 34.08it/s]Training CobwebTree:  94%|| 6891/7317 [02:47<00:12, 34.44it/s]Training CobwebTree:  94%|| 6895/7317 [02:47<00:12, 34.39it/s]Training CobwebTree:  94%|| 6900/7317 [02:47<00:11, 36.09it/s]Training CobwebTree:  94%|| 6905/7317 [02:47<00:10, 37.66it/s]Training CobwebTree:  94%|| 6910/7317 [02:47<00:10, 38.62it/s]Training CobwebTree:  94%|| 6914/7317 [02:47<00:10, 38.65it/s]Training CobwebTree:  95%|| 6919/7317 [02:48<00:09, 40.16it/s]Training CobwebTree:  95%|| 6924/7317 [02:48<00:10, 37.94it/s]Training CobwebTree:  95%|| 6928/7317 [02:48<00:10, 37.28it/s]Training CobwebTree:  95%|| 6933/7317 [02:48<00:10, 38.36it/s]Training CobwebTree:  95%|| 6937/7317 [02:48<00:09, 38.44it/s]Training CobwebTree:  95%|| 6941/7317 [02:48<00:09, 38.55it/s]Training CobwebTree:  95%|| 6945/7317 [02:48<00:09, 38.55it/s]Training CobwebTree:  95%|| 6949/7317 [02:48<00:09, 38.12it/s]Training CobwebTree:  95%|| 6953/7317 [02:48<00:09, 36.98it/s]Training CobwebTree:  95%|| 6958/7317 [02:49<00:09, 38.06it/s]Training CobwebTree:  95%|| 6962/7317 [02:49<00:09, 36.53it/s]Training CobwebTree:  95%|| 6966/7317 [02:49<00:09, 36.39it/s]Training CobwebTree:  95%|| 6970/7317 [02:49<00:09, 36.05it/s]Training CobwebTree:  95%|| 6974/7317 [02:49<00:09, 36.78it/s]Training CobwebTree:  95%|| 6978/7317 [02:49<00:09, 35.55it/s]Training CobwebTree:  95%|| 6982/7317 [02:49<00:09, 34.43it/s]Training CobwebTree:  95%|| 6986/7317 [02:49<00:09, 34.97it/s]Training CobwebTree:  96%|| 6990/7317 [02:49<00:09, 36.13it/s]Training CobwebTree:  96%|| 6994/7317 [02:50<00:09, 35.83it/s]Training CobwebTree:  96%|| 6999/7317 [02:50<00:08, 37.55it/s]Training CobwebTree:  96%|| 7004/7317 [02:50<00:08, 37.76it/s]Training CobwebTree:  96%|| 7008/7317 [02:50<00:08, 36.41it/s]Training CobwebTree:  96%|| 7013/7317 [02:50<00:08, 37.30it/s]Training CobwebTree:  96%|| 7017/7317 [02:50<00:08, 37.19it/s]Training CobwebTree:  96%|| 7021/7317 [02:50<00:08, 36.88it/s]Training CobwebTree:  96%|| 7025/7317 [02:50<00:07, 37.16it/s]Training CobwebTree:  96%|| 7029/7317 [02:51<00:07, 36.53it/s]Training CobwebTree:  96%|| 7033/7317 [02:51<00:07, 36.22it/s]Training CobwebTree:  96%|| 7037/7317 [02:51<00:07, 36.44it/s]Training CobwebTree:  96%|| 7041/7317 [02:51<00:07, 37.17it/s]Training CobwebTree:  96%|| 7045/7317 [02:51<00:07, 35.87it/s]Training CobwebTree:  96%|| 7049/7317 [02:51<00:07, 34.83it/s]Training CobwebTree:  96%|| 7054/7317 [02:51<00:06, 38.59it/s]Training CobwebTree:  96%|| 7059/7317 [02:51<00:06, 38.69it/s]Training CobwebTree:  97%|| 7063/7317 [02:51<00:06, 37.98it/s]Training CobwebTree:  97%|| 7067/7317 [02:52<00:06, 37.33it/s]Training CobwebTree:  97%|| 7071/7317 [02:52<00:06, 36.38it/s]Training CobwebTree:  97%|| 7076/7317 [02:52<00:06, 37.46it/s]Training CobwebTree:  97%|| 7080/7317 [02:52<00:06, 36.27it/s]Training CobwebTree:  97%|| 7084/7317 [02:52<00:06, 36.88it/s]Training CobwebTree:  97%|| 7088/7317 [02:52<00:06, 36.22it/s]Training CobwebTree:  97%|| 7092/7317 [02:52<00:06, 36.99it/s]Training CobwebTree:  97%|| 7096/7317 [02:52<00:06, 35.72it/s]Training CobwebTree:  97%|| 7101/7317 [02:52<00:05, 36.60it/s]Training CobwebTree:  97%|| 7105/7317 [02:53<00:05, 36.36it/s]Training CobwebTree:  97%|| 7109/7317 [02:53<00:05, 36.64it/s]Training CobwebTree:  97%|| 7113/7317 [02:53<00:05, 34.09it/s]Training CobwebTree:  97%|| 7117/7317 [02:53<00:05, 35.25it/s]Training CobwebTree:  97%|| 7121/7317 [02:53<00:05, 36.07it/s]Training CobwebTree:  97%|| 7125/7317 [02:53<00:05, 36.40it/s]Training CobwebTree:  97%|| 7129/7317 [02:53<00:05, 37.00it/s]Training CobwebTree:  97%|| 7133/7317 [02:53<00:05, 35.82it/s]Training CobwebTree:  98%|| 7137/7317 [02:54<00:05, 35.08it/s]Training CobwebTree:  98%|| 7141/7317 [02:54<00:04, 36.06it/s]Training CobwebTree:  98%|| 7145/7317 [02:54<00:04, 36.20it/s]Training CobwebTree:  98%|| 7149/7317 [02:54<00:04, 36.38it/s]Training CobwebTree:  98%|| 7153/7317 [02:54<00:04, 35.78it/s]Training CobwebTree:  98%|| 7157/7317 [02:54<00:04, 35.05it/s]Training CobwebTree:  98%|| 7162/7317 [02:54<00:04, 36.51it/s]Training CobwebTree:  98%|| 7167/7317 [02:54<00:03, 37.93it/s]Training CobwebTree:  98%|| 7171/7317 [02:54<00:03, 37.07it/s]Training CobwebTree:  98%|| 7175/7317 [02:55<00:03, 36.70it/s]Training CobwebTree:  98%|| 7179/7317 [02:55<00:03, 35.90it/s]Training CobwebTree:  98%|| 7183/7317 [02:55<00:03, 35.71it/s]Training CobwebTree:  98%|| 7187/7317 [02:55<00:03, 36.87it/s]Training CobwebTree:  98%|| 7191/7317 [02:55<00:03, 36.08it/s]Training CobwebTree:  98%|| 7195/7317 [02:55<00:03, 36.90it/s]Training CobwebTree:  98%|| 7199/7317 [02:55<00:03, 35.60it/s]Training CobwebTree:  98%|| 7203/7317 [02:55<00:03, 36.47it/s]Training CobwebTree:  98%|| 7207/7317 [02:55<00:02, 37.35it/s]Training CobwebTree:  99%|| 7211/7317 [02:56<00:02, 37.52it/s]Training CobwebTree:  99%|| 7215/7317 [02:56<00:02, 37.46it/s]Training CobwebTree:  99%|| 7219/7317 [02:56<00:02, 37.36it/s]Training CobwebTree:  99%|| 7223/7317 [02:56<00:02, 37.79it/s]Training CobwebTree:  99%|| 7228/7317 [02:56<00:02, 38.53it/s]Training CobwebTree:  99%|| 7232/7317 [02:56<00:02, 36.32it/s]Training CobwebTree:  99%|| 7236/7317 [02:56<00:02, 36.76it/s]Training CobwebTree:  99%|| 7240/7317 [02:56<00:02, 35.77it/s]Training CobwebTree:  99%|| 7244/7317 [02:56<00:02, 35.43it/s]Training CobwebTree:  99%|| 7248/7317 [02:57<00:01, 36.04it/s]Training CobwebTree:  99%|| 7252/7317 [02:57<00:01, 35.99it/s]Training CobwebTree:  99%|| 7256/7317 [02:57<00:01, 36.01it/s]Training CobwebTree:  99%|| 7260/7317 [02:57<00:01, 36.88it/s]Training CobwebTree:  99%|| 7264/7317 [02:57<00:01, 37.40it/s]Training CobwebTree:  99%|| 7268/7317 [02:57<00:01, 36.14it/s]Training CobwebTree:  99%|| 7272/7317 [02:57<00:01, 35.43it/s]Training CobwebTree:  99%|| 7277/7317 [02:57<00:01, 37.25it/s]Training CobwebTree: 100%|| 7281/7317 [02:57<00:01, 35.41it/s]Training CobwebTree: 100%|| 7286/7317 [02:58<00:00, 36.89it/s]Training CobwebTree: 100%|| 7290/7317 [02:58<00:00, 36.35it/s]Training CobwebTree: 100%|| 7295/7317 [02:58<00:00, 36.92it/s]Training CobwebTree: 100%|| 7299/7317 [02:58<00:00, 36.78it/s]Training CobwebTree: 100%|| 7303/7317 [02:58<00:00, 37.40it/s]Training CobwebTree: 100%|| 7307/7317 [02:58<00:00, 36.90it/s]Training CobwebTree: 100%|| 7311/7317 [02:58<00:00, 34.23it/s]Training CobwebTree: 100%|| 7316/7317 [02:58<00:00, 36.07it/s]Training CobwebTree: 100%|| 7317/7317 [02:58<00:00, 40.89it/s]
2025-12-22 12:18:06,541 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:18:08,959 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-22 12:18:08,965 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-22 12:18:08,968 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-22 12:18:08,972 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-22 12:18:08,979 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-22 12:18:08,987 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-22 12:18:08,992 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-22 12:18:09,009 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-22 12:18:09,017 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-22 12:18:09,023 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-22 12:18:09,059 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-22 12:18:09,064 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-22 12:18:09,070 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-22 12:18:09,156 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-22 12:18:09,395 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-22 12:18:09,414 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-22 12:18:09,528 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-22 12:18:09,781 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-22 12:18:09,967 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-22 12:18:09,985 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-22 12:18:10,211 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-22 12:18:10,676 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-22 12:18:10,786 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-22 12:18:10,790 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-22 12:18:10,855 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-22 12:18:10,977 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-22 12:18:11,057 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-22 12:18:11,244 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-22 12:18:11,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,558 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,755 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,796 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:11,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:11,962 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,682 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,701 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,716 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,722 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:12,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:12,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,091 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,138 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,263 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,541 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,680 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,755 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:13,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:13,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,136 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,841 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,881 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,919 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:14,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:14,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:15,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:15,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:17,614 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:18:18,023 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-22 12:18:20,871 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:18:23,704 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-22 12:18:23,707 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-22 12:18:23,708 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-22 12:18:23,711 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-22 12:18:23,714 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-22 12:18:23,716 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-22 12:18:23,718 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-22 12:18:23,719 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-22 12:18:23,721 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-22 12:18:23,723 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-22 12:18:23,725 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-22 12:18:23,726 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-22 12:18:23,728 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-22 12:18:23,731 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-22 12:18:23,733 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-22 12:18:23,736 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-22 12:18:23,740 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-22 12:18:23,744 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-22 12:18:23,745 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-22 12:18:23,747 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-22 12:18:23,750 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-22 12:18:23,752 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-22 12:18:23,754 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-22 12:18:23,756 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-22 12:18:23,758 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-22 12:18:23,761 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-22 12:18:23,763 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-22 12:18:23,766 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-22 12:18:23,786 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-22 12:18:23,787 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-22 12:18:23,810 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-22 12:18:23,812 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-22 12:18:23,905 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-22 12:18:23,943 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-22 12:18:23,946 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-22 12:18:23,960 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-22 12:18:24,027 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-22 12:18:24,041 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-22 12:18:24,099 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-22 12:18:24,102 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-22 12:18:24,161 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-22 12:18:24,198 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-22 12:18:24,202 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-22 12:18:24,217 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-22 12:18:24,307 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-22 12:18:24,322 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-22 12:18:24,407 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-22 12:18:24,409 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-22 12:18:24,412 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-22 12:18:24,414 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-22 12:18:24,503 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-22 12:18:24,516 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-22 12:18:24,574 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-22 12:18:24,577 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-22 12:18:24,643 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-22 12:18:24,657 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-22 12:18:24,663 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-22 12:18:24,732 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-22 12:18:24,781 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-22 12:18:24,783 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-22 12:18:24,786 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-22 12:18:24,882 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-22 12:18:24,897 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-22 12:18:24,903 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-22 12:18:24,905 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-22 12:18:24,971 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-22 12:18:24,973 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-22 12:18:25,043 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-22 12:18:25,048 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-22 12:18:25,064 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-22 12:18:25,084 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-22 12:18:25,099 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-22 12:18:25,132 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-22 12:18:25,134 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-22 12:18:25,199 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-22 12:18:25,201 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-22 12:18:25,216 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-22 12:18:25,269 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-22 12:18:25,286 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-22 12:18:25,371 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-22 12:18:25,385 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-22 12:18:25,453 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-22 12:18:25,456 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-22 12:18:25,458 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-22 12:18:25,511 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-22 12:18:25,513 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-22 12:18:25,528 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-22 12:18:25,590 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-22 12:18:25,641 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-22 12:18:25,644 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-22 12:18:25,646 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-22 12:18:25,694 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-22 12:18:25,730 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-22 12:18:25,768 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-22 12:18:25,770 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-22 12:18:25,846 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-22 12:18:25,887 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-22 12:18:25,890 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-22 12:18:25,945 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-22 12:18:25,973 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-22 12:18:26,026 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-22 12:18:26,047 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-22 12:18:26,060 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-22 12:18:26,122 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-22 12:18:26,138 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-22 12:18:26,195 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-22 12:18:26,197 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-22 12:18:26,199 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-22 12:18:26,247 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-22 12:18:26,249 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-22 12:18:26,251 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-22 12:18:26,305 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-22 12:18:26,308 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-22 12:18:26,361 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-22 12:18:26,363 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-22 12:18:26,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,364 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,367 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,367 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,436 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,685 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,956 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:26,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:26,975 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,054 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,060 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,070 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,689 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:27,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:27,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:28,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:28,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:28,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:28,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:28,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:18:28,372 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:18:31,054 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:18:31,278 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-22 12:18:32,411 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.5766001011205771, 'coherence_npmi': 0.04804915517678855, 'topic_diversity': 0.9, 'inter_topic_similarity': 0.45141658186912537}
2025-12-22 12:18:32,412 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6598418435191882, 'coherence_npmi': 0.10714683682197317, 'topic_diversity': 0.828, 'inter_topic_similarity': 0.2755028307437897}
2025-12-22 12:18:32,412 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.5786848142421227, 'coherence_npmi': 0.04262545809472807, 'topic_diversity': 0.7830303030303031, 'inter_topic_similarity': 0.21610087156295776}
2025-12-22 12:18:32,412 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model HDBSCAN for hierarchical conversion
2025-12-22 12:18:37,380 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model KMeans for hierarchical conversion
2025-12-22 12:18:51,279 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model BERTopicCobwebWrapper for hierarchical conversion
2025-12-22 12:19:15,196 INFO __main__: Hierarchical Model 0 (HDBSCAN) metrics: {'hier_coherence_npmi': 0.1751731256600117, 'hier_topic_uniqueness': 0.9, 'hier_topic_diversity': 0.9, 'hier_topic_specialization': 0.12015062755545486, 'hier_affinity_child': 0.8057782649993896, 'hier_affinity_non_child': 0.0, 'hier_coherence_clnpmi': 0.04493474961960795, 'hier_PC_TD': 0.35, 'hier_PnonC_TD': nan, 'hier_sibling_TD': 0.6, 'hier_sibling_clnpmi': 0.05056973128704587}
2025-12-22 12:19:15,196 INFO __main__: Hierarchical Model 1 (KMeans) metrics: {'hier_coherence_npmi': 0.1947305403928277, 'hier_topic_uniqueness': 0.7762816814764183, 'hier_topic_diversity': 0.7762816814764183, 'hier_topic_specialization': 0.22687945106732857, 'hier_affinity_child': 0.812312126159668, 'hier_affinity_non_child': 0.35606518387794495, 'hier_coherence_clnpmi': 0.13377749374513329, 'hier_PC_TD': 0.23291160593792176, 'hier_PnonC_TD': 0.7514049190559957, 'hier_sibling_TD': 0.9670388091440724, 'hier_sibling_clnpmi': 0.16282581842580585}
2025-12-22 12:19:15,196 INFO __main__: Hierarchical Model 2 (BERTopicCobwebWrapper) metrics: {'hier_coherence_npmi': 0.2210143915283784, 'hier_topic_uniqueness': 0.6966444805194806, 'hier_topic_diversity': 0.6966444805194806, 'hier_topic_specialization': 0.3288897223160005, 'hier_affinity_child': 0.7592413425445557, 'hier_affinity_non_child': 0.21820183098316193, 'hier_coherence_clnpmi': 0.1273200757561703, 'hier_PC_TD': 0.2530053862483259, 'hier_PnonC_TD': 0.7863505518276914, 'hier_sibling_TD': 0.9430670995670997, 'hier_sibling_clnpmi': 0.16070451678259662}
