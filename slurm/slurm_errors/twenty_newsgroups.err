2025-12-21 06:13:26,181 INFO __main__: Starting benchmark for dataset=20newsgroups
2025-12-21 06:13:27,388 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-21 06:13:27,881 INFO gensim.corpora.dictionary: built Dictionary<70709 unique tokens: ['88', '89', 'best', 'bonnevilles', 'book']...> from 7317 documents (total 691444 corpus positions)
2025-12-21 06:13:27,884 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<70709 unique tokens: ['88', '89', 'best', 'bonnevilles', 'book']...> from 7317 documents (total 691444 corpus positions)", 'datetime': '2025-12-21T06:13:27.882068', 'gensim': '4.4.0', 'python': '3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-200-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-21 06:13:28,349 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-21 06:13:28,349 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-21 06:13:30,359 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 7317 docs
2025-12-21 06:14:52,458 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 06:14:54,727 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-21 06:14:54,731 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-21 06:14:54,733 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-21 06:14:54,736 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-21 06:14:54,741 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-21 06:14:54,745 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-21 06:14:54,748 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-21 06:14:54,750 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-21 06:14:54,756 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-21 06:14:54,761 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-21 06:14:54,766 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-21 06:14:54,770 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-21 06:14:54,775 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-21 06:14:54,778 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-21 06:14:54,790 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-21 06:14:54,796 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-21 06:14:54,800 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-21 06:14:54,808 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-21 06:14:54,814 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-21 06:14:54,818 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-21 06:14:54,826 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-21 06:14:54,848 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-21 06:14:54,853 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-21 06:14:54,855 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-21 06:14:54,859 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-21 06:14:54,878 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-21 06:14:54,897 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-21 06:14:54,923 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-21 06:14:55,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,657 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,673 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,681 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,703 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,726 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,732 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,763 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,780 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,782 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,865 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:55,934 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:55,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,051 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,082 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,125 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,133 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,561 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,694 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,728 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,774 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:56,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:56,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:57,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:57,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:57,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:57,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:57,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:57,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:57,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:57,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:57,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:57,767 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:57,800 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:57,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:58,034 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:58,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:14:58,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:14:58,093 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:01,520 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 06:15:01,550 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-21 06:15:02,025 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 06:15:05,191 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-21 06:15:05,192 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-21 06:15:05,193 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-21 06:15:05,194 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-21 06:15:05,195 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-21 06:15:05,196 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-21 06:15:05,196 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-21 06:15:05,197 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-21 06:15:05,198 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-21 06:15:05,199 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-21 06:15:05,200 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-21 06:15:05,201 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-21 06:15:05,202 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-21 06:15:05,203 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-21 06:15:05,204 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-21 06:15:05,206 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-21 06:15:05,207 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-21 06:15:05,209 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-21 06:15:05,210 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-21 06:15:05,211 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-21 06:15:05,212 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-21 06:15:05,213 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-21 06:15:05,214 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-21 06:15:05,214 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-21 06:15:05,215 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-21 06:15:05,216 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-21 06:15:05,217 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-21 06:15:05,219 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-21 06:15:05,220 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-21 06:15:05,220 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-21 06:15:05,224 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-21 06:15:05,225 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-21 06:15:05,226 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-21 06:15:05,227 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-21 06:15:05,228 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-21 06:15:05,229 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-21 06:15:05,229 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-21 06:15:05,231 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-21 06:15:05,232 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-21 06:15:05,233 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-21 06:15:05,234 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-21 06:15:05,234 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-21 06:15:05,236 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-21 06:15:05,236 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-21 06:15:05,237 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-21 06:15:05,239 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-21 06:15:05,240 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-21 06:15:05,242 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-21 06:15:05,243 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-21 06:15:05,244 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-21 06:15:05,245 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-21 06:15:05,246 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-21 06:15:05,248 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-21 06:15:05,249 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-21 06:15:05,251 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-21 06:15:05,253 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-21 06:15:05,254 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-21 06:15:05,255 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-21 06:15:05,257 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-21 06:15:05,259 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-21 06:15:05,260 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-21 06:15:05,262 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-21 06:15:05,263 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-21 06:15:05,265 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-21 06:15:05,266 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-21 06:15:05,268 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-21 06:15:05,270 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-21 06:15:05,272 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-21 06:15:05,273 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-21 06:15:05,275 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-21 06:15:05,276 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-21 06:15:05,278 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-21 06:15:05,280 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-21 06:15:05,281 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-21 06:15:05,284 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-21 06:15:05,285 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-21 06:15:05,287 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-21 06:15:05,288 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-21 06:15:05,290 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-21 06:15:05,292 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-21 06:15:05,294 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-21 06:15:05,295 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-21 06:15:05,296 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-21 06:15:05,298 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-21 06:15:05,299 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-21 06:15:05,301 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-21 06:15:05,302 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-21 06:15:05,315 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-21 06:15:05,317 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-21 06:15:05,319 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-21 06:15:05,320 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-21 06:15:05,322 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-21 06:15:05,322 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-21 06:15:05,326 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-21 06:15:05,328 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-21 06:15:05,329 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-21 06:15:05,334 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-21 06:15:05,336 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-21 06:15:05,338 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-21 06:15:05,339 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-21 06:15:05,430 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-21 06:15:05,437 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-21 06:15:05,452 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-21 06:15:05,454 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-21 06:15:05,478 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-21 06:15:05,484 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-21 06:15:05,524 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-21 06:15:05,525 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-21 06:15:05,527 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-21 06:15:05,531 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-21 06:15:05,540 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-21 06:15:05,557 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-21 06:15:05,680 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-21 06:15:05,681 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-21 06:15:05,682 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-21 06:15:06,035 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,035 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,036 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,037 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,037 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,037 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,043 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,043 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,043 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,044 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,044 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,044 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,045 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,047 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,047 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,051 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,105 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,117 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,136 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,206 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,209 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,110 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,293 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,296 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,322 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,541 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,682 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,699 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,763 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,770 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,792 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,796 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,848 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,782 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,931 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,932 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,935 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,969 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:06,977 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,991 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:06,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,088 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,129 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,172 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,058 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,275 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,514 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,700 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:07,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:07,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:08,115 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:08,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:08,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:08,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:08,780 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:08,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:08,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:15:08,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:15:12,935 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 06:15:12,967 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-21 06:15:13,415 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 7317 docs
2025-12-21 06:16:15,332 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 06:16:18,719 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-21 06:16:18,723 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-21 06:16:18,726 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-21 06:16:18,730 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-21 06:16:18,736 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-21 06:16:18,743 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-21 06:16:18,748 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-21 06:16:18,750 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-21 06:16:18,757 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-21 06:16:18,765 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-21 06:16:18,771 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-21 06:16:18,776 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-21 06:16:18,782 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-21 06:16:18,787 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-21 06:16:18,846 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-21 06:16:19,004 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-21 06:16:19,127 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-21 06:16:19,333 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-21 06:16:19,465 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-21 06:16:19,557 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-21 06:16:19,642 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-21 06:16:20,084 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-21 06:16:20,197 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-21 06:16:20,281 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-21 06:16:20,287 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-21 06:16:20,369 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-21 06:16:20,435 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-21 06:16:20,568 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-21 06:16:20,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,620 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,722 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:20,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,977 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:20,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,327 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,779 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,866 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:21,932 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:21,973 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,156 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,122 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:22,901 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,948 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:22,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,364 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,534 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,796 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,856 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:23,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:23,990 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:24,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:24,048 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:24,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:24,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:24,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:24,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:24,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:24,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:24,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:28,814 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 06:16:29,011 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-21 06:16:29,886 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 06:16:33,788 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-21 06:16:33,790 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-21 06:16:33,791 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-21 06:16:33,792 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-21 06:16:33,793 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-21 06:16:33,794 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-21 06:16:33,795 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-21 06:16:33,796 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-21 06:16:33,797 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-21 06:16:33,798 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-21 06:16:33,799 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-21 06:16:33,800 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-21 06:16:33,801 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-21 06:16:33,803 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-21 06:16:33,803 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-21 06:16:33,805 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-21 06:16:33,807 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-21 06:16:33,809 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-21 06:16:33,810 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-21 06:16:33,811 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-21 06:16:33,813 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-21 06:16:33,813 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-21 06:16:33,814 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-21 06:16:33,815 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-21 06:16:33,816 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-21 06:16:33,817 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-21 06:16:33,818 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-21 06:16:33,820 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-21 06:16:33,821 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-21 06:16:33,822 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-21 06:16:33,828 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-21 06:16:33,830 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-21 06:16:33,832 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-21 06:16:33,834 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-21 06:16:33,836 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-21 06:16:33,837 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-21 06:16:33,839 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-21 06:16:33,841 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-21 06:16:33,844 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-21 06:16:33,846 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-21 06:16:33,848 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-21 06:16:33,849 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-21 06:16:33,852 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-21 06:16:33,853 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-21 06:16:33,855 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-21 06:16:33,885 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-21 06:16:33,916 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-21 06:16:33,918 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-21 06:16:33,919 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-21 06:16:33,936 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-21 06:16:33,938 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-21 06:16:33,948 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-21 06:16:33,949 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-21 06:16:33,951 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-21 06:16:33,962 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-21 06:16:33,964 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-21 06:16:34,013 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-21 06:16:34,028 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-21 06:16:34,030 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-21 06:16:34,032 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-21 06:16:34,048 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-21 06:16:34,050 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-21 06:16:34,060 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-21 06:16:34,216 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-21 06:16:34,272 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-21 06:16:34,350 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-21 06:16:34,365 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-21 06:16:34,404 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-21 06:16:34,406 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-21 06:16:34,483 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-21 06:16:34,496 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-21 06:16:34,502 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-21 06:16:34,595 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-21 06:16:34,608 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-21 06:16:34,702 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-21 06:16:34,705 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-21 06:16:34,707 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-21 06:16:34,795 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-21 06:16:34,798 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-21 06:16:34,866 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-21 06:16:34,881 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-21 06:16:34,882 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-21 06:16:34,960 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-21 06:16:34,962 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-21 06:16:34,964 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-21 06:16:35,037 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-21 06:16:35,039 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-21 06:16:35,041 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-21 06:16:35,046 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-21 06:16:35,151 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-21 06:16:35,153 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-21 06:16:35,168 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-21 06:16:35,252 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-21 06:16:35,304 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-21 06:16:35,371 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-21 06:16:35,385 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-21 06:16:35,443 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-21 06:16:35,458 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-21 06:16:35,515 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-21 06:16:35,517 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-21 06:16:35,520 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-21 06:16:35,615 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-21 06:16:35,628 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-21 06:16:35,634 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-21 06:16:35,703 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-21 06:16:35,706 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-21 06:16:35,708 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-21 06:16:35,748 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-21 06:16:35,804 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-21 06:16:35,830 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-21 06:16:35,870 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-21 06:16:35,886 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-21 06:16:35,941 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-21 06:16:35,943 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-21 06:16:35,944 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-21 06:16:35,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,951 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,951 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,953 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,953 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,953 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,956 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,956 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,957 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,957 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,957 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,962 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:35,962 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,962 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,963 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,963 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,964 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,964 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,966 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,968 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,969 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,969 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,969 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:35,969 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,973 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,974 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,974 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,974 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,974 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,975 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,975 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,975 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,977 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,977 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:35,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:35,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,110 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,634 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,728 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,858 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:36,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:36,994 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,127 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,657 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:37,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:37,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:38,036 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:38,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:38,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:38,151 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:38,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:38,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:38,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:38,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:38,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:38,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:38,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:38,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:16:38,865 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:16:43,344 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 06:16:43,493 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-21 06:16:44,219 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 7317 docs
Training CobwebTree:   0%|          | 0/7317 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 18/7317 [00:00<00:43, 169.10it/s]Training CobwebTree:   0%|          | 35/7317 [00:00<01:00, 120.64it/s]Training CobwebTree:   1%|          | 48/7317 [00:00<01:11, 102.12it/s]Training CobwebTree:   1%|          | 59/7317 [00:00<01:15, 95.50it/s] Training CobwebTree:   1%|          | 69/7317 [00:00<01:21, 88.84it/s]Training CobwebTree:   1%|          | 79/7317 [00:00<01:29, 80.85it/s]Training CobwebTree:   1%|          | 88/7317 [00:00<01:30, 80.15it/s]Training CobwebTree:   1%|         | 97/7317 [00:01<01:36, 74.62it/s]Training CobwebTree:   1%|         | 105/7317 [00:01<01:37, 74.09it/s]Training CobwebTree:   2%|         | 113/7317 [00:01<01:39, 72.15it/s]Training CobwebTree:   2%|         | 121/7317 [00:01<01:40, 71.93it/s]Training CobwebTree:   2%|         | 129/7317 [00:01<01:43, 69.67it/s]Training CobwebTree:   2%|         | 137/7317 [00:01<01:41, 70.72it/s]Training CobwebTree:   2%|         | 145/7317 [00:01<01:42, 70.19it/s]Training CobwebTree:   2%|         | 153/7317 [00:01<01:46, 67.53it/s]Training CobwebTree:   2%|         | 160/7317 [00:02<01:46, 67.44it/s]Training CobwebTree:   2%|         | 168/7317 [00:02<01:45, 67.57it/s]Training CobwebTree:   2%|         | 175/7317 [00:02<01:54, 62.41it/s]Training CobwebTree:   2%|         | 182/7317 [00:02<01:54, 62.28it/s]Training CobwebTree:   3%|         | 189/7317 [00:02<01:53, 62.75it/s]Training CobwebTree:   3%|         | 196/7317 [00:02<01:56, 61.01it/s]Training CobwebTree:   3%|         | 203/7317 [00:02<02:04, 56.92it/s]Training CobwebTree:   3%|         | 209/7317 [00:02<02:08, 55.22it/s]Training CobwebTree:   3%|         | 217/7317 [00:02<01:57, 60.20it/s]Training CobwebTree:   3%|         | 224/7317 [00:03<02:00, 59.11it/s]Training CobwebTree:   3%|         | 230/7317 [00:03<02:03, 57.42it/s]Training CobwebTree:   3%|         | 237/7317 [00:03<02:02, 57.88it/s]Training CobwebTree:   3%|         | 243/7317 [00:03<02:05, 56.51it/s]Training CobwebTree:   3%|         | 249/7317 [00:03<02:10, 54.30it/s]Training CobwebTree:   3%|         | 255/7317 [00:03<02:13, 52.92it/s]Training CobwebTree:   4%|         | 261/7317 [00:03<02:09, 54.54it/s]Training CobwebTree:   4%|         | 267/7317 [00:03<02:11, 53.42it/s]Training CobwebTree:   4%|         | 273/7317 [00:04<02:07, 55.06it/s]Training CobwebTree:   4%|         | 279/7317 [00:04<02:09, 54.52it/s]Training CobwebTree:   4%|         | 285/7317 [00:04<02:10, 53.69it/s]Training CobwebTree:   4%|         | 291/7317 [00:04<02:10, 53.82it/s]Training CobwebTree:   4%|         | 298/7317 [00:04<02:02, 57.49it/s]Training CobwebTree:   4%|         | 304/7317 [00:04<02:03, 56.98it/s]Training CobwebTree:   4%|         | 310/7317 [00:04<02:01, 57.62it/s]Training CobwebTree:   4%|         | 316/7317 [00:04<02:07, 54.80it/s]Training CobwebTree:   4%|         | 322/7317 [00:04<02:04, 56.14it/s]Training CobwebTree:   4%|         | 328/7317 [00:05<02:05, 55.83it/s]Training CobwebTree:   5%|         | 334/7317 [00:05<02:08, 54.51it/s]Training CobwebTree:   5%|         | 340/7317 [00:05<02:09, 53.70it/s]Training CobwebTree:   5%|         | 346/7317 [00:05<02:12, 52.57it/s]Training CobwebTree:   5%|         | 352/7317 [00:05<02:13, 52.10it/s]Training CobwebTree:   5%|         | 358/7317 [00:05<02:15, 51.47it/s]Training CobwebTree:   5%|         | 364/7317 [00:05<02:24, 48.02it/s]Training CobwebTree:   5%|         | 370/7317 [00:05<02:17, 50.38it/s]Training CobwebTree:   5%|         | 376/7317 [00:05<02:13, 52.01it/s]Training CobwebTree:   5%|         | 383/7317 [00:06<02:07, 54.17it/s]Training CobwebTree:   5%|         | 389/7317 [00:06<02:09, 53.67it/s]Training CobwebTree:   5%|         | 395/7317 [00:06<02:11, 52.60it/s]Training CobwebTree:   5%|         | 401/7317 [00:06<02:14, 51.30it/s]Training CobwebTree:   6%|         | 407/7317 [00:06<02:17, 50.41it/s]Training CobwebTree:   6%|         | 413/7317 [00:06<02:16, 50.69it/s]Training CobwebTree:   6%|         | 419/7317 [00:06<02:17, 50.25it/s]Training CobwebTree:   6%|         | 425/7317 [00:06<02:20, 49.15it/s]Training CobwebTree:   6%|         | 430/7317 [00:07<02:22, 48.27it/s]Training CobwebTree:   6%|         | 436/7317 [00:07<02:16, 50.45it/s]Training CobwebTree:   6%|         | 442/7317 [00:07<02:16, 50.39it/s]Training CobwebTree:   6%|         | 448/7317 [00:07<02:19, 49.08it/s]Training CobwebTree:   6%|         | 453/7317 [00:07<02:19, 49.09it/s]Training CobwebTree:   6%|         | 459/7317 [00:07<02:21, 48.52it/s]Training CobwebTree:   6%|         | 465/7317 [00:07<02:18, 49.58it/s]Training CobwebTree:   6%|         | 470/7317 [00:07<02:28, 46.04it/s]Training CobwebTree:   6%|         | 475/7317 [00:07<02:26, 46.63it/s]Training CobwebTree:   7%|         | 481/7317 [00:08<02:21, 48.41it/s]Training CobwebTree:   7%|         | 486/7317 [00:08<02:24, 47.40it/s]Training CobwebTree:   7%|         | 491/7317 [00:08<02:27, 46.37it/s]Training CobwebTree:   7%|         | 496/7317 [00:08<02:31, 45.17it/s]Training CobwebTree:   7%|         | 501/7317 [00:08<02:33, 44.47it/s]Training CobwebTree:   7%|         | 506/7317 [00:08<02:32, 44.63it/s]Training CobwebTree:   7%|         | 512/7317 [00:08<02:29, 45.39it/s]Training CobwebTree:   7%|         | 518/7317 [00:08<02:24, 47.19it/s]Training CobwebTree:   7%|         | 523/7317 [00:09<02:29, 45.50it/s]Training CobwebTree:   7%|         | 529/7317 [00:09<02:19, 48.63it/s]Training CobwebTree:   7%|         | 534/7317 [00:09<02:20, 48.26it/s]Training CobwebTree:   7%|         | 539/7317 [00:09<02:19, 48.47it/s]Training CobwebTree:   7%|         | 545/7317 [00:09<02:19, 48.53it/s]Training CobwebTree:   8%|         | 550/7317 [00:09<02:23, 47.03it/s]Training CobwebTree:   8%|         | 556/7317 [00:09<02:18, 48.82it/s]Training CobwebTree:   8%|         | 562/7317 [00:09<02:18, 48.69it/s]Training CobwebTree:   8%|         | 568/7317 [00:09<02:16, 49.57it/s]Training CobwebTree:   8%|         | 574/7317 [00:10<02:11, 51.26it/s]Training CobwebTree:   8%|         | 580/7317 [00:10<02:14, 49.91it/s]Training CobwebTree:   8%|         | 586/7317 [00:10<02:13, 50.36it/s]Training CobwebTree:   8%|         | 592/7317 [00:10<02:11, 51.10it/s]Training CobwebTree:   8%|         | 598/7317 [00:10<02:09, 51.97it/s]Training CobwebTree:   8%|         | 604/7317 [00:10<02:12, 50.70it/s]Training CobwebTree:   8%|         | 610/7317 [00:10<02:13, 50.18it/s]Training CobwebTree:   8%|         | 616/7317 [00:10<02:17, 48.58it/s]Training CobwebTree:   8%|         | 621/7317 [00:10<02:20, 47.71it/s]Training CobwebTree:   9%|         | 627/7317 [00:11<02:17, 48.54it/s]Training CobwebTree:   9%|         | 633/7317 [00:11<02:16, 48.98it/s]Training CobwebTree:   9%|         | 638/7317 [00:11<02:18, 48.35it/s]Training CobwebTree:   9%|         | 643/7317 [00:11<02:16, 48.77it/s]Training CobwebTree:   9%|         | 649/7317 [00:11<02:20, 47.42it/s]Training CobwebTree:   9%|         | 654/7317 [00:11<02:21, 47.10it/s]Training CobwebTree:   9%|         | 660/7317 [00:11<02:19, 47.84it/s]Training CobwebTree:   9%|         | 666/7317 [00:11<02:14, 49.29it/s]Training CobwebTree:   9%|         | 672/7317 [00:12<02:14, 49.49it/s]Training CobwebTree:   9%|         | 677/7317 [00:12<02:18, 48.06it/s]Training CobwebTree:   9%|         | 682/7317 [00:12<02:17, 48.26it/s]Training CobwebTree:   9%|         | 687/7317 [00:12<02:19, 47.69it/s]Training CobwebTree:   9%|         | 693/7317 [00:12<02:17, 48.01it/s]Training CobwebTree:  10%|         | 698/7317 [00:12<02:22, 46.60it/s]Training CobwebTree:  10%|         | 703/7317 [00:12<02:20, 47.15it/s]Training CobwebTree:  10%|         | 708/7317 [00:12<02:24, 45.88it/s]Training CobwebTree:  10%|         | 713/7317 [00:12<02:20, 46.93it/s]Training CobwebTree:  10%|         | 718/7317 [00:13<02:24, 45.64it/s]Training CobwebTree:  10%|         | 724/7317 [00:13<02:21, 46.67it/s]Training CobwebTree:  10%|         | 729/7317 [00:13<02:20, 46.90it/s]Training CobwebTree:  10%|         | 734/7317 [00:13<02:18, 47.41it/s]Training CobwebTree:  10%|         | 739/7317 [00:13<02:16, 48.09it/s]Training CobwebTree:  10%|         | 744/7317 [00:13<02:16, 48.12it/s]Training CobwebTree:  10%|         | 750/7317 [00:13<02:16, 48.02it/s]Training CobwebTree:  10%|         | 755/7317 [00:13<02:19, 47.17it/s]Training CobwebTree:  10%|         | 760/7317 [00:13<02:21, 46.36it/s]Training CobwebTree:  10%|         | 765/7317 [00:14<02:18, 47.23it/s]Training CobwebTree:  11%|         | 770/7317 [00:14<02:23, 45.63it/s]Training CobwebTree:  11%|         | 775/7317 [00:14<02:20, 46.62it/s]Training CobwebTree:  11%|         | 780/7317 [00:14<02:22, 45.82it/s]Training CobwebTree:  11%|         | 785/7317 [00:14<02:22, 45.91it/s]Training CobwebTree:  11%|         | 790/7317 [00:14<02:21, 46.13it/s]Training CobwebTree:  11%|         | 795/7317 [00:14<02:23, 45.40it/s]Training CobwebTree:  11%|         | 800/7317 [00:14<02:22, 45.85it/s]Training CobwebTree:  11%|         | 805/7317 [00:14<02:20, 46.35it/s]Training CobwebTree:  11%|         | 810/7317 [00:14<02:21, 46.08it/s]Training CobwebTree:  11%|         | 816/7317 [00:15<02:15, 48.10it/s]Training CobwebTree:  11%|         | 821/7317 [00:15<02:14, 48.18it/s]Training CobwebTree:  11%|        | 826/7317 [00:15<02:15, 47.89it/s]Training CobwebTree:  11%|        | 832/7317 [00:15<02:13, 48.61it/s]Training CobwebTree:  11%|        | 838/7317 [00:15<02:06, 51.35it/s]Training CobwebTree:  12%|        | 844/7317 [00:15<02:09, 49.89it/s]Training CobwebTree:  12%|        | 850/7317 [00:15<02:14, 48.02it/s]Training CobwebTree:  12%|        | 855/7317 [00:15<02:17, 47.00it/s]Training CobwebTree:  12%|        | 860/7317 [00:16<02:19, 46.43it/s]Training CobwebTree:  12%|        | 865/7317 [00:16<02:16, 47.19it/s]Training CobwebTree:  12%|        | 870/7317 [00:16<02:14, 47.80it/s]Training CobwebTree:  12%|        | 875/7317 [00:16<02:15, 47.39it/s]Training CobwebTree:  12%|        | 880/7317 [00:16<02:19, 46.23it/s]Training CobwebTree:  12%|        | 885/7317 [00:16<02:22, 45.19it/s]Training CobwebTree:  12%|        | 890/7317 [00:16<02:21, 45.49it/s]Training CobwebTree:  12%|        | 895/7317 [00:16<02:24, 44.33it/s]Training CobwebTree:  12%|        | 900/7317 [00:16<02:24, 44.47it/s]Training CobwebTree:  12%|        | 906/7317 [00:17<02:15, 47.41it/s]Training CobwebTree:  12%|        | 911/7317 [00:17<02:15, 47.26it/s]Training CobwebTree:  13%|        | 916/7317 [00:17<02:16, 46.78it/s]Training CobwebTree:  13%|        | 921/7317 [00:17<02:17, 46.63it/s]Training CobwebTree:  13%|        | 927/7317 [00:17<02:16, 46.98it/s]Training CobwebTree:  13%|        | 932/7317 [00:17<02:27, 43.19it/s]Training CobwebTree:  13%|        | 937/7317 [00:17<02:25, 43.94it/s]Training CobwebTree:  13%|        | 942/7317 [00:17<02:21, 45.05it/s]Training CobwebTree:  13%|        | 947/7317 [00:17<02:22, 44.69it/s]Training CobwebTree:  13%|        | 952/7317 [00:18<02:28, 42.96it/s]Training CobwebTree:  13%|        | 957/7317 [00:18<02:22, 44.75it/s]Training CobwebTree:  13%|        | 962/7317 [00:18<02:24, 44.05it/s]Training CobwebTree:  13%|        | 967/7317 [00:18<02:24, 44.08it/s]Training CobwebTree:  13%|        | 972/7317 [00:18<02:24, 43.95it/s]Training CobwebTree:  13%|        | 977/7317 [00:18<02:26, 43.39it/s]Training CobwebTree:  13%|        | 982/7317 [00:18<02:25, 43.39it/s]Training CobwebTree:  13%|        | 987/7317 [00:18<02:29, 42.39it/s]Training CobwebTree:  14%|        | 992/7317 [00:18<02:25, 43.44it/s]Training CobwebTree:  14%|        | 997/7317 [00:19<02:26, 43.15it/s]Training CobwebTree:  14%|        | 1002/7317 [00:19<02:24, 43.85it/s]Training CobwebTree:  14%|        | 1007/7317 [00:19<02:20, 44.88it/s]Training CobwebTree:  14%|        | 1012/7317 [00:19<02:21, 44.46it/s]Training CobwebTree:  14%|        | 1017/7317 [00:19<02:21, 44.58it/s]Training CobwebTree:  14%|        | 1022/7317 [00:19<02:25, 43.39it/s]Training CobwebTree:  14%|        | 1027/7317 [00:19<02:24, 43.56it/s]Training CobwebTree:  14%|        | 1032/7317 [00:19<02:24, 43.58it/s]Training CobwebTree:  14%|        | 1037/7317 [00:19<02:23, 43.87it/s]Training CobwebTree:  14%|        | 1042/7317 [00:20<02:19, 45.06it/s]Training CobwebTree:  14%|        | 1047/7317 [00:20<02:19, 44.97it/s]Training CobwebTree:  14%|        | 1052/7317 [00:20<02:17, 45.59it/s]Training CobwebTree:  14%|        | 1057/7317 [00:20<02:17, 45.62it/s]Training CobwebTree:  15%|        | 1062/7317 [00:20<02:26, 42.68it/s]Training CobwebTree:  15%|        | 1067/7317 [00:20<02:22, 43.87it/s]Training CobwebTree:  15%|        | 1072/7317 [00:20<02:24, 43.14it/s]Training CobwebTree:  15%|        | 1077/7317 [00:20<02:21, 44.04it/s]Training CobwebTree:  15%|        | 1082/7317 [00:21<02:24, 43.09it/s]Training CobwebTree:  15%|        | 1087/7317 [00:21<02:25, 42.92it/s]Training CobwebTree:  15%|        | 1092/7317 [00:21<02:19, 44.54it/s]Training CobwebTree:  15%|        | 1097/7317 [00:21<02:18, 44.95it/s]Training CobwebTree:  15%|        | 1102/7317 [00:21<02:23, 43.45it/s]Training CobwebTree:  15%|        | 1107/7317 [00:21<02:19, 44.39it/s]Training CobwebTree:  15%|        | 1112/7317 [00:21<02:19, 44.33it/s]Training CobwebTree:  15%|        | 1117/7317 [00:21<02:17, 44.94it/s]Training CobwebTree:  15%|        | 1122/7317 [00:21<02:19, 44.42it/s]Training CobwebTree:  15%|        | 1127/7317 [00:22<02:18, 44.82it/s]Training CobwebTree:  15%|        | 1132/7317 [00:22<02:18, 44.66it/s]Training CobwebTree:  16%|        | 1137/7317 [00:22<02:17, 44.88it/s]Training CobwebTree:  16%|        | 1142/7317 [00:22<02:16, 45.09it/s]Training CobwebTree:  16%|        | 1147/7317 [00:22<02:17, 44.97it/s]Training CobwebTree:  16%|        | 1152/7317 [00:22<02:22, 43.12it/s]Training CobwebTree:  16%|        | 1157/7317 [00:22<02:23, 43.07it/s]Training CobwebTree:  16%|        | 1162/7317 [00:22<02:21, 43.44it/s]Training CobwebTree:  16%|        | 1167/7317 [00:22<02:22, 43.16it/s]Training CobwebTree:  16%|        | 1172/7317 [00:23<02:17, 44.64it/s]Training CobwebTree:  16%|        | 1177/7317 [00:23<02:15, 45.22it/s]Training CobwebTree:  16%|        | 1182/7317 [00:23<02:15, 45.40it/s]Training CobwebTree:  16%|        | 1187/7317 [00:23<02:12, 46.25it/s]Training CobwebTree:  16%|        | 1192/7317 [00:23<02:17, 44.47it/s]Training CobwebTree:  16%|        | 1197/7317 [00:23<02:20, 43.58it/s]Training CobwebTree:  16%|        | 1202/7317 [00:23<02:21, 43.07it/s]Training CobwebTree:  16%|        | 1207/7317 [00:23<02:22, 42.88it/s]Training CobwebTree:  17%|        | 1212/7317 [00:23<02:21, 43.17it/s]Training CobwebTree:  17%|        | 1217/7317 [00:24<02:24, 42.26it/s]Training CobwebTree:  17%|        | 1222/7317 [00:24<02:23, 42.55it/s]Training CobwebTree:  17%|        | 1227/7317 [00:24<02:21, 43.17it/s]Training CobwebTree:  17%|        | 1232/7317 [00:24<02:21, 42.98it/s]Training CobwebTree:  17%|        | 1237/7317 [00:24<02:17, 44.12it/s]Training CobwebTree:  17%|        | 1242/7317 [00:24<02:22, 42.54it/s]Training CobwebTree:  17%|        | 1247/7317 [00:24<02:20, 43.12it/s]Training CobwebTree:  17%|        | 1252/7317 [00:24<02:21, 42.79it/s]Training CobwebTree:  17%|        | 1257/7317 [00:25<02:22, 42.49it/s]Training CobwebTree:  17%|        | 1262/7317 [00:25<02:17, 44.05it/s]Training CobwebTree:  17%|        | 1267/7317 [00:25<02:19, 43.27it/s]Training CobwebTree:  17%|        | 1273/7317 [00:25<02:13, 45.13it/s]Training CobwebTree:  17%|        | 1278/7317 [00:25<02:13, 45.26it/s]Training CobwebTree:  18%|        | 1283/7317 [00:25<02:10, 46.36it/s]Training CobwebTree:  18%|        | 1288/7317 [00:25<02:12, 45.33it/s]Training CobwebTree:  18%|        | 1293/7317 [00:25<02:20, 42.86it/s]Training CobwebTree:  18%|        | 1298/7317 [00:25<02:22, 42.30it/s]Training CobwebTree:  18%|        | 1303/7317 [00:26<02:22, 42.14it/s]Training CobwebTree:  18%|        | 1308/7317 [00:26<02:16, 43.93it/s]Training CobwebTree:  18%|        | 1313/7317 [00:26<02:22, 42.22it/s]Training CobwebTree:  18%|        | 1318/7317 [00:26<02:20, 42.66it/s]Training CobwebTree:  18%|        | 1323/7317 [00:26<02:28, 40.43it/s]Training CobwebTree:  18%|        | 1328/7317 [00:26<02:27, 40.47it/s]Training CobwebTree:  18%|        | 1333/7317 [00:26<02:32, 39.28it/s]Training CobwebTree:  18%|        | 1338/7317 [00:26<02:22, 41.82it/s]Training CobwebTree:  18%|        | 1343/7317 [00:27<02:21, 42.27it/s]Training CobwebTree:  18%|        | 1348/7317 [00:27<02:22, 41.93it/s]Training CobwebTree:  18%|        | 1353/7317 [00:27<02:19, 42.61it/s]Training CobwebTree:  19%|        | 1358/7317 [00:27<02:21, 42.24it/s]Training CobwebTree:  19%|        | 1363/7317 [00:27<02:18, 43.00it/s]Training CobwebTree:  19%|        | 1368/7317 [00:27<02:24, 41.28it/s]Training CobwebTree:  19%|        | 1373/7317 [00:27<02:20, 42.24it/s]Training CobwebTree:  19%|        | 1378/7317 [00:27<02:22, 41.67it/s]Training CobwebTree:  19%|        | 1383/7317 [00:27<02:20, 42.12it/s]Training CobwebTree:  19%|        | 1388/7317 [00:28<02:17, 43.00it/s]Training CobwebTree:  19%|        | 1393/7317 [00:28<02:14, 44.11it/s]Training CobwebTree:  19%|        | 1398/7317 [00:28<02:13, 44.50it/s]Training CobwebTree:  19%|        | 1403/7317 [00:28<02:18, 42.63it/s]Training CobwebTree:  19%|        | 1408/7317 [00:28<02:13, 44.34it/s]Training CobwebTree:  19%|        | 1413/7317 [00:28<02:16, 43.14it/s]Training CobwebTree:  19%|        | 1418/7317 [00:28<02:23, 41.02it/s]Training CobwebTree:  19%|        | 1423/7317 [00:28<02:21, 41.62it/s]Training CobwebTree:  20%|        | 1428/7317 [00:29<02:22, 41.26it/s]Training CobwebTree:  20%|        | 1433/7317 [00:29<02:17, 42.92it/s]Training CobwebTree:  20%|        | 1438/7317 [00:29<02:16, 43.14it/s]Training CobwebTree:  20%|        | 1443/7317 [00:29<02:21, 41.39it/s]Training CobwebTree:  20%|        | 1448/7317 [00:29<02:20, 41.81it/s]Training CobwebTree:  20%|        | 1453/7317 [00:29<02:23, 40.94it/s]Training CobwebTree:  20%|        | 1458/7317 [00:29<02:15, 43.10it/s]Training CobwebTree:  20%|        | 1463/7317 [00:29<02:15, 43.12it/s]Training CobwebTree:  20%|        | 1468/7317 [00:29<02:19, 41.84it/s]Training CobwebTree:  20%|        | 1473/7317 [00:30<02:14, 43.33it/s]Training CobwebTree:  20%|        | 1478/7317 [00:30<02:12, 44.13it/s]Training CobwebTree:  20%|        | 1483/7317 [00:30<02:12, 43.96it/s]Training CobwebTree:  20%|        | 1488/7317 [00:30<02:10, 44.67it/s]Training CobwebTree:  20%|        | 1493/7317 [00:30<02:17, 42.40it/s]Training CobwebTree:  20%|        | 1498/7317 [00:30<02:15, 43.03it/s]Training CobwebTree:  21%|        | 1503/7317 [00:30<02:19, 41.63it/s]Training CobwebTree:  21%|        | 1508/7317 [00:30<02:18, 41.99it/s]Training CobwebTree:  21%|        | 1513/7317 [00:31<02:17, 42.29it/s]Training CobwebTree:  21%|        | 1518/7317 [00:31<02:12, 43.88it/s]Training CobwebTree:  21%|        | 1523/7317 [00:31<02:12, 43.62it/s]Training CobwebTree:  21%|        | 1528/7317 [00:31<02:15, 42.74it/s]Training CobwebTree:  21%|        | 1533/7317 [00:31<02:19, 41.52it/s]Training CobwebTree:  21%|        | 1538/7317 [00:31<02:25, 39.83it/s]Training CobwebTree:  21%|        | 1543/7317 [00:31<02:28, 38.90it/s]Training CobwebTree:  21%|        | 1547/7317 [00:31<02:33, 37.71it/s]Training CobwebTree:  21%|        | 1552/7317 [00:31<02:25, 39.66it/s]Training CobwebTree:  21%|       | 1557/7317 [00:32<02:23, 40.16it/s]Training CobwebTree:  21%|       | 1562/7317 [00:32<02:26, 39.20it/s]Training CobwebTree:  21%|       | 1568/7317 [00:32<02:14, 42.80it/s]Training CobwebTree:  21%|       | 1573/7317 [00:32<02:15, 42.45it/s]Training CobwebTree:  22%|       | 1578/7317 [00:32<02:15, 42.29it/s]Training CobwebTree:  22%|       | 1583/7317 [00:32<02:15, 42.47it/s]Training CobwebTree:  22%|       | 1588/7317 [00:32<02:15, 42.32it/s]Training CobwebTree:  22%|       | 1593/7317 [00:32<02:20, 40.75it/s]Training CobwebTree:  22%|       | 1598/7317 [00:33<02:19, 40.96it/s]Training CobwebTree:  22%|       | 1603/7317 [00:33<02:18, 41.39it/s]Training CobwebTree:  22%|       | 1608/7317 [00:33<02:19, 40.97it/s]Training CobwebTree:  22%|       | 1613/7317 [00:33<02:24, 39.61it/s]Training CobwebTree:  22%|       | 1618/7317 [00:33<02:21, 40.30it/s]Training CobwebTree:  22%|       | 1623/7317 [00:33<02:19, 40.87it/s]Training CobwebTree:  22%|       | 1628/7317 [00:33<02:15, 42.03it/s]Training CobwebTree:  22%|       | 1633/7317 [00:33<02:15, 41.82it/s]Training CobwebTree:  22%|       | 1638/7317 [00:34<02:16, 41.60it/s]Training CobwebTree:  22%|       | 1643/7317 [00:34<02:16, 41.70it/s]Training CobwebTree:  23%|       | 1649/7317 [00:34<02:08, 44.04it/s]Training CobwebTree:  23%|       | 1654/7317 [00:34<02:14, 41.99it/s]Training CobwebTree:  23%|       | 1659/7317 [00:34<02:19, 40.63it/s]Training CobwebTree:  23%|       | 1664/7317 [00:34<02:15, 41.60it/s]Training CobwebTree:  23%|       | 1669/7317 [00:34<02:12, 42.64it/s]Training CobwebTree:  23%|       | 1674/7317 [00:34<02:07, 44.27it/s]Training CobwebTree:  23%|       | 1679/7317 [00:35<02:12, 42.69it/s]Training CobwebTree:  23%|       | 1684/7317 [00:35<02:09, 43.50it/s]Training CobwebTree:  23%|       | 1689/7317 [00:35<02:10, 43.27it/s]Training CobwebTree:  23%|       | 1694/7317 [00:35<02:15, 41.58it/s]Training CobwebTree:  23%|       | 1699/7317 [00:35<02:13, 42.04it/s]Training CobwebTree:  23%|       | 1704/7317 [00:35<02:20, 40.01it/s]Training CobwebTree:  23%|       | 1709/7317 [00:35<02:18, 40.41it/s]Training CobwebTree:  23%|       | 1714/7317 [00:35<02:16, 40.96it/s]Training CobwebTree:  23%|       | 1719/7317 [00:35<02:15, 41.46it/s]Training CobwebTree:  24%|       | 1724/7317 [00:36<02:13, 41.78it/s]Training CobwebTree:  24%|       | 1729/7317 [00:36<02:14, 41.56it/s]Training CobwebTree:  24%|       | 1734/7317 [00:36<02:13, 41.90it/s]Training CobwebTree:  24%|       | 1739/7317 [00:36<02:20, 39.76it/s]Training CobwebTree:  24%|       | 1744/7317 [00:36<02:17, 40.40it/s]Training CobwebTree:  24%|       | 1749/7317 [00:36<02:18, 40.28it/s]Training CobwebTree:  24%|       | 1754/7317 [00:36<02:16, 40.77it/s]Training CobwebTree:  24%|       | 1759/7317 [00:36<02:24, 38.37it/s]Training CobwebTree:  24%|       | 1763/7317 [00:37<02:24, 38.40it/s]Training CobwebTree:  24%|       | 1767/7317 [00:37<02:25, 38.18it/s]Training CobwebTree:  24%|       | 1772/7317 [00:37<02:22, 38.94it/s]Training CobwebTree:  24%|       | 1776/7317 [00:37<02:25, 38.14it/s]Training CobwebTree:  24%|       | 1780/7317 [00:37<02:28, 37.36it/s]Training CobwebTree:  24%|       | 1785/7317 [00:37<02:20, 39.49it/s]Training CobwebTree:  24%|       | 1789/7317 [00:37<02:19, 39.59it/s]Training CobwebTree:  25%|       | 1793/7317 [00:37<02:20, 39.44it/s]Training CobwebTree:  25%|       | 1798/7317 [00:37<02:17, 40.25it/s]Training CobwebTree:  25%|       | 1803/7317 [00:38<02:20, 39.34it/s]Training CobwebTree:  25%|       | 1807/7317 [00:38<02:22, 38.61it/s]Training CobwebTree:  25%|       | 1811/7317 [00:38<02:24, 38.18it/s]Training CobwebTree:  25%|       | 1815/7317 [00:38<02:29, 36.73it/s]Training CobwebTree:  25%|       | 1819/7317 [00:38<02:32, 36.03it/s]Training CobwebTree:  25%|       | 1823/7317 [00:38<02:28, 36.88it/s]Training CobwebTree:  25%|       | 1828/7317 [00:38<02:25, 37.80it/s]Training CobwebTree:  25%|       | 1832/7317 [00:38<02:29, 36.69it/s]Training CobwebTree:  25%|       | 1836/7317 [00:39<02:25, 37.57it/s]Training CobwebTree:  25%|       | 1840/7317 [00:39<02:24, 37.87it/s]Training CobwebTree:  25%|       | 1844/7317 [00:39<02:27, 37.11it/s]Training CobwebTree:  25%|       | 1849/7317 [00:39<02:22, 38.43it/s]Training CobwebTree:  25%|       | 1854/7317 [00:39<02:15, 40.21it/s]Training CobwebTree:  25%|       | 1859/7317 [00:39<02:15, 40.14it/s]Training CobwebTree:  25%|       | 1864/7317 [00:39<02:18, 39.48it/s]Training CobwebTree:  26%|       | 1868/7317 [00:39<02:23, 38.07it/s]Training CobwebTree:  26%|       | 1872/7317 [00:39<02:23, 38.00it/s]Training CobwebTree:  26%|       | 1877/7317 [00:40<02:22, 38.14it/s]Training CobwebTree:  26%|       | 1882/7317 [00:40<02:16, 39.73it/s]Training CobwebTree:  26%|       | 1886/7317 [00:40<02:18, 39.35it/s]Training CobwebTree:  26%|       | 1891/7317 [00:40<02:14, 40.33it/s]Training CobwebTree:  26%|       | 1896/7317 [00:40<02:14, 40.27it/s]Training CobwebTree:  26%|       | 1901/7317 [00:40<02:15, 40.06it/s]Training CobwebTree:  26%|       | 1906/7317 [00:40<02:16, 39.69it/s]Training CobwebTree:  26%|       | 1911/7317 [00:40<02:16, 39.61it/s]Training CobwebTree:  26%|       | 1916/7317 [00:41<02:10, 41.25it/s]Training CobwebTree:  26%|       | 1921/7317 [00:41<02:14, 40.26it/s]Training CobwebTree:  26%|       | 1926/7317 [00:41<02:13, 40.38it/s]Training CobwebTree:  26%|       | 1931/7317 [00:41<02:15, 39.74it/s]Training CobwebTree:  26%|       | 1936/7317 [00:41<02:14, 39.92it/s]Training CobwebTree:  27%|       | 1941/7317 [00:41<02:17, 39.02it/s]Training CobwebTree:  27%|       | 1945/7317 [00:41<02:21, 37.99it/s]Training CobwebTree:  27%|       | 1949/7317 [00:41<02:21, 37.98it/s]Training CobwebTree:  27%|       | 1953/7317 [00:41<02:22, 37.66it/s]Training CobwebTree:  27%|       | 1958/7317 [00:42<02:20, 38.23it/s]Training CobwebTree:  27%|       | 1962/7317 [00:42<02:21, 37.72it/s]Training CobwebTree:  27%|       | 1966/7317 [00:42<02:25, 36.89it/s]Training CobwebTree:  27%|       | 1970/7317 [00:42<02:26, 36.47it/s]Training CobwebTree:  27%|       | 1974/7317 [00:42<02:25, 36.65it/s]Training CobwebTree:  27%|       | 1978/7317 [00:42<02:27, 36.31it/s]Training CobwebTree:  27%|       | 1982/7317 [00:42<02:30, 35.53it/s]Training CobwebTree:  27%|       | 1986/7317 [00:42<02:28, 35.82it/s]Training CobwebTree:  27%|       | 1990/7317 [00:43<02:25, 36.64it/s]Training CobwebTree:  27%|       | 1994/7317 [00:43<02:22, 37.47it/s]Training CobwebTree:  27%|       | 1998/7317 [00:43<02:23, 36.97it/s]Training CobwebTree:  27%|       | 2002/7317 [00:43<02:26, 36.36it/s]Training CobwebTree:  27%|       | 2006/7317 [00:43<02:27, 36.09it/s]Training CobwebTree:  27%|       | 2010/7317 [00:43<02:27, 35.91it/s]Training CobwebTree:  28%|       | 2015/7317 [00:43<02:23, 36.97it/s]Training CobwebTree:  28%|       | 2020/7317 [00:43<02:20, 37.76it/s]Training CobwebTree:  28%|       | 2024/7317 [00:43<02:28, 35.76it/s]Training CobwebTree:  28%|       | 2028/7317 [00:44<02:28, 35.55it/s]Training CobwebTree:  28%|       | 2032/7317 [00:44<02:31, 34.82it/s]Training CobwebTree:  28%|       | 2036/7317 [00:44<02:31, 34.78it/s]Training CobwebTree:  28%|       | 2040/7317 [00:44<02:35, 34.03it/s]Training CobwebTree:  28%|       | 2045/7317 [00:44<02:25, 36.17it/s]Training CobwebTree:  28%|       | 2049/7317 [00:44<02:25, 36.13it/s]Training CobwebTree:  28%|       | 2054/7317 [00:44<02:20, 37.34it/s]Training CobwebTree:  28%|       | 2059/7317 [00:44<02:18, 37.98it/s]Training CobwebTree:  28%|       | 2063/7317 [00:45<02:24, 36.30it/s]Training CobwebTree:  28%|       | 2067/7317 [00:45<02:31, 34.71it/s]Training CobwebTree:  28%|       | 2071/7317 [00:45<02:25, 36.02it/s]Training CobwebTree:  28%|       | 2075/7317 [00:45<02:24, 36.33it/s]Training CobwebTree:  28%|       | 2079/7317 [00:45<02:22, 36.88it/s]Training CobwebTree:  28%|       | 2084/7317 [00:45<02:16, 38.41it/s]Training CobwebTree:  29%|       | 2088/7317 [00:45<02:17, 37.95it/s]Training CobwebTree:  29%|       | 2093/7317 [00:45<02:11, 39.76it/s]Training CobwebTree:  29%|       | 2097/7317 [00:45<02:11, 39.74it/s]Training CobwebTree:  29%|       | 2101/7317 [00:46<02:20, 37.02it/s]Training CobwebTree:  29%|       | 2105/7317 [00:46<02:22, 36.68it/s]Training CobwebTree:  29%|       | 2109/7317 [00:46<02:21, 36.83it/s]Training CobwebTree:  29%|       | 2113/7317 [00:46<02:20, 37.08it/s]Training CobwebTree:  29%|       | 2118/7317 [00:46<02:14, 38.66it/s]Training CobwebTree:  29%|       | 2122/7317 [00:46<02:15, 38.24it/s]Training CobwebTree:  29%|       | 2126/7317 [00:46<02:15, 38.39it/s]Training CobwebTree:  29%|       | 2130/7317 [00:46<02:22, 36.50it/s]Training CobwebTree:  29%|       | 2134/7317 [00:46<02:20, 36.84it/s]Training CobwebTree:  29%|       | 2138/7317 [00:47<02:17, 37.60it/s]Training CobwebTree:  29%|       | 2142/7317 [00:47<02:16, 37.82it/s]Training CobwebTree:  29%|       | 2146/7317 [00:47<02:22, 36.26it/s]Training CobwebTree:  29%|       | 2151/7317 [00:47<02:15, 38.20it/s]Training CobwebTree:  29%|       | 2155/7317 [00:47<02:16, 37.71it/s]Training CobwebTree:  30%|       | 2160/7317 [00:47<02:13, 38.57it/s]Training CobwebTree:  30%|       | 2164/7317 [00:47<02:12, 38.93it/s]Training CobwebTree:  30%|       | 2168/7317 [00:47<02:14, 38.22it/s]Training CobwebTree:  30%|       | 2173/7317 [00:47<02:12, 38.77it/s]Training CobwebTree:  30%|       | 2177/7317 [00:48<02:18, 37.15it/s]Training CobwebTree:  30%|       | 2181/7317 [00:48<02:19, 36.75it/s]Training CobwebTree:  30%|       | 2186/7317 [00:48<02:15, 37.94it/s]Training CobwebTree:  30%|       | 2190/7317 [00:48<02:13, 38.34it/s]Training CobwebTree:  30%|       | 2195/7317 [00:48<02:10, 39.35it/s]Training CobwebTree:  30%|       | 2199/7317 [00:48<02:16, 37.59it/s]Training CobwebTree:  30%|       | 2204/7317 [00:48<02:10, 39.27it/s]Training CobwebTree:  30%|       | 2209/7317 [00:48<02:10, 39.20it/s]Training CobwebTree:  30%|       | 2213/7317 [00:48<02:16, 37.44it/s]Training CobwebTree:  30%|       | 2217/7317 [00:49<02:24, 35.31it/s]Training CobwebTree:  30%|       | 2221/7317 [00:49<02:25, 34.95it/s]Training CobwebTree:  30%|       | 2225/7317 [00:49<02:29, 33.96it/s]Training CobwebTree:  30%|       | 2229/7317 [00:49<02:23, 35.47it/s]Training CobwebTree:  31%|       | 2233/7317 [00:49<02:20, 36.25it/s]Training CobwebTree:  31%|       | 2238/7317 [00:49<02:15, 37.58it/s]Training CobwebTree:  31%|       | 2242/7317 [00:49<02:20, 36.23it/s]Training CobwebTree:  31%|       | 2246/7317 [00:49<02:20, 36.17it/s]Training CobwebTree:  31%|       | 2250/7317 [00:50<02:21, 35.86it/s]Training CobwebTree:  31%|       | 2254/7317 [00:50<02:25, 34.91it/s]Training CobwebTree:  31%|       | 2258/7317 [00:50<02:19, 36.18it/s]Training CobwebTree:  31%|       | 2262/7317 [00:50<02:26, 34.55it/s]Training CobwebTree:  31%|       | 2266/7317 [00:50<02:22, 35.34it/s]Training CobwebTree:  31%|       | 2270/7317 [00:50<02:23, 35.22it/s]Training CobwebTree:  31%|       | 2275/7317 [00:50<02:17, 36.76it/s]Training CobwebTree:  31%|       | 2279/7317 [00:50<02:16, 36.87it/s]Training CobwebTree:  31%|       | 2283/7317 [00:50<02:23, 35.06it/s]Training CobwebTree:  31%|      | 2288/7317 [00:51<02:15, 37.11it/s]Training CobwebTree:  31%|      | 2293/7317 [00:51<02:11, 38.15it/s]Training CobwebTree:  31%|      | 2297/7317 [00:51<02:15, 37.10it/s]Training CobwebTree:  31%|      | 2301/7317 [00:51<02:15, 36.94it/s]Training CobwebTree:  32%|      | 2305/7317 [00:51<02:17, 36.50it/s]Training CobwebTree:  32%|      | 2309/7317 [00:51<02:17, 36.38it/s]Training CobwebTree:  32%|      | 2313/7317 [00:51<02:21, 35.42it/s]Training CobwebTree:  32%|      | 2317/7317 [00:51<02:18, 36.13it/s]Training CobwebTree:  32%|      | 2321/7317 [00:52<02:19, 35.91it/s]Training CobwebTree:  32%|      | 2325/7317 [00:52<02:17, 36.33it/s]Training CobwebTree:  32%|      | 2330/7317 [00:52<02:12, 37.69it/s]Training CobwebTree:  32%|      | 2334/7317 [00:52<02:16, 36.64it/s]Training CobwebTree:  32%|      | 2338/7317 [00:52<02:14, 36.92it/s]Training CobwebTree:  32%|      | 2342/7317 [00:52<02:17, 36.11it/s]Training CobwebTree:  32%|      | 2346/7317 [00:52<02:15, 36.63it/s]Training CobwebTree:  32%|      | 2350/7317 [00:52<02:16, 36.44it/s]Training CobwebTree:  32%|      | 2354/7317 [00:52<02:17, 35.98it/s]Training CobwebTree:  32%|      | 2359/7317 [00:53<02:11, 37.62it/s]Training CobwebTree:  32%|      | 2363/7317 [00:53<02:10, 38.08it/s]Training CobwebTree:  32%|      | 2367/7317 [00:53<02:11, 37.64it/s]Training CobwebTree:  32%|      | 2371/7317 [00:53<02:10, 37.85it/s]Training CobwebTree:  32%|      | 2376/7317 [00:53<02:08, 38.35it/s]Training CobwebTree:  33%|      | 2380/7317 [00:53<02:07, 38.74it/s]Training CobwebTree:  33%|      | 2385/7317 [00:53<02:02, 40.30it/s]Training CobwebTree:  33%|      | 2390/7317 [00:53<02:06, 38.86it/s]Training CobwebTree:  33%|      | 2394/7317 [00:53<02:06, 38.90it/s]Training CobwebTree:  33%|      | 2399/7317 [00:54<02:04, 39.59it/s]Training CobwebTree:  33%|      | 2403/7317 [00:54<02:09, 38.08it/s]Training CobwebTree:  33%|      | 2407/7317 [00:54<02:10, 37.51it/s]Training CobwebTree:  33%|      | 2411/7317 [00:54<02:13, 36.74it/s]Training CobwebTree:  33%|      | 2415/7317 [00:54<02:10, 37.48it/s]Training CobwebTree:  33%|      | 2419/7317 [00:54<02:08, 38.13it/s]Training CobwebTree:  33%|      | 2424/7317 [00:54<02:05, 39.12it/s]Training CobwebTree:  33%|      | 2428/7317 [00:54<02:04, 39.18it/s]Training CobwebTree:  33%|      | 2432/7317 [00:54<02:08, 38.09it/s]Training CobwebTree:  33%|      | 2436/7317 [00:55<02:06, 38.50it/s]Training CobwebTree:  33%|      | 2440/7317 [00:55<02:09, 37.73it/s]Training CobwebTree:  33%|      | 2445/7317 [00:55<02:07, 38.30it/s]Training CobwebTree:  33%|      | 2449/7317 [00:55<02:12, 36.68it/s]Training CobwebTree:  34%|      | 2453/7317 [00:55<02:13, 36.48it/s]Training CobwebTree:  34%|      | 2457/7317 [00:55<02:11, 36.94it/s]Training CobwebTree:  34%|      | 2462/7317 [00:55<02:09, 37.47it/s]Training CobwebTree:  34%|      | 2466/7317 [00:55<02:07, 38.05it/s]Training CobwebTree:  34%|      | 2470/7317 [00:55<02:09, 37.37it/s]Training CobwebTree:  34%|      | 2475/7317 [00:56<02:05, 38.46it/s]Training CobwebTree:  34%|      | 2479/7317 [00:56<02:09, 37.32it/s]Training CobwebTree:  34%|      | 2483/7317 [00:56<02:09, 37.19it/s]Training CobwebTree:  34%|      | 2487/7317 [00:56<02:13, 36.29it/s]Training CobwebTree:  34%|      | 2491/7317 [00:56<02:09, 37.26it/s]Training CobwebTree:  34%|      | 2495/7317 [00:56<02:09, 37.29it/s]Training CobwebTree:  34%|      | 2499/7317 [00:56<02:08, 37.52it/s]Training CobwebTree:  34%|      | 2504/7317 [00:56<02:00, 39.96it/s]Training CobwebTree:  34%|      | 2508/7317 [00:56<02:03, 39.09it/s]Training CobwebTree:  34%|      | 2512/7317 [00:57<02:07, 37.68it/s]Training CobwebTree:  34%|      | 2516/7317 [00:57<02:09, 37.03it/s]Training CobwebTree:  34%|      | 2520/7317 [00:57<02:14, 35.63it/s]Training CobwebTree:  34%|      | 2524/7317 [00:57<02:14, 35.75it/s]Training CobwebTree:  35%|      | 2528/7317 [00:57<02:13, 35.90it/s]Training CobwebTree:  35%|      | 2533/7317 [00:57<02:06, 37.91it/s]Training CobwebTree:  35%|      | 2537/7317 [00:57<02:08, 37.07it/s]Training CobwebTree:  35%|      | 2541/7317 [00:57<02:06, 37.62it/s]Training CobwebTree:  35%|      | 2545/7317 [00:57<02:16, 34.86it/s]Training CobwebTree:  35%|      | 2549/7317 [00:58<02:15, 35.24it/s]Training CobwebTree:  35%|      | 2553/7317 [00:58<02:14, 35.33it/s]Training CobwebTree:  35%|      | 2557/7317 [00:58<02:15, 35.06it/s]Training CobwebTree:  35%|      | 2561/7317 [00:58<02:10, 36.33it/s]Training CobwebTree:  35%|      | 2566/7317 [00:58<02:06, 37.56it/s]Training CobwebTree:  35%|      | 2571/7317 [00:58<02:04, 38.23it/s]Training CobwebTree:  35%|      | 2575/7317 [00:58<02:06, 37.61it/s]Training CobwebTree:  35%|      | 2579/7317 [00:58<02:09, 36.69it/s]Training CobwebTree:  35%|      | 2584/7317 [00:59<02:03, 38.28it/s]Training CobwebTree:  35%|      | 2588/7317 [00:59<02:07, 37.22it/s]Training CobwebTree:  35%|      | 2593/7317 [00:59<02:01, 38.88it/s]Training CobwebTree:  35%|      | 2597/7317 [00:59<02:06, 37.42it/s]Training CobwebTree:  36%|      | 2601/7317 [00:59<02:08, 36.79it/s]Training CobwebTree:  36%|      | 2605/7317 [00:59<02:10, 35.97it/s]Training CobwebTree:  36%|      | 2610/7317 [00:59<02:03, 38.09it/s]Training CobwebTree:  36%|      | 2615/7317 [00:59<01:57, 40.02it/s]Training CobwebTree:  36%|      | 2620/7317 [00:59<02:01, 38.70it/s]Training CobwebTree:  36%|      | 2624/7317 [01:00<02:09, 36.36it/s]Training CobwebTree:  36%|      | 2628/7317 [01:00<02:06, 36.93it/s]Training CobwebTree:  36%|      | 2633/7317 [01:00<02:06, 36.94it/s]Training CobwebTree:  36%|      | 2637/7317 [01:00<02:12, 35.39it/s]Training CobwebTree:  36%|      | 2641/7317 [01:00<02:09, 36.09it/s]Training CobwebTree:  36%|      | 2645/7317 [01:00<02:06, 36.83it/s]Training CobwebTree:  36%|      | 2649/7317 [01:00<02:09, 36.13it/s]Training CobwebTree:  36%|      | 2653/7317 [01:00<02:06, 36.97it/s]Training CobwebTree:  36%|      | 2657/7317 [01:01<02:09, 35.88it/s]Training CobwebTree:  36%|      | 2661/7317 [01:01<02:06, 36.75it/s]Training CobwebTree:  36%|      | 2666/7317 [01:01<02:00, 38.48it/s]Training CobwebTree:  36%|      | 2670/7317 [01:01<02:01, 38.29it/s]Training CobwebTree:  37%|      | 2674/7317 [01:01<02:04, 37.34it/s]Training CobwebTree:  37%|      | 2678/7317 [01:01<02:02, 37.76it/s]Training CobwebTree:  37%|      | 2682/7317 [01:01<02:07, 36.38it/s]Training CobwebTree:  37%|      | 2686/7317 [01:01<02:10, 35.56it/s]Training CobwebTree:  37%|      | 2690/7317 [01:01<02:10, 35.37it/s]Training CobwebTree:  37%|      | 2694/7317 [01:02<02:09, 35.81it/s]Training CobwebTree:  37%|      | 2698/7317 [01:02<02:13, 34.59it/s]Training CobwebTree:  37%|      | 2702/7317 [01:02<02:13, 34.57it/s]Training CobwebTree:  37%|      | 2706/7317 [01:02<02:13, 34.66it/s]Training CobwebTree:  37%|      | 2710/7317 [01:02<02:10, 35.25it/s]Training CobwebTree:  37%|      | 2714/7317 [01:02<02:12, 34.63it/s]Training CobwebTree:  37%|      | 2718/7317 [01:02<02:11, 35.10it/s]Training CobwebTree:  37%|      | 2722/7317 [01:02<02:09, 35.40it/s]Training CobwebTree:  37%|      | 2726/7317 [01:02<02:08, 35.74it/s]Training CobwebTree:  37%|      | 2730/7317 [01:03<02:09, 35.38it/s]Training CobwebTree:  37%|      | 2735/7317 [01:03<02:01, 37.60it/s]Training CobwebTree:  37%|      | 2740/7317 [01:03<01:58, 38.72it/s]Training CobwebTree:  38%|      | 2744/7317 [01:03<02:07, 35.97it/s]Training CobwebTree:  38%|      | 2749/7317 [01:03<02:00, 37.93it/s]Training CobwebTree:  38%|      | 2753/7317 [01:03<02:00, 37.73it/s]Training CobwebTree:  38%|      | 2758/7317 [01:03<01:58, 38.34it/s]Training CobwebTree:  38%|      | 2762/7317 [01:03<01:57, 38.62it/s]Training CobwebTree:  38%|      | 2766/7317 [01:03<02:05, 36.13it/s]Training CobwebTree:  38%|      | 2770/7317 [01:04<02:08, 35.30it/s]Training CobwebTree:  38%|      | 2775/7317 [01:04<02:04, 36.34it/s]Training CobwebTree:  38%|      | 2779/7317 [01:04<02:04, 36.32it/s]Training CobwebTree:  38%|      | 2783/7317 [01:04<02:05, 36.06it/s]Training CobwebTree:  38%|      | 2787/7317 [01:04<02:04, 36.52it/s]Training CobwebTree:  38%|      | 2792/7317 [01:04<01:58, 38.18it/s]Training CobwebTree:  38%|      | 2796/7317 [01:04<01:58, 38.26it/s]Training CobwebTree:  38%|      | 2800/7317 [01:04<02:00, 37.47it/s]Training CobwebTree:  38%|      | 2804/7317 [01:05<02:04, 36.28it/s]Training CobwebTree:  38%|      | 2808/7317 [01:05<02:06, 35.55it/s]Training CobwebTree:  38%|      | 2812/7317 [01:05<02:02, 36.64it/s]Training CobwebTree:  38%|      | 2817/7317 [01:05<02:00, 37.39it/s]Training CobwebTree:  39%|      | 2821/7317 [01:05<02:00, 37.39it/s]Training CobwebTree:  39%|      | 2825/7317 [01:05<02:07, 35.37it/s]Training CobwebTree:  39%|      | 2829/7317 [01:05<02:08, 35.05it/s]Training CobwebTree:  39%|      | 2833/7317 [01:05<02:05, 35.71it/s]Training CobwebTree:  39%|      | 2837/7317 [01:05<02:02, 36.57it/s]Training CobwebTree:  39%|      | 2841/7317 [01:06<02:02, 36.56it/s]Training CobwebTree:  39%|      | 2845/7317 [01:06<02:03, 36.27it/s]Training CobwebTree:  39%|      | 2849/7317 [01:06<02:04, 35.99it/s]Training CobwebTree:  39%|      | 2853/7317 [01:06<02:05, 35.47it/s]Training CobwebTree:  39%|      | 2857/7317 [01:06<02:06, 35.39it/s]Training CobwebTree:  39%|      | 2861/7317 [01:06<02:01, 36.64it/s]Training CobwebTree:  39%|      | 2865/7317 [01:06<02:00, 36.90it/s]Training CobwebTree:  39%|      | 2869/7317 [01:06<01:58, 37.63it/s]Training CobwebTree:  39%|      | 2873/7317 [01:06<02:06, 35.23it/s]Training CobwebTree:  39%|      | 2877/7317 [01:07<02:02, 36.10it/s]Training CobwebTree:  39%|      | 2881/7317 [01:07<02:01, 36.47it/s]Training CobwebTree:  39%|      | 2885/7317 [01:07<02:08, 34.52it/s]Training CobwebTree:  39%|      | 2889/7317 [01:07<02:11, 33.64it/s]Training CobwebTree:  40%|      | 2893/7317 [01:07<02:10, 33.99it/s]Training CobwebTree:  40%|      | 2897/7317 [01:07<02:04, 35.44it/s]Training CobwebTree:  40%|      | 2901/7317 [01:07<02:08, 34.26it/s]Training CobwebTree:  40%|      | 2905/7317 [01:07<02:08, 34.41it/s]Training CobwebTree:  40%|      | 2909/7317 [01:07<02:05, 35.12it/s]Training CobwebTree:  40%|      | 2913/7317 [01:08<02:02, 35.84it/s]Training CobwebTree:  40%|      | 2917/7317 [01:08<02:02, 35.82it/s]Training CobwebTree:  40%|      | 2921/7317 [01:08<01:59, 36.80it/s]Training CobwebTree:  40%|      | 2925/7317 [01:08<01:56, 37.61it/s]Training CobwebTree:  40%|      | 2929/7317 [01:08<01:55, 38.11it/s]Training CobwebTree:  40%|      | 2934/7317 [01:08<01:50, 39.65it/s]Training CobwebTree:  40%|      | 2938/7317 [01:08<01:56, 37.58it/s]Training CobwebTree:  40%|      | 2942/7317 [01:08<01:58, 36.79it/s]Training CobwebTree:  40%|      | 2946/7317 [01:08<02:01, 35.90it/s]Training CobwebTree:  40%|      | 2950/7317 [01:09<02:07, 34.36it/s]Training CobwebTree:  40%|      | 2954/7317 [01:09<02:08, 34.01it/s]Training CobwebTree:  40%|      | 2958/7317 [01:09<02:06, 34.45it/s]Training CobwebTree:  40%|      | 2962/7317 [01:09<02:04, 35.07it/s]Training CobwebTree:  41%|      | 2966/7317 [01:09<02:04, 34.92it/s]Training CobwebTree:  41%|      | 2970/7317 [01:09<02:05, 34.62it/s]Training CobwebTree:  41%|      | 2974/7317 [01:09<02:04, 34.99it/s]Training CobwebTree:  41%|      | 2978/7317 [01:09<01:59, 36.23it/s]Training CobwebTree:  41%|      | 2982/7317 [01:10<02:03, 35.17it/s]Training CobwebTree:  41%|      | 2987/7317 [01:10<01:55, 37.53it/s]Training CobwebTree:  41%|      | 2991/7317 [01:10<01:58, 36.38it/s]Training CobwebTree:  41%|      | 2995/7317 [01:10<02:03, 35.03it/s]Training CobwebTree:  41%|      | 2999/7317 [01:10<02:05, 34.31it/s]Training CobwebTree:  41%|      | 3003/7317 [01:10<02:02, 35.08it/s]Training CobwebTree:  41%|      | 3007/7317 [01:10<02:00, 35.66it/s]Training CobwebTree:  41%|      | 3011/7317 [01:10<01:57, 36.64it/s]Training CobwebTree:  41%|      | 3016/7317 [01:10<01:55, 37.21it/s]Training CobwebTree:  41%|     | 3020/7317 [01:11<01:57, 36.64it/s]Training CobwebTree:  41%|     | 3025/7317 [01:11<01:53, 37.80it/s]Training CobwebTree:  41%|     | 3030/7317 [01:11<01:52, 38.11it/s]Training CobwebTree:  41%|     | 3034/7317 [01:11<01:55, 37.08it/s]Training CobwebTree:  42%|     | 3038/7317 [01:11<01:55, 37.05it/s]Training CobwebTree:  42%|     | 3042/7317 [01:11<01:53, 37.68it/s]Training CobwebTree:  42%|     | 3046/7317 [01:11<02:04, 34.22it/s]Training CobwebTree:  42%|     | 3050/7317 [01:11<02:01, 35.23it/s]Training CobwebTree:  42%|     | 3054/7317 [01:11<01:59, 35.61it/s]Training CobwebTree:  42%|     | 3058/7317 [01:12<01:57, 36.21it/s]Training CobwebTree:  42%|     | 3062/7317 [01:12<01:57, 36.27it/s]Training CobwebTree:  42%|     | 3066/7317 [01:12<01:55, 36.67it/s]Training CobwebTree:  42%|     | 3071/7317 [01:12<01:50, 38.45it/s]Training CobwebTree:  42%|     | 3075/7317 [01:12<01:51, 37.97it/s]Training CobwebTree:  42%|     | 3079/7317 [01:12<01:53, 37.39it/s]Training CobwebTree:  42%|     | 3083/7317 [01:12<01:53, 37.32it/s]Training CobwebTree:  42%|     | 3087/7317 [01:12<01:52, 37.67it/s]Training CobwebTree:  42%|     | 3091/7317 [01:12<01:50, 38.12it/s]Training CobwebTree:  42%|     | 3095/7317 [01:13<01:51, 37.84it/s]Training CobwebTree:  42%|     | 3099/7317 [01:13<01:51, 37.72it/s]Training CobwebTree:  42%|     | 3103/7317 [01:13<01:52, 37.37it/s]Training CobwebTree:  42%|     | 3108/7317 [01:13<01:51, 37.65it/s]Training CobwebTree:  43%|     | 3112/7317 [01:13<01:57, 35.78it/s]Training CobwebTree:  43%|     | 3116/7317 [01:13<01:54, 36.70it/s]Training CobwebTree:  43%|     | 3120/7317 [01:13<01:53, 36.87it/s]Training CobwebTree:  43%|     | 3124/7317 [01:13<01:55, 36.21it/s]Training CobwebTree:  43%|     | 3129/7317 [01:13<01:49, 38.23it/s]Training CobwebTree:  43%|     | 3133/7317 [01:14<01:50, 37.97it/s]Training CobwebTree:  43%|     | 3138/7317 [01:14<01:48, 38.54it/s]Training CobwebTree:  43%|     | 3142/7317 [01:14<01:48, 38.54it/s]Training CobwebTree:  43%|     | 3146/7317 [01:14<01:48, 38.38it/s]Training CobwebTree:  43%|     | 3150/7317 [01:14<01:49, 38.04it/s]Training CobwebTree:  43%|     | 3154/7317 [01:14<01:49, 38.18it/s]Training CobwebTree:  43%|     | 3158/7317 [01:14<01:53, 36.65it/s]Training CobwebTree:  43%|     | 3162/7317 [01:14<01:56, 35.54it/s]Training CobwebTree:  43%|     | 3166/7317 [01:14<01:53, 36.42it/s]Training CobwebTree:  43%|     | 3170/7317 [01:15<01:53, 36.51it/s]Training CobwebTree:  43%|     | 3174/7317 [01:15<01:53, 36.48it/s]Training CobwebTree:  43%|     | 3179/7317 [01:15<01:48, 38.12it/s]Training CobwebTree:  44%|     | 3183/7317 [01:15<01:51, 37.21it/s]Training CobwebTree:  44%|     | 3187/7317 [01:15<01:48, 37.95it/s]Training CobwebTree:  44%|     | 3191/7317 [01:15<01:52, 36.77it/s]Training CobwebTree:  44%|     | 3195/7317 [01:15<01:52, 36.61it/s]Training CobwebTree:  44%|     | 3199/7317 [01:15<01:57, 34.96it/s]Training CobwebTree:  44%|     | 3203/7317 [01:15<01:55, 35.77it/s]Training CobwebTree:  44%|     | 3207/7317 [01:16<01:53, 36.09it/s]Training CobwebTree:  44%|     | 3211/7317 [01:16<01:51, 36.69it/s]Training CobwebTree:  44%|     | 3215/7317 [01:16<01:50, 37.07it/s]Training CobwebTree:  44%|     | 3219/7317 [01:16<01:56, 35.06it/s]Training CobwebTree:  44%|     | 3223/7317 [01:16<01:54, 35.66it/s]Training CobwebTree:  44%|     | 3227/7317 [01:16<01:59, 34.29it/s]Training CobwebTree:  44%|     | 3231/7317 [01:16<02:02, 33.23it/s]Training CobwebTree:  44%|     | 3236/7317 [01:16<01:56, 35.13it/s]Training CobwebTree:  44%|     | 3241/7317 [01:17<01:50, 36.88it/s]Training CobwebTree:  44%|     | 3245/7317 [01:17<01:50, 36.86it/s]Training CobwebTree:  44%|     | 3249/7317 [01:17<01:56, 34.86it/s]Training CobwebTree:  44%|     | 3253/7317 [01:17<01:54, 35.43it/s]Training CobwebTree:  45%|     | 3258/7317 [01:17<01:47, 37.67it/s]Training CobwebTree:  45%|     | 3263/7317 [01:17<01:46, 38.20it/s]Training CobwebTree:  45%|     | 3267/7317 [01:17<01:45, 38.42it/s]Training CobwebTree:  45%|     | 3271/7317 [01:17<01:48, 37.19it/s]Training CobwebTree:  45%|     | 3275/7317 [01:17<01:52, 35.98it/s]Training CobwebTree:  45%|     | 3279/7317 [01:18<01:51, 36.23it/s]Training CobwebTree:  45%|     | 3283/7317 [01:18<01:55, 34.97it/s]Training CobwebTree:  45%|     | 3287/7317 [01:18<01:55, 34.82it/s]Training CobwebTree:  45%|     | 3291/7317 [01:18<01:59, 33.79it/s]Training CobwebTree:  45%|     | 3295/7317 [01:18<01:57, 34.29it/s]Training CobwebTree:  45%|     | 3299/7317 [01:18<01:56, 34.37it/s]Training CobwebTree:  45%|     | 3303/7317 [01:18<01:52, 35.57it/s]Training CobwebTree:  45%|     | 3307/7317 [01:18<01:59, 33.57it/s]Training CobwebTree:  45%|     | 3311/7317 [01:19<02:02, 32.83it/s]Training CobwebTree:  45%|     | 3315/7317 [01:19<02:10, 30.77it/s]Training CobwebTree:  45%|     | 3319/7317 [01:19<02:06, 31.50it/s]Training CobwebTree:  45%|     | 3324/7317 [01:19<01:56, 34.28it/s]Training CobwebTree:  45%|     | 3328/7317 [01:19<01:57, 34.03it/s]Training CobwebTree:  46%|     | 3332/7317 [01:19<01:53, 35.09it/s]Training CobwebTree:  46%|     | 3336/7317 [01:19<01:53, 35.12it/s]Training CobwebTree:  46%|     | 3341/7317 [01:19<01:47, 37.13it/s]Training CobwebTree:  46%|     | 3345/7317 [01:20<01:47, 36.87it/s]Training CobwebTree:  46%|     | 3349/7317 [01:20<01:47, 37.02it/s]Training CobwebTree:  46%|     | 3353/7317 [01:20<01:50, 35.85it/s]Training CobwebTree:  46%|     | 3357/7317 [01:20<01:51, 35.49it/s]Training CobwebTree:  46%|     | 3362/7317 [01:20<01:47, 36.67it/s]Training CobwebTree:  46%|     | 3366/7317 [01:20<01:47, 36.83it/s]Training CobwebTree:  46%|     | 3370/7317 [01:20<01:48, 36.51it/s]Training CobwebTree:  46%|     | 3374/7317 [01:20<01:53, 34.87it/s]Training CobwebTree:  46%|     | 3379/7317 [01:20<01:46, 36.87it/s]Training CobwebTree:  46%|     | 3383/7317 [01:21<01:47, 36.65it/s]Training CobwebTree:  46%|     | 3387/7317 [01:21<01:51, 35.32it/s]Training CobwebTree:  46%|     | 3391/7317 [01:21<01:48, 36.05it/s]Training CobwebTree:  46%|     | 3395/7317 [01:21<01:51, 35.30it/s]Training CobwebTree:  46%|     | 3399/7317 [01:21<01:55, 33.88it/s]Training CobwebTree:  47%|     | 3403/7317 [01:21<01:50, 35.46it/s]Training CobwebTree:  47%|     | 3407/7317 [01:21<01:51, 34.98it/s]Training CobwebTree:  47%|     | 3412/7317 [01:21<01:44, 37.54it/s]Training CobwebTree:  47%|     | 3417/7317 [01:21<01:39, 39.04it/s]Training CobwebTree:  47%|     | 3421/7317 [01:22<01:44, 37.30it/s]Training CobwebTree:  47%|     | 3425/7317 [01:22<01:43, 37.77it/s]Training CobwebTree:  47%|     | 3429/7317 [01:22<01:44, 37.33it/s]Training CobwebTree:  47%|     | 3433/7317 [01:22<01:45, 36.66it/s]Training CobwebTree:  47%|     | 3437/7317 [01:22<01:45, 36.68it/s]Training CobwebTree:  47%|     | 3441/7317 [01:22<01:43, 37.35it/s]Training CobwebTree:  47%|     | 3445/7317 [01:22<01:42, 37.74it/s]Training CobwebTree:  47%|     | 3449/7317 [01:22<01:44, 36.96it/s]Training CobwebTree:  47%|     | 3453/7317 [01:22<01:46, 36.19it/s]Training CobwebTree:  47%|     | 3457/7317 [01:23<01:48, 35.66it/s]Training CobwebTree:  47%|     | 3461/7317 [01:23<01:49, 35.21it/s]Training CobwebTree:  47%|     | 3465/7317 [01:23<01:48, 35.64it/s]Training CobwebTree:  47%|     | 3469/7317 [01:23<01:46, 36.01it/s]Training CobwebTree:  47%|     | 3474/7317 [01:23<01:44, 36.87it/s]Training CobwebTree:  48%|     | 3479/7317 [01:23<01:40, 38.06it/s]Training CobwebTree:  48%|     | 3483/7317 [01:23<01:41, 37.89it/s]Training CobwebTree:  48%|     | 3488/7317 [01:23<01:41, 37.58it/s]Training CobwebTree:  48%|     | 3492/7317 [01:24<01:43, 37.00it/s]Training CobwebTree:  48%|     | 3496/7317 [01:24<01:46, 35.92it/s]Training CobwebTree:  48%|     | 3500/7317 [01:24<01:47, 35.48it/s]Training CobwebTree:  48%|     | 3504/7317 [01:24<01:46, 35.65it/s]Training CobwebTree:  48%|     | 3508/7317 [01:24<01:46, 35.63it/s]Training CobwebTree:  48%|     | 3512/7317 [01:24<01:45, 36.04it/s]Training CobwebTree:  48%|     | 3516/7317 [01:24<01:51, 34.23it/s]Training CobwebTree:  48%|     | 3520/7317 [01:24<01:53, 33.42it/s]Training CobwebTree:  48%|     | 3524/7317 [01:24<01:49, 34.59it/s]Training CobwebTree:  48%|     | 3528/7317 [01:25<01:48, 34.92it/s]Training CobwebTree:  48%|     | 3532/7317 [01:25<01:49, 34.43it/s]Training CobwebTree:  48%|     | 3536/7317 [01:25<01:51, 34.06it/s]Training CobwebTree:  48%|     | 3540/7317 [01:25<01:55, 32.72it/s]Training CobwebTree:  48%|     | 3544/7317 [01:25<01:50, 34.07it/s]Training CobwebTree:  48%|     | 3548/7317 [01:25<01:49, 34.43it/s]Training CobwebTree:  49%|     | 3552/7317 [01:25<01:48, 34.72it/s]Training CobwebTree:  49%|     | 3556/7317 [01:25<01:45, 35.60it/s]Training CobwebTree:  49%|     | 3560/7317 [01:25<01:45, 35.59it/s]Training CobwebTree:  49%|     | 3564/7317 [01:26<01:43, 36.32it/s]Training CobwebTree:  49%|     | 3569/7317 [01:26<01:40, 37.43it/s]Training CobwebTree:  49%|     | 3573/7317 [01:26<01:43, 36.22it/s]Training CobwebTree:  49%|     | 3577/7317 [01:26<01:42, 36.55it/s]Training CobwebTree:  49%|     | 3581/7317 [01:26<01:42, 36.38it/s]Training CobwebTree:  49%|     | 3585/7317 [01:26<01:48, 34.35it/s]Training CobwebTree:  49%|     | 3590/7317 [01:26<01:41, 36.68it/s]Training CobwebTree:  49%|     | 3594/7317 [01:26<01:45, 35.16it/s]Training CobwebTree:  49%|     | 3598/7317 [01:27<01:44, 35.67it/s]Training CobwebTree:  49%|     | 3602/7317 [01:27<01:45, 35.23it/s]Training CobwebTree:  49%|     | 3606/7317 [01:27<01:43, 35.91it/s]Training CobwebTree:  49%|     | 3610/7317 [01:27<01:42, 36.23it/s]Training CobwebTree:  49%|     | 3614/7317 [01:27<01:40, 36.84it/s]Training CobwebTree:  49%|     | 3618/7317 [01:27<01:45, 35.13it/s]Training CobwebTree:  50%|     | 3622/7317 [01:27<01:45, 34.97it/s]Training CobwebTree:  50%|     | 3626/7317 [01:27<01:47, 34.46it/s]Training CobwebTree:  50%|     | 3630/7317 [01:27<01:50, 33.24it/s]Training CobwebTree:  50%|     | 3634/7317 [01:28<01:46, 34.54it/s]Training CobwebTree:  50%|     | 3638/7317 [01:28<01:42, 35.97it/s]Training CobwebTree:  50%|     | 3643/7317 [01:28<01:35, 38.42it/s]Training CobwebTree:  50%|     | 3647/7317 [01:28<01:36, 38.17it/s]Training CobwebTree:  50%|     | 3651/7317 [01:28<01:41, 36.29it/s]Training CobwebTree:  50%|     | 3655/7317 [01:28<01:39, 36.76it/s]Training CobwebTree:  50%|     | 3659/7317 [01:28<01:43, 35.48it/s]Training CobwebTree:  50%|     | 3663/7317 [01:28<01:43, 35.23it/s]Training CobwebTree:  50%|     | 3667/7317 [01:28<01:41, 36.13it/s]Training CobwebTree:  50%|     | 3672/7317 [01:29<01:35, 38.13it/s]Training CobwebTree:  50%|     | 3677/7317 [01:29<01:35, 38.18it/s]Training CobwebTree:  50%|     | 3681/7317 [01:29<01:34, 38.45it/s]Training CobwebTree:  50%|     | 3685/7317 [01:29<01:36, 37.55it/s]Training CobwebTree:  50%|     | 3689/7317 [01:29<01:35, 37.81it/s]Training CobwebTree:  50%|     | 3693/7317 [01:29<01:37, 37.27it/s]Training CobwebTree:  51%|     | 3697/7317 [01:29<01:39, 36.43it/s]Training CobwebTree:  51%|     | 3701/7317 [01:29<01:44, 34.56it/s]Training CobwebTree:  51%|     | 3705/7317 [01:30<01:42, 35.33it/s]Training CobwebTree:  51%|     | 3709/7317 [01:30<01:38, 36.59it/s]Training CobwebTree:  51%|     | 3713/7317 [01:30<01:38, 36.63it/s]Training CobwebTree:  51%|     | 3717/7317 [01:30<01:40, 35.73it/s]Training CobwebTree:  51%|     | 3721/7317 [01:30<01:38, 36.47it/s]Training CobwebTree:  51%|     | 3725/7317 [01:30<01:40, 35.83it/s]Training CobwebTree:  51%|     | 3729/7317 [01:30<01:40, 35.85it/s]Training CobwebTree:  51%|     | 3734/7317 [01:30<01:34, 37.86it/s]Training CobwebTree:  51%|     | 3738/7317 [01:30<01:35, 37.55it/s]Training CobwebTree:  51%|     | 3742/7317 [01:30<01:34, 37.91it/s]Training CobwebTree:  51%|     | 3746/7317 [01:31<01:36, 37.12it/s]Training CobwebTree:  51%|    | 3750/7317 [01:31<01:35, 37.48it/s]Training CobwebTree:  51%|    | 3754/7317 [01:31<01:43, 34.49it/s]Training CobwebTree:  51%|    | 3758/7317 [01:31<01:41, 34.94it/s]Training CobwebTree:  51%|    | 3762/7317 [01:31<01:41, 35.10it/s]Training CobwebTree:  51%|    | 3767/7317 [01:31<01:38, 36.12it/s]Training CobwebTree:  52%|    | 3771/7317 [01:31<01:39, 35.57it/s]Training CobwebTree:  52%|    | 3775/7317 [01:31<01:38, 35.89it/s]Training CobwebTree:  52%|    | 3779/7317 [01:32<01:41, 34.74it/s]Training CobwebTree:  52%|    | 3783/7317 [01:32<01:44, 33.81it/s]Training CobwebTree:  52%|    | 3787/7317 [01:32<01:42, 34.34it/s]Training CobwebTree:  52%|    | 3792/7317 [01:32<01:34, 37.24it/s]Training CobwebTree:  52%|    | 3796/7317 [01:32<01:37, 36.01it/s]Training CobwebTree:  52%|    | 3800/7317 [01:32<01:36, 36.62it/s]Training CobwebTree:  52%|    | 3804/7317 [01:32<01:37, 36.00it/s]Training CobwebTree:  52%|    | 3808/7317 [01:32<01:41, 34.52it/s]Training CobwebTree:  52%|    | 3812/7317 [01:32<01:41, 34.65it/s]Training CobwebTree:  52%|    | 3816/7317 [01:33<01:39, 35.16it/s]Training CobwebTree:  52%|    | 3820/7317 [01:33<01:42, 34.07it/s]Training CobwebTree:  52%|    | 3824/7317 [01:33<01:43, 33.79it/s]Training CobwebTree:  52%|    | 3828/7317 [01:33<01:40, 34.83it/s]Training CobwebTree:  52%|    | 3832/7317 [01:33<01:40, 34.85it/s]Training CobwebTree:  52%|    | 3836/7317 [01:33<01:36, 36.11it/s]Training CobwebTree:  52%|    | 3840/7317 [01:33<01:37, 35.75it/s]Training CobwebTree:  53%|    | 3844/7317 [01:33<01:36, 35.93it/s]Training CobwebTree:  53%|    | 3848/7317 [01:34<01:36, 35.98it/s]Training CobwebTree:  53%|    | 3852/7317 [01:34<01:37, 35.55it/s]Training CobwebTree:  53%|    | 3856/7317 [01:34<01:39, 34.61it/s]Training CobwebTree:  53%|    | 3860/7317 [01:34<01:40, 34.40it/s]Training CobwebTree:  53%|    | 3864/7317 [01:34<01:40, 34.49it/s]Training CobwebTree:  53%|    | 3868/7317 [01:34<01:39, 34.63it/s]Training CobwebTree:  53%|    | 3872/7317 [01:34<01:37, 35.40it/s]Training CobwebTree:  53%|    | 3876/7317 [01:34<01:36, 35.68it/s]Training CobwebTree:  53%|    | 3880/7317 [01:34<01:34, 36.21it/s]Training CobwebTree:  53%|    | 3884/7317 [01:35<01:33, 36.62it/s]Training CobwebTree:  53%|    | 3888/7317 [01:35<01:34, 36.20it/s]Training CobwebTree:  53%|    | 3892/7317 [01:35<01:37, 35.15it/s]Training CobwebTree:  53%|    | 3896/7317 [01:35<01:35, 35.88it/s]Training CobwebTree:  53%|    | 3901/7317 [01:35<01:33, 36.67it/s]Training CobwebTree:  53%|    | 3905/7317 [01:35<01:33, 36.48it/s]Training CobwebTree:  53%|    | 3910/7317 [01:35<01:29, 38.02it/s]Training CobwebTree:  53%|    | 3914/7317 [01:35<01:34, 36.07it/s]Training CobwebTree:  54%|    | 3918/7317 [01:35<01:35, 35.63it/s]Training CobwebTree:  54%|    | 3922/7317 [01:36<01:34, 36.09it/s]Training CobwebTree:  54%|    | 3926/7317 [01:36<01:34, 36.07it/s]Training CobwebTree:  54%|    | 3930/7317 [01:36<01:32, 36.47it/s]Training CobwebTree:  54%|    | 3934/7317 [01:36<01:32, 36.74it/s]Training CobwebTree:  54%|    | 3938/7317 [01:36<01:33, 36.18it/s]Training CobwebTree:  54%|    | 3943/7317 [01:36<01:28, 37.93it/s]Training CobwebTree:  54%|    | 3947/7317 [01:36<01:29, 37.74it/s]Training CobwebTree:  54%|    | 3951/7317 [01:36<01:34, 35.67it/s]Training CobwebTree:  54%|    | 3955/7317 [01:36<01:32, 36.46it/s]Training CobwebTree:  54%|    | 3959/7317 [01:37<01:31, 36.70it/s]Training CobwebTree:  54%|    | 3963/7317 [01:37<01:32, 36.11it/s]Training CobwebTree:  54%|    | 3967/7317 [01:37<01:35, 35.02it/s]Training CobwebTree:  54%|    | 3971/7317 [01:37<01:39, 33.58it/s]Training CobwebTree:  54%|    | 3975/7317 [01:37<01:39, 33.65it/s]Training CobwebTree:  54%|    | 3979/7317 [01:37<01:39, 33.59it/s]Training CobwebTree:  54%|    | 3983/7317 [01:37<01:37, 34.25it/s]Training CobwebTree:  54%|    | 3987/7317 [01:37<01:34, 35.35it/s]Training CobwebTree:  55%|    | 3991/7317 [01:38<01:33, 35.41it/s]Training CobwebTree:  55%|    | 3995/7317 [01:38<01:33, 35.45it/s]Training CobwebTree:  55%|    | 3999/7317 [01:38<01:33, 35.49it/s]Training CobwebTree:  55%|    | 4003/7317 [01:38<01:34, 35.00it/s]Training CobwebTree:  55%|    | 4007/7317 [01:38<01:33, 35.35it/s]Training CobwebTree:  55%|    | 4011/7317 [01:38<01:30, 36.48it/s]Training CobwebTree:  55%|    | 4015/7317 [01:38<01:32, 35.70it/s]Training CobwebTree:  55%|    | 4019/7317 [01:38<01:33, 35.34it/s]Training CobwebTree:  55%|    | 4024/7317 [01:38<01:29, 36.86it/s]Training CobwebTree:  55%|    | 4029/7317 [01:39<01:23, 39.46it/s]Training CobwebTree:  55%|    | 4033/7317 [01:39<01:28, 37.02it/s]Training CobwebTree:  55%|    | 4038/7317 [01:39<01:26, 38.06it/s]Training CobwebTree:  55%|    | 4042/7317 [01:39<01:30, 36.34it/s]Training CobwebTree:  55%|    | 4046/7317 [01:39<01:31, 35.70it/s]Training CobwebTree:  55%|    | 4050/7317 [01:39<01:31, 35.71it/s]Training CobwebTree:  55%|    | 4054/7317 [01:39<01:29, 36.46it/s]Training CobwebTree:  55%|    | 4058/7317 [01:39<01:32, 35.30it/s]Training CobwebTree:  56%|    | 4062/7317 [01:39<01:30, 35.81it/s]Training CobwebTree:  56%|    | 4067/7317 [01:40<01:26, 37.58it/s]Training CobwebTree:  56%|    | 4071/7317 [01:40<01:28, 36.50it/s]Training CobwebTree:  56%|    | 4075/7317 [01:40<01:35, 34.02it/s]Training CobwebTree:  56%|    | 4079/7317 [01:40<01:33, 34.68it/s]Training CobwebTree:  56%|    | 4083/7317 [01:40<01:35, 33.88it/s]Training CobwebTree:  56%|    | 4088/7317 [01:40<01:31, 35.24it/s]Training CobwebTree:  56%|    | 4092/7317 [01:40<01:32, 34.69it/s]Training CobwebTree:  56%|    | 4096/7317 [01:40<01:30, 35.42it/s]Training CobwebTree:  56%|    | 4100/7317 [01:41<01:32, 34.94it/s]Training CobwebTree:  56%|    | 4104/7317 [01:41<01:31, 35.22it/s]Training CobwebTree:  56%|    | 4108/7317 [01:41<01:37, 33.01it/s]Training CobwebTree:  56%|    | 4112/7317 [01:41<01:35, 33.73it/s]Training CobwebTree:  56%|    | 4116/7317 [01:41<01:32, 34.48it/s]Training CobwebTree:  56%|    | 4120/7317 [01:41<01:30, 35.16it/s]Training CobwebTree:  56%|    | 4124/7317 [01:41<01:29, 35.55it/s]Training CobwebTree:  56%|    | 4128/7317 [01:41<01:28, 35.92it/s]Training CobwebTree:  56%|    | 4132/7317 [01:41<01:31, 34.94it/s]Training CobwebTree:  57%|    | 4136/7317 [01:42<01:31, 34.86it/s]Training CobwebTree:  57%|    | 4140/7317 [01:42<01:32, 34.35it/s]Training CobwebTree:  57%|    | 4144/7317 [01:42<01:33, 33.86it/s]Training CobwebTree:  57%|    | 4148/7317 [01:42<01:35, 33.24it/s]Training CobwebTree:  57%|    | 4152/7317 [01:42<01:31, 34.42it/s]Training CobwebTree:  57%|    | 4156/7317 [01:42<01:29, 35.18it/s]Training CobwebTree:  57%|    | 4160/7317 [01:42<01:28, 35.86it/s]Training CobwebTree:  57%|    | 4164/7317 [01:42<01:30, 34.84it/s]Training CobwebTree:  57%|    | 4168/7317 [01:43<01:29, 35.20it/s]Training CobwebTree:  57%|    | 4172/7317 [01:43<01:28, 35.36it/s]Training CobwebTree:  57%|    | 4176/7317 [01:43<01:27, 36.00it/s]Training CobwebTree:  57%|    | 4180/7317 [01:43<01:24, 37.04it/s]Training CobwebTree:  57%|    | 4184/7317 [01:43<01:28, 35.48it/s]Training CobwebTree:  57%|    | 4188/7317 [01:43<01:27, 35.63it/s]Training CobwebTree:  57%|    | 4192/7317 [01:43<01:26, 36.02it/s]Training CobwebTree:  57%|    | 4197/7317 [01:43<01:24, 36.94it/s]Training CobwebTree:  57%|    | 4201/7317 [01:43<01:23, 37.19it/s]Training CobwebTree:  57%|    | 4206/7317 [01:44<01:22, 37.84it/s]Training CobwebTree:  58%|    | 4211/7317 [01:44<01:21, 38.19it/s]Training CobwebTree:  58%|    | 4215/7317 [01:44<01:21, 38.29it/s]Training CobwebTree:  58%|    | 4219/7317 [01:44<01:22, 37.63it/s]Training CobwebTree:  58%|    | 4223/7317 [01:44<01:28, 34.85it/s]Training CobwebTree:  58%|    | 4227/7317 [01:44<01:28, 34.96it/s]Training CobwebTree:  58%|    | 4232/7317 [01:44<01:23, 36.94it/s]Training CobwebTree:  58%|    | 4236/7317 [01:44<01:28, 35.01it/s]Training CobwebTree:  58%|    | 4240/7317 [01:44<01:25, 36.02it/s]Training CobwebTree:  58%|    | 4244/7317 [01:45<01:24, 36.45it/s]Training CobwebTree:  58%|    | 4249/7317 [01:45<01:21, 37.48it/s]Training CobwebTree:  58%|    | 4253/7317 [01:45<01:20, 37.99it/s]Training CobwebTree:  58%|    | 4257/7317 [01:45<01:20, 37.78it/s]Training CobwebTree:  58%|    | 4261/7317 [01:45<01:21, 37.65it/s]Training CobwebTree:  58%|    | 4265/7317 [01:45<01:20, 37.93it/s]Training CobwebTree:  58%|    | 4270/7317 [01:45<01:20, 37.74it/s]Training CobwebTree:  58%|    | 4274/7317 [01:45<01:26, 35.13it/s]Training CobwebTree:  58%|    | 4278/7317 [01:46<01:27, 34.63it/s]Training CobwebTree:  59%|    | 4282/7317 [01:46<01:27, 34.55it/s]Training CobwebTree:  59%|    | 4286/7317 [01:46<01:27, 34.80it/s]Training CobwebTree:  59%|    | 4290/7317 [01:46<01:25, 35.43it/s]Training CobwebTree:  59%|    | 4294/7317 [01:46<01:26, 34.94it/s]Training CobwebTree:  59%|    | 4298/7317 [01:46<01:29, 33.58it/s]Training CobwebTree:  59%|    | 4302/7317 [01:46<01:26, 34.88it/s]Training CobwebTree:  59%|    | 4306/7317 [01:46<01:26, 34.79it/s]Training CobwebTree:  59%|    | 4310/7317 [01:46<01:26, 34.58it/s]Training CobwebTree:  59%|    | 4314/7317 [01:47<01:29, 33.63it/s]Training CobwebTree:  59%|    | 4318/7317 [01:47<01:30, 33.00it/s]Training CobwebTree:  59%|    | 4322/7317 [01:47<01:26, 34.78it/s]Training CobwebTree:  59%|    | 4326/7317 [01:47<01:27, 34.31it/s]Training CobwebTree:  59%|    | 4330/7317 [01:47<01:26, 34.45it/s]Training CobwebTree:  59%|    | 4334/7317 [01:47<01:25, 34.84it/s]Training CobwebTree:  59%|    | 4338/7317 [01:47<01:24, 35.38it/s]Training CobwebTree:  59%|    | 4342/7317 [01:47<01:22, 36.21it/s]Training CobwebTree:  59%|    | 4346/7317 [01:47<01:26, 34.26it/s]Training CobwebTree:  59%|    | 4350/7317 [01:48<01:25, 34.73it/s]Training CobwebTree:  60%|    | 4354/7317 [01:48<01:23, 35.62it/s]Training CobwebTree:  60%|    | 4358/7317 [01:48<01:22, 35.88it/s]Training CobwebTree:  60%|    | 4362/7317 [01:48<01:23, 35.40it/s]Training CobwebTree:  60%|    | 4366/7317 [01:48<01:20, 36.53it/s]Training CobwebTree:  60%|    | 4371/7317 [01:48<01:17, 38.10it/s]Training CobwebTree:  60%|    | 4375/7317 [01:48<01:18, 37.31it/s]Training CobwebTree:  60%|    | 4379/7317 [01:48<01:17, 37.82it/s]Training CobwebTree:  60%|    | 4383/7317 [01:48<01:19, 36.95it/s]Training CobwebTree:  60%|    | 4387/7317 [01:49<01:21, 35.99it/s]Training CobwebTree:  60%|    | 4391/7317 [01:49<01:22, 35.47it/s]Training CobwebTree:  60%|    | 4395/7317 [01:49<01:20, 36.29it/s]Training CobwebTree:  60%|    | 4399/7317 [01:49<01:19, 36.67it/s]Training CobwebTree:  60%|    | 4403/7317 [01:49<01:21, 35.88it/s]Training CobwebTree:  60%|    | 4407/7317 [01:49<01:22, 35.17it/s]Training CobwebTree:  60%|    | 4411/7317 [01:49<01:22, 35.42it/s]Training CobwebTree:  60%|    | 4415/7317 [01:49<01:19, 36.37it/s]Training CobwebTree:  60%|    | 4419/7317 [01:49<01:19, 36.24it/s]Training CobwebTree:  60%|    | 4424/7317 [01:50<01:16, 37.68it/s]Training CobwebTree:  61%|    | 4428/7317 [01:50<01:17, 37.47it/s]Training CobwebTree:  61%|    | 4432/7317 [01:50<01:16, 37.77it/s]Training CobwebTree:  61%|    | 4436/7317 [01:50<01:17, 37.05it/s]Training CobwebTree:  61%|    | 4440/7317 [01:50<01:21, 35.09it/s]Training CobwebTree:  61%|    | 4444/7317 [01:50<01:22, 35.01it/s]Training CobwebTree:  61%|    | 4449/7317 [01:50<01:16, 37.60it/s]Training CobwebTree:  61%|    | 4453/7317 [01:50<01:17, 36.85it/s]Training CobwebTree:  61%|    | 4458/7317 [01:51<01:14, 38.19it/s]Training CobwebTree:  61%|    | 4462/7317 [01:51<01:18, 36.28it/s]Training CobwebTree:  61%|    | 4466/7317 [01:51<01:19, 35.84it/s]Training CobwebTree:  61%|    | 4470/7317 [01:51<01:19, 35.80it/s]Training CobwebTree:  61%|    | 4474/7317 [01:51<01:18, 36.06it/s]Training CobwebTree:  61%|    | 4478/7317 [01:51<01:19, 35.78it/s]Training CobwebTree:  61%|   | 4483/7317 [01:51<01:16, 37.04it/s]Training CobwebTree:  61%|   | 4487/7317 [01:51<01:17, 36.75it/s]Training CobwebTree:  61%|   | 4491/7317 [01:51<01:18, 36.12it/s]Training CobwebTree:  61%|   | 4495/7317 [01:52<01:18, 35.79it/s]Training CobwebTree:  61%|   | 4499/7317 [01:52<01:17, 36.29it/s]Training CobwebTree:  62%|   | 4503/7317 [01:52<01:16, 36.73it/s]Training CobwebTree:  62%|   | 4507/7317 [01:52<01:17, 36.29it/s]Training CobwebTree:  62%|   | 4511/7317 [01:52<01:18, 35.56it/s]Training CobwebTree:  62%|   | 4516/7317 [01:52<01:15, 37.12it/s]Training CobwebTree:  62%|   | 4520/7317 [01:52<01:15, 37.13it/s]Training CobwebTree:  62%|   | 4524/7317 [01:52<01:15, 36.97it/s]Training CobwebTree:  62%|   | 4528/7317 [01:52<01:21, 34.18it/s]Training CobwebTree:  62%|   | 4532/7317 [01:53<01:18, 35.43it/s]Training CobwebTree:  62%|   | 4537/7317 [01:53<01:15, 36.75it/s]Training CobwebTree:  62%|   | 4541/7317 [01:53<01:16, 36.39it/s]Training CobwebTree:  62%|   | 4545/7317 [01:53<01:18, 35.51it/s]Training CobwebTree:  62%|   | 4549/7317 [01:53<01:20, 34.27it/s]Training CobwebTree:  62%|   | 4553/7317 [01:53<01:19, 34.69it/s]Training CobwebTree:  62%|   | 4557/7317 [01:53<01:19, 34.63it/s]Training CobwebTree:  62%|   | 4562/7317 [01:53<01:13, 37.25it/s]Training CobwebTree:  62%|   | 4566/7317 [01:54<01:13, 37.49it/s]Training CobwebTree:  62%|   | 4570/7317 [01:54<01:15, 36.36it/s]Training CobwebTree:  63%|   | 4575/7317 [01:54<01:13, 37.18it/s]Training CobwebTree:  63%|   | 4579/7317 [01:54<01:13, 37.07it/s]Training CobwebTree:  63%|   | 4584/7317 [01:54<01:11, 38.33it/s]Training CobwebTree:  63%|   | 4589/7317 [01:54<01:08, 39.66it/s]Training CobwebTree:  63%|   | 4593/7317 [01:54<01:10, 38.57it/s]Training CobwebTree:  63%|   | 4597/7317 [01:54<01:13, 37.21it/s]Training CobwebTree:  63%|   | 4601/7317 [01:54<01:13, 37.20it/s]Training CobwebTree:  63%|   | 4605/7317 [01:55<01:13, 37.13it/s]Training CobwebTree:  63%|   | 4609/7317 [01:55<01:15, 35.97it/s]Training CobwebTree:  63%|   | 4613/7317 [01:55<01:19, 34.13it/s]Training CobwebTree:  63%|   | 4617/7317 [01:55<01:15, 35.61it/s]Training CobwebTree:  63%|   | 4621/7317 [01:55<01:17, 34.91it/s]Training CobwebTree:  63%|   | 4625/7317 [01:55<01:17, 34.91it/s]Training CobwebTree:  63%|   | 4629/7317 [01:55<01:16, 35.15it/s]Training CobwebTree:  63%|   | 4633/7317 [01:55<01:14, 36.14it/s]Training CobwebTree:  63%|   | 4637/7317 [01:55<01:15, 35.55it/s]Training CobwebTree:  63%|   | 4641/7317 [01:56<01:16, 34.82it/s]Training CobwebTree:  63%|   | 4645/7317 [01:56<01:17, 34.49it/s]Training CobwebTree:  64%|   | 4649/7317 [01:56<01:14, 35.64it/s]Training CobwebTree:  64%|   | 4653/7317 [01:56<01:15, 35.24it/s]Training CobwebTree:  64%|   | 4657/7317 [01:56<01:17, 34.27it/s]Training CobwebTree:  64%|   | 4661/7317 [01:56<01:22, 32.36it/s]Training CobwebTree:  64%|   | 4665/7317 [01:56<01:20, 32.94it/s]Training CobwebTree:  64%|   | 4669/7317 [01:56<01:19, 33.35it/s]Training CobwebTree:  64%|   | 4673/7317 [01:57<01:15, 34.94it/s]Training CobwebTree:  64%|   | 4677/7317 [01:57<01:12, 36.30it/s]Training CobwebTree:  64%|   | 4681/7317 [01:57<01:14, 35.47it/s]Training CobwebTree:  64%|   | 4685/7317 [01:57<01:12, 36.45it/s]Training CobwebTree:  64%|   | 4689/7317 [01:57<01:12, 36.16it/s]Training CobwebTree:  64%|   | 4693/7317 [01:57<01:17, 33.96it/s]Training CobwebTree:  64%|   | 4697/7317 [01:57<01:18, 33.56it/s]Training CobwebTree:  64%|   | 4701/7317 [01:57<01:18, 33.50it/s]Training CobwebTree:  64%|   | 4705/7317 [01:57<01:20, 32.36it/s]Training CobwebTree:  64%|   | 4709/7317 [01:58<01:19, 32.86it/s]Training CobwebTree:  64%|   | 4713/7317 [01:58<01:15, 34.29it/s]Training CobwebTree:  64%|   | 4717/7317 [01:58<01:18, 33.06it/s]Training CobwebTree:  65%|   | 4721/7317 [01:58<01:16, 34.02it/s]Training CobwebTree:  65%|   | 4725/7317 [01:58<01:14, 34.99it/s]Training CobwebTree:  65%|   | 4729/7317 [01:58<01:13, 35.43it/s]Training CobwebTree:  65%|   | 4733/7317 [01:58<01:13, 35.31it/s]Training CobwebTree:  65%|   | 4737/7317 [01:58<01:11, 35.92it/s]Training CobwebTree:  65%|   | 4742/7317 [01:59<01:08, 37.46it/s]Training CobwebTree:  65%|   | 4746/7317 [01:59<01:10, 36.22it/s]Training CobwebTree:  65%|   | 4750/7317 [01:59<01:11, 35.90it/s]Training CobwebTree:  65%|   | 4754/7317 [01:59<01:12, 35.48it/s]Training CobwebTree:  65%|   | 4758/7317 [01:59<01:11, 35.95it/s]Training CobwebTree:  65%|   | 4762/7317 [01:59<01:15, 33.89it/s]Training CobwebTree:  65%|   | 4766/7317 [01:59<01:17, 32.82it/s]Training CobwebTree:  65%|   | 4770/7317 [01:59<01:16, 33.38it/s]Training CobwebTree:  65%|   | 4774/7317 [01:59<01:17, 32.98it/s]Training CobwebTree:  65%|   | 4778/7317 [02:00<01:14, 34.20it/s]Training CobwebTree:  65%|   | 4782/7317 [02:00<01:13, 34.65it/s]Training CobwebTree:  65%|   | 4786/7317 [02:00<01:14, 33.89it/s]Training CobwebTree:  65%|   | 4790/7317 [02:00<01:12, 34.83it/s]Training CobwebTree:  66%|   | 4794/7317 [02:00<01:14, 34.06it/s]Training CobwebTree:  66%|   | 4798/7317 [02:00<01:12, 34.94it/s]Training CobwebTree:  66%|   | 4802/7317 [02:00<01:11, 35.25it/s]Training CobwebTree:  66%|   | 4806/7317 [02:00<01:16, 32.93it/s]Training CobwebTree:  66%|   | 4810/7317 [02:01<01:14, 33.71it/s]Training CobwebTree:  66%|   | 4814/7317 [02:01<01:12, 34.76it/s]Training CobwebTree:  66%|   | 4818/7317 [02:01<01:13, 33.88it/s]Training CobwebTree:  66%|   | 4822/7317 [02:01<01:11, 35.12it/s]Training CobwebTree:  66%|   | 4826/7317 [02:01<01:10, 35.20it/s]Training CobwebTree:  66%|   | 4830/7317 [02:01<01:12, 34.30it/s]Training CobwebTree:  66%|   | 4834/7317 [02:01<01:10, 35.15it/s]Training CobwebTree:  66%|   | 4838/7317 [02:01<01:11, 34.63it/s]Training CobwebTree:  66%|   | 4843/7317 [02:01<01:07, 36.68it/s]Training CobwebTree:  66%|   | 4847/7317 [02:02<01:13, 33.48it/s]Training CobwebTree:  66%|   | 4851/7317 [02:02<01:12, 33.96it/s]Training CobwebTree:  66%|   | 4855/7317 [02:02<01:10, 35.10it/s]Training CobwebTree:  66%|   | 4859/7317 [02:02<01:12, 34.07it/s]Training CobwebTree:  66%|   | 4863/7317 [02:02<01:16, 32.09it/s]Training CobwebTree:  67%|   | 4867/7317 [02:02<01:14, 32.76it/s]Training CobwebTree:  67%|   | 4871/7317 [02:02<01:15, 32.36it/s]Training CobwebTree:  67%|   | 4875/7317 [02:02<01:12, 33.77it/s]Training CobwebTree:  67%|   | 4879/7317 [02:03<01:11, 33.98it/s]Training CobwebTree:  67%|   | 4883/7317 [02:03<01:13, 33.14it/s]Training CobwebTree:  67%|   | 4887/7317 [02:03<01:11, 34.12it/s]Training CobwebTree:  67%|   | 4891/7317 [02:03<01:09, 34.89it/s]Training CobwebTree:  67%|   | 4896/7317 [02:03<01:05, 36.72it/s]Training CobwebTree:  67%|   | 4900/7317 [02:03<01:06, 36.57it/s]Training CobwebTree:  67%|   | 4904/7317 [02:03<01:06, 36.35it/s]Training CobwebTree:  67%|   | 4909/7317 [02:03<01:04, 37.49it/s]Training CobwebTree:  67%|   | 4913/7317 [02:03<01:08, 34.98it/s]Training CobwebTree:  67%|   | 4917/7317 [02:04<01:07, 35.63it/s]Training CobwebTree:  67%|   | 4921/7317 [02:04<01:05, 36.56it/s]Training CobwebTree:  67%|   | 4925/7317 [02:04<01:07, 35.49it/s]Training CobwebTree:  67%|   | 4929/7317 [02:04<01:07, 35.56it/s]Training CobwebTree:  67%|   | 4933/7317 [02:04<01:08, 34.94it/s]Training CobwebTree:  67%|   | 4937/7317 [02:04<01:08, 34.92it/s]Training CobwebTree:  68%|   | 4941/7317 [02:04<01:09, 34.25it/s]Training CobwebTree:  68%|   | 4945/7317 [02:04<01:08, 34.45it/s]Training CobwebTree:  68%|   | 4949/7317 [02:05<01:10, 33.67it/s]Training CobwebTree:  68%|   | 4953/7317 [02:05<01:07, 34.90it/s]Training CobwebTree:  68%|   | 4958/7317 [02:05<01:05, 35.93it/s]Training CobwebTree:  68%|   | 4962/7317 [02:05<01:05, 36.21it/s]Training CobwebTree:  68%|   | 4966/7317 [02:05<01:07, 34.68it/s]Training CobwebTree:  68%|   | 4970/7317 [02:05<01:06, 35.29it/s]Training CobwebTree:  68%|   | 4974/7317 [02:05<01:04, 36.16it/s]Training CobwebTree:  68%|   | 4978/7317 [02:05<01:05, 35.72it/s]Training CobwebTree:  68%|   | 4982/7317 [02:05<01:04, 36.41it/s]Training CobwebTree:  68%|   | 4986/7317 [02:06<01:05, 35.37it/s]Training CobwebTree:  68%|   | 4990/7317 [02:06<01:07, 34.69it/s]Training CobwebTree:  68%|   | 4994/7317 [02:06<01:05, 35.37it/s]Training CobwebTree:  68%|   | 4999/7317 [02:06<01:02, 36.82it/s]Training CobwebTree:  68%|   | 5003/7317 [02:06<01:04, 36.10it/s]Training CobwebTree:  68%|   | 5007/7317 [02:06<01:03, 36.22it/s]Training CobwebTree:  68%|   | 5011/7317 [02:06<01:03, 36.52it/s]Training CobwebTree:  69%|   | 5015/7317 [02:06<01:04, 35.44it/s]Training CobwebTree:  69%|   | 5019/7317 [02:06<01:07, 34.02it/s]Training CobwebTree:  69%|   | 5023/7317 [02:07<01:08, 33.68it/s]Training CobwebTree:  69%|   | 5027/7317 [02:07<01:09, 32.94it/s]Training CobwebTree:  69%|   | 5031/7317 [02:07<01:08, 33.17it/s]Training CobwebTree:  69%|   | 5035/7317 [02:07<01:07, 33.57it/s]Training CobwebTree:  69%|   | 5039/7317 [02:07<01:06, 34.25it/s]Training CobwebTree:  69%|   | 5043/7317 [02:07<01:07, 33.76it/s]Training CobwebTree:  69%|   | 5047/7317 [02:07<01:07, 33.67it/s]Training CobwebTree:  69%|   | 5051/7317 [02:07<01:07, 33.46it/s]Training CobwebTree:  69%|   | 5055/7317 [02:08<01:06, 33.83it/s]Training CobwebTree:  69%|   | 5059/7317 [02:08<01:07, 33.55it/s]Training CobwebTree:  69%|   | 5063/7317 [02:08<01:09, 32.34it/s]Training CobwebTree:  69%|   | 5068/7317 [02:08<01:05, 34.44it/s]Training CobwebTree:  69%|   | 5072/7317 [02:08<01:07, 33.10it/s]Training CobwebTree:  69%|   | 5076/7317 [02:08<01:05, 34.16it/s]Training CobwebTree:  69%|   | 5080/7317 [02:08<01:05, 33.97it/s]Training CobwebTree:  69%|   | 5085/7317 [02:08<01:03, 34.91it/s]Training CobwebTree:  70%|   | 5089/7317 [02:09<01:03, 35.17it/s]Training CobwebTree:  70%|   | 5093/7317 [02:09<01:03, 34.75it/s]Training CobwebTree:  70%|   | 5097/7317 [02:09<01:02, 35.38it/s]Training CobwebTree:  70%|   | 5101/7317 [02:09<01:03, 34.71it/s]Training CobwebTree:  70%|   | 5105/7317 [02:09<01:03, 34.83it/s]Training CobwebTree:  70%|   | 5109/7317 [02:09<01:03, 34.80it/s]Training CobwebTree:  70%|   | 5113/7317 [02:09<01:02, 35.02it/s]Training CobwebTree:  70%|   | 5117/7317 [02:09<01:04, 34.36it/s]Training CobwebTree:  70%|   | 5121/7317 [02:09<01:03, 34.45it/s]Training CobwebTree:  70%|   | 5126/7317 [02:10<00:59, 37.11it/s]Training CobwebTree:  70%|   | 5130/7317 [02:10<00:59, 36.91it/s]Training CobwebTree:  70%|   | 5134/7317 [02:10<00:59, 36.73it/s]Training CobwebTree:  70%|   | 5138/7317 [02:10<01:00, 36.22it/s]Training CobwebTree:  70%|   | 5142/7317 [02:10<01:01, 35.10it/s]Training CobwebTree:  70%|   | 5146/7317 [02:10<01:01, 35.51it/s]Training CobwebTree:  70%|   | 5150/7317 [02:10<01:01, 35.30it/s]Training CobwebTree:  70%|   | 5154/7317 [02:10<01:00, 35.69it/s]Training CobwebTree:  70%|   | 5158/7317 [02:11<01:01, 35.37it/s]Training CobwebTree:  71%|   | 5162/7317 [02:11<01:00, 35.38it/s]Training CobwebTree:  71%|   | 5166/7317 [02:11<01:02, 34.34it/s]Training CobwebTree:  71%|   | 5170/7317 [02:11<01:01, 34.93it/s]Training CobwebTree:  71%|   | 5174/7317 [02:11<00:59, 36.01it/s]Training CobwebTree:  71%|   | 5178/7317 [02:11<00:58, 36.25it/s]Training CobwebTree:  71%|   | 5182/7317 [02:11<01:01, 34.73it/s]Training CobwebTree:  71%|   | 5186/7317 [02:11<01:00, 35.19it/s]Training CobwebTree:  71%|   | 5191/7317 [02:11<00:57, 36.80it/s]Training CobwebTree:  71%|   | 5195/7317 [02:12<01:02, 34.00it/s]Training CobwebTree:  71%|   | 5199/7317 [02:12<01:02, 33.85it/s]Training CobwebTree:  71%|   | 5203/7317 [02:12<01:02, 34.09it/s]Training CobwebTree:  71%|   | 5207/7317 [02:12<01:02, 33.79it/s]Training CobwebTree:  71%|   | 5211/7317 [02:12<01:03, 33.27it/s]Training CobwebTree:  71%|  | 5215/7317 [02:12<01:02, 33.55it/s]Training CobwebTree:  71%|  | 5219/7317 [02:12<01:02, 33.63it/s]Training CobwebTree:  71%|  | 5223/7317 [02:12<01:03, 33.22it/s]Training CobwebTree:  71%|  | 5227/7317 [02:13<01:00, 34.77it/s]Training CobwebTree:  71%|  | 5231/7317 [02:13<00:58, 35.48it/s]Training CobwebTree:  72%|  | 5235/7317 [02:13<00:59, 35.26it/s]Training CobwebTree:  72%|  | 5239/7317 [02:13<00:59, 34.73it/s]Training CobwebTree:  72%|  | 5243/7317 [02:13<01:01, 33.93it/s]Training CobwebTree:  72%|  | 5248/7317 [02:13<00:58, 35.29it/s]Training CobwebTree:  72%|  | 5252/7317 [02:13<00:57, 35.72it/s]Training CobwebTree:  72%|  | 5256/7317 [02:13<00:59, 34.82it/s]Training CobwebTree:  72%|  | 5260/7317 [02:13<00:58, 35.33it/s]Training CobwebTree:  72%|  | 5264/7317 [02:14<00:56, 36.25it/s]Training CobwebTree:  72%|  | 5268/7317 [02:14<00:58, 35.07it/s]Training CobwebTree:  72%|  | 5272/7317 [02:14<00:56, 35.97it/s]Training CobwebTree:  72%|  | 5276/7317 [02:14<00:56, 35.88it/s]Training CobwebTree:  72%|  | 5280/7317 [02:14<00:55, 36.41it/s]Training CobwebTree:  72%|  | 5284/7317 [02:14<00:54, 37.07it/s]Training CobwebTree:  72%|  | 5288/7317 [02:14<00:54, 37.32it/s]Training CobwebTree:  72%|  | 5292/7317 [02:14<00:53, 37.52it/s]Training CobwebTree:  72%|  | 5296/7317 [02:14<00:54, 36.83it/s]Training CobwebTree:  72%|  | 5300/7317 [02:15<00:56, 35.81it/s]Training CobwebTree:  72%|  | 5304/7317 [02:15<00:57, 34.98it/s]Training CobwebTree:  73%|  | 5308/7317 [02:15<00:56, 35.33it/s]Training CobwebTree:  73%|  | 5312/7317 [02:15<00:57, 34.79it/s]Training CobwebTree:  73%|  | 5316/7317 [02:15<00:57, 34.99it/s]Training CobwebTree:  73%|  | 5320/7317 [02:15<00:58, 33.92it/s]Training CobwebTree:  73%|  | 5324/7317 [02:15<00:57, 34.60it/s]Training CobwebTree:  73%|  | 5328/7317 [02:15<00:58, 34.09it/s]Training CobwebTree:  73%|  | 5332/7317 [02:15<00:56, 35.35it/s]Training CobwebTree:  73%|  | 5336/7317 [02:16<00:55, 35.51it/s]Training CobwebTree:  73%|  | 5340/7317 [02:16<00:53, 36.62it/s]Training CobwebTree:  73%|  | 5344/7317 [02:16<00:56, 35.00it/s]Training CobwebTree:  73%|  | 5348/7317 [02:16<00:57, 34.37it/s]Training CobwebTree:  73%|  | 5352/7317 [02:16<00:56, 34.70it/s]Training CobwebTree:  73%|  | 5356/7317 [02:16<00:57, 34.30it/s]Training CobwebTree:  73%|  | 5360/7317 [02:16<00:55, 34.96it/s]Training CobwebTree:  73%|  | 5364/7317 [02:16<00:54, 35.67it/s]Training CobwebTree:  73%|  | 5368/7317 [02:16<00:55, 35.42it/s]Training CobwebTree:  73%|  | 5372/7317 [02:17<00:56, 34.53it/s]Training CobwebTree:  73%|  | 5376/7317 [02:17<00:59, 32.74it/s]Training CobwebTree:  74%|  | 5380/7317 [02:17<00:56, 33.99it/s]Training CobwebTree:  74%|  | 5384/7317 [02:17<00:55, 34.98it/s]Training CobwebTree:  74%|  | 5388/7317 [02:17<00:53, 36.18it/s]Training CobwebTree:  74%|  | 5392/7317 [02:17<00:54, 35.00it/s]Training CobwebTree:  74%|  | 5396/7317 [02:17<00:55, 34.90it/s]Training CobwebTree:  74%|  | 5400/7317 [02:17<00:54, 35.26it/s]Training CobwebTree:  74%|  | 5404/7317 [02:18<00:54, 34.95it/s]Training CobwebTree:  74%|  | 5408/7317 [02:18<00:53, 35.49it/s]Training CobwebTree:  74%|  | 5412/7317 [02:18<00:52, 36.40it/s]Training CobwebTree:  74%|  | 5416/7317 [02:18<00:51, 36.68it/s]Training CobwebTree:  74%|  | 5421/7317 [02:18<00:50, 37.58it/s]Training CobwebTree:  74%|  | 5425/7317 [02:18<00:50, 37.76it/s]Training CobwebTree:  74%|  | 5429/7317 [02:18<00:50, 37.56it/s]Training CobwebTree:  74%|  | 5433/7317 [02:18<00:50, 37.34it/s]Training CobwebTree:  74%|  | 5437/7317 [02:18<00:50, 37.21it/s]Training CobwebTree:  74%|  | 5441/7317 [02:19<00:50, 37.11it/s]Training CobwebTree:  74%|  | 5445/7317 [02:19<00:50, 37.34it/s]Training CobwebTree:  74%|  | 5449/7317 [02:19<00:50, 37.12it/s]Training CobwebTree:  75%|  | 5453/7317 [02:19<00:52, 35.19it/s]Training CobwebTree:  75%|  | 5457/7317 [02:19<00:53, 34.64it/s]Training CobwebTree:  75%|  | 5461/7317 [02:19<00:57, 32.17it/s]Training CobwebTree:  75%|  | 5465/7317 [02:19<00:56, 33.01it/s]Training CobwebTree:  75%|  | 5469/7317 [02:19<00:56, 32.75it/s]Training CobwebTree:  75%|  | 5473/7317 [02:19<00:55, 33.26it/s]Training CobwebTree:  75%|  | 5477/7317 [02:20<00:55, 33.18it/s]Training CobwebTree:  75%|  | 5481/7317 [02:20<00:55, 32.82it/s]Training CobwebTree:  75%|  | 5485/7317 [02:20<00:54, 33.78it/s]Training CobwebTree:  75%|  | 5489/7317 [02:20<00:53, 34.31it/s]Training CobwebTree:  75%|  | 5493/7317 [02:20<00:53, 33.84it/s]Training CobwebTree:  75%|  | 5497/7317 [02:20<00:54, 33.53it/s]Training CobwebTree:  75%|  | 5501/7317 [02:20<00:53, 33.94it/s]Training CobwebTree:  75%|  | 5505/7317 [02:20<00:53, 33.82it/s]Training CobwebTree:  75%|  | 5509/7317 [02:21<00:54, 33.08it/s]Training CobwebTree:  75%|  | 5513/7317 [02:21<00:52, 34.49it/s]Training CobwebTree:  75%|  | 5517/7317 [02:21<00:52, 34.40it/s]Training CobwebTree:  75%|  | 5521/7317 [02:21<00:52, 34.45it/s]Training CobwebTree:  76%|  | 5525/7317 [02:21<00:51, 35.05it/s]Training CobwebTree:  76%|  | 5529/7317 [02:21<00:49, 36.15it/s]Training CobwebTree:  76%|  | 5533/7317 [02:21<00:49, 36.30it/s]Training CobwebTree:  76%|  | 5537/7317 [02:21<00:48, 36.56it/s]Training CobwebTree:  76%|  | 5542/7317 [02:21<00:47, 37.05it/s]Training CobwebTree:  76%|  | 5546/7317 [02:22<00:48, 36.83it/s]Training CobwebTree:  76%|  | 5550/7317 [02:22<00:47, 37.24it/s]Training CobwebTree:  76%|  | 5554/7317 [02:22<00:50, 34.96it/s]Training CobwebTree:  76%|  | 5558/7317 [02:22<00:49, 35.78it/s]Training CobwebTree:  76%|  | 5562/7317 [02:22<00:50, 35.04it/s]Training CobwebTree:  76%|  | 5566/7317 [02:22<00:49, 35.19it/s]Training CobwebTree:  76%|  | 5570/7317 [02:22<00:49, 35.51it/s]Training CobwebTree:  76%|  | 5574/7317 [02:22<00:49, 35.18it/s]Training CobwebTree:  76%|  | 5578/7317 [02:22<00:49, 34.82it/s]Training CobwebTree:  76%|  | 5582/7317 [02:23<00:48, 35.75it/s]Training CobwebTree:  76%|  | 5586/7317 [02:23<00:47, 36.70it/s]Training CobwebTree:  76%|  | 5590/7317 [02:23<00:49, 34.64it/s]Training CobwebTree:  76%|  | 5594/7317 [02:23<00:49, 35.04it/s]Training CobwebTree:  77%|  | 5598/7317 [02:23<00:50, 34.18it/s]Training CobwebTree:  77%|  | 5602/7317 [02:23<00:51, 33.34it/s]Training CobwebTree:  77%|  | 5607/7317 [02:23<00:47, 35.76it/s]Training CobwebTree:  77%|  | 5611/7317 [02:23<00:49, 34.69it/s]Training CobwebTree:  77%|  | 5615/7317 [02:24<00:47, 35.82it/s]Training CobwebTree:  77%|  | 5619/7317 [02:24<00:47, 36.12it/s]Training CobwebTree:  77%|  | 5623/7317 [02:24<00:46, 36.75it/s]Training CobwebTree:  77%|  | 5627/7317 [02:24<00:45, 37.56it/s]Training CobwebTree:  77%|  | 5631/7317 [02:24<00:44, 37.51it/s]Training CobwebTree:  77%|  | 5635/7317 [02:24<00:46, 36.06it/s]Training CobwebTree:  77%|  | 5639/7317 [02:24<00:47, 35.48it/s]Training CobwebTree:  77%|  | 5643/7317 [02:24<00:47, 35.58it/s]Training CobwebTree:  77%|  | 5647/7317 [02:24<00:45, 36.70it/s]Training CobwebTree:  77%|  | 5651/7317 [02:24<00:44, 37.34it/s]Training CobwebTree:  77%|  | 5655/7317 [02:25<00:45, 36.47it/s]Training CobwebTree:  77%|  | 5659/7317 [02:25<00:48, 34.24it/s]Training CobwebTree:  77%|  | 5663/7317 [02:25<00:46, 35.38it/s]Training CobwebTree:  77%|  | 5667/7317 [02:25<00:48, 34.18it/s]Training CobwebTree:  78%|  | 5671/7317 [02:25<00:47, 34.39it/s]Training CobwebTree:  78%|  | 5675/7317 [02:25<00:48, 33.78it/s]Training CobwebTree:  78%|  | 5679/7317 [02:25<00:47, 34.46it/s]Training CobwebTree:  78%|  | 5683/7317 [02:25<00:48, 33.93it/s]Training CobwebTree:  78%|  | 5687/7317 [02:26<00:46, 35.32it/s]Training CobwebTree:  78%|  | 5691/7317 [02:26<00:46, 34.84it/s]Training CobwebTree:  78%|  | 5695/7317 [02:26<00:46, 34.66it/s]Training CobwebTree:  78%|  | 5699/7317 [02:26<00:45, 35.66it/s]Training CobwebTree:  78%|  | 5703/7317 [02:26<00:44, 36.49it/s]Training CobwebTree:  78%|  | 5707/7317 [02:26<00:42, 37.46it/s]Training CobwebTree:  78%|  | 5711/7317 [02:26<00:46, 34.76it/s]Training CobwebTree:  78%|  | 5715/7317 [02:26<00:47, 33.54it/s]Training CobwebTree:  78%|  | 5719/7317 [02:26<00:47, 33.91it/s]Training CobwebTree:  78%|  | 5723/7317 [02:27<00:46, 34.09it/s]Training CobwebTree:  78%|  | 5727/7317 [02:27<00:46, 34.53it/s]Training CobwebTree:  78%|  | 5731/7317 [02:27<00:45, 34.55it/s]Training CobwebTree:  78%|  | 5735/7317 [02:27<00:45, 34.96it/s]Training CobwebTree:  78%|  | 5739/7317 [02:27<00:44, 35.51it/s]Training CobwebTree:  78%|  | 5743/7317 [02:27<00:45, 34.29it/s]Training CobwebTree:  79%|  | 5747/7317 [02:27<00:45, 34.62it/s]Training CobwebTree:  79%|  | 5751/7317 [02:27<00:43, 36.00it/s]Training CobwebTree:  79%|  | 5756/7317 [02:28<00:42, 36.58it/s]Training CobwebTree:  79%|  | 5760/7317 [02:28<00:43, 35.54it/s]Training CobwebTree:  79%|  | 5764/7317 [02:28<00:46, 33.42it/s]Training CobwebTree:  79%|  | 5768/7317 [02:28<00:44, 34.46it/s]Training CobwebTree:  79%|  | 5772/7317 [02:28<00:46, 33.12it/s]Training CobwebTree:  79%|  | 5776/7317 [02:28<00:48, 31.79it/s]Training CobwebTree:  79%|  | 5780/7317 [02:28<00:47, 32.28it/s]Training CobwebTree:  79%|  | 5784/7317 [02:28<00:46, 33.12it/s]Training CobwebTree:  79%|  | 5788/7317 [02:28<00:45, 33.25it/s]Training CobwebTree:  79%|  | 5792/7317 [02:29<00:46, 32.70it/s]Training CobwebTree:  79%|  | 5796/7317 [02:29<00:45, 33.61it/s]Training CobwebTree:  79%|  | 5800/7317 [02:29<00:44, 34.28it/s]Training CobwebTree:  79%|  | 5804/7317 [02:29<00:44, 33.78it/s]Training CobwebTree:  79%|  | 5808/7317 [02:29<00:43, 34.65it/s]Training CobwebTree:  79%|  | 5812/7317 [02:29<00:43, 34.88it/s]Training CobwebTree:  79%|  | 5816/7317 [02:29<00:43, 34.54it/s]Training CobwebTree:  80%|  | 5820/7317 [02:29<00:44, 33.80it/s]Training CobwebTree:  80%|  | 5824/7317 [02:30<00:43, 34.29it/s]Training CobwebTree:  80%|  | 5828/7317 [02:30<00:43, 34.30it/s]Training CobwebTree:  80%|  | 5832/7317 [02:30<00:43, 34.51it/s]Training CobwebTree:  80%|  | 5836/7317 [02:30<00:45, 32.83it/s]Training CobwebTree:  80%|  | 5840/7317 [02:30<00:47, 31.33it/s]Training CobwebTree:  80%|  | 5844/7317 [02:30<00:44, 33.30it/s]Training CobwebTree:  80%|  | 5848/7317 [02:30<00:45, 32.12it/s]Training CobwebTree:  80%|  | 5852/7317 [02:30<00:43, 33.49it/s]Training CobwebTree:  80%|  | 5856/7317 [02:31<00:43, 33.87it/s]Training CobwebTree:  80%|  | 5860/7317 [02:31<00:43, 33.26it/s]Training CobwebTree:  80%|  | 5864/7317 [02:31<00:42, 34.43it/s]Training CobwebTree:  80%|  | 5868/7317 [02:31<00:41, 35.31it/s]Training CobwebTree:  80%|  | 5872/7317 [02:31<00:40, 35.35it/s]Training CobwebTree:  80%|  | 5876/7317 [02:31<00:41, 34.49it/s]Training CobwebTree:  80%|  | 5880/7317 [02:31<00:43, 33.23it/s]Training CobwebTree:  80%|  | 5884/7317 [02:31<00:44, 32.55it/s]Training CobwebTree:  80%|  | 5888/7317 [02:31<00:44, 31.88it/s]Training CobwebTree:  81%|  | 5892/7317 [02:32<00:43, 32.80it/s]Training CobwebTree:  81%|  | 5896/7317 [02:32<00:43, 32.98it/s]Training CobwebTree:  81%|  | 5900/7317 [02:32<00:42, 33.48it/s]Training CobwebTree:  81%|  | 5904/7317 [02:32<00:43, 32.61it/s]Training CobwebTree:  81%|  | 5908/7317 [02:32<00:44, 31.42it/s]Training CobwebTree:  81%|  | 5912/7317 [02:32<00:42, 33.44it/s]Training CobwebTree:  81%|  | 5916/7317 [02:32<00:43, 32.40it/s]Training CobwebTree:  81%|  | 5920/7317 [02:32<00:44, 31.59it/s]Training CobwebTree:  81%|  | 5925/7317 [02:33<00:40, 34.76it/s]Training CobwebTree:  81%|  | 5929/7317 [02:33<00:40, 34.40it/s]Training CobwebTree:  81%|  | 5933/7317 [02:33<00:40, 33.87it/s]Training CobwebTree:  81%|  | 5937/7317 [02:33<00:40, 34.34it/s]Training CobwebTree:  81%|  | 5941/7317 [02:33<00:38, 35.71it/s]Training CobwebTree:  81%|  | 5945/7317 [02:33<00:38, 35.35it/s]Training CobwebTree:  81%| | 5949/7317 [02:33<00:40, 33.50it/s]Training CobwebTree:  81%| | 5953/7317 [02:33<00:41, 32.78it/s]Training CobwebTree:  81%| | 5957/7317 [02:34<00:41, 32.80it/s]Training CobwebTree:  81%| | 5961/7317 [02:34<00:42, 31.85it/s]Training CobwebTree:  82%| | 5965/7317 [02:34<00:42, 32.15it/s]Training CobwebTree:  82%| | 5970/7317 [02:34<00:40, 33.54it/s]Training CobwebTree:  82%| | 5974/7317 [02:34<00:38, 34.63it/s]Training CobwebTree:  82%| | 5979/7317 [02:34<00:37, 35.92it/s]Training CobwebTree:  82%| | 5983/7317 [02:34<00:37, 35.72it/s]Training CobwebTree:  82%| | 5987/7317 [02:34<00:37, 35.51it/s]Training CobwebTree:  82%| | 5991/7317 [02:35<00:38, 34.61it/s]Training CobwebTree:  82%| | 5995/7317 [02:35<00:38, 34.07it/s]Training CobwebTree:  82%| | 5999/7317 [02:35<00:38, 33.95it/s]Training CobwebTree:  82%| | 6003/7317 [02:35<00:37, 34.66it/s]Training CobwebTree:  82%| | 6007/7317 [02:35<00:38, 34.02it/s]Training CobwebTree:  82%| | 6011/7317 [02:35<00:37, 34.43it/s]Training CobwebTree:  82%| | 6015/7317 [02:35<00:38, 34.15it/s]Training CobwebTree:  82%| | 6019/7317 [02:35<00:36, 35.65it/s]Training CobwebTree:  82%| | 6023/7317 [02:35<00:37, 34.09it/s]Training CobwebTree:  82%| | 6027/7317 [02:36<00:38, 33.59it/s]Training CobwebTree:  82%| | 6031/7317 [02:36<00:39, 32.39it/s]Training CobwebTree:  82%| | 6035/7317 [02:36<00:37, 34.27it/s]Training CobwebTree:  83%| | 6039/7317 [02:36<00:39, 31.98it/s]Training CobwebTree:  83%| | 6043/7317 [02:36<00:38, 33.38it/s]Training CobwebTree:  83%| | 6047/7317 [02:36<00:37, 34.27it/s]Training CobwebTree:  83%| | 6051/7317 [02:36<00:35, 35.41it/s]Training CobwebTree:  83%| | 6055/7317 [02:36<00:36, 34.66it/s]Training CobwebTree:  83%| | 6059/7317 [02:36<00:35, 35.92it/s]Training CobwebTree:  83%| | 6063/7317 [02:37<00:34, 36.64it/s]Training CobwebTree:  83%| | 6067/7317 [02:37<00:35, 34.86it/s]Training CobwebTree:  83%| | 6071/7317 [02:37<00:35, 35.57it/s]Training CobwebTree:  83%| | 6075/7317 [02:37<00:34, 36.13it/s]Training CobwebTree:  83%| | 6079/7317 [02:37<00:34, 36.07it/s]Training CobwebTree:  83%| | 6083/7317 [02:37<00:36, 33.77it/s]Training CobwebTree:  83%| | 6087/7317 [02:37<00:36, 33.69it/s]Training CobwebTree:  83%| | 6091/7317 [02:37<00:35, 34.46it/s]Training CobwebTree:  83%| | 6095/7317 [02:38<00:35, 33.99it/s]Training CobwebTree:  83%| | 6099/7317 [02:38<00:35, 34.56it/s]Training CobwebTree:  83%| | 6103/7317 [02:38<00:38, 31.75it/s]Training CobwebTree:  83%| | 6107/7317 [02:38<00:37, 31.95it/s]Training CobwebTree:  84%| | 6111/7317 [02:38<00:36, 32.71it/s]Training CobwebTree:  84%| | 6115/7317 [02:38<00:36, 33.22it/s]Training CobwebTree:  84%| | 6119/7317 [02:38<00:38, 31.34it/s]Training CobwebTree:  84%| | 6123/7317 [02:38<00:35, 33.41it/s]Training CobwebTree:  84%| | 6127/7317 [02:39<00:34, 34.32it/s]Training CobwebTree:  84%| | 6131/7317 [02:39<00:33, 35.06it/s]Training CobwebTree:  84%| | 6135/7317 [02:39<00:33, 34.96it/s]Training CobwebTree:  84%| | 6139/7317 [02:39<00:34, 33.86it/s]Training CobwebTree:  84%| | 6143/7317 [02:39<00:34, 34.49it/s]Training CobwebTree:  84%| | 6147/7317 [02:39<00:32, 35.50it/s]Training CobwebTree:  84%| | 6151/7317 [02:39<00:33, 34.87it/s]Training CobwebTree:  84%| | 6155/7317 [02:39<00:33, 34.85it/s]Training CobwebTree:  84%| | 6159/7317 [02:39<00:34, 33.35it/s]Training CobwebTree:  84%| | 6163/7317 [02:40<00:34, 33.06it/s]Training CobwebTree:  84%| | 6167/7317 [02:40<00:34, 33.61it/s]Training CobwebTree:  84%| | 6171/7317 [02:40<00:35, 31.87it/s]Training CobwebTree:  84%| | 6176/7317 [02:40<00:32, 34.73it/s]Training CobwebTree:  84%| | 6180/7317 [02:40<00:33, 33.45it/s]Training CobwebTree:  85%| | 6184/7317 [02:40<00:35, 32.31it/s]Training CobwebTree:  85%| | 6188/7317 [02:40<00:35, 32.02it/s]Training CobwebTree:  85%| | 6192/7317 [02:40<00:34, 32.81it/s]Training CobwebTree:  85%| | 6196/7317 [02:41<00:34, 32.66it/s]Training CobwebTree:  85%| | 6200/7317 [02:41<00:33, 33.63it/s]Training CobwebTree:  85%| | 6205/7317 [02:41<00:30, 36.41it/s]Training CobwebTree:  85%| | 6209/7317 [02:41<00:31, 34.91it/s]Training CobwebTree:  85%| | 6213/7317 [02:41<00:31, 35.23it/s]Training CobwebTree:  85%| | 6217/7317 [02:41<00:30, 35.84it/s]Training CobwebTree:  85%| | 6221/7317 [02:41<00:31, 35.32it/s]Training CobwebTree:  85%| | 6225/7317 [02:41<00:32, 33.89it/s]Training CobwebTree:  85%| | 6229/7317 [02:42<00:32, 33.74it/s]Training CobwebTree:  85%| | 6233/7317 [02:42<00:32, 33.25it/s]Training CobwebTree:  85%| | 6237/7317 [02:42<00:31, 34.30it/s]Training CobwebTree:  85%| | 6241/7317 [02:42<00:31, 34.28it/s]Training CobwebTree:  85%| | 6245/7317 [02:42<00:31, 34.29it/s]Training CobwebTree:  85%| | 6250/7317 [02:42<00:30, 34.87it/s]Training CobwebTree:  85%| | 6254/7317 [02:42<00:32, 32.70it/s]Training CobwebTree:  86%| | 6258/7317 [02:42<00:33, 31.87it/s]Training CobwebTree:  86%| | 6262/7317 [02:42<00:31, 33.71it/s]Training CobwebTree:  86%| | 6266/7317 [02:43<00:30, 33.97it/s]Training CobwebTree:  86%| | 6271/7317 [02:43<00:29, 35.80it/s]Training CobwebTree:  86%| | 6275/7317 [02:43<00:29, 35.32it/s]Training CobwebTree:  86%| | 6279/7317 [02:43<00:28, 35.97it/s]Training CobwebTree:  86%| | 6283/7317 [02:43<00:28, 36.56it/s]Training CobwebTree:  86%| | 6287/7317 [02:43<00:30, 33.86it/s]Training CobwebTree:  86%| | 6291/7317 [02:43<00:30, 33.42it/s]Training CobwebTree:  86%| | 6295/7317 [02:43<00:30, 33.40it/s]Training CobwebTree:  86%| | 6299/7317 [02:44<00:31, 32.70it/s]Training CobwebTree:  86%| | 6303/7317 [02:44<00:31, 32.53it/s]Training CobwebTree:  86%| | 6307/7317 [02:44<00:30, 32.73it/s]Training CobwebTree:  86%| | 6311/7317 [02:44<00:32, 30.89it/s]Training CobwebTree:  86%| | 6315/7317 [02:44<00:30, 32.74it/s]Training CobwebTree:  86%| | 6319/7317 [02:44<00:31, 31.40it/s]Training CobwebTree:  86%| | 6324/7317 [02:44<00:29, 34.24it/s]Training CobwebTree:  86%| | 6328/7317 [02:44<00:28, 34.47it/s]Training CobwebTree:  87%| | 6332/7317 [02:45<00:28, 34.53it/s]Training CobwebTree:  87%| | 6336/7317 [02:45<00:28, 34.94it/s]Training CobwebTree:  87%| | 6340/7317 [02:45<00:29, 33.46it/s]Training CobwebTree:  87%| | 6344/7317 [02:45<00:28, 34.29it/s]Training CobwebTree:  87%| | 6348/7317 [02:45<00:27, 35.50it/s]Training CobwebTree:  87%| | 6352/7317 [02:45<00:26, 35.96it/s]Training CobwebTree:  87%| | 6356/7317 [02:45<00:27, 35.37it/s]Training CobwebTree:  87%| | 6360/7317 [02:45<00:26, 35.52it/s]Training CobwebTree:  87%| | 6364/7317 [02:45<00:27, 34.61it/s]Training CobwebTree:  87%| | 6368/7317 [02:46<00:27, 35.14it/s]Training CobwebTree:  87%| | 6372/7317 [02:46<00:26, 35.30it/s]Training CobwebTree:  87%| | 6376/7317 [02:46<00:27, 34.01it/s]Training CobwebTree:  87%| | 6380/7317 [02:46<00:27, 33.78it/s]Training CobwebTree:  87%| | 6384/7317 [02:46<00:27, 33.95it/s]Training CobwebTree:  87%| | 6388/7317 [02:46<00:27, 34.34it/s]Training CobwebTree:  87%| | 6392/7317 [02:46<00:26, 34.82it/s]Training CobwebTree:  87%| | 6396/7317 [02:46<00:27, 33.40it/s]Training CobwebTree:  87%| | 6400/7317 [02:47<00:27, 33.84it/s]Training CobwebTree:  88%| | 6404/7317 [02:47<00:25, 35.47it/s]Training CobwebTree:  88%| | 6408/7317 [02:47<00:27, 32.49it/s]Training CobwebTree:  88%| | 6412/7317 [02:47<00:27, 33.50it/s]Training CobwebTree:  88%| | 6416/7317 [02:47<00:27, 33.21it/s]Training CobwebTree:  88%| | 6421/7317 [02:47<00:24, 35.98it/s]Training CobwebTree:  88%| | 6425/7317 [02:47<00:27, 32.68it/s]Training CobwebTree:  88%| | 6429/7317 [02:47<00:26, 33.52it/s]Training CobwebTree:  88%| | 6433/7317 [02:47<00:25, 34.05it/s]Training CobwebTree:  88%| | 6437/7317 [02:48<00:25, 34.42it/s]Training CobwebTree:  88%| | 6441/7317 [02:48<00:27, 32.28it/s]Training CobwebTree:  88%| | 6445/7317 [02:48<00:28, 30.92it/s]Training CobwebTree:  88%| | 6449/7317 [02:48<00:28, 30.90it/s]Training CobwebTree:  88%| | 6453/7317 [02:48<00:27, 31.61it/s]Training CobwebTree:  88%| | 6457/7317 [02:48<00:28, 30.34it/s]Training CobwebTree:  88%| | 6461/7317 [02:48<00:26, 32.41it/s]Training CobwebTree:  88%| | 6465/7317 [02:48<00:25, 33.65it/s]Training CobwebTree:  88%| | 6469/7317 [02:49<00:25, 33.63it/s]Training CobwebTree:  88%| | 6473/7317 [02:49<00:24, 34.24it/s]Training CobwebTree:  89%| | 6478/7317 [02:49<00:23, 35.30it/s]Training CobwebTree:  89%| | 6483/7317 [02:49<00:22, 36.45it/s]Training CobwebTree:  89%| | 6487/7317 [02:49<00:22, 36.69it/s]Training CobwebTree:  89%| | 6492/7317 [02:49<00:21, 37.68it/s]Training CobwebTree:  89%| | 6496/7317 [02:49<00:22, 37.12it/s]Training CobwebTree:  89%| | 6500/7317 [02:49<00:22, 35.71it/s]Training CobwebTree:  89%| | 6504/7317 [02:50<00:22, 35.44it/s]Training CobwebTree:  89%| | 6508/7317 [02:50<00:23, 34.34it/s]Training CobwebTree:  89%| | 6512/7317 [02:50<00:22, 35.26it/s]Training CobwebTree:  89%| | 6516/7317 [02:50<00:22, 35.54it/s]Training CobwebTree:  89%| | 6520/7317 [02:50<00:23, 34.48it/s]Training CobwebTree:  89%| | 6524/7317 [02:50<00:23, 34.43it/s]Training CobwebTree:  89%| | 6529/7317 [02:50<00:21, 35.97it/s]Training CobwebTree:  89%| | 6533/7317 [02:50<00:21, 36.07it/s]Training CobwebTree:  89%| | 6537/7317 [02:51<00:21, 35.96it/s]Training CobwebTree:  89%| | 6541/7317 [02:51<00:22, 34.78it/s]Training CobwebTree:  89%| | 6545/7317 [02:51<00:21, 35.34it/s]Training CobwebTree:  90%| | 6549/7317 [02:51<00:21, 34.94it/s]Training CobwebTree:  90%| | 6553/7317 [02:51<00:21, 34.74it/s]Training CobwebTree:  90%| | 6557/7317 [02:51<00:21, 34.71it/s]Training CobwebTree:  90%| | 6562/7317 [02:51<00:20, 37.39it/s]Training CobwebTree:  90%| | 6566/7317 [02:51<00:19, 37.70it/s]Training CobwebTree:  90%| | 6570/7317 [02:51<00:20, 36.49it/s]Training CobwebTree:  90%| | 6574/7317 [02:52<00:20, 36.49it/s]Training CobwebTree:  90%| | 6578/7317 [02:52<00:20, 35.67it/s]Training CobwebTree:  90%| | 6582/7317 [02:52<00:20, 35.68it/s]Training CobwebTree:  90%| | 6586/7317 [02:52<00:21, 34.43it/s]Training CobwebTree:  90%| | 6590/7317 [02:52<00:20, 34.97it/s]Training CobwebTree:  90%| | 6594/7317 [02:52<00:20, 35.10it/s]Training CobwebTree:  90%| | 6598/7317 [02:52<00:20, 34.65it/s]Training CobwebTree:  90%| | 6602/7317 [02:52<00:20, 34.21it/s]Training CobwebTree:  90%| | 6606/7317 [02:52<00:21, 33.29it/s]Training CobwebTree:  90%| | 6610/7317 [02:53<00:21, 33.14it/s]Training CobwebTree:  90%| | 6614/7317 [02:53<00:21, 32.36it/s]Training CobwebTree:  90%| | 6618/7317 [02:53<00:21, 33.24it/s]Training CobwebTree:  91%| | 6622/7317 [02:53<00:21, 33.03it/s]Training CobwebTree:  91%| | 6626/7317 [02:53<00:21, 32.53it/s]Training CobwebTree:  91%| | 6630/7317 [02:53<00:21, 32.48it/s]Training CobwebTree:  91%| | 6634/7317 [02:53<00:21, 31.37it/s]Training CobwebTree:  91%| | 6638/7317 [02:53<00:21, 31.17it/s]Training CobwebTree:  91%| | 6642/7317 [02:54<00:20, 32.32it/s]Training CobwebTree:  91%| | 6647/7317 [02:54<00:19, 33.68it/s]Training CobwebTree:  91%| | 6651/7317 [02:54<00:19, 33.93it/s]Training CobwebTree:  91%| | 6655/7317 [02:54<00:19, 33.11it/s]Training CobwebTree:  91%| | 6659/7317 [02:54<00:19, 33.71it/s]Training CobwebTree:  91%| | 6663/7317 [02:54<00:18, 34.77it/s]Training CobwebTree:  91%| | 6667/7317 [02:54<00:18, 35.43it/s]Training CobwebTree:  91%| | 6671/7317 [02:54<00:19, 33.68it/s]Training CobwebTree:  91%| | 6675/7317 [02:55<00:19, 32.81it/s]Training CobwebTree:  91%|| 6679/7317 [02:55<00:18, 33.87it/s]Training CobwebTree:  91%|| 6683/7317 [02:55<00:18, 33.97it/s]Training CobwebTree:  91%|| 6687/7317 [02:55<00:18, 34.13it/s]Training CobwebTree:  91%|| 6691/7317 [02:55<00:18, 33.05it/s]Training CobwebTree:  91%|| 6695/7317 [02:55<00:19, 31.73it/s]Training CobwebTree:  92%|| 6699/7317 [02:55<00:18, 33.33it/s]Training CobwebTree:  92%|| 6703/7317 [02:55<00:18, 33.63it/s]Training CobwebTree:  92%|| 6707/7317 [02:56<00:17, 34.32it/s]Training CobwebTree:  92%|| 6711/7317 [02:56<00:17, 34.67it/s]Training CobwebTree:  92%|| 6715/7317 [02:56<00:17, 33.79it/s]Training CobwebTree:  92%|| 6719/7317 [02:56<00:17, 33.94it/s]Training CobwebTree:  92%|| 6723/7317 [02:56<00:17, 34.72it/s]Training CobwebTree:  92%|| 6727/7317 [02:56<00:17, 34.69it/s]Training CobwebTree:  92%|| 6731/7317 [02:56<00:17, 32.80it/s]Training CobwebTree:  92%|| 6735/7317 [02:56<00:18, 32.27it/s]Training CobwebTree:  92%|| 6739/7317 [02:56<00:17, 33.15it/s]Training CobwebTree:  92%|| 6743/7317 [02:57<00:17, 32.55it/s]Training CobwebTree:  92%|| 6747/7317 [02:57<00:17, 32.18it/s]Training CobwebTree:  92%|| 6751/7317 [02:57<00:17, 32.57it/s]Training CobwebTree:  92%|| 6755/7317 [02:57<00:17, 32.73it/s]Training CobwebTree:  92%|| 6759/7317 [02:57<00:17, 31.27it/s]Training CobwebTree:  92%|| 6763/7317 [02:57<00:17, 32.54it/s]Training CobwebTree:  92%|| 6767/7317 [02:57<00:16, 33.47it/s]Training CobwebTree:  93%|| 6771/7317 [02:57<00:16, 33.03it/s]Training CobwebTree:  93%|| 6775/7317 [02:58<00:15, 34.15it/s]Training CobwebTree:  93%|| 6779/7317 [02:58<00:15, 34.08it/s]Training CobwebTree:  93%|| 6783/7317 [02:58<00:16, 33.01it/s]Training CobwebTree:  93%|| 6787/7317 [02:58<00:16, 32.65it/s]Training CobwebTree:  93%|| 6791/7317 [02:58<00:15, 33.85it/s]Training CobwebTree:  93%|| 6796/7317 [02:58<00:14, 35.26it/s]Training CobwebTree:  93%|| 6800/7317 [02:58<00:15, 33.76it/s]Training CobwebTree:  93%|| 6804/7317 [02:58<00:15, 34.06it/s]Training CobwebTree:  93%|| 6808/7317 [02:59<00:15, 32.34it/s]Training CobwebTree:  93%|| 6812/7317 [02:59<00:15, 32.49it/s]Training CobwebTree:  93%|| 6816/7317 [02:59<00:15, 32.91it/s]Training CobwebTree:  93%|| 6820/7317 [02:59<00:15, 32.85it/s]Training CobwebTree:  93%|| 6824/7317 [02:59<00:15, 32.77it/s]Training CobwebTree:  93%|| 6828/7317 [02:59<00:14, 34.35it/s]Training CobwebTree:  93%|| 6832/7317 [03:00<00:27, 17.85it/s]Training CobwebTree:  93%|| 6837/7317 [03:00<00:21, 22.02it/s]Training CobwebTree:  93%|| 6841/7317 [03:00<00:18, 25.25it/s]Training CobwebTree:  94%|| 6845/7317 [03:00<00:17, 27.34it/s]Training CobwebTree:  94%|| 6849/7317 [03:00<00:15, 29.86it/s]Training CobwebTree:  94%|| 6853/7317 [03:00<00:15, 30.67it/s]Training CobwebTree:  94%|| 6857/7317 [03:00<00:14, 32.19it/s]Training CobwebTree:  94%|| 6861/7317 [03:00<00:13, 32.63it/s]Training CobwebTree:  94%|| 6865/7317 [03:01<00:13, 32.64it/s]Training CobwebTree:  94%|| 6869/7317 [03:01<00:12, 34.46it/s]Training CobwebTree:  94%|| 6873/7317 [03:01<00:13, 33.42it/s]Training CobwebTree:  94%|| 6877/7317 [03:01<00:13, 32.43it/s]Training CobwebTree:  94%|| 6881/7317 [03:01<00:13, 32.48it/s]Training CobwebTree:  94%|| 6885/7317 [03:01<00:13, 33.09it/s]Training CobwebTree:  94%|| 6889/7317 [03:01<00:12, 33.99it/s]Training CobwebTree:  94%|| 6893/7317 [03:01<00:12, 33.13it/s]Training CobwebTree:  94%|| 6897/7317 [03:02<00:12, 32.35it/s]Training CobwebTree:  94%|| 6901/7317 [03:02<00:12, 33.24it/s]Training CobwebTree:  94%|| 6905/7317 [03:02<00:12, 33.62it/s]Training CobwebTree:  94%|| 6909/7317 [03:02<00:12, 33.68it/s]Training CobwebTree:  94%|| 6913/7317 [03:02<00:12, 32.78it/s]Training CobwebTree:  95%|| 6917/7317 [03:02<00:11, 34.56it/s]Training CobwebTree:  95%|| 6921/7317 [03:02<00:11, 34.33it/s]Training CobwebTree:  95%|| 6925/7317 [03:02<00:11, 33.14it/s]Training CobwebTree:  95%|| 6929/7317 [03:02<00:11, 33.09it/s]Training CobwebTree:  95%|| 6933/7317 [03:03<00:11, 33.51it/s]Training CobwebTree:  95%|| 6937/7317 [03:03<00:11, 34.07it/s]Training CobwebTree:  95%|| 6942/7317 [03:03<00:10, 35.74it/s]Training CobwebTree:  95%|| 6946/7317 [03:03<00:10, 34.44it/s]Training CobwebTree:  95%|| 6950/7317 [03:03<00:10, 35.11it/s]Training CobwebTree:  95%|| 6954/7317 [03:03<00:10, 35.69it/s]Training CobwebTree:  95%|| 6958/7317 [03:03<00:10, 34.65it/s]Training CobwebTree:  95%|| 6962/7317 [03:03<00:10, 34.46it/s]Training CobwebTree:  95%|| 6966/7317 [03:04<00:10, 34.54it/s]Training CobwebTree:  95%|| 6970/7317 [03:04<00:10, 33.78it/s]Training CobwebTree:  95%|| 6974/7317 [03:04<00:10, 33.51it/s]Training CobwebTree:  95%|| 6978/7317 [03:04<00:10, 33.23it/s]Training CobwebTree:  95%|| 6982/7317 [03:04<00:09, 34.58it/s]Training CobwebTree:  95%|| 6986/7317 [03:04<00:09, 33.26it/s]Training CobwebTree:  96%|| 6990/7317 [03:04<00:09, 34.76it/s]Training CobwebTree:  96%|| 6994/7317 [03:04<00:09, 33.19it/s]Training CobwebTree:  96%|| 6998/7317 [03:04<00:09, 34.07it/s]Training CobwebTree:  96%|| 7002/7317 [03:05<00:09, 33.60it/s]Training CobwebTree:  96%|| 7006/7317 [03:05<00:09, 34.02it/s]Training CobwebTree:  96%|| 7010/7317 [03:05<00:08, 34.75it/s]Training CobwebTree:  96%|| 7014/7317 [03:05<00:08, 35.33it/s]Training CobwebTree:  96%|| 7018/7317 [03:05<00:08, 34.50it/s]Training CobwebTree:  96%|| 7022/7317 [03:05<00:08, 35.57it/s]Training CobwebTree:  96%|| 7026/7317 [03:05<00:08, 34.99it/s]Training CobwebTree:  96%|| 7030/7317 [03:05<00:08, 35.82it/s]Training CobwebTree:  96%|| 7034/7317 [03:05<00:08, 34.73it/s]Training CobwebTree:  96%|| 7038/7317 [03:06<00:08, 33.76it/s]Training CobwebTree:  96%|| 7042/7317 [03:06<00:08, 33.52it/s]Training CobwebTree:  96%|| 7046/7317 [03:06<00:08, 33.20it/s]Training CobwebTree:  96%|| 7051/7317 [03:06<00:07, 35.77it/s]Training CobwebTree:  96%|| 7056/7317 [03:06<00:06, 39.16it/s]Training CobwebTree:  96%|| 7060/7317 [03:06<00:07, 36.58it/s]Training CobwebTree:  97%|| 7064/7317 [03:06<00:07, 36.11it/s]Training CobwebTree:  97%|| 7068/7317 [03:06<00:07, 34.20it/s]Training CobwebTree:  97%|| 7072/7317 [03:07<00:07, 34.68it/s]Training CobwebTree:  97%|| 7076/7317 [03:07<00:07, 33.40it/s]Training CobwebTree:  97%|| 7080/7317 [03:07<00:07, 33.57it/s]Training CobwebTree:  97%|| 7084/7317 [03:07<00:06, 33.43it/s]Training CobwebTree:  97%|| 7088/7317 [03:07<00:06, 32.93it/s]Training CobwebTree:  97%|| 7092/7317 [03:07<00:06, 32.54it/s]Training CobwebTree:  97%|| 7096/7317 [03:07<00:06, 32.69it/s]Training CobwebTree:  97%|| 7100/7317 [03:07<00:06, 34.25it/s]Training CobwebTree:  97%|| 7104/7317 [03:08<00:06, 32.70it/s]Training CobwebTree:  97%|| 7108/7317 [03:08<00:06, 31.69it/s]Training CobwebTree:  97%|| 7112/7317 [03:08<00:06, 32.09it/s]Training CobwebTree:  97%|| 7116/7317 [03:08<00:06, 31.99it/s]Training CobwebTree:  97%|| 7120/7317 [03:08<00:06, 32.52it/s]Training CobwebTree:  97%|| 7124/7317 [03:08<00:06, 31.17it/s]Training CobwebTree:  97%|| 7129/7317 [03:08<00:05, 33.84it/s]Training CobwebTree:  97%|| 7133/7317 [03:08<00:05, 31.88it/s]Training CobwebTree:  98%|| 7137/7317 [03:09<00:05, 31.72it/s]Training CobwebTree:  98%|| 7141/7317 [03:09<00:05, 33.53it/s]Training CobwebTree:  98%|| 7146/7317 [03:09<00:04, 35.94it/s]Training CobwebTree:  98%|| 7150/7317 [03:09<00:04, 35.31it/s]Training CobwebTree:  98%|| 7154/7317 [03:09<00:04, 33.97it/s]Training CobwebTree:  98%|| 7158/7317 [03:09<00:04, 32.82it/s]Training CobwebTree:  98%|| 7162/7317 [03:09<00:04, 31.78it/s]Training CobwebTree:  98%|| 7166/7317 [03:09<00:04, 32.78it/s]Training CobwebTree:  98%|| 7170/7317 [03:10<00:04, 32.24it/s]Training CobwebTree:  98%|| 7174/7317 [03:10<00:04, 31.58it/s]Training CobwebTree:  98%|| 7178/7317 [03:10<00:04, 31.99it/s]Training CobwebTree:  98%|| 7182/7317 [03:10<00:04, 31.46it/s]Training CobwebTree:  98%|| 7186/7317 [03:10<00:03, 32.80it/s]Training CobwebTree:  98%|| 7190/7317 [03:10<00:03, 34.26it/s]Training CobwebTree:  98%|| 7194/7317 [03:10<00:03, 34.33it/s]Training CobwebTree:  98%|| 7198/7317 [03:10<00:03, 34.72it/s]Training CobwebTree:  98%|| 7202/7317 [03:11<00:03, 34.18it/s]Training CobwebTree:  98%|| 7206/7317 [03:11<00:03, 33.80it/s]Training CobwebTree:  99%|| 7210/7317 [03:11<00:03, 33.86it/s]Training CobwebTree:  99%|| 7214/7317 [03:11<00:03, 33.15it/s]Training CobwebTree:  99%|| 7218/7317 [03:11<00:02, 33.46it/s]Training CobwebTree:  99%|| 7222/7317 [03:11<00:02, 34.49it/s]Training CobwebTree:  99%|| 7226/7317 [03:11<00:02, 33.60it/s]Training CobwebTree:  99%|| 7230/7317 [03:11<00:02, 34.07it/s]Training CobwebTree:  99%|| 7234/7317 [03:11<00:02, 31.86it/s]Training CobwebTree:  99%|| 7238/7317 [03:12<00:02, 32.08it/s]Training CobwebTree:  99%|| 7242/7317 [03:12<00:02, 31.94it/s]Training CobwebTree:  99%|| 7246/7317 [03:12<00:02, 30.68it/s]Training CobwebTree:  99%|| 7250/7317 [03:12<00:02, 31.49it/s]Training CobwebTree:  99%|| 7254/7317 [03:12<00:01, 32.14it/s]Training CobwebTree:  99%|| 7258/7317 [03:12<00:01, 32.64it/s]Training CobwebTree:  99%|| 7262/7317 [03:12<00:01, 33.83it/s]Training CobwebTree:  99%|| 7266/7317 [03:12<00:01, 33.01it/s]Training CobwebTree:  99%|| 7270/7317 [03:13<00:01, 32.31it/s]Training CobwebTree:  99%|| 7274/7317 [03:13<00:01, 32.44it/s]Training CobwebTree:  99%|| 7278/7317 [03:13<00:01, 31.62it/s]Training CobwebTree: 100%|| 7282/7317 [03:13<00:01, 31.55it/s]Training CobwebTree: 100%|| 7286/7317 [03:13<00:00, 31.82it/s]Training CobwebTree: 100%|| 7290/7317 [03:13<00:00, 32.24it/s]Training CobwebTree: 100%|| 7294/7317 [03:13<00:00, 33.52it/s]Training CobwebTree: 100%|| 7298/7317 [03:13<00:00, 33.01it/s]Training CobwebTree: 100%|| 7302/7317 [03:14<00:00, 34.81it/s]Training CobwebTree: 100%|| 7306/7317 [03:14<00:00, 34.07it/s]Training CobwebTree: 100%|| 7310/7317 [03:14<00:00, 32.53it/s]Training CobwebTree: 100%|| 7314/7317 [03:14<00:00, 31.31it/s]Training CobwebTree: 100%|| 7317/7317 [03:14<00:00, 37.61it/s]
2025-12-21 06:21:15,747 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 06:21:19,618 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (-27795 virtual)
2025-12-21 06:21:19,623 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (-26682 virtual)
2025-12-21 06:21:19,626 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (-23185 virtual)
2025-12-21 06:21:19,629 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (-16292 virtual)
2025-12-21 06:21:19,635 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (-18608 virtual)
2025-12-21 06:21:19,643 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (-28593 virtual)
2025-12-21 06:21:19,647 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (-28613 virtual)
2025-12-21 06:21:19,650 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (-28200 virtual)
2025-12-21 06:21:19,655 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (-10350 virtual)
2025-12-21 06:21:19,658 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (-13670 virtual)
2025-12-21 06:21:19,662 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (-17781 virtual)
2025-12-21 06:21:19,665 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (-17298 virtual)
2025-12-21 06:21:19,671 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (-19617 virtual)
2025-12-21 06:21:19,677 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (-20801 virtual)
2025-12-21 06:21:19,691 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-39359 virtual)
2025-12-21 06:21:19,698 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (-44349 virtual)
2025-12-21 06:21:19,703 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (-48268 virtual)
2025-12-21 06:21:19,713 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (-57638 virtual)
2025-12-21 06:21:19,720 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (-64105 virtual)
2025-12-21 06:21:19,726 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (-65618 virtual)
2025-12-21 06:21:19,733 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (-70438 virtual)
2025-12-21 06:21:20,024 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (-88562 virtual)
2025-12-21 06:21:20,029 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (-90583 virtual)
2025-12-21 06:21:20,032 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (-88457 virtual)
2025-12-21 06:21:20,196 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (-91122 virtual)
2025-12-21 06:21:20,276 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-95074 virtual)
2025-12-21 06:21:20,333 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (-92885 virtual)
2025-12-21 06:21:20,585 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (-102297 virtual)
2025-12-21 06:21:20,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,739 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,739 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,783 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,849 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,854 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:20,915 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:20,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,061 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,122 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,249 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,285 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,465 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,841 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:21,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:21,992 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,204 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,207 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,460 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,656 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,767 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,854 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:22,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:22,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,263 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,476 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,673 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:23,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:23,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,367 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,685 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,739 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:24,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:24,974 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:25,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:25,069 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:29,388 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 06:21:29,576 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 325714 virtual documents
2025-12-21 06:21:30,419 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 06:21:35,752 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (4102 virtual)
2025-12-21 06:21:35,755 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (6821 virtual)
2025-12-21 06:21:35,756 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (10012 virtual)
2025-12-21 06:21:35,757 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (13175 virtual)
2025-12-21 06:21:35,758 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (18193 virtual)
2025-12-21 06:21:35,759 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (24166 virtual)
2025-12-21 06:21:35,760 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (26860 virtual)
2025-12-21 06:21:35,761 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (30637 virtual)
2025-12-21 06:21:35,763 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (36618 virtual)
2025-12-21 06:21:35,764 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (40222 virtual)
2025-12-21 06:21:35,765 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (45221 virtual)
2025-12-21 06:21:35,766 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (48803 virtual)
2025-12-21 06:21:35,767 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (54550 virtual)
2025-12-21 06:21:35,769 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (61805 virtual)
2025-12-21 06:21:35,770 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (65443 virtual)
2025-12-21 06:21:35,772 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (75718 virtual)
2025-12-21 06:21:35,774 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (85615 virtual)
2025-12-21 06:21:35,776 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (98908 virtual)
2025-12-21 06:21:35,778 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (103504 virtual)
2025-12-21 06:21:35,779 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (107458 virtual)
2025-12-21 06:21:35,780 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (115792 virtual)
2025-12-21 06:21:35,781 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (119883 virtual)
2025-12-21 06:21:35,782 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (123870 virtual)
2025-12-21 06:21:35,783 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (126477 virtual)
2025-12-21 06:21:35,784 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (129653 virtual)
2025-12-21 06:21:35,786 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (137807 virtual)
2025-12-21 06:21:35,787 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (141654 virtual)
2025-12-21 06:21:35,788 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (150587 virtual)
2025-12-21 06:21:35,790 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (157400 virtual)
2025-12-21 06:21:35,791 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (160656 virtual)
2025-12-21 06:21:35,795 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (188050 virtual)
2025-12-21 06:21:35,796 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (192522 virtual)
2025-12-21 06:21:35,798 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (197427 virtual)
2025-12-21 06:21:35,799 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (203930 virtual)
2025-12-21 06:21:35,801 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (208950 virtual)
2025-12-21 06:21:35,802 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (212204 virtual)
2025-12-21 06:21:35,804 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (216523 virtual)
2025-12-21 06:21:35,807 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (225419 virtual)
2025-12-21 06:21:35,809 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (231642 virtual)
2025-12-21 06:21:35,811 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (238702 virtual)
2025-12-21 06:21:35,813 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (244063 virtual)
2025-12-21 06:21:35,815 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (247310 virtual)
2025-12-21 06:21:35,817 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (255583 virtual)
2025-12-21 06:21:35,819 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (259605 virtual)
2025-12-21 06:21:35,821 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (265467 virtual)
2025-12-21 06:21:35,824 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (273599 virtual)
2025-12-21 06:21:35,825 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (276944 virtual)
2025-12-21 06:21:35,827 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (280428 virtual)
2025-12-21 06:21:35,828 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (283467 virtual)
2025-12-21 06:21:35,830 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (286217 virtual)
2025-12-21 06:21:35,832 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (292208 virtual)
2025-12-21 06:21:35,833 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (295281 virtual)
2025-12-21 06:21:35,835 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (300269 virtual)
2025-12-21 06:21:35,837 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (305244 virtual)
2025-12-21 06:21:35,839 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (312641 virtual)
2025-12-21 06:21:35,841 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (317950 virtual)
2025-12-21 06:21:35,843 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (320861 virtual)
2025-12-21 06:21:35,889 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (326822 virtual)
2025-12-21 06:21:35,891 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (333251 virtual)
2025-12-21 06:21:35,892 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (337048 virtual)
2025-12-21 06:21:35,894 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (341136 virtual)
2025-12-21 06:21:35,921 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (348532 virtual)
2025-12-21 06:21:35,923 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (352494 virtual)
2025-12-21 06:21:35,924 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (356604 virtual)
2025-12-21 06:21:35,926 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (360863 virtual)
2025-12-21 06:21:35,932 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (363771 virtual)
2025-12-21 06:21:35,940 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (369214 virtual)
2025-12-21 06:21:35,949 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (377562 virtual)
2025-12-21 06:21:35,960 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (381941 virtual)
2025-12-21 06:21:35,965 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (386658 virtual)
2025-12-21 06:21:35,967 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (389807 virtual)
2025-12-21 06:21:36,094 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (396695 virtual)
2025-12-21 06:21:36,095 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (401352 virtual)
2025-12-21 06:21:36,097 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (404776 virtual)
2025-12-21 06:21:36,105 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (414382 virtual)
2025-12-21 06:21:36,168 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (418521 virtual)
2025-12-21 06:21:36,170 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (423376 virtual)
2025-12-21 06:21:36,172 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (427601 virtual)
2025-12-21 06:21:36,174 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (435162 virtual)
2025-12-21 06:21:36,269 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (441132 virtual)
2025-12-21 06:21:36,271 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (446159 virtual)
2025-12-21 06:21:36,276 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (449839 virtual)
2025-12-21 06:21:36,277 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (452978 virtual)
2025-12-21 06:21:36,292 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (456946 virtual)
2025-12-21 06:21:36,412 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (461159 virtual)
2025-12-21 06:21:36,414 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (465356 virtual)
2025-12-21 06:21:36,416 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (470767 virtual)
2025-12-21 06:21:36,424 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (474303 virtual)
2025-12-21 06:21:36,436 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (478656 virtual)
2025-12-21 06:21:36,480 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (483369 virtual)
2025-12-21 06:21:36,573 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (489140 virtual)
2025-12-21 06:21:36,574 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (493085 virtual)
2025-12-21 06:21:36,576 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (495960 virtual)
2025-12-21 06:21:36,756 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (513038 virtual)
2025-12-21 06:21:36,758 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (517896 virtual)
2025-12-21 06:21:36,760 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (523375 virtual)
2025-12-21 06:21:36,917 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (530217 virtual)
2025-12-21 06:21:36,920 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (538743 virtual)
2025-12-21 06:21:37,060 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (543995 virtual)
2025-12-21 06:21:37,120 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (548568 virtual)
2025-12-21 06:21:37,157 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (555278 virtual)
2025-12-21 06:21:37,159 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (560089 virtual)
2025-12-21 06:21:37,262 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (563732 virtual)
2025-12-21 06:21:37,265 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (570526 virtual)
2025-12-21 06:21:37,354 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (579115 virtual)
2025-12-21 06:21:37,368 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (583088 virtual)
2025-12-21 06:21:37,370 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (586718 virtual)
2025-12-21 06:21:37,458 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (589978 virtual)
2025-12-21 06:21:37,461 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (596075 virtual)
2025-12-21 06:21:37,463 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (599722 virtual)
2025-12-21 06:21:37,542 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (604923 virtual)
2025-12-21 06:21:37,546 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (614503 virtual)
2025-12-21 06:21:37,634 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (619104 virtual)
2025-12-21 06:21:37,637 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (623568 virtual)
2025-12-21 06:21:37,638 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (625591 virtual)
2025-12-21 06:21:37,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,656 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,656 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,657 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,657 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,657 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,659 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,659 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,660 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,660 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,660 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,683 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,691 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,692 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,699 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,798 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,818 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,818 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,833 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,921 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:37,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,842 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,990 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:37,914 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,048 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,048 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,098 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,145 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,245 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,670 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,715 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,742 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,768 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,769 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,886 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:38,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:38,977 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,089 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,280 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,688 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:39,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:39,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:40,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:40,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:40,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:40,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:40,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:40,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:40,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:40,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:40,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:40,586 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:40,692 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:40,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:40,711 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 06:21:40,712 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 06:21:45,619 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 06:21:45,768 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 628280 virtual documents
2025-12-21 06:21:46,438 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.5766001011205771, 'coherence_npmi': 0.04804915517678855, 'topic_diversity': 0.9, 'inter_topic_similarity': 0.4503951370716095}
2025-12-21 06:21:46,438 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6674919140326898, 'coherence_npmi': 0.08821722759861127, 'topic_diversity': 0.802, 'inter_topic_similarity': 0.27645018696784973}
2025-12-21 06:21:46,438 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.676000747010618, 'coherence_npmi': 0.11242356294372408, 'topic_diversity': 0.81, 'inter_topic_similarity': 0.2657122015953064}
