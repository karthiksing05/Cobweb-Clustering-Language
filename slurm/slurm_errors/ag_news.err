2025-12-23 23:34:39,664 INFO __main__: Starting benchmark for dataset=agnews
2025-12-23 23:34:42,643 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-23 23:34:42,782 INFO gensim.corpora.dictionary: built Dictionary<21775 unique tokens: ['band', 'bears', 'black', 'claw', 'cynics']...> from 10000 documents (total 256241 corpus positions)
2025-12-23 23:34:42,785 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<21775 unique tokens: ['band', 'bears', 'black', 'claw', 'cynics']...> from 10000 documents (total 256241 corpus positions)", 'datetime': '2025-12-23T23:34:42.782284', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-153-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-23 23:34:44,420 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-23 23:34:44,420 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-23 23:34:49,311 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10000 docs
2025-12-23 23:36:41,462 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:36:48,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,534 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,534 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,540 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,540 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,541 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,546 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,552 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,587 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,596 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,596 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,624 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,624 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,624 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,640 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,640 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,679 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,679 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,680 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,680 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,690 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,691 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,712 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,838 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,858 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,867 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,868 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:48,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:48,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:49,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:49,029 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:49,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:49,103 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:49,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:49,120 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:49,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:49,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:49,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:49,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:49,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:50,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:50,362 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:50,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:36:50,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:36:53,496 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:36:55,259 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-23 23:37:21,790 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:37:28,977 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-23 23:37:28,978 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-23 23:37:28,979 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-23 23:37:28,980 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-23 23:37:28,981 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-23 23:37:28,982 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-23 23:37:28,983 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-23 23:37:28,984 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-23 23:37:28,984 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-23 23:37:28,985 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-23 23:37:28,986 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-23 23:37:28,987 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-23 23:37:28,987 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-23 23:37:28,988 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-23 23:37:28,989 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-23 23:37:28,990 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-23 23:37:28,991 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-23 23:37:28,991 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-23 23:37:28,992 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-23 23:37:28,993 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-23 23:37:28,994 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-23 23:37:28,994 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-23 23:37:28,995 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-23 23:37:28,996 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-23 23:37:28,997 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-23 23:37:28,997 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-23 23:37:28,998 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-23 23:37:28,999 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-23 23:37:29,000 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-23 23:37:29,000 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-23 23:37:29,001 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-23 23:37:29,002 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-23 23:37:29,003 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-23 23:37:29,004 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-23 23:37:29,004 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-23 23:37:29,005 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-23 23:37:29,006 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-23 23:37:29,007 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-23 23:37:29,008 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-23 23:37:29,009 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-23 23:37:29,009 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-23 23:37:29,010 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-23 23:37:29,011 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-23 23:37:29,011 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-23 23:37:29,012 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-23 23:37:29,013 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-23 23:37:29,014 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-23 23:37:29,014 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-23 23:37:29,015 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-23 23:37:29,016 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-23 23:37:29,017 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-23 23:37:29,017 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-23 23:37:29,018 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-23 23:37:29,019 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-23 23:37:29,020 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-23 23:37:29,020 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-23 23:37:29,021 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-23 23:37:29,022 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-23 23:37:29,022 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-23 23:37:29,023 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-23 23:37:29,024 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-23 23:37:29,025 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-23 23:37:29,026 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-23 23:37:29,026 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-23 23:37:29,027 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-23 23:37:29,028 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-23 23:37:29,029 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-23 23:37:29,030 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-23 23:37:29,030 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-23 23:37:29,031 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-23 23:37:29,032 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-23 23:37:29,032 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-23 23:37:29,033 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-23 23:37:29,034 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-23 23:37:29,034 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-23 23:37:29,035 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-23 23:37:29,036 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-23 23:37:29,037 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-23 23:37:29,038 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-23 23:37:29,038 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-23 23:37:29,039 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-23 23:37:29,040 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-23 23:37:29,041 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-23 23:37:29,041 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-23 23:37:29,042 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-23 23:37:29,043 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-23 23:37:29,044 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-23 23:37:29,044 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-23 23:37:29,045 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-23 23:37:29,046 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-23 23:37:29,046 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-23 23:37:29,047 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-23 23:37:29,048 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-23 23:37:29,048 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-23 23:37:29,049 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-23 23:37:29,050 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-23 23:37:29,050 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-23 23:37:29,051 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-23 23:37:29,052 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-23 23:37:29,053 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-23 23:37:29,053 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-23 23:37:29,054 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-23 23:37:29,055 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-23 23:37:29,056 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-23 23:37:29,056 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-23 23:37:29,057 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-23 23:37:29,058 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-23 23:37:29,059 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-23 23:37:29,059 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-23 23:37:29,060 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-23 23:37:29,061 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-23 23:37:29,061 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-23 23:37:29,062 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-23 23:37:29,063 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-23 23:37:29,064 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-23 23:37:29,064 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-23 23:37:29,065 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-23 23:37:29,066 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-23 23:37:29,066 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-23 23:37:29,067 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-23 23:37:29,068 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-23 23:37:29,069 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-23 23:37:29,069 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-23 23:37:29,070 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-23 23:37:29,071 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-23 23:37:29,072 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-23 23:37:29,072 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-23 23:37:29,073 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-23 23:37:29,074 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-23 23:37:29,074 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-23 23:37:29,075 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-23 23:37:29,076 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-23 23:37:29,076 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-23 23:37:29,077 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-23 23:37:29,078 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-23 23:37:29,078 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-23 23:37:29,079 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-23 23:37:29,099 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-23 23:37:29,100 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-23 23:37:29,101 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-23 23:37:29,111 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-23 23:37:29,130 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-23 23:37:29,131 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-23 23:37:29,132 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-23 23:37:29,133 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-23 23:37:29,133 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-23 23:37:29,135 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-23 23:37:29,215 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-23 23:37:29,216 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-23 23:37:29,217 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-23 23:37:29,218 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-23 23:37:29,219 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-23 23:37:29,219 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-23 23:37:29,220 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-23 23:37:29,221 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-23 23:37:29,221 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-23 23:37:29,222 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-23 23:37:30,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,207 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,233 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,244 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,244 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,244 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,245 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,278 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,281 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,282 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,284 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,287 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,290 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,293 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,302 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,319 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,322 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,325 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,332 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,356 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,368 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,368 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,402 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,402 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,402 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,672 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,787 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,790 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,792 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,792 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,792 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,915 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,942 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:30,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:30,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,320 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:31,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:31,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:32,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:37:32,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:37:36,067 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:37:37,736 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-23 23:37:39,518 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10000 docs
2025-12-23 23:38:49,811 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:38:55,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,830 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,833 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,833 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,834 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,838 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,839 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,839 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,840 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,840 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,840 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,850 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,850 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,870 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:55,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:55,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,959 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:39:00,097 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-23 23:39:00,490 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:39:07,262 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-23 23:39:07,264 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-23 23:39:07,266 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-23 23:39:07,267 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-23 23:39:07,267 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-23 23:39:07,268 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-23 23:39:07,269 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-23 23:39:07,270 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-23 23:39:07,271 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-23 23:39:07,272 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-23 23:39:07,272 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-23 23:39:07,273 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-23 23:39:07,274 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-23 23:39:07,274 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-23 23:39:07,275 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-23 23:39:07,276 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-23 23:39:07,277 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-23 23:39:07,278 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-23 23:39:07,278 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-23 23:39:07,279 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-23 23:39:07,280 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-23 23:39:07,280 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-23 23:39:07,282 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-23 23:39:07,282 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-23 23:39:07,283 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-23 23:39:07,284 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-23 23:39:07,284 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-23 23:39:07,285 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-23 23:39:07,286 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-23 23:39:07,287 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-23 23:39:07,287 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-23 23:39:07,288 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-23 23:39:07,289 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-23 23:39:07,289 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-23 23:39:07,290 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-23 23:39:07,290 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-23 23:39:07,291 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-23 23:39:07,292 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-23 23:39:07,293 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-23 23:39:07,293 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-23 23:39:07,294 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-23 23:39:07,295 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-23 23:39:07,295 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-23 23:39:07,296 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-23 23:39:07,297 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-23 23:39:07,298 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-23 23:39:07,298 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-23 23:39:07,299 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-23 23:39:07,300 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-23 23:39:07,300 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-23 23:39:07,301 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-23 23:39:07,302 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-23 23:39:07,302 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-23 23:39:07,303 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-23 23:39:07,303 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-23 23:39:07,304 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-23 23:39:07,305 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-23 23:39:07,305 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-23 23:39:07,306 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-23 23:39:07,307 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-23 23:39:07,307 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-23 23:39:07,308 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-23 23:39:07,309 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-23 23:39:07,309 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-23 23:39:07,310 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-23 23:39:07,311 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-23 23:39:07,311 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-23 23:39:07,312 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-23 23:39:07,312 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-23 23:39:07,313 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-23 23:39:07,314 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-23 23:39:07,314 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-23 23:39:07,315 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-23 23:39:07,315 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-23 23:39:07,316 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-23 23:39:07,317 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-23 23:39:07,317 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-23 23:39:07,318 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-23 23:39:07,319 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-23 23:39:07,319 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-23 23:39:07,320 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-23 23:39:07,321 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-23 23:39:07,321 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-23 23:39:07,322 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-23 23:39:07,322 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-23 23:39:07,323 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-23 23:39:07,324 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-23 23:39:07,324 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-23 23:39:07,325 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-23 23:39:07,325 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-23 23:39:07,326 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-23 23:39:07,327 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-23 23:39:07,327 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-23 23:39:07,328 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-23 23:39:07,328 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-23 23:39:07,329 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-23 23:39:07,330 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-23 23:39:07,330 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-23 23:39:07,331 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-23 23:39:07,331 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-23 23:39:07,332 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-23 23:39:07,333 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-23 23:39:07,333 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-23 23:39:07,334 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-23 23:39:07,335 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-23 23:39:07,335 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-23 23:39:07,336 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-23 23:39:07,337 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-23 23:39:07,337 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-23 23:39:07,338 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-23 23:39:07,339 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-23 23:39:07,339 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-23 23:39:07,340 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-23 23:39:07,340 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-23 23:39:07,341 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-23 23:39:07,341 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-23 23:39:07,342 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-23 23:39:07,343 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-23 23:39:07,343 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-23 23:39:07,344 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-23 23:39:07,344 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-23 23:39:07,345 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-23 23:39:07,346 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-23 23:39:07,346 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-23 23:39:07,347 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-23 23:39:07,348 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-23 23:39:07,348 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-23 23:39:07,349 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-23 23:39:07,349 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-23 23:39:07,350 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-23 23:39:07,351 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-23 23:39:07,351 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-23 23:39:07,352 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-23 23:39:07,353 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-23 23:39:07,353 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-23 23:39:07,354 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-23 23:39:07,355 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-23 23:39:07,356 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-23 23:39:07,356 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-23 23:39:07,357 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-23 23:39:07,357 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-23 23:39:07,358 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-23 23:39:07,359 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-23 23:39:07,379 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-23 23:39:07,380 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-23 23:39:07,380 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-23 23:39:07,381 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-23 23:39:07,386 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-23 23:39:07,386 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-23 23:39:07,387 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-23 23:39:07,391 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-23 23:39:07,391 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-23 23:39:07,392 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-23 23:39:07,393 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-23 23:39:07,398 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-23 23:39:07,398 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-23 23:39:07,398 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-23 23:39:07,901 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,906 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,906 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,914 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,921 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,922 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,923 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,945 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,946 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:07,946 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:07,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,963 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,964 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,964 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:07,965 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:07,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,968 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,973 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,990 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:07,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:07,997 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:07,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,000 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,020 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,020 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,021 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,021 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,021 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,040 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,040 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,053 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,072 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,072 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,073 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,109 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,110 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,115 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,118 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,220 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,229 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,230 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,256 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,260 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,275 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,277 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,277 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,278 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,278 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,280 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,280 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,281 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,281 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,282 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,282 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,284 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,284 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,285 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,285 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,287 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,287 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,288 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,288 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,290 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,290 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,291 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,291 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,293 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,293 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,293 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,308 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,324 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:08,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,396 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,396 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,396 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,398 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,432 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:09,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:09,566 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:12,438 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:39:12,572 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-23 23:39:12,839 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10000 docs
Training CobwebTree:   0%|          | 0/10000 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 20/10000 [00:00<00:50, 197.07it/s]Training CobwebTree:   0%|          | 40/10000 [00:00<00:59, 167.35it/s]Training CobwebTree:   1%|          | 58/10000 [00:00<01:11, 138.12it/s]Training CobwebTree:   1%|          | 73/10000 [00:00<01:23, 118.61it/s]Training CobwebTree:   1%|          | 86/10000 [00:00<01:31, 108.07it/s]Training CobwebTree:   1%|          | 98/10000 [00:00<01:35, 103.47it/s]Training CobwebTree:   1%|          | 109/10000 [00:00<01:47, 91.75it/s]Training CobwebTree:   1%|          | 119/10000 [00:01<01:54, 86.05it/s]Training CobwebTree:   1%|         | 128/10000 [00:01<01:56, 84.85it/s]Training CobwebTree:   1%|         | 137/10000 [00:01<02:04, 79.32it/s]Training CobwebTree:   1%|         | 145/10000 [00:01<02:07, 76.99it/s]Training CobwebTree:   2%|         | 153/10000 [00:01<02:15, 72.89it/s]Training CobwebTree:   2%|         | 161/10000 [00:01<02:20, 69.86it/s]Training CobwebTree:   2%|         | 169/10000 [00:01<02:16, 72.15it/s]Training CobwebTree:   2%|         | 177/10000 [00:01<02:13, 73.50it/s]Training CobwebTree:   2%|         | 185/10000 [00:02<02:17, 71.34it/s]Training CobwebTree:   2%|         | 193/10000 [00:02<02:13, 73.31it/s]Training CobwebTree:   2%|         | 201/10000 [00:02<02:11, 74.74it/s]Training CobwebTree:   2%|         | 209/10000 [00:02<02:18, 70.91it/s]Training CobwebTree:   2%|         | 217/10000 [00:02<02:17, 71.11it/s]Training CobwebTree:   2%|         | 225/10000 [00:02<02:18, 70.74it/s]Training CobwebTree:   2%|         | 233/10000 [00:02<02:22, 68.38it/s]Training CobwebTree:   2%|         | 241/10000 [00:02<02:21, 68.96it/s]Training CobwebTree:   2%|         | 248/10000 [00:02<02:27, 66.19it/s]Training CobwebTree:   3%|         | 256/10000 [00:03<02:21, 68.82it/s]Training CobwebTree:   3%|         | 264/10000 [00:03<02:18, 70.20it/s]Training CobwebTree:   3%|         | 272/10000 [00:03<02:16, 71.35it/s]Training CobwebTree:   3%|         | 280/10000 [00:03<02:14, 72.08it/s]Training CobwebTree:   3%|         | 288/10000 [00:03<02:13, 72.92it/s]Training CobwebTree:   3%|         | 296/10000 [00:03<02:18, 70.10it/s]Training CobwebTree:   3%|         | 304/10000 [00:03<02:21, 68.52it/s]Training CobwebTree:   3%|         | 311/10000 [00:03<02:22, 68.20it/s]Training CobwebTree:   3%|         | 318/10000 [00:03<02:24, 66.77it/s]Training CobwebTree:   3%|         | 325/10000 [00:04<02:35, 62.34it/s]Training CobwebTree:   3%|         | 332/10000 [00:04<02:31, 63.75it/s]Training CobwebTree:   3%|         | 339/10000 [00:04<02:29, 64.45it/s]Training CobwebTree:   3%|         | 346/10000 [00:04<02:27, 65.65it/s]Training CobwebTree:   4%|         | 353/10000 [00:04<02:31, 63.82it/s]Training CobwebTree:   4%|         | 360/10000 [00:04<02:39, 60.42it/s]Training CobwebTree:   4%|         | 367/10000 [00:04<02:39, 60.37it/s]Training CobwebTree:   4%|         | 375/10000 [00:04<02:33, 62.89it/s]Training CobwebTree:   4%|         | 382/10000 [00:05<02:31, 63.44it/s]Training CobwebTree:   4%|         | 390/10000 [00:05<02:29, 64.44it/s]Training CobwebTree:   4%|         | 397/10000 [00:05<02:30, 63.65it/s]Training CobwebTree:   4%|         | 405/10000 [00:05<02:24, 66.38it/s]Training CobwebTree:   4%|         | 412/10000 [00:05<02:28, 64.71it/s]Training CobwebTree:   4%|         | 420/10000 [00:05<02:23, 66.86it/s]Training CobwebTree:   4%|         | 428/10000 [00:05<02:18, 69.18it/s]Training CobwebTree:   4%|         | 435/10000 [00:05<02:25, 65.57it/s]Training CobwebTree:   4%|         | 442/10000 [00:05<02:26, 65.28it/s]Training CobwebTree:   4%|         | 449/10000 [00:06<02:25, 65.53it/s]Training CobwebTree:   5%|         | 463/10000 [00:06<01:50, 86.27it/s]Training CobwebTree:   5%|         | 475/10000 [00:06<01:42, 93.36it/s]Training CobwebTree:   5%|         | 486/10000 [00:06<01:38, 96.98it/s]Training CobwebTree:   5%|         | 496/10000 [00:06<01:42, 92.67it/s]Training CobwebTree:   5%|         | 506/10000 [00:06<01:55, 81.88it/s]Training CobwebTree:   5%|         | 515/10000 [00:06<02:05, 75.33it/s]Training CobwebTree:   5%|         | 523/10000 [00:06<02:06, 75.14it/s]Training CobwebTree:   5%|         | 531/10000 [00:06<02:10, 72.61it/s]Training CobwebTree:   5%|         | 539/10000 [00:07<02:09, 72.94it/s]Training CobwebTree:   5%|         | 547/10000 [00:07<02:15, 69.80it/s]Training CobwebTree:   6%|         | 555/10000 [00:07<02:18, 67.96it/s]Training CobwebTree:   6%|         | 564/10000 [00:07<02:09, 72.71it/s]Training CobwebTree:   6%|         | 572/10000 [00:07<02:09, 73.04it/s]Training CobwebTree:   6%|         | 580/10000 [00:07<02:10, 72.10it/s]Training CobwebTree:   6%|         | 588/10000 [00:07<02:12, 71.30it/s]Training CobwebTree:   6%|         | 596/10000 [00:07<02:10, 72.01it/s]Training CobwebTree:   6%|         | 605/10000 [00:07<02:04, 75.46it/s]Training CobwebTree:   6%|         | 613/10000 [00:08<02:06, 74.30it/s]Training CobwebTree:   6%|         | 621/10000 [00:08<02:08, 72.77it/s]Training CobwebTree:   6%|         | 629/10000 [00:08<02:08, 72.92it/s]Training CobwebTree:   6%|         | 637/10000 [00:08<02:12, 70.68it/s]Training CobwebTree:   6%|         | 645/10000 [00:08<02:17, 68.20it/s]Training CobwebTree:   7%|         | 653/10000 [00:08<02:13, 69.79it/s]Training CobwebTree:   7%|         | 661/10000 [00:08<02:23, 64.97it/s]Training CobwebTree:   7%|         | 668/10000 [00:08<02:23, 65.00it/s]Training CobwebTree:   7%|         | 675/10000 [00:09<02:23, 65.06it/s]Training CobwebTree:   7%|         | 685/10000 [00:09<02:10, 71.44it/s]Training CobwebTree:   7%|         | 693/10000 [00:09<02:12, 70.03it/s]Training CobwebTree:   7%|         | 701/10000 [00:09<02:16, 68.09it/s]Training CobwebTree:   7%|         | 708/10000 [00:09<02:18, 67.02it/s]Training CobwebTree:   7%|         | 715/10000 [00:09<02:17, 67.36it/s]Training CobwebTree:   7%|         | 722/10000 [00:09<02:17, 67.60it/s]Training CobwebTree:   7%|         | 729/10000 [00:09<02:18, 66.88it/s]Training CobwebTree:   7%|         | 736/10000 [00:09<02:18, 66.87it/s]Training CobwebTree:   7%|         | 744/10000 [00:10<02:15, 68.24it/s]Training CobwebTree:   8%|         | 752/10000 [00:10<02:11, 70.59it/s]Training CobwebTree:   8%|         | 760/10000 [00:10<02:18, 66.55it/s]Training CobwebTree:   8%|         | 768/10000 [00:10<02:14, 68.48it/s]Training CobwebTree:   8%|         | 776/10000 [00:10<02:10, 70.91it/s]Training CobwebTree:   8%|         | 784/10000 [00:10<02:15, 67.93it/s]Training CobwebTree:   8%|         | 791/10000 [00:10<02:23, 64.12it/s]Training CobwebTree:   8%|         | 798/10000 [00:10<02:23, 64.02it/s]Training CobwebTree:   8%|         | 805/10000 [00:10<02:28, 61.91it/s]Training CobwebTree:   8%|         | 812/10000 [00:11<02:25, 62.99it/s]Training CobwebTree:   8%|         | 819/10000 [00:11<02:23, 64.11it/s]Training CobwebTree:   8%|         | 826/10000 [00:11<02:36, 58.57it/s]Training CobwebTree:   8%|         | 833/10000 [00:11<02:33, 59.76it/s]Training CobwebTree:   8%|         | 841/10000 [00:11<02:26, 62.44it/s]Training CobwebTree:   8%|         | 848/10000 [00:11<02:27, 62.21it/s]Training CobwebTree:   9%|         | 855/10000 [00:11<02:33, 59.53it/s]Training CobwebTree:   9%|         | 862/10000 [00:11<02:32, 59.96it/s]Training CobwebTree:   9%|         | 869/10000 [00:12<02:31, 60.29it/s]Training CobwebTree:   9%|         | 876/10000 [00:12<02:28, 61.26it/s]Training CobwebTree:   9%|         | 883/10000 [00:12<02:30, 60.65it/s]Training CobwebTree:   9%|         | 890/10000 [00:12<02:42, 55.99it/s]Training CobwebTree:   9%|         | 897/10000 [00:12<02:38, 57.34it/s]Training CobwebTree:   9%|         | 904/10000 [00:12<02:35, 58.68it/s]Training CobwebTree:   9%|         | 910/10000 [00:12<02:35, 58.43it/s]Training CobwebTree:   9%|         | 916/10000 [00:12<02:35, 58.59it/s]Training CobwebTree:   9%|         | 922/10000 [00:12<02:35, 58.47it/s]Training CobwebTree:   9%|         | 929/10000 [00:13<02:34, 58.68it/s]Training CobwebTree:   9%|         | 935/10000 [00:13<02:38, 57.27it/s]Training CobwebTree:   9%|         | 942/10000 [00:13<02:31, 59.67it/s]Training CobwebTree:   9%|         | 948/10000 [00:13<02:31, 59.69it/s]Training CobwebTree:  10%|         | 954/10000 [00:13<02:33, 59.10it/s]Training CobwebTree:  10%|         | 960/10000 [00:13<02:36, 57.83it/s]Training CobwebTree:  10%|         | 966/10000 [00:13<02:39, 56.81it/s]Training CobwebTree:  10%|         | 972/10000 [00:13<02:37, 57.40it/s]Training CobwebTree:  10%|         | 978/10000 [00:13<02:36, 57.57it/s]Training CobwebTree:  10%|         | 985/10000 [00:14<02:33, 58.60it/s]Training CobwebTree:  10%|         | 991/10000 [00:14<02:34, 58.28it/s]Training CobwebTree:  10%|         | 997/10000 [00:14<02:40, 56.00it/s]Training CobwebTree:  10%|         | 1003/10000 [00:14<02:38, 56.59it/s]Training CobwebTree:  10%|         | 1009/10000 [00:14<02:37, 56.98it/s]Training CobwebTree:  10%|         | 1015/10000 [00:14<02:36, 57.60it/s]Training CobwebTree:  10%|         | 1021/10000 [00:14<02:38, 56.74it/s]Training CobwebTree:  10%|         | 1028/10000 [00:14<02:34, 58.13it/s]Training CobwebTree:  10%|         | 1035/10000 [00:14<02:27, 60.98it/s]Training CobwebTree:  10%|         | 1042/10000 [00:15<02:25, 61.58it/s]Training CobwebTree:  10%|         | 1049/10000 [00:15<02:26, 61.08it/s]Training CobwebTree:  11%|         | 1056/10000 [00:15<02:21, 63.28it/s]Training CobwebTree:  11%|         | 1063/10000 [00:15<02:28, 60.38it/s]Training CobwebTree:  11%|         | 1070/10000 [00:15<02:25, 61.30it/s]Training CobwebTree:  11%|         | 1077/10000 [00:15<02:22, 62.40it/s]Training CobwebTree:  11%|         | 1084/10000 [00:15<02:28, 60.03it/s]Training CobwebTree:  11%|         | 1091/10000 [00:15<02:24, 61.57it/s]Training CobwebTree:  11%|         | 1099/10000 [00:15<02:16, 65.21it/s]Training CobwebTree:  11%|         | 1106/10000 [00:16<02:26, 60.85it/s]Training CobwebTree:  11%|         | 1113/10000 [00:16<02:22, 62.49it/s]Training CobwebTree:  11%|         | 1120/10000 [00:16<02:26, 60.78it/s]Training CobwebTree:  11%|        | 1127/10000 [00:16<02:21, 62.65it/s]Training CobwebTree:  11%|        | 1134/10000 [00:16<04:31, 32.69it/s]Training CobwebTree:  11%|        | 1140/10000 [00:16<03:58, 37.17it/s]Training CobwebTree:  11%|        | 1147/10000 [00:17<03:26, 42.83it/s]Training CobwebTree:  12%|        | 1155/10000 [00:17<02:59, 49.27it/s]Training CobwebTree:  12%|        | 1162/10000 [00:17<02:46, 53.14it/s]Training CobwebTree:  12%|        | 1169/10000 [00:17<02:37, 56.10it/s]Training CobwebTree:  12%|        | 1176/10000 [00:17<02:35, 56.80it/s]Training CobwebTree:  12%|        | 1183/10000 [00:17<02:34, 57.25it/s]Training CobwebTree:  12%|        | 1190/10000 [00:17<02:28, 59.32it/s]Training CobwebTree:  12%|        | 1197/10000 [00:17<02:25, 60.56it/s]Training CobwebTree:  12%|        | 1204/10000 [00:17<02:23, 61.37it/s]Training CobwebTree:  12%|        | 1211/10000 [00:18<02:21, 62.00it/s]Training CobwebTree:  12%|        | 1218/10000 [00:18<02:22, 61.65it/s]Training CobwebTree:  12%|        | 1225/10000 [00:18<02:18, 63.17it/s]Training CobwebTree:  12%|        | 1232/10000 [00:18<02:20, 62.44it/s]Training CobwebTree:  12%|        | 1240/10000 [00:18<02:16, 64.05it/s]Training CobwebTree:  12%|        | 1247/10000 [00:18<02:20, 62.08it/s]Training CobwebTree:  13%|        | 1254/10000 [00:18<02:21, 61.96it/s]Training CobwebTree:  13%|        | 1261/10000 [00:18<02:23, 60.94it/s]Training CobwebTree:  13%|        | 1268/10000 [00:18<02:22, 61.46it/s]Training CobwebTree:  13%|        | 1275/10000 [00:19<02:19, 62.63it/s]Training CobwebTree:  13%|        | 1282/10000 [00:19<02:26, 59.48it/s]Training CobwebTree:  13%|        | 1289/10000 [00:19<02:26, 59.27it/s]Training CobwebTree:  13%|        | 1296/10000 [00:19<02:25, 59.98it/s]Training CobwebTree:  13%|        | 1303/10000 [00:19<02:24, 60.28it/s]Training CobwebTree:  13%|        | 1310/10000 [00:19<02:18, 62.89it/s]Training CobwebTree:  13%|        | 1317/10000 [00:19<02:19, 62.35it/s]Training CobwebTree:  13%|        | 1324/10000 [00:19<02:16, 63.66it/s]Training CobwebTree:  13%|        | 1331/10000 [00:19<02:19, 62.34it/s]Training CobwebTree:  13%|        | 1338/10000 [00:20<02:26, 58.97it/s]Training CobwebTree:  13%|        | 1344/10000 [00:20<02:32, 56.69it/s]Training CobwebTree:  14%|        | 1350/10000 [00:20<02:38, 54.57it/s]Training CobwebTree:  14%|        | 1356/10000 [00:20<02:36, 55.24it/s]Training CobwebTree:  14%|        | 1362/10000 [00:20<02:43, 52.95it/s]Training CobwebTree:  14%|        | 1368/10000 [00:20<02:37, 54.81it/s]Training CobwebTree:  14%|        | 1374/10000 [00:20<02:40, 53.85it/s]Training CobwebTree:  14%|        | 1380/10000 [00:20<02:36, 55.07it/s]Training CobwebTree:  14%|        | 1387/10000 [00:21<02:31, 57.02it/s]Training CobwebTree:  14%|        | 1394/10000 [00:21<02:27, 58.17it/s]Training CobwebTree:  14%|        | 1400/10000 [00:21<02:30, 57.18it/s]Training CobwebTree:  14%|        | 1407/10000 [00:21<02:24, 59.28it/s]Training CobwebTree:  14%|        | 1413/10000 [00:21<02:27, 58.40it/s]Training CobwebTree:  14%|        | 1419/10000 [00:21<02:27, 58.36it/s]Training CobwebTree:  14%|        | 1425/10000 [00:21<02:26, 58.53it/s]Training CobwebTree:  14%|        | 1431/10000 [00:21<02:34, 55.64it/s]Training CobwebTree:  14%|        | 1438/10000 [00:21<02:26, 58.43it/s]Training CobwebTree:  14%|        | 1444/10000 [00:21<02:26, 58.22it/s]Training CobwebTree:  14%|        | 1450/10000 [00:22<02:32, 56.23it/s]Training CobwebTree:  15%|        | 1456/10000 [00:22<02:37, 54.20it/s]Training CobwebTree:  15%|        | 1462/10000 [00:22<02:35, 54.77it/s]Training CobwebTree:  15%|        | 1469/10000 [00:22<02:31, 56.33it/s]Training CobwebTree:  15%|        | 1476/10000 [00:22<02:25, 58.68it/s]Training CobwebTree:  15%|        | 1482/10000 [00:22<02:27, 57.84it/s]Training CobwebTree:  15%|        | 1489/10000 [00:22<02:23, 59.30it/s]Training CobwebTree:  15%|        | 1496/10000 [00:22<02:19, 60.90it/s]Training CobwebTree:  15%|        | 1503/10000 [00:23<02:22, 59.48it/s]Training CobwebTree:  15%|        | 1509/10000 [00:23<02:27, 57.68it/s]Training CobwebTree:  15%|        | 1515/10000 [00:23<02:30, 56.53it/s]Training CobwebTree:  15%|        | 1521/10000 [00:23<02:27, 57.46it/s]Training CobwebTree:  15%|        | 1529/10000 [00:23<02:17, 61.51it/s]Training CobwebTree:  15%|        | 1536/10000 [00:23<02:14, 63.11it/s]Training CobwebTree:  15%|        | 1543/10000 [00:23<02:16, 62.04it/s]Training CobwebTree:  16%|        | 1551/10000 [00:23<02:10, 64.72it/s]Training CobwebTree:  16%|        | 1558/10000 [00:23<02:14, 62.56it/s]Training CobwebTree:  16%|        | 1565/10000 [00:24<02:15, 62.17it/s]Training CobwebTree:  16%|        | 1572/10000 [00:24<02:26, 57.72it/s]Training CobwebTree:  16%|        | 1579/10000 [00:24<02:21, 59.53it/s]Training CobwebTree:  16%|        | 1586/10000 [00:24<02:26, 57.33it/s]Training CobwebTree:  16%|        | 1593/10000 [00:24<02:23, 58.50it/s]Training CobwebTree:  16%|        | 1600/10000 [00:24<02:19, 60.27it/s]Training CobwebTree:  16%|        | 1608/10000 [00:24<02:12, 63.17it/s]Training CobwebTree:  16%|        | 1615/10000 [00:24<02:14, 62.19it/s]Training CobwebTree:  16%|        | 1622/10000 [00:24<02:11, 63.55it/s]Training CobwebTree:  16%|        | 1629/10000 [00:25<02:15, 61.75it/s]Training CobwebTree:  16%|        | 1636/10000 [00:25<02:24, 57.77it/s]Training CobwebTree:  16%|        | 1642/10000 [00:25<02:28, 56.29it/s]Training CobwebTree:  16%|        | 1648/10000 [00:25<02:27, 56.67it/s]Training CobwebTree:  17%|        | 1654/10000 [00:25<02:32, 54.61it/s]Training CobwebTree:  17%|        | 1660/10000 [00:25<02:33, 54.43it/s]Training CobwebTree:  17%|        | 1666/10000 [00:25<02:31, 55.18it/s]Training CobwebTree:  17%|        | 1672/10000 [00:25<02:30, 55.34it/s]Training CobwebTree:  17%|        | 1679/10000 [00:26<02:25, 57.18it/s]Training CobwebTree:  17%|        | 1685/10000 [00:26<02:24, 57.73it/s]Training CobwebTree:  17%|        | 1691/10000 [00:26<02:28, 55.78it/s]Training CobwebTree:  17%|        | 1698/10000 [00:26<02:20, 58.90it/s]Training CobwebTree:  17%|        | 1704/10000 [00:26<02:21, 58.73it/s]Training CobwebTree:  17%|        | 1711/10000 [00:26<02:14, 61.66it/s]Training CobwebTree:  17%|        | 1718/10000 [00:26<02:15, 61.12it/s]Training CobwebTree:  17%|        | 1725/10000 [00:26<02:15, 61.08it/s]Training CobwebTree:  17%|        | 1732/10000 [00:26<02:17, 60.00it/s]Training CobwebTree:  17%|        | 1739/10000 [00:26<02:16, 60.73it/s]Training CobwebTree:  17%|        | 1746/10000 [00:27<02:20, 58.91it/s]Training CobwebTree:  18%|        | 1752/10000 [00:27<02:21, 58.33it/s]Training CobwebTree:  18%|        | 1758/10000 [00:27<02:23, 57.41it/s]Training CobwebTree:  18%|        | 1765/10000 [00:27<02:16, 60.39it/s]Training CobwebTree:  18%|        | 1772/10000 [00:27<02:18, 59.59it/s]Training CobwebTree:  18%|        | 1778/10000 [00:27<02:22, 57.75it/s]Training CobwebTree:  18%|        | 1784/10000 [00:27<02:24, 56.80it/s]Training CobwebTree:  18%|        | 1790/10000 [00:27<02:23, 57.36it/s]Training CobwebTree:  18%|        | 1797/10000 [00:27<02:20, 58.28it/s]Training CobwebTree:  18%|        | 1803/10000 [00:28<02:27, 55.65it/s]Training CobwebTree:  18%|        | 1809/10000 [00:28<02:25, 56.24it/s]Training CobwebTree:  18%|        | 1815/10000 [00:28<02:24, 56.68it/s]Training CobwebTree:  18%|        | 1821/10000 [00:28<02:24, 56.41it/s]Training CobwebTree:  18%|        | 1827/10000 [00:28<02:24, 56.54it/s]Training CobwebTree:  18%|        | 1833/10000 [00:28<02:27, 55.41it/s]Training CobwebTree:  18%|        | 1839/10000 [00:28<02:24, 56.46it/s]Training CobwebTree:  18%|        | 1846/10000 [00:28<02:18, 58.84it/s]Training CobwebTree:  19%|        | 1852/10000 [00:28<02:21, 57.70it/s]Training CobwebTree:  19%|        | 1858/10000 [00:29<02:22, 57.20it/s]Training CobwebTree:  19%|        | 1864/10000 [00:29<02:26, 55.57it/s]Training CobwebTree:  19%|        | 1870/10000 [00:29<02:28, 54.59it/s]Training CobwebTree:  19%|        | 1876/10000 [00:29<02:27, 55.10it/s]Training CobwebTree:  19%|        | 1882/10000 [00:29<02:25, 55.71it/s]Training CobwebTree:  19%|        | 1889/10000 [00:29<02:17, 58.98it/s]Training CobwebTree:  19%|        | 1895/10000 [00:29<02:24, 56.11it/s]Training CobwebTree:  19%|        | 1901/10000 [00:29<02:23, 56.55it/s]Training CobwebTree:  19%|        | 1907/10000 [00:29<02:28, 54.32it/s]Training CobwebTree:  19%|        | 1914/10000 [00:30<02:23, 56.54it/s]Training CobwebTree:  19%|        | 1920/10000 [00:30<02:24, 55.92it/s]Training CobwebTree:  19%|        | 1927/10000 [00:30<02:21, 56.85it/s]Training CobwebTree:  19%|        | 1933/10000 [00:30<02:21, 56.83it/s]Training CobwebTree:  19%|        | 1940/10000 [00:30<02:18, 58.39it/s]Training CobwebTree:  19%|        | 1946/10000 [00:30<02:20, 57.37it/s]Training CobwebTree:  20%|        | 1952/10000 [00:30<02:21, 56.77it/s]Training CobwebTree:  20%|        | 1958/10000 [00:30<02:20, 57.26it/s]Training CobwebTree:  20%|        | 1964/10000 [00:30<02:25, 55.36it/s]Training CobwebTree:  20%|        | 1970/10000 [00:31<02:22, 56.49it/s]Training CobwebTree:  20%|        | 1976/10000 [00:31<02:20, 57.07it/s]Training CobwebTree:  20%|        | 1982/10000 [00:31<02:22, 56.12it/s]Training CobwebTree:  20%|        | 1988/10000 [00:31<02:24, 55.54it/s]Training CobwebTree:  20%|        | 1994/10000 [00:31<02:21, 56.58it/s]Training CobwebTree:  20%|        | 2000/10000 [00:31<02:21, 56.49it/s]Training CobwebTree:  20%|        | 2006/10000 [00:31<02:24, 55.41it/s]Training CobwebTree:  20%|        | 2013/10000 [00:31<02:21, 56.34it/s]Training CobwebTree:  20%|        | 2020/10000 [00:31<02:13, 59.84it/s]Training CobwebTree:  20%|        | 2027/10000 [00:32<02:14, 59.16it/s]Training CobwebTree:  20%|        | 2033/10000 [00:32<02:16, 58.44it/s]Training CobwebTree:  20%|        | 2039/10000 [00:32<02:21, 56.28it/s]Training CobwebTree:  20%|        | 2045/10000 [00:32<02:27, 53.92it/s]Training CobwebTree:  21%|        | 2051/10000 [00:32<02:24, 54.95it/s]Training CobwebTree:  21%|        | 2057/10000 [00:32<02:24, 54.99it/s]Training CobwebTree:  21%|        | 2063/10000 [00:32<02:22, 55.71it/s]Training CobwebTree:  21%|        | 2069/10000 [00:32<02:22, 55.72it/s]Training CobwebTree:  21%|        | 2075/10000 [00:32<02:21, 56.16it/s]Training CobwebTree:  21%|        | 2081/10000 [00:33<02:27, 53.74it/s]Training CobwebTree:  21%|        | 2087/10000 [00:33<02:23, 55.27it/s]Training CobwebTree:  21%|        | 2093/10000 [00:33<02:23, 55.24it/s]Training CobwebTree:  21%|        | 2099/10000 [00:33<02:22, 55.35it/s]Training CobwebTree:  21%|        | 2105/10000 [00:33<02:29, 52.69it/s]Training CobwebTree:  21%|        | 2111/10000 [00:33<02:25, 54.11it/s]Training CobwebTree:  21%|        | 2117/10000 [00:33<02:22, 55.48it/s]Training CobwebTree:  21%|        | 2124/10000 [00:33<02:17, 57.38it/s]Training CobwebTree:  21%|       | 2131/10000 [00:33<02:14, 58.34it/s]Training CobwebTree:  21%|       | 2137/10000 [00:34<02:16, 57.73it/s]Training CobwebTree:  21%|       | 2143/10000 [00:34<02:22, 55.28it/s]Training CobwebTree:  21%|       | 2149/10000 [00:34<02:23, 54.66it/s]Training CobwebTree:  22%|       | 2155/10000 [00:34<02:22, 55.09it/s]Training CobwebTree:  22%|       | 2162/10000 [00:34<02:17, 57.06it/s]Training CobwebTree:  22%|       | 2168/10000 [00:34<02:17, 57.11it/s]Training CobwebTree:  22%|       | 2174/10000 [00:34<02:15, 57.79it/s]Training CobwebTree:  22%|       | 2181/10000 [00:34<02:09, 60.33it/s]Training CobwebTree:  22%|       | 2188/10000 [00:34<02:13, 58.49it/s]Training CobwebTree:  22%|       | 2194/10000 [00:35<02:16, 57.10it/s]Training CobwebTree:  22%|       | 2200/10000 [00:35<02:17, 56.78it/s]Training CobwebTree:  22%|       | 2206/10000 [00:35<02:15, 57.45it/s]Training CobwebTree:  22%|       | 2212/10000 [00:35<02:16, 56.90it/s]Training CobwebTree:  22%|       | 2219/10000 [00:35<02:14, 57.71it/s]Training CobwebTree:  22%|       | 2225/10000 [00:35<02:13, 58.30it/s]Training CobwebTree:  22%|       | 2232/10000 [00:35<02:09, 59.82it/s]Training CobwebTree:  22%|       | 2239/10000 [00:35<02:09, 59.84it/s]Training CobwebTree:  22%|       | 2245/10000 [00:35<02:11, 58.76it/s]Training CobwebTree:  23%|       | 2251/10000 [00:36<02:15, 57.13it/s]Training CobwebTree:  23%|       | 2258/10000 [00:36<02:10, 59.22it/s]Training CobwebTree:  23%|       | 2264/10000 [00:36<02:13, 57.99it/s]Training CobwebTree:  23%|       | 2270/10000 [00:36<02:12, 58.14it/s]Training CobwebTree:  23%|       | 2276/10000 [00:36<02:16, 56.60it/s]Training CobwebTree:  23%|       | 2283/10000 [00:36<02:14, 57.49it/s]Training CobwebTree:  23%|       | 2289/10000 [00:36<02:20, 55.03it/s]Training CobwebTree:  23%|       | 2295/10000 [00:36<02:18, 55.63it/s]Training CobwebTree:  23%|       | 2301/10000 [00:36<02:17, 56.03it/s]Training CobwebTree:  23%|       | 2308/10000 [00:37<02:13, 57.46it/s]Training CobwebTree:  23%|       | 2314/10000 [00:37<02:15, 56.71it/s]Training CobwebTree:  23%|       | 2320/10000 [00:37<02:14, 57.00it/s]Training CobwebTree:  23%|       | 2326/10000 [00:37<02:22, 53.96it/s]Training CobwebTree:  23%|       | 2333/10000 [00:37<02:18, 55.37it/s]Training CobwebTree:  23%|       | 2339/10000 [00:37<02:19, 55.02it/s]Training CobwebTree:  23%|       | 2345/10000 [00:37<02:25, 52.70it/s]Training CobwebTree:  24%|       | 2351/10000 [00:37<02:25, 52.50it/s]Training CobwebTree:  24%|       | 2357/10000 [00:37<02:22, 53.66it/s]Training CobwebTree:  24%|       | 2363/10000 [00:38<02:25, 52.43it/s]Training CobwebTree:  24%|       | 2369/10000 [00:38<02:26, 52.20it/s]Training CobwebTree:  24%|       | 2376/10000 [00:38<02:19, 54.82it/s]Training CobwebTree:  24%|       | 2382/10000 [00:38<02:18, 55.11it/s]Training CobwebTree:  24%|       | 2388/10000 [00:38<02:17, 55.25it/s]Training CobwebTree:  24%|       | 2395/10000 [00:38<02:15, 56.06it/s]Training CobwebTree:  24%|       | 2402/10000 [00:38<02:11, 57.75it/s]Training CobwebTree:  24%|       | 2408/10000 [00:38<02:18, 54.90it/s]Training CobwebTree:  24%|       | 2414/10000 [00:38<02:19, 54.49it/s]Training CobwebTree:  24%|       | 2420/10000 [00:39<02:22, 53.32it/s]Training CobwebTree:  24%|       | 2427/10000 [00:39<02:15, 55.81it/s]Training CobwebTree:  24%|       | 2433/10000 [00:39<02:19, 54.10it/s]Training CobwebTree:  24%|       | 2439/10000 [00:39<02:18, 54.70it/s]Training CobwebTree:  24%|       | 2446/10000 [00:39<02:12, 57.22it/s]Training CobwebTree:  25%|       | 2452/10000 [00:39<02:13, 56.64it/s]Training CobwebTree:  25%|       | 2459/10000 [00:39<02:10, 57.98it/s]Training CobwebTree:  25%|       | 2465/10000 [00:39<02:12, 57.02it/s]Training CobwebTree:  25%|       | 2471/10000 [00:39<02:17, 54.73it/s]Training CobwebTree:  25%|       | 2477/10000 [00:40<02:21, 53.24it/s]Training CobwebTree:  25%|       | 2483/10000 [00:40<02:25, 51.71it/s]Training CobwebTree:  25%|       | 2489/10000 [00:40<02:29, 50.24it/s]Training CobwebTree:  25%|       | 2495/10000 [00:40<02:22, 52.51it/s]Training CobwebTree:  25%|       | 2501/10000 [00:40<02:27, 50.76it/s]Training CobwebTree:  25%|       | 2507/10000 [00:40<02:25, 51.34it/s]Training CobwebTree:  25%|       | 2513/10000 [00:40<02:23, 52.24it/s]Training CobwebTree:  25%|       | 2519/10000 [00:40<02:27, 50.74it/s]Training CobwebTree:  25%|       | 2525/10000 [00:41<02:23, 52.21it/s]Training CobwebTree:  25%|       | 2531/10000 [00:41<02:22, 52.24it/s]Training CobwebTree:  25%|       | 2537/10000 [00:41<02:29, 49.99it/s]Training CobwebTree:  25%|       | 2543/10000 [00:41<02:25, 51.26it/s]Training CobwebTree:  25%|       | 2549/10000 [00:41<02:25, 51.17it/s]Training CobwebTree:  26%|       | 2555/10000 [00:41<02:26, 50.72it/s]Training CobwebTree:  26%|       | 2561/10000 [00:41<02:28, 50.10it/s]Training CobwebTree:  26%|       | 2567/10000 [00:41<02:24, 51.54it/s]Training CobwebTree:  26%|       | 2573/10000 [00:42<02:24, 51.37it/s]Training CobwebTree:  26%|       | 2579/10000 [00:42<02:25, 51.14it/s]Training CobwebTree:  26%|       | 2585/10000 [00:42<02:18, 53.41it/s]Training CobwebTree:  26%|       | 2591/10000 [00:42<02:17, 53.74it/s]Training CobwebTree:  26%|       | 2597/10000 [00:42<02:18, 53.55it/s]Training CobwebTree:  26%|       | 2603/10000 [00:42<02:20, 52.52it/s]Training CobwebTree:  26%|       | 2609/10000 [00:42<02:16, 54.25it/s]Training CobwebTree:  26%|       | 2615/10000 [00:42<02:17, 53.70it/s]Training CobwebTree:  26%|       | 2621/10000 [00:42<02:25, 50.87it/s]Training CobwebTree:  26%|       | 2627/10000 [00:43<02:19, 52.71it/s]Training CobwebTree:  26%|       | 2633/10000 [00:43<02:16, 54.12it/s]Training CobwebTree:  26%|       | 2639/10000 [00:43<02:16, 53.75it/s]Training CobwebTree:  26%|       | 2645/10000 [00:43<02:27, 49.99it/s]Training CobwebTree:  27%|       | 2651/10000 [00:43<02:30, 48.88it/s]Training CobwebTree:  27%|       | 2657/10000 [00:43<02:26, 50.28it/s]Training CobwebTree:  27%|       | 2663/10000 [00:43<02:23, 51.05it/s]Training CobwebTree:  27%|       | 2669/10000 [00:43<02:23, 50.96it/s]Training CobwebTree:  27%|       | 2675/10000 [00:43<02:19, 52.44it/s]Training CobwebTree:  27%|       | 2682/10000 [00:44<02:11, 55.56it/s]Training CobwebTree:  27%|       | 2688/10000 [00:44<02:18, 52.98it/s]Training CobwebTree:  27%|       | 2694/10000 [00:44<02:16, 53.52it/s]Training CobwebTree:  27%|       | 2700/10000 [00:44<02:22, 51.19it/s]Training CobwebTree:  27%|       | 2706/10000 [00:44<02:23, 50.66it/s]Training CobwebTree:  27%|       | 2712/10000 [00:44<02:23, 50.91it/s]Training CobwebTree:  27%|       | 2718/10000 [00:44<02:27, 49.51it/s]Training CobwebTree:  27%|       | 2724/10000 [00:44<02:22, 50.93it/s]Training CobwebTree:  27%|       | 2730/10000 [00:45<02:20, 51.91it/s]Training CobwebTree:  27%|       | 2736/10000 [00:45<02:17, 52.94it/s]Training CobwebTree:  27%|       | 2742/10000 [00:45<02:19, 52.18it/s]Training CobwebTree:  27%|       | 2748/10000 [00:45<02:18, 52.43it/s]Training CobwebTree:  28%|       | 2755/10000 [00:45<02:13, 54.17it/s]Training CobwebTree:  28%|       | 2761/10000 [00:45<02:18, 52.20it/s]Training CobwebTree:  28%|       | 2767/10000 [00:45<02:17, 52.68it/s]Training CobwebTree:  28%|       | 2773/10000 [00:45<02:18, 52.03it/s]Training CobwebTree:  28%|       | 2779/10000 [00:45<02:27, 49.07it/s]Training CobwebTree:  28%|       | 2785/10000 [00:46<02:24, 49.95it/s]Training CobwebTree:  28%|       | 2791/10000 [00:46<02:21, 50.89it/s]Training CobwebTree:  28%|       | 2797/10000 [00:46<02:17, 52.57it/s]Training CobwebTree:  28%|       | 2803/10000 [00:46<02:15, 52.96it/s]Training CobwebTree:  28%|       | 2809/10000 [00:46<02:11, 54.64it/s]Training CobwebTree:  28%|       | 2815/10000 [00:46<02:13, 53.85it/s]Training CobwebTree:  28%|       | 2821/10000 [00:46<02:13, 53.74it/s]Training CobwebTree:  28%|       | 2827/10000 [00:46<02:17, 52.30it/s]Training CobwebTree:  28%|       | 2833/10000 [00:46<02:15, 52.94it/s]Training CobwebTree:  28%|       | 2839/10000 [00:47<02:16, 52.55it/s]Training CobwebTree:  28%|       | 2845/10000 [00:47<02:11, 54.49it/s]Training CobwebTree:  29%|       | 2851/10000 [00:47<02:14, 53.17it/s]Training CobwebTree:  29%|       | 2857/10000 [00:47<02:13, 53.40it/s]Training CobwebTree:  29%|       | 2863/10000 [00:47<02:17, 52.05it/s]Training CobwebTree:  29%|       | 2869/10000 [00:47<02:18, 51.43it/s]Training CobwebTree:  29%|       | 2875/10000 [00:47<02:16, 52.14it/s]Training CobwebTree:  29%|       | 2881/10000 [00:47<02:18, 51.50it/s]Training CobwebTree:  29%|       | 2887/10000 [00:48<02:13, 53.23it/s]Training CobwebTree:  29%|       | 2893/10000 [00:48<02:11, 53.90it/s]Training CobwebTree:  29%|       | 2899/10000 [00:48<02:15, 52.53it/s]Training CobwebTree:  29%|       | 2905/10000 [00:48<02:14, 52.79it/s]Training CobwebTree:  29%|       | 2912/10000 [00:48<02:07, 55.76it/s]Training CobwebTree:  29%|       | 2919/10000 [00:48<02:04, 57.01it/s]Training CobwebTree:  29%|       | 2925/10000 [00:48<02:04, 56.72it/s]Training CobwebTree:  29%|       | 2931/10000 [00:48<02:13, 52.90it/s]Training CobwebTree:  29%|       | 2937/10000 [00:48<02:14, 52.70it/s]Training CobwebTree:  29%|       | 2943/10000 [00:49<02:10, 54.11it/s]Training CobwebTree:  29%|       | 2949/10000 [00:49<02:11, 53.58it/s]Training CobwebTree:  30%|       | 2955/10000 [00:49<02:11, 53.55it/s]Training CobwebTree:  30%|       | 2961/10000 [00:49<02:14, 52.22it/s]Training CobwebTree:  30%|       | 2967/10000 [00:49<02:11, 53.63it/s]Training CobwebTree:  30%|       | 2973/10000 [00:49<02:17, 51.22it/s]Training CobwebTree:  30%|       | 2979/10000 [00:49<02:18, 50.85it/s]Training CobwebTree:  30%|       | 2985/10000 [00:49<02:13, 52.50it/s]Training CobwebTree:  30%|       | 2991/10000 [00:49<02:14, 52.00it/s]Training CobwebTree:  30%|       | 2998/10000 [00:50<02:06, 55.53it/s]Training CobwebTree:  30%|       | 3005/10000 [00:50<02:03, 56.44it/s]Training CobwebTree:  30%|       | 3011/10000 [00:50<02:10, 53.75it/s]Training CobwebTree:  30%|       | 3017/10000 [00:50<02:06, 55.07it/s]Training CobwebTree:  30%|       | 3024/10000 [00:50<02:03, 56.50it/s]Training CobwebTree:  30%|       | 3031/10000 [00:50<01:59, 58.18it/s]Training CobwebTree:  30%|       | 3037/10000 [00:50<02:04, 55.94it/s]Training CobwebTree:  30%|       | 3043/10000 [00:50<02:07, 54.72it/s]Training CobwebTree:  30%|       | 3049/10000 [00:50<02:09, 53.85it/s]Training CobwebTree:  31%|       | 3055/10000 [00:51<02:10, 53.21it/s]Training CobwebTree:  31%|       | 3061/10000 [00:51<02:12, 52.30it/s]Training CobwebTree:  31%|       | 3067/10000 [00:51<02:11, 52.72it/s]Training CobwebTree:  31%|       | 3073/10000 [00:51<02:13, 52.03it/s]Training CobwebTree:  31%|       | 3079/10000 [00:51<02:19, 49.45it/s]Training CobwebTree:  31%|       | 3084/10000 [00:51<02:20, 49.32it/s]Training CobwebTree:  31%|       | 3089/10000 [00:51<02:20, 49.16it/s]Training CobwebTree:  31%|       | 3095/10000 [00:51<02:20, 49.30it/s]Training CobwebTree:  31%|       | 3101/10000 [00:52<02:17, 50.08it/s]Training CobwebTree:  31%|       | 3107/10000 [00:52<02:15, 50.82it/s]Training CobwebTree:  31%|       | 3113/10000 [00:52<02:13, 51.59it/s]Training CobwebTree:  31%|       | 3119/10000 [00:52<02:16, 50.59it/s]Training CobwebTree:  31%|      | 3125/10000 [00:52<02:10, 52.74it/s]Training CobwebTree:  31%|      | 3131/10000 [00:52<02:08, 53.36it/s]Training CobwebTree:  31%|      | 3137/10000 [00:52<02:12, 51.74it/s]Training CobwebTree:  31%|      | 3143/10000 [00:52<02:10, 52.70it/s]Training CobwebTree:  31%|      | 3149/10000 [00:52<02:13, 51.25it/s]Training CobwebTree:  32%|      | 3155/10000 [00:53<02:13, 51.17it/s]Training CobwebTree:  32%|      | 3161/10000 [00:53<02:16, 49.92it/s]Training CobwebTree:  32%|      | 3167/10000 [00:53<02:16, 50.21it/s]Training CobwebTree:  32%|      | 3173/10000 [00:53<02:16, 49.89it/s]Training CobwebTree:  32%|      | 3178/10000 [00:53<02:17, 49.74it/s]Training CobwebTree:  32%|      | 3183/10000 [00:53<02:17, 49.45it/s]Training CobwebTree:  32%|      | 3189/10000 [00:53<02:12, 51.26it/s]Training CobwebTree:  32%|      | 3195/10000 [00:53<02:13, 51.05it/s]Training CobwebTree:  32%|      | 3201/10000 [00:53<02:09, 52.66it/s]Training CobwebTree:  32%|      | 3207/10000 [00:54<02:06, 53.83it/s]Training CobwebTree:  32%|      | 3213/10000 [00:54<02:08, 52.72it/s]Training CobwebTree:  32%|      | 3219/10000 [00:54<02:09, 52.30it/s]Training CobwebTree:  32%|      | 3226/10000 [00:54<02:01, 55.59it/s]Training CobwebTree:  32%|      | 3232/10000 [00:54<02:05, 53.88it/s]Training CobwebTree:  32%|      | 3238/10000 [00:54<02:05, 53.69it/s]Training CobwebTree:  32%|      | 3244/10000 [00:54<02:07, 52.98it/s]Training CobwebTree:  32%|      | 3250/10000 [00:54<02:07, 52.94it/s]Training CobwebTree:  33%|      | 3256/10000 [00:54<02:06, 53.25it/s]Training CobwebTree:  33%|      | 3262/10000 [00:55<02:05, 53.54it/s]Training CobwebTree:  33%|      | 3268/10000 [00:55<02:08, 52.46it/s]Training CobwebTree:  33%|      | 3274/10000 [00:55<02:12, 50.94it/s]Training CobwebTree:  33%|      | 3280/10000 [00:55<02:14, 50.07it/s]Training CobwebTree:  33%|      | 3286/10000 [00:55<02:13, 50.26it/s]Training CobwebTree:  33%|      | 3292/10000 [00:55<02:10, 51.52it/s]Training CobwebTree:  33%|      | 3298/10000 [00:55<02:14, 49.94it/s]Training CobwebTree:  33%|      | 3304/10000 [00:55<02:17, 48.78it/s]Training CobwebTree:  33%|      | 3309/10000 [00:56<02:18, 48.30it/s]Training CobwebTree:  33%|      | 3315/10000 [00:56<02:11, 50.93it/s]Training CobwebTree:  33%|      | 3321/10000 [00:56<02:09, 51.75it/s]Training CobwebTree:  33%|      | 3327/10000 [00:56<02:04, 53.41it/s]Training CobwebTree:  33%|      | 3333/10000 [00:56<02:08, 51.92it/s]Training CobwebTree:  33%|      | 3339/10000 [00:56<02:06, 52.52it/s]Training CobwebTree:  33%|      | 3345/10000 [00:56<02:07, 52.37it/s]Training CobwebTree:  34%|      | 3351/10000 [00:56<02:11, 50.71it/s]Training CobwebTree:  34%|      | 3357/10000 [00:56<02:08, 51.62it/s]Training CobwebTree:  34%|      | 3363/10000 [00:57<02:11, 50.34it/s]Training CobwebTree:  34%|      | 3369/10000 [00:57<02:16, 48.75it/s]Training CobwebTree:  34%|      | 3374/10000 [00:57<02:17, 48.33it/s]Training CobwebTree:  34%|      | 3380/10000 [00:57<02:12, 50.15it/s]Training CobwebTree:  34%|      | 3386/10000 [00:57<02:13, 49.43it/s]Training CobwebTree:  34%|      | 3391/10000 [00:57<02:15, 48.82it/s]Training CobwebTree:  34%|      | 3397/10000 [00:57<02:10, 50.53it/s]Training CobwebTree:  34%|      | 3403/10000 [00:57<02:05, 52.43it/s]Training CobwebTree:  34%|      | 3409/10000 [00:58<02:10, 50.66it/s]Training CobwebTree:  34%|      | 3415/10000 [00:58<02:11, 50.18it/s]Training CobwebTree:  34%|      | 3421/10000 [00:58<02:09, 50.89it/s]Training CobwebTree:  34%|      | 3427/10000 [00:58<02:10, 50.52it/s]Training CobwebTree:  34%|      | 3433/10000 [00:58<02:04, 52.84it/s]Training CobwebTree:  34%|      | 3439/10000 [00:58<02:08, 50.92it/s]Training CobwebTree:  34%|      | 3445/10000 [00:58<02:07, 51.56it/s]Training CobwebTree:  35%|      | 3451/10000 [00:58<02:02, 53.35it/s]Training CobwebTree:  35%|      | 3457/10000 [00:58<02:04, 52.66it/s]Training CobwebTree:  35%|      | 3463/10000 [00:59<02:05, 51.99it/s]Training CobwebTree:  35%|      | 3469/10000 [00:59<02:00, 54.02it/s]Training CobwebTree:  35%|      | 3475/10000 [00:59<02:04, 52.38it/s]Training CobwebTree:  35%|      | 3481/10000 [00:59<02:09, 50.44it/s]Training CobwebTree:  35%|      | 3487/10000 [00:59<02:08, 50.78it/s]Training CobwebTree:  35%|      | 3493/10000 [00:59<02:04, 52.12it/s]Training CobwebTree:  35%|      | 3499/10000 [00:59<02:05, 51.87it/s]Training CobwebTree:  35%|      | 3506/10000 [00:59<01:58, 54.69it/s]Training CobwebTree:  35%|      | 3512/10000 [01:00<02:04, 52.09it/s]Training CobwebTree:  35%|      | 3518/10000 [01:00<02:06, 51.19it/s]Training CobwebTree:  35%|      | 3524/10000 [01:00<02:04, 52.17it/s]Training CobwebTree:  35%|      | 3531/10000 [01:00<01:57, 55.15it/s]Training CobwebTree:  35%|      | 3537/10000 [01:00<01:56, 55.40it/s]Training CobwebTree:  35%|      | 3543/10000 [01:00<02:01, 53.24it/s]Training CobwebTree:  35%|      | 3549/10000 [01:00<02:00, 53.65it/s]Training CobwebTree:  36%|      | 3555/10000 [01:00<02:04, 51.75it/s]Training CobwebTree:  36%|      | 3561/10000 [01:00<02:04, 51.82it/s]Training CobwebTree:  36%|      | 3567/10000 [01:01<02:07, 50.57it/s]Training CobwebTree:  36%|      | 3573/10000 [01:01<02:10, 49.39it/s]Training CobwebTree:  36%|      | 3578/10000 [01:01<02:10, 49.26it/s]Training CobwebTree:  36%|      | 3583/10000 [01:01<02:12, 48.54it/s]Training CobwebTree:  36%|      | 3589/10000 [01:01<02:07, 50.14it/s]Training CobwebTree:  36%|      | 3595/10000 [01:01<02:08, 49.69it/s]Training CobwebTree:  36%|      | 3601/10000 [01:01<02:08, 49.79it/s]Training CobwebTree:  36%|      | 3606/10000 [01:01<02:10, 49.02it/s]Training CobwebTree:  36%|      | 3612/10000 [01:01<02:04, 51.46it/s]Training CobwebTree:  36%|      | 3618/10000 [01:02<02:04, 51.32it/s]Training CobwebTree:  36%|      | 3624/10000 [01:02<02:05, 50.77it/s]Training CobwebTree:  36%|      | 3630/10000 [01:02<02:00, 53.07it/s]Training CobwebTree:  36%|      | 3636/10000 [01:02<01:59, 53.08it/s]Training CobwebTree:  36%|      | 3642/10000 [01:02<02:07, 49.98it/s]Training CobwebTree:  36%|      | 3649/10000 [01:02<01:59, 53.17it/s]Training CobwebTree:  37%|      | 3655/10000 [01:02<02:01, 52.05it/s]Training CobwebTree:  37%|      | 3661/10000 [01:02<02:02, 51.61it/s]Training CobwebTree:  37%|      | 3667/10000 [01:03<02:04, 50.84it/s]Training CobwebTree:  37%|      | 3673/10000 [01:03<02:00, 52.48it/s]Training CobwebTree:  37%|      | 3679/10000 [01:03<01:56, 54.45it/s]Training CobwebTree:  37%|      | 3685/10000 [01:03<02:05, 50.27it/s]Training CobwebTree:  37%|      | 3691/10000 [01:03<02:02, 51.44it/s]Training CobwebTree:  37%|      | 3697/10000 [01:03<02:06, 49.65it/s]Training CobwebTree:  37%|      | 3703/10000 [01:03<02:05, 50.17it/s]Training CobwebTree:  37%|      | 3709/10000 [01:03<02:03, 51.08it/s]Training CobwebTree:  37%|      | 3715/10000 [01:03<02:07, 49.29it/s]Training CobwebTree:  37%|      | 3721/10000 [01:04<02:06, 49.65it/s]Training CobwebTree:  37%|      | 3727/10000 [01:04<02:04, 50.28it/s]Training CobwebTree:  37%|      | 3733/10000 [01:04<02:05, 50.01it/s]Training CobwebTree:  37%|      | 3739/10000 [01:04<02:03, 50.83it/s]Training CobwebTree:  37%|      | 3745/10000 [01:04<01:59, 52.51it/s]Training CobwebTree:  38%|      | 3751/10000 [01:04<01:59, 52.21it/s]Training CobwebTree:  38%|      | 3757/10000 [01:04<01:59, 52.20it/s]Training CobwebTree:  38%|      | 3763/10000 [01:04<01:56, 53.66it/s]Training CobwebTree:  38%|      | 3769/10000 [01:05<01:59, 52.00it/s]Training CobwebTree:  38%|      | 3775/10000 [01:05<01:56, 53.32it/s]Training CobwebTree:  38%|      | 3781/10000 [01:05<01:55, 53.70it/s]Training CobwebTree:  38%|      | 3787/10000 [01:05<01:58, 52.63it/s]Training CobwebTree:  38%|      | 3793/10000 [01:05<01:59, 51.85it/s]Training CobwebTree:  38%|      | 3799/10000 [01:05<02:04, 49.89it/s]Training CobwebTree:  38%|      | 3805/10000 [01:05<02:06, 48.83it/s]Training CobwebTree:  38%|      | 3811/10000 [01:05<02:03, 50.22it/s]Training CobwebTree:  38%|      | 3817/10000 [01:05<01:59, 51.73it/s]Training CobwebTree:  38%|      | 3823/10000 [01:06<02:00, 51.12it/s]Training CobwebTree:  38%|      | 3829/10000 [01:06<02:02, 50.53it/s]Training CobwebTree:  38%|      | 3835/10000 [01:06<01:58, 52.02it/s]Training CobwebTree:  38%|      | 3841/10000 [01:06<01:58, 51.83it/s]Training CobwebTree:  38%|      | 3847/10000 [01:06<02:00, 50.94it/s]Training CobwebTree:  39%|      | 3853/10000 [01:06<02:02, 50.34it/s]Training CobwebTree:  39%|      | 3859/10000 [01:06<02:00, 50.86it/s]Training CobwebTree:  39%|      | 3865/10000 [01:06<02:02, 50.05it/s]Training CobwebTree:  39%|      | 3871/10000 [01:06<01:57, 52.23it/s]Training CobwebTree:  39%|      | 3877/10000 [01:07<02:04, 49.34it/s]Training CobwebTree:  39%|      | 3883/10000 [01:07<02:00, 50.88it/s]Training CobwebTree:  39%|      | 3889/10000 [01:07<01:57, 51.96it/s]Training CobwebTree:  39%|      | 3895/10000 [01:07<02:02, 49.69it/s]Training CobwebTree:  39%|      | 3901/10000 [01:07<02:04, 48.89it/s]Training CobwebTree:  39%|      | 3906/10000 [01:07<02:07, 47.84it/s]Training CobwebTree:  39%|      | 3911/10000 [01:07<02:06, 48.07it/s]Training CobwebTree:  39%|      | 3916/10000 [01:07<02:05, 48.45it/s]Training CobwebTree:  39%|      | 3923/10000 [01:08<01:56, 52.04it/s]Training CobwebTree:  39%|      | 3929/10000 [01:08<01:58, 51.08it/s]Training CobwebTree:  39%|      | 3935/10000 [01:08<01:56, 51.85it/s]Training CobwebTree:  39%|      | 3941/10000 [01:08<01:58, 51.21it/s]Training CobwebTree:  39%|      | 3947/10000 [01:08<02:00, 50.39it/s]Training CobwebTree:  40%|      | 3953/10000 [01:08<02:02, 49.54it/s]Training CobwebTree:  40%|      | 3958/10000 [01:08<02:04, 48.65it/s]Training CobwebTree:  40%|      | 3964/10000 [01:08<02:01, 49.50it/s]Training CobwebTree:  40%|      | 3970/10000 [01:08<01:55, 52.05it/s]Training CobwebTree:  40%|      | 3976/10000 [01:09<02:00, 49.87it/s]Training CobwebTree:  40%|      | 3982/10000 [01:09<01:59, 50.30it/s]Training CobwebTree:  40%|      | 3988/10000 [01:09<01:56, 51.50it/s]Training CobwebTree:  40%|      | 3994/10000 [01:09<01:57, 50.97it/s]Training CobwebTree:  40%|      | 4000/10000 [01:09<01:58, 50.82it/s]Training CobwebTree:  40%|      | 4006/10000 [01:09<01:55, 51.70it/s]Training CobwebTree:  40%|      | 4012/10000 [01:09<01:54, 52.24it/s]Training CobwebTree:  40%|      | 4019/10000 [01:09<01:50, 54.20it/s]Training CobwebTree:  40%|      | 4025/10000 [01:10<01:53, 52.60it/s]Training CobwebTree:  40%|      | 4031/10000 [01:10<01:51, 53.65it/s]Training CobwebTree:  40%|      | 4037/10000 [01:10<01:48, 54.74it/s]Training CobwebTree:  40%|      | 4043/10000 [01:10<01:52, 52.98it/s]Training CobwebTree:  40%|      | 4049/10000 [01:10<01:57, 50.58it/s]Training CobwebTree:  41%|      | 4055/10000 [01:10<01:58, 50.23it/s]Training CobwebTree:  41%|      | 4061/10000 [01:10<01:53, 52.40it/s]Training CobwebTree:  41%|      | 4067/10000 [01:10<01:51, 53.37it/s]Training CobwebTree:  41%|      | 4074/10000 [01:10<01:44, 56.83it/s]Training CobwebTree:  41%|      | 4080/10000 [01:11<01:43, 56.95it/s]Training CobwebTree:  41%|      | 4086/10000 [01:11<01:47, 55.08it/s]Training CobwebTree:  41%|      | 4092/10000 [01:11<01:52, 52.61it/s]Training CobwebTree:  41%|      | 4098/10000 [01:11<01:53, 51.94it/s]Training CobwebTree:  41%|      | 4104/10000 [01:11<01:51, 52.66it/s]Training CobwebTree:  41%|      | 4110/10000 [01:11<01:56, 50.65it/s]Training CobwebTree:  41%|      | 4116/10000 [01:11<01:53, 51.73it/s]Training CobwebTree:  41%|      | 4122/10000 [01:11<01:58, 49.47it/s]Training CobwebTree:  41%|     | 4128/10000 [01:11<01:53, 51.56it/s]Training CobwebTree:  41%|     | 4135/10000 [01:12<01:48, 53.94it/s]Training CobwebTree:  41%|     | 4141/10000 [01:12<01:51, 52.64it/s]Training CobwebTree:  41%|     | 4147/10000 [01:12<01:49, 53.30it/s]Training CobwebTree:  42%|     | 4153/10000 [01:12<01:53, 51.59it/s]Training CobwebTree:  42%|     | 4159/10000 [01:12<01:52, 51.90it/s]Training CobwebTree:  42%|     | 4165/10000 [01:12<01:54, 50.77it/s]Training CobwebTree:  42%|     | 4171/10000 [01:12<01:53, 51.15it/s]Training CobwebTree:  42%|     | 4177/10000 [01:12<01:59, 48.84it/s]Training CobwebTree:  42%|     | 4183/10000 [01:13<01:56, 49.74it/s]Training CobwebTree:  42%|     | 4189/10000 [01:13<01:55, 50.29it/s]Training CobwebTree:  42%|     | 4195/10000 [01:13<01:55, 50.25it/s]Training CobwebTree:  42%|     | 4201/10000 [01:13<01:56, 49.83it/s]Training CobwebTree:  42%|     | 4206/10000 [01:13<01:58, 49.01it/s]Training CobwebTree:  42%|     | 4212/10000 [01:13<01:56, 49.88it/s]Training CobwebTree:  42%|     | 4217/10000 [01:13<01:58, 48.87it/s]Training CobwebTree:  42%|     | 4223/10000 [01:13<01:56, 49.73it/s]Training CobwebTree:  42%|     | 4229/10000 [01:13<01:51, 51.84it/s]Training CobwebTree:  42%|     | 4235/10000 [01:14<01:47, 53.84it/s]Training CobwebTree:  42%|     | 4241/10000 [01:14<01:51, 51.44it/s]Training CobwebTree:  42%|     | 4247/10000 [01:14<01:51, 51.81it/s]Training CobwebTree:  43%|     | 4253/10000 [01:14<01:54, 50.35it/s]Training CobwebTree:  43%|     | 4259/10000 [01:14<01:51, 51.42it/s]Training CobwebTree:  43%|     | 4265/10000 [01:14<01:50, 51.84it/s]Training CobwebTree:  43%|     | 4272/10000 [01:14<01:43, 55.16it/s]Training CobwebTree:  43%|     | 4278/10000 [01:14<01:47, 53.40it/s]Training CobwebTree:  43%|     | 4284/10000 [01:15<01:45, 54.32it/s]Training CobwebTree:  43%|     | 4290/10000 [01:15<01:48, 52.59it/s]Training CobwebTree:  43%|     | 4296/10000 [01:15<01:46, 53.40it/s]Training CobwebTree:  43%|     | 4302/10000 [01:15<01:48, 52.40it/s]Training CobwebTree:  43%|     | 4308/10000 [01:15<01:50, 51.39it/s]Training CobwebTree:  43%|     | 4314/10000 [01:15<01:54, 49.83it/s]Training CobwebTree:  43%|     | 4320/10000 [01:15<01:52, 50.54it/s]Training CobwebTree:  43%|     | 4326/10000 [01:15<01:50, 51.54it/s]Training CobwebTree:  43%|     | 4332/10000 [01:15<01:52, 50.33it/s]Training CobwebTree:  43%|     | 4338/10000 [01:16<01:50, 51.23it/s]Training CobwebTree:  43%|     | 4344/10000 [01:16<01:49, 51.75it/s]Training CobwebTree:  44%|     | 4350/10000 [01:16<01:48, 52.16it/s]Training CobwebTree:  44%|     | 4356/10000 [01:16<01:47, 52.66it/s]Training CobwebTree:  44%|     | 4362/10000 [01:16<01:50, 51.05it/s]Training CobwebTree:  44%|     | 4368/10000 [01:16<01:53, 49.82it/s]Training CobwebTree:  44%|     | 4374/10000 [01:16<01:48, 51.68it/s]Training CobwebTree:  44%|     | 4380/10000 [01:16<01:49, 51.25it/s]Training CobwebTree:  44%|     | 4386/10000 [01:17<01:45, 53.04it/s]Training CobwebTree:  44%|     | 4392/10000 [01:17<01:43, 54.19it/s]Training CobwebTree:  44%|     | 4398/10000 [01:17<01:46, 52.61it/s]Training CobwebTree:  44%|     | 4404/10000 [01:17<01:46, 52.51it/s]Training CobwebTree:  44%|     | 4410/10000 [01:17<01:43, 53.79it/s]Training CobwebTree:  44%|     | 4416/10000 [01:17<01:46, 52.47it/s]Training CobwebTree:  44%|     | 4422/10000 [01:17<01:44, 53.32it/s]Training CobwebTree:  44%|     | 4428/10000 [01:17<01:46, 52.47it/s]Training CobwebTree:  44%|     | 4435/10000 [01:17<01:43, 54.00it/s]Training CobwebTree:  44%|     | 4441/10000 [01:18<01:45, 52.88it/s]Training CobwebTree:  44%|     | 4447/10000 [01:18<01:45, 52.64it/s]Training CobwebTree:  45%|     | 4453/10000 [01:18<01:48, 51.06it/s]Training CobwebTree:  45%|     | 4459/10000 [01:18<01:53, 48.89it/s]Training CobwebTree:  45%|     | 4465/10000 [01:18<01:48, 50.93it/s]Training CobwebTree:  45%|     | 4471/10000 [01:18<01:46, 51.92it/s]Training CobwebTree:  45%|     | 4477/10000 [01:18<01:47, 51.43it/s]Training CobwebTree:  45%|     | 4483/10000 [01:18<01:46, 51.60it/s]Training CobwebTree:  45%|     | 4489/10000 [01:18<01:50, 50.05it/s]Training CobwebTree:  45%|     | 4495/10000 [01:19<01:50, 49.96it/s]Training CobwebTree:  45%|     | 4501/10000 [01:19<01:46, 51.67it/s]Training CobwebTree:  45%|     | 4507/10000 [01:19<01:49, 50.30it/s]Training CobwebTree:  45%|     | 4513/10000 [01:19<01:49, 50.20it/s]Training CobwebTree:  45%|     | 4519/10000 [01:19<01:44, 52.45it/s]Training CobwebTree:  45%|     | 4525/10000 [01:19<01:45, 51.91it/s]Training CobwebTree:  45%|     | 4531/10000 [01:19<01:50, 49.32it/s]Training CobwebTree:  45%|     | 4537/10000 [01:19<01:48, 50.22it/s]Training CobwebTree:  45%|     | 4543/10000 [01:20<01:47, 50.59it/s]Training CobwebTree:  45%|     | 4549/10000 [01:20<01:51, 49.11it/s]Training CobwebTree:  46%|     | 4555/10000 [01:20<01:50, 49.13it/s]Training CobwebTree:  46%|     | 4561/10000 [01:20<01:46, 51.23it/s]Training CobwebTree:  46%|     | 4567/10000 [01:20<01:47, 50.67it/s]Training CobwebTree:  46%|     | 4574/10000 [01:20<01:38, 54.91it/s]Training CobwebTree:  46%|     | 4580/10000 [01:20<01:39, 54.35it/s]Training CobwebTree:  46%|     | 4586/10000 [01:20<01:42, 52.91it/s]Training CobwebTree:  46%|     | 4592/10000 [01:21<01:46, 50.89it/s]Training CobwebTree:  46%|     | 4598/10000 [01:21<01:47, 50.27it/s]Training CobwebTree:  46%|     | 4604/10000 [01:21<01:49, 49.25it/s]Training CobwebTree:  46%|     | 4609/10000 [01:21<01:51, 48.49it/s]Training CobwebTree:  46%|     | 4614/10000 [01:21<01:50, 48.69it/s]Training CobwebTree:  46%|     | 4620/10000 [01:21<01:45, 50.97it/s]Training CobwebTree:  46%|     | 4626/10000 [01:21<01:46, 50.33it/s]Training CobwebTree:  46%|     | 4632/10000 [01:21<01:47, 49.80it/s]Training CobwebTree:  46%|     | 4638/10000 [01:21<01:44, 51.52it/s]Training CobwebTree:  46%|     | 4644/10000 [01:22<01:42, 52.06it/s]Training CobwebTree:  46%|     | 4650/10000 [01:22<01:47, 49.91it/s]Training CobwebTree:  47%|     | 4656/10000 [01:22<01:49, 48.86it/s]Training CobwebTree:  47%|     | 4662/10000 [01:22<01:52, 47.64it/s]Training CobwebTree:  47%|     | 4668/10000 [01:22<01:46, 49.88it/s]Training CobwebTree:  47%|     | 4674/10000 [01:22<01:47, 49.58it/s]Training CobwebTree:  47%|     | 4679/10000 [01:22<01:50, 48.03it/s]Training CobwebTree:  47%|     | 4684/10000 [01:22<01:50, 48.02it/s]Training CobwebTree:  47%|     | 4690/10000 [01:22<01:45, 50.29it/s]Training CobwebTree:  47%|     | 4696/10000 [01:23<01:46, 49.95it/s]Training CobwebTree:  47%|     | 4702/10000 [01:23<01:45, 50.28it/s]Training CobwebTree:  47%|     | 4708/10000 [01:23<01:42, 51.88it/s]Training CobwebTree:  47%|     | 4714/10000 [01:23<01:42, 51.39it/s]Training CobwebTree:  47%|     | 4720/10000 [01:23<01:40, 52.32it/s]Training CobwebTree:  47%|     | 4726/10000 [01:23<01:39, 53.04it/s]Training CobwebTree:  47%|     | 4732/10000 [01:23<01:37, 54.28it/s]Training CobwebTree:  47%|     | 4738/10000 [01:23<01:44, 50.37it/s]Training CobwebTree:  47%|     | 4744/10000 [01:24<01:46, 49.30it/s]Training CobwebTree:  48%|     | 4751/10000 [01:24<01:38, 53.36it/s]Training CobwebTree:  48%|     | 4757/10000 [01:24<01:40, 52.04it/s]Training CobwebTree:  48%|     | 4763/10000 [01:24<01:40, 52.28it/s]Training CobwebTree:  48%|     | 4769/10000 [01:24<01:39, 52.44it/s]Training CobwebTree:  48%|     | 4775/10000 [01:24<01:37, 53.32it/s]Training CobwebTree:  48%|     | 4781/10000 [01:24<01:40, 51.92it/s]Training CobwebTree:  48%|     | 4787/10000 [01:24<01:38, 52.75it/s]Training CobwebTree:  48%|     | 4793/10000 [01:24<01:44, 49.65it/s]Training CobwebTree:  48%|     | 4799/10000 [01:25<01:44, 50.01it/s]Training CobwebTree:  48%|     | 4805/10000 [01:25<01:42, 50.48it/s]Training CobwebTree:  48%|     | 4811/10000 [01:25<01:45, 49.33it/s]Training CobwebTree:  48%|     | 4817/10000 [01:25<01:40, 51.32it/s]Training CobwebTree:  48%|     | 4823/10000 [01:25<01:41, 51.03it/s]Training CobwebTree:  48%|     | 4829/10000 [01:25<01:39, 51.86it/s]Training CobwebTree:  48%|     | 4835/10000 [01:25<01:39, 52.16it/s]Training CobwebTree:  48%|     | 4841/10000 [01:25<01:40, 51.28it/s]Training CobwebTree:  48%|     | 4847/10000 [01:26<01:40, 51.14it/s]Training CobwebTree:  49%|     | 4853/10000 [01:26<01:42, 50.23it/s]Training CobwebTree:  49%|     | 4859/10000 [01:26<01:41, 50.66it/s]Training CobwebTree:  49%|     | 4865/10000 [01:26<01:44, 49.08it/s]Training CobwebTree:  49%|     | 4871/10000 [01:26<01:40, 51.02it/s]Training CobwebTree:  49%|     | 4877/10000 [01:26<01:38, 52.09it/s]Training CobwebTree:  49%|     | 4883/10000 [01:26<01:40, 51.11it/s]Training CobwebTree:  49%|     | 4889/10000 [01:26<01:38, 51.94it/s]Training CobwebTree:  49%|     | 4895/10000 [01:26<01:42, 49.89it/s]Training CobwebTree:  49%|     | 4901/10000 [01:27<01:39, 51.23it/s]Training CobwebTree:  49%|     | 4907/10000 [01:27<01:35, 53.50it/s]Training CobwebTree:  49%|     | 4913/10000 [01:27<01:39, 51.32it/s]Training CobwebTree:  49%|     | 4919/10000 [01:27<01:36, 52.49it/s]Training CobwebTree:  49%|     | 4925/10000 [01:27<01:35, 53.34it/s]Training CobwebTree:  49%|     | 4931/10000 [01:27<01:37, 51.81it/s]Training CobwebTree:  49%|     | 4937/10000 [01:27<01:44, 48.41it/s]Training CobwebTree:  49%|     | 4942/10000 [01:27<01:45, 47.76it/s]Training CobwebTree:  49%|     | 4947/10000 [01:28<01:45, 48.01it/s]Training CobwebTree:  50%|     | 4953/10000 [01:28<01:42, 49.08it/s]Training CobwebTree:  50%|     | 4960/10000 [01:28<01:36, 52.30it/s]Training CobwebTree:  50%|     | 4966/10000 [01:28<01:39, 50.58it/s]Training CobwebTree:  50%|     | 4972/10000 [01:28<01:37, 51.72it/s]Training CobwebTree:  50%|     | 4978/10000 [01:28<01:39, 50.35it/s]Training CobwebTree:  50%|     | 4984/10000 [01:28<01:41, 49.49it/s]Training CobwebTree:  50%|     | 4990/10000 [01:28<01:38, 50.79it/s]Training CobwebTree:  50%|     | 4996/10000 [01:28<01:41, 49.16it/s]Training CobwebTree:  50%|     | 5002/10000 [01:29<01:41, 49.11it/s]Training CobwebTree:  50%|     | 5008/10000 [01:29<01:40, 49.43it/s]Training CobwebTree:  50%|     | 5014/10000 [01:29<01:39, 49.93it/s]Training CobwebTree:  50%|     | 5020/10000 [01:29<01:39, 50.28it/s]Training CobwebTree:  50%|     | 5026/10000 [01:29<01:35, 51.87it/s]Training CobwebTree:  50%|     | 5032/10000 [01:29<01:36, 51.63it/s]Training CobwebTree:  50%|     | 5039/10000 [01:29<01:31, 54.49it/s]Training CobwebTree:  50%|     | 5045/10000 [01:29<01:33, 53.02it/s]Training CobwebTree:  51%|     | 5051/10000 [01:30<01:38, 50.49it/s]Training CobwebTree:  51%|     | 5057/10000 [01:30<01:39, 49.71it/s]Training CobwebTree:  51%|     | 5063/10000 [01:30<01:36, 50.91it/s]Training CobwebTree:  51%|     | 5069/10000 [01:30<01:40, 48.92it/s]Training CobwebTree:  51%|     | 5074/10000 [01:30<01:45, 46.87it/s]Training CobwebTree:  51%|     | 5079/10000 [01:30<01:47, 45.77it/s]Training CobwebTree:  51%|     | 5085/10000 [01:30<01:45, 46.75it/s]Training CobwebTree:  51%|     | 5091/10000 [01:30<01:39, 49.28it/s]Training CobwebTree:  51%|     | 5097/10000 [01:31<01:38, 49.62it/s]Training CobwebTree:  51%|     | 5103/10000 [01:31<01:38, 49.84it/s]Training CobwebTree:  51%|     | 5109/10000 [01:31<01:39, 49.09it/s]Training CobwebTree:  51%|     | 5114/10000 [01:31<01:39, 49.10it/s]Training CobwebTree:  51%|     | 5119/10000 [01:31<01:43, 47.29it/s]Training CobwebTree:  51%|    | 5125/10000 [01:31<01:36, 50.54it/s]Training CobwebTree:  51%|    | 5131/10000 [01:31<01:40, 48.30it/s]Training CobwebTree:  51%|    | 5136/10000 [01:31<01:41, 48.13it/s]Training CobwebTree:  51%|    | 5141/10000 [01:31<01:41, 47.72it/s]Training CobwebTree:  51%|    | 5147/10000 [01:32<01:37, 50.02it/s]Training CobwebTree:  52%|    | 5153/10000 [01:32<01:35, 50.55it/s]Training CobwebTree:  52%|    | 5159/10000 [01:32<01:37, 49.64it/s]Training CobwebTree:  52%|    | 5164/10000 [01:32<01:41, 47.56it/s]Training CobwebTree:  52%|    | 5170/10000 [01:32<01:35, 50.68it/s]Training CobwebTree:  52%|    | 5176/10000 [01:32<01:36, 50.10it/s]Training CobwebTree:  52%|    | 5182/10000 [01:32<01:38, 48.68it/s]Training CobwebTree:  52%|    | 5187/10000 [01:32<01:38, 48.94it/s]Training CobwebTree:  52%|    | 5192/10000 [01:32<01:38, 49.04it/s]Training CobwebTree:  52%|    | 5197/10000 [01:33<01:38, 48.96it/s]Training CobwebTree:  52%|    | 5203/10000 [01:33<01:33, 51.21it/s]Training CobwebTree:  52%|    | 5209/10000 [01:33<01:37, 49.29it/s]Training CobwebTree:  52%|    | 5215/10000 [01:33<01:34, 50.53it/s]Training CobwebTree:  52%|    | 5221/10000 [01:33<01:35, 50.24it/s]Training CobwebTree:  52%|    | 5227/10000 [01:33<01:33, 50.94it/s]Training CobwebTree:  52%|    | 5233/10000 [01:33<01:35, 50.08it/s]Training CobwebTree:  52%|    | 5239/10000 [01:33<01:34, 50.12it/s]Training CobwebTree:  52%|    | 5245/10000 [01:33<01:36, 49.48it/s]Training CobwebTree:  53%|    | 5251/10000 [01:34<01:34, 50.35it/s]Training CobwebTree:  53%|    | 5257/10000 [01:34<01:37, 48.74it/s]Training CobwebTree:  53%|    | 5263/10000 [01:34<01:37, 48.52it/s]Training CobwebTree:  53%|    | 5269/10000 [01:34<01:35, 49.36it/s]Training CobwebTree:  53%|    | 5274/10000 [01:34<01:36, 49.18it/s]Training CobwebTree:  53%|    | 5279/10000 [01:34<01:36, 48.75it/s]Training CobwebTree:  53%|    | 5284/10000 [01:34<01:37, 48.45it/s]Training CobwebTree:  53%|    | 5289/10000 [01:34<01:36, 48.82it/s]Training CobwebTree:  53%|    | 5294/10000 [01:34<01:35, 49.05it/s]Training CobwebTree:  53%|    | 5299/10000 [01:35<01:37, 47.99it/s]Training CobwebTree:  53%|    | 5304/10000 [01:35<01:39, 47.36it/s]Training CobwebTree:  53%|    | 5309/10000 [01:35<01:39, 47.24it/s]Training CobwebTree:  53%|    | 5315/10000 [01:35<01:34, 49.47it/s]Training CobwebTree:  53%|    | 5320/10000 [01:35<01:38, 47.46it/s]Training CobwebTree:  53%|    | 5326/10000 [01:35<01:36, 48.38it/s]Training CobwebTree:  53%|    | 5331/10000 [01:35<01:35, 48.76it/s]Training CobwebTree:  53%|    | 5336/10000 [01:35<01:36, 48.16it/s]Training CobwebTree:  53%|    | 5341/10000 [01:35<01:38, 47.49it/s]Training CobwebTree:  53%|    | 5347/10000 [01:36<01:34, 49.46it/s]Training CobwebTree:  54%|    | 5352/10000 [01:36<01:35, 48.43it/s]Training CobwebTree:  54%|    | 5358/10000 [01:36<01:33, 49.42it/s]Training CobwebTree:  54%|    | 5363/10000 [01:36<01:34, 49.19it/s]Training CobwebTree:  54%|    | 5369/10000 [01:36<01:31, 50.34it/s]Training CobwebTree:  54%|    | 5375/10000 [01:36<01:28, 52.34it/s]Training CobwebTree:  54%|    | 5381/10000 [01:36<01:27, 52.77it/s]Training CobwebTree:  54%|    | 5388/10000 [01:36<01:22, 55.73it/s]Training CobwebTree:  54%|    | 5394/10000 [01:36<01:25, 53.87it/s]Training CobwebTree:  54%|    | 5400/10000 [01:37<01:24, 54.57it/s]Training CobwebTree:  54%|    | 5406/10000 [01:37<01:27, 52.55it/s]Training CobwebTree:  54%|    | 5412/10000 [01:37<01:29, 51.06it/s]Training CobwebTree:  54%|    | 5418/10000 [01:37<01:29, 51.25it/s]Training CobwebTree:  54%|    | 5424/10000 [01:37<01:33, 49.04it/s]Training CobwebTree:  54%|    | 5429/10000 [01:37<01:36, 47.36it/s]Training CobwebTree:  54%|    | 5434/10000 [01:37<01:35, 47.91it/s]Training CobwebTree:  54%|    | 5440/10000 [01:37<01:31, 49.88it/s]Training CobwebTree:  54%|    | 5446/10000 [01:38<01:32, 49.36it/s]Training CobwebTree:  55%|    | 5453/10000 [01:38<01:27, 52.19it/s]Training CobwebTree:  55%|    | 5459/10000 [01:38<01:26, 52.47it/s]Training CobwebTree:  55%|    | 5465/10000 [01:38<01:23, 54.37it/s]Training CobwebTree:  55%|    | 5471/10000 [01:38<01:24, 53.74it/s]Training CobwebTree:  55%|    | 5477/10000 [01:38<01:27, 51.63it/s]Training CobwebTree:  55%|    | 5483/10000 [01:38<01:32, 48.66it/s]Training CobwebTree:  55%|    | 5489/10000 [01:38<01:30, 49.80it/s]Training CobwebTree:  55%|    | 5495/10000 [01:38<01:26, 52.17it/s]Training CobwebTree:  55%|    | 5501/10000 [01:39<01:25, 52.51it/s]Training CobwebTree:  55%|    | 5507/10000 [01:39<01:27, 51.10it/s]Training CobwebTree:  55%|    | 5513/10000 [01:39<01:33, 48.15it/s]Training CobwebTree:  55%|    | 5519/10000 [01:39<01:28, 50.67it/s]Training CobwebTree:  55%|    | 5525/10000 [01:39<01:27, 50.95it/s]Training CobwebTree:  55%|    | 5532/10000 [01:39<01:24, 53.07it/s]Training CobwebTree:  55%|    | 5538/10000 [01:39<01:23, 53.45it/s]Training CobwebTree:  55%|    | 5544/10000 [01:39<01:25, 52.11it/s]Training CobwebTree:  56%|    | 5550/10000 [01:40<01:24, 52.38it/s]Training CobwebTree:  56%|    | 5557/10000 [01:40<01:21, 54.81it/s]Training CobwebTree:  56%|    | 5564/10000 [01:40<01:16, 57.86it/s]Training CobwebTree:  56%|    | 5570/10000 [01:40<01:18, 56.09it/s]Training CobwebTree:  56%|    | 5576/10000 [01:40<01:24, 52.50it/s]Training CobwebTree:  56%|    | 5582/10000 [01:40<01:25, 51.71it/s]Training CobwebTree:  56%|    | 5588/10000 [01:40<01:26, 50.85it/s]Training CobwebTree:  56%|    | 5594/10000 [01:40<01:26, 50.91it/s]Training CobwebTree:  56%|    | 5600/10000 [01:40<01:26, 50.86it/s]Training CobwebTree:  56%|    | 5606/10000 [01:41<01:27, 50.18it/s]Training CobwebTree:  56%|    | 5612/10000 [01:41<01:28, 49.63it/s]Training CobwebTree:  56%|    | 5618/10000 [01:41<01:27, 50.04it/s]Training CobwebTree:  56%|    | 5624/10000 [01:41<01:31, 47.68it/s]Training CobwebTree:  56%|    | 5629/10000 [01:41<01:32, 47.21it/s]Training CobwebTree:  56%|    | 5634/10000 [01:41<01:33, 46.50it/s]Training CobwebTree:  56%|    | 5640/10000 [01:41<01:30, 47.95it/s]Training CobwebTree:  56%|    | 5645/10000 [01:41<01:31, 47.76it/s]Training CobwebTree:  56%|    | 5650/10000 [01:42<01:30, 48.31it/s]Training CobwebTree:  57%|    | 5655/10000 [01:42<01:30, 48.24it/s]Training CobwebTree:  57%|    | 5660/10000 [01:42<01:31, 47.68it/s]Training CobwebTree:  57%|    | 5667/10000 [01:42<01:23, 52.14it/s]Training CobwebTree:  57%|    | 5673/10000 [01:42<01:24, 51.20it/s]Training CobwebTree:  57%|    | 5679/10000 [01:42<01:25, 50.71it/s]Training CobwebTree:  57%|    | 5685/10000 [01:42<01:25, 50.44it/s]Training CobwebTree:  57%|    | 5691/10000 [01:42<01:25, 50.21it/s]Training CobwebTree:  57%|    | 5697/10000 [01:42<01:27, 49.19it/s]Training CobwebTree:  57%|    | 5703/10000 [01:43<01:23, 51.75it/s]Training CobwebTree:  57%|    | 5709/10000 [01:43<01:24, 50.86it/s]Training CobwebTree:  57%|    | 5715/10000 [01:43<01:26, 49.30it/s]Training CobwebTree:  57%|    | 5720/10000 [01:43<01:27, 48.94it/s]Training CobwebTree:  57%|    | 5725/10000 [01:43<01:27, 48.85it/s]Training CobwebTree:  57%|    | 5730/10000 [01:43<01:29, 47.54it/s]Training CobwebTree:  57%|    | 5735/10000 [01:43<01:29, 47.45it/s]Training CobwebTree:  57%|    | 5741/10000 [01:43<01:26, 49.49it/s]Training CobwebTree:  57%|    | 5747/10000 [01:43<01:21, 52.26it/s]Training CobwebTree:  58%|    | 5753/10000 [01:44<01:22, 51.27it/s]Training CobwebTree:  58%|    | 5759/10000 [01:44<01:23, 50.86it/s]Training CobwebTree:  58%|    | 5765/10000 [01:44<01:24, 50.38it/s]Training CobwebTree:  58%|    | 5771/10000 [01:44<01:22, 51.27it/s]Training CobwebTree:  58%|    | 5777/10000 [01:44<01:23, 50.36it/s]Training CobwebTree:  58%|    | 5783/10000 [01:44<01:22, 50.94it/s]Training CobwebTree:  58%|    | 5789/10000 [01:44<01:22, 51.20it/s]Training CobwebTree:  58%|    | 5795/10000 [01:44<01:21, 51.89it/s]Training CobwebTree:  58%|    | 5801/10000 [01:45<01:20, 52.45it/s]Training CobwebTree:  58%|    | 5807/10000 [01:45<01:21, 51.54it/s]Training CobwebTree:  58%|    | 5813/10000 [01:45<01:24, 49.40it/s]Training CobwebTree:  58%|    | 5819/10000 [01:45<01:22, 50.97it/s]Training CobwebTree:  58%|    | 5825/10000 [01:45<01:20, 51.69it/s]Training CobwebTree:  58%|    | 5831/10000 [01:45<01:21, 50.86it/s]Training CobwebTree:  58%|    | 5837/10000 [01:45<01:23, 50.09it/s]Training CobwebTree:  58%|    | 5843/10000 [01:45<01:31, 45.31it/s]Training CobwebTree:  58%|    | 5848/10000 [01:46<01:30, 46.07it/s]Training CobwebTree:  59%|    | 5854/10000 [01:46<01:24, 48.79it/s]Training CobwebTree:  59%|    | 5859/10000 [01:46<01:25, 48.38it/s]Training CobwebTree:  59%|    | 5865/10000 [01:46<01:20, 51.44it/s]Training CobwebTree:  59%|    | 5871/10000 [01:46<01:20, 51.25it/s]Training CobwebTree:  59%|    | 5877/10000 [01:46<01:20, 51.08it/s]Training CobwebTree:  59%|    | 5883/10000 [01:46<01:21, 50.64it/s]Training CobwebTree:  59%|    | 5889/10000 [01:46<01:21, 50.66it/s]Training CobwebTree:  59%|    | 5895/10000 [01:46<01:19, 51.59it/s]Training CobwebTree:  59%|    | 5901/10000 [01:47<01:18, 52.25it/s]Training CobwebTree:  59%|    | 5907/10000 [01:47<01:16, 53.42it/s]Training CobwebTree:  59%|    | 5913/10000 [01:47<01:20, 50.54it/s]Training CobwebTree:  59%|    | 5919/10000 [01:47<01:19, 51.16it/s]Training CobwebTree:  59%|    | 5925/10000 [01:47<01:23, 48.68it/s]Training CobwebTree:  59%|    | 5930/10000 [01:47<01:25, 47.87it/s]Training CobwebTree:  59%|    | 5935/10000 [01:47<01:29, 45.50it/s]Training CobwebTree:  59%|    | 5941/10000 [01:47<01:25, 47.75it/s]Training CobwebTree:  59%|    | 5946/10000 [01:47<01:23, 48.30it/s]Training CobwebTree:  60%|    | 5951/10000 [01:48<01:23, 48.46it/s]Training CobwebTree:  60%|    | 5956/10000 [01:48<01:23, 48.59it/s]Training CobwebTree:  60%|    | 5962/10000 [01:48<01:20, 50.31it/s]Training CobwebTree:  60%|    | 5968/10000 [01:48<01:17, 52.01it/s]Training CobwebTree:  60%|    | 5974/10000 [01:48<01:18, 51.26it/s]Training CobwebTree:  60%|    | 5980/10000 [01:48<01:17, 51.59it/s]Training CobwebTree:  60%|    | 5986/10000 [01:48<01:16, 52.20it/s]Training CobwebTree:  60%|    | 5992/10000 [01:48<01:19, 50.26it/s]Training CobwebTree:  60%|    | 5998/10000 [01:48<01:16, 52.21it/s]Training CobwebTree:  60%|    | 6004/10000 [01:49<01:17, 51.61it/s]Training CobwebTree:  60%|    | 6010/10000 [01:49<01:16, 51.93it/s]Training CobwebTree:  60%|    | 6016/10000 [01:49<01:18, 50.47it/s]Training CobwebTree:  60%|    | 6022/10000 [01:49<01:19, 50.28it/s]Training CobwebTree:  60%|    | 6028/10000 [01:49<01:15, 52.29it/s]Training CobwebTree:  60%|    | 6034/10000 [01:49<01:16, 51.92it/s]Training CobwebTree:  60%|    | 6040/10000 [01:49<01:16, 51.70it/s]Training CobwebTree:  60%|    | 6046/10000 [01:49<01:19, 49.78it/s]Training CobwebTree:  61%|    | 6052/10000 [01:50<01:20, 49.02it/s]Training CobwebTree:  61%|    | 6058/10000 [01:50<01:20, 49.00it/s]Training CobwebTree:  61%|    | 6063/10000 [01:50<01:20, 48.73it/s]Training CobwebTree:  61%|    | 6068/10000 [01:50<01:22, 47.95it/s]Training CobwebTree:  61%|    | 6073/10000 [01:50<01:24, 46.44it/s]Training CobwebTree:  61%|    | 6078/10000 [01:50<01:25, 45.69it/s]Training CobwebTree:  61%|    | 6083/10000 [01:50<01:25, 45.88it/s]Training CobwebTree:  61%|    | 6088/10000 [01:50<01:23, 46.62it/s]Training CobwebTree:  61%|    | 6093/10000 [01:50<01:22, 47.31it/s]Training CobwebTree:  61%|    | 6098/10000 [01:51<01:22, 47.54it/s]Training CobwebTree:  61%|    | 6104/10000 [01:51<01:17, 50.58it/s]Training CobwebTree:  61%|    | 6110/10000 [01:51<01:15, 51.58it/s]Training CobwebTree:  61%|    | 6116/10000 [01:51<01:16, 50.79it/s]Training CobwebTree:  61%|    | 6122/10000 [01:51<01:16, 50.54it/s]Training CobwebTree:  61%|   | 6128/10000 [01:51<01:16, 50.65it/s]Training CobwebTree:  61%|   | 6134/10000 [01:51<01:17, 49.80it/s]Training CobwebTree:  61%|   | 6140/10000 [01:51<01:16, 50.36it/s]Training CobwebTree:  61%|   | 6146/10000 [01:51<01:18, 49.17it/s]Training CobwebTree:  62%|   | 6152/10000 [01:52<01:17, 49.48it/s]Training CobwebTree:  62%|   | 6158/10000 [01:52<01:13, 52.07it/s]Training CobwebTree:  62%|   | 6164/10000 [01:52<01:13, 51.85it/s]Training CobwebTree:  62%|   | 6170/10000 [01:52<01:13, 52.05it/s]Training CobwebTree:  62%|   | 6176/10000 [01:52<01:14, 51.59it/s]Training CobwebTree:  62%|   | 6182/10000 [01:52<01:11, 53.65it/s]Training CobwebTree:  62%|   | 6188/10000 [01:52<01:11, 53.64it/s]Training CobwebTree:  62%|   | 6194/10000 [01:52<01:14, 50.89it/s]Training CobwebTree:  62%|   | 6200/10000 [01:52<01:15, 50.58it/s]Training CobwebTree:  62%|   | 6206/10000 [01:53<01:16, 49.46it/s]Training CobwebTree:  62%|   | 6211/10000 [01:53<01:17, 48.89it/s]Training CobwebTree:  62%|   | 6217/10000 [01:53<01:15, 49.87it/s]Training CobwebTree:  62%|   | 6222/10000 [01:53<01:18, 47.87it/s]Training CobwebTree:  62%|   | 6227/10000 [01:53<01:19, 47.57it/s]Training CobwebTree:  62%|   | 6233/10000 [01:53<01:17, 48.64it/s]Training CobwebTree:  62%|   | 6238/10000 [01:53<01:18, 47.64it/s]Training CobwebTree:  62%|   | 6243/10000 [01:53<01:19, 46.98it/s]Training CobwebTree:  62%|   | 6248/10000 [01:54<01:21, 46.23it/s]Training CobwebTree:  63%|   | 6254/10000 [01:54<01:18, 47.65it/s]Training CobwebTree:  63%|   | 6259/10000 [01:54<01:18, 47.59it/s]Training CobwebTree:  63%|   | 6264/10000 [01:54<01:17, 48.13it/s]Training CobwebTree:  63%|   | 6269/10000 [01:54<01:17, 48.22it/s]Training CobwebTree:  63%|   | 6275/10000 [01:54<01:13, 50.98it/s]Training CobwebTree:  63%|   | 6281/10000 [01:54<01:15, 49.16it/s]Training CobwebTree:  63%|   | 6286/10000 [01:54<01:21, 45.70it/s]Training CobwebTree:  63%|   | 6292/10000 [01:54<01:16, 48.71it/s]Training CobwebTree:  63%|   | 6298/10000 [01:55<01:14, 49.51it/s]Training CobwebTree:  63%|   | 6303/10000 [01:55<01:17, 47.89it/s]Training CobwebTree:  63%|   | 6309/10000 [01:55<01:14, 49.23it/s]Training CobwebTree:  63%|   | 6315/10000 [01:55<01:14, 49.77it/s]Training CobwebTree:  63%|   | 6321/10000 [01:55<01:13, 49.79it/s]Training CobwebTree:  63%|   | 6327/10000 [01:55<01:14, 49.53it/s]Training CobwebTree:  63%|   | 6333/10000 [01:55<01:11, 51.11it/s]Training CobwebTree:  63%|   | 6339/10000 [01:55<01:14, 49.44it/s]Training CobwebTree:  63%|   | 6345/10000 [01:55<01:10, 51.82it/s]Training CobwebTree:  64%|   | 6351/10000 [01:56<01:11, 51.31it/s]Training CobwebTree:  64%|   | 6357/10000 [01:56<01:08, 53.07it/s]Training CobwebTree:  64%|   | 6363/10000 [01:56<01:10, 51.27it/s]Training CobwebTree:  64%|   | 6369/10000 [01:56<01:10, 51.37it/s]Training CobwebTree:  64%|   | 6375/10000 [01:56<01:13, 49.53it/s]Training CobwebTree:  64%|   | 6380/10000 [01:56<01:15, 48.20it/s]Training CobwebTree:  64%|   | 6385/10000 [01:56<01:14, 48.41it/s]Training CobwebTree:  64%|   | 6391/10000 [01:56<01:10, 50.88it/s]Training CobwebTree:  64%|   | 6397/10000 [01:56<01:10, 51.30it/s]Training CobwebTree:  64%|   | 6403/10000 [01:57<01:13, 49.20it/s]Training CobwebTree:  64%|   | 6408/10000 [01:57<01:13, 48.76it/s]Training CobwebTree:  64%|   | 6414/10000 [01:57<01:12, 49.33it/s]Training CobwebTree:  64%|   | 6420/10000 [01:57<01:10, 50.92it/s]Training CobwebTree:  64%|   | 6426/10000 [01:57<01:09, 51.70it/s]Training CobwebTree:  64%|   | 6432/10000 [01:57<01:08, 52.26it/s]Training CobwebTree:  64%|   | 6438/10000 [01:57<01:07, 52.85it/s]Training CobwebTree:  64%|   | 6444/10000 [01:57<01:09, 51.36it/s]Training CobwebTree:  64%|   | 6450/10000 [01:58<01:07, 52.35it/s]Training CobwebTree:  65%|   | 6456/10000 [01:58<01:08, 51.83it/s]Training CobwebTree:  65%|   | 6462/10000 [01:58<01:07, 52.22it/s]Training CobwebTree:  65%|   | 6468/10000 [01:58<01:09, 51.09it/s]Training CobwebTree:  65%|   | 6474/10000 [01:58<01:09, 50.62it/s]Training CobwebTree:  65%|   | 6480/10000 [01:58<01:11, 48.99it/s]Training CobwebTree:  65%|   | 6486/10000 [01:58<01:09, 50.89it/s]Training CobwebTree:  65%|   | 6492/10000 [01:58<01:09, 50.47it/s]Training CobwebTree:  65%|   | 6498/10000 [01:59<01:12, 48.63it/s]Training CobwebTree:  65%|   | 6503/10000 [01:59<01:11, 48.90it/s]Training CobwebTree:  65%|   | 6508/10000 [01:59<01:13, 47.22it/s]Training CobwebTree:  65%|   | 6513/10000 [01:59<01:14, 47.01it/s]Training CobwebTree:  65%|   | 6519/10000 [01:59<01:09, 50.21it/s]Training CobwebTree:  65%|   | 6525/10000 [01:59<01:11, 48.90it/s]Training CobwebTree:  65%|   | 6530/10000 [01:59<01:10, 48.91it/s]Training CobwebTree:  65%|   | 6536/10000 [01:59<01:07, 51.09it/s]Training CobwebTree:  65%|   | 6542/10000 [01:59<01:14, 46.34it/s]Training CobwebTree:  65%|   | 6547/10000 [02:00<01:14, 46.55it/s]Training CobwebTree:  66%|   | 6553/10000 [02:00<01:10, 48.55it/s]Training CobwebTree:  66%|   | 6558/10000 [02:00<01:14, 46.44it/s]Training CobwebTree:  66%|   | 6564/10000 [02:00<01:09, 49.58it/s]Training CobwebTree:  66%|   | 6570/10000 [02:00<01:07, 51.08it/s]Training CobwebTree:  66%|   | 6576/10000 [02:00<01:06, 51.62it/s]Training CobwebTree:  66%|   | 6582/10000 [02:00<01:06, 51.70it/s]Training CobwebTree:  66%|   | 6588/10000 [02:00<01:04, 52.76it/s]Training CobwebTree:  66%|   | 6594/10000 [02:00<01:02, 54.12it/s]Training CobwebTree:  66%|   | 6600/10000 [02:01<01:01, 55.30it/s]Training CobwebTree:  66%|   | 6606/10000 [02:01<01:01, 55.15it/s]Training CobwebTree:  66%|   | 6612/10000 [02:01<01:03, 53.51it/s]Training CobwebTree:  66%|   | 6618/10000 [02:01<01:01, 54.70it/s]Training CobwebTree:  66%|   | 6624/10000 [02:01<01:06, 50.61it/s]Training CobwebTree:  66%|   | 6630/10000 [02:01<01:07, 49.83it/s]Training CobwebTree:  66%|   | 6636/10000 [02:01<01:10, 47.92it/s]Training CobwebTree:  66%|   | 6642/10000 [02:01<01:07, 49.47it/s]Training CobwebTree:  66%|   | 6648/10000 [02:01<01:07, 49.51it/s]Training CobwebTree:  67%|   | 6654/10000 [02:02<01:05, 51.28it/s]Training CobwebTree:  67%|   | 6660/10000 [02:02<01:10, 47.58it/s]Training CobwebTree:  67%|   | 6665/10000 [02:02<01:11, 46.95it/s]Training CobwebTree:  67%|   | 6670/10000 [02:02<01:10, 47.53it/s]Training CobwebTree:  67%|   | 6676/10000 [02:02<01:06, 50.25it/s]Training CobwebTree:  67%|   | 6682/10000 [02:02<01:06, 49.77it/s]Training CobwebTree:  67%|   | 6688/10000 [02:02<01:05, 50.48it/s]Training CobwebTree:  67%|   | 6694/10000 [02:02<01:04, 51.45it/s]Training CobwebTree:  67%|   | 6700/10000 [02:03<01:04, 51.37it/s]Training CobwebTree:  67%|   | 6706/10000 [02:03<01:05, 49.94it/s]Training CobwebTree:  67%|   | 6712/10000 [02:03<01:08, 48.01it/s]Training CobwebTree:  67%|   | 6717/10000 [02:03<01:07, 48.42it/s]Training CobwebTree:  67%|   | 6722/10000 [02:03<01:10, 46.80it/s]Training CobwebTree:  67%|   | 6728/10000 [02:03<01:07, 48.34it/s]Training CobwebTree:  67%|   | 6733/10000 [02:03<01:08, 47.88it/s]Training CobwebTree:  67%|   | 6739/10000 [02:03<01:05, 49.82it/s]Training CobwebTree:  67%|   | 6744/10000 [02:03<01:06, 48.98it/s]Training CobwebTree:  67%|   | 6749/10000 [02:04<01:06, 49.03it/s]Training CobwebTree:  68%|   | 6754/10000 [02:04<01:07, 48.06it/s]Training CobwebTree:  68%|   | 6760/10000 [02:04<01:06, 48.98it/s]Training CobwebTree:  68%|   | 6766/10000 [02:04<01:06, 48.93it/s]Training CobwebTree:  68%|   | 6771/10000 [02:04<01:09, 46.21it/s]Training CobwebTree:  68%|   | 6777/10000 [02:04<01:07, 48.05it/s]Training CobwebTree:  68%|   | 6782/10000 [02:04<01:07, 47.71it/s]Training CobwebTree:  68%|   | 6787/10000 [02:04<01:06, 48.00it/s]Training CobwebTree:  68%|   | 6793/10000 [02:04<01:05, 48.62it/s]Training CobwebTree:  68%|   | 6798/10000 [02:05<01:08, 46.72it/s]Training CobwebTree:  68%|   | 6804/10000 [02:05<01:06, 47.99it/s]Training CobwebTree:  68%|   | 6810/10000 [02:05<01:03, 50.40it/s]Training CobwebTree:  68%|   | 6816/10000 [02:05<01:03, 50.49it/s]Training CobwebTree:  68%|   | 6822/10000 [02:05<01:04, 49.54it/s]Training CobwebTree:  68%|   | 6827/10000 [02:05<01:06, 47.96it/s]Training CobwebTree:  68%|   | 6832/10000 [02:05<01:06, 47.69it/s]Training CobwebTree:  68%|   | 6838/10000 [02:05<01:02, 50.63it/s]Training CobwebTree:  68%|   | 6844/10000 [02:05<00:59, 52.68it/s]Training CobwebTree:  68%|   | 6850/10000 [02:06<01:01, 51.06it/s]Training CobwebTree:  69%|   | 6856/10000 [02:06<01:02, 50.25it/s]Training CobwebTree:  69%|   | 6862/10000 [02:06<01:01, 50.62it/s]Training CobwebTree:  69%|   | 6868/10000 [02:06<01:02, 49.90it/s]Training CobwebTree:  69%|   | 6874/10000 [02:06<00:59, 52.16it/s]Training CobwebTree:  69%|   | 6880/10000 [02:06<01:00, 51.62it/s]Training CobwebTree:  69%|   | 6886/10000 [02:06<01:03, 49.23it/s]Training CobwebTree:  69%|   | 6892/10000 [02:06<01:03, 49.19it/s]Training CobwebTree:  69%|   | 6897/10000 [02:07<01:03, 48.59it/s]Training CobwebTree:  69%|   | 6902/10000 [02:07<01:05, 47.66it/s]Training CobwebTree:  69%|   | 6907/10000 [02:07<01:05, 47.16it/s]Training CobwebTree:  69%|   | 6912/10000 [02:07<01:04, 47.51it/s]Training CobwebTree:  69%|   | 6917/10000 [02:07<01:04, 47.85it/s]Training CobwebTree:  69%|   | 6922/10000 [02:07<01:04, 47.65it/s]Training CobwebTree:  69%|   | 6928/10000 [02:07<01:01, 49.64it/s]Training CobwebTree:  69%|   | 6934/10000 [02:07<00:59, 51.37it/s]Training CobwebTree:  69%|   | 6940/10000 [02:07<00:59, 51.24it/s]Training CobwebTree:  69%|   | 6946/10000 [02:08<00:58, 51.99it/s]Training CobwebTree:  70%|   | 6952/10000 [02:08<00:59, 50.98it/s]Training CobwebTree:  70%|   | 6958/10000 [02:08<00:59, 51.01it/s]Training CobwebTree:  70%|   | 6964/10000 [02:08<01:03, 47.76it/s]Training CobwebTree:  70%|   | 6970/10000 [02:08<01:02, 48.86it/s]Training CobwebTree:  70%|   | 6976/10000 [02:08<01:01, 48.93it/s]Training CobwebTree:  70%|   | 6981/10000 [02:08<01:02, 48.38it/s]Training CobwebTree:  70%|   | 6986/10000 [02:08<01:04, 46.90it/s]Training CobwebTree:  70%|   | 6991/10000 [02:08<01:04, 46.86it/s]Training CobwebTree:  70%|   | 6997/10000 [02:09<01:00, 49.90it/s]Training CobwebTree:  70%|   | 7003/10000 [02:09<01:00, 49.88it/s]Training CobwebTree:  70%|   | 7009/10000 [02:09<01:03, 47.15it/s]Training CobwebTree:  70%|   | 7014/10000 [02:09<01:03, 47.09it/s]Training CobwebTree:  70%|   | 7020/10000 [02:09<01:01, 48.42it/s]Training CobwebTree:  70%|   | 7025/10000 [02:09<01:01, 48.68it/s]Training CobwebTree:  70%|   | 7031/10000 [02:09<00:59, 50.16it/s]Training CobwebTree:  70%|   | 7037/10000 [02:09<01:00, 49.25it/s]Training CobwebTree:  70%|   | 7042/10000 [02:10<01:00, 48.94it/s]Training CobwebTree:  70%|   | 7048/10000 [02:10<01:00, 48.88it/s]Training CobwebTree:  71%|   | 7054/10000 [02:10<00:58, 50.02it/s]Training CobwebTree:  71%|   | 7060/10000 [02:10<00:58, 50.30it/s]Training CobwebTree:  71%|   | 7066/10000 [02:10<00:57, 50.66it/s]Training CobwebTree:  71%|   | 7072/10000 [02:10<00:59, 49.05it/s]Training CobwebTree:  71%|   | 7077/10000 [02:10<01:00, 48.33it/s]Training CobwebTree:  71%|   | 7082/10000 [02:10<01:01, 47.73it/s]Training CobwebTree:  71%|   | 7088/10000 [02:10<00:59, 48.98it/s]Training CobwebTree:  71%|   | 7093/10000 [02:11<01:00, 48.18it/s]Training CobwebTree:  71%|   | 7098/10000 [02:11<01:01, 47.51it/s]Training CobwebTree:  71%|   | 7103/10000 [02:11<01:00, 48.15it/s]Training CobwebTree:  71%|   | 7109/10000 [02:11<00:59, 48.70it/s]Training CobwebTree:  71%|   | 7115/10000 [02:11<00:58, 49.49it/s]Training CobwebTree:  71%|   | 7120/10000 [02:11<00:58, 49.52it/s]Training CobwebTree:  71%|  | 7125/10000 [02:11<00:57, 49.57it/s]Training CobwebTree:  71%|  | 7131/10000 [02:11<00:58, 49.36it/s]Training CobwebTree:  71%|  | 7136/10000 [02:11<00:59, 48.09it/s]Training CobwebTree:  71%|  | 7141/10000 [02:12<01:01, 46.86it/s]Training CobwebTree:  71%|  | 7146/10000 [02:12<01:00, 46.88it/s]Training CobwebTree:  72%|  | 7151/10000 [02:12<01:00, 46.98it/s]Training CobwebTree:  72%|  | 7157/10000 [02:12<00:58, 48.86it/s]Training CobwebTree:  72%|  | 7163/10000 [02:12<00:56, 50.42it/s]Training CobwebTree:  72%|  | 7169/10000 [02:12<00:56, 50.20it/s]Training CobwebTree:  72%|  | 7175/10000 [02:12<00:58, 48.02it/s]Training CobwebTree:  72%|  | 7181/10000 [02:12<00:56, 50.03it/s]Training CobwebTree:  72%|  | 7187/10000 [02:12<00:54, 51.87it/s]Training CobwebTree:  72%|  | 7193/10000 [02:13<00:53, 52.38it/s]Training CobwebTree:  72%|  | 7199/10000 [02:13<00:57, 49.06it/s]Training CobwebTree:  72%|  | 7204/10000 [02:13<00:58, 47.52it/s]Training CobwebTree:  72%|  | 7209/10000 [02:13<00:58, 47.89it/s]Training CobwebTree:  72%|  | 7215/10000 [02:13<00:55, 49.93it/s]Training CobwebTree:  72%|  | 7221/10000 [02:13<00:57, 48.55it/s]Training CobwebTree:  72%|  | 7226/10000 [02:13<00:57, 48.21it/s]Training CobwebTree:  72%|  | 7232/10000 [02:13<00:55, 49.90it/s]Training CobwebTree:  72%|  | 7238/10000 [02:14<00:58, 47.31it/s]Training CobwebTree:  72%|  | 7245/10000 [02:14<00:53, 51.12it/s]Training CobwebTree:  73%|  | 7252/10000 [02:14<00:50, 54.41it/s]Training CobwebTree:  73%|  | 7258/10000 [02:14<00:50, 54.19it/s]Training CobwebTree:  73%|  | 7264/10000 [02:14<00:53, 51.53it/s]Training CobwebTree:  73%|  | 7270/10000 [02:14<00:51, 52.66it/s]Training CobwebTree:  73%|  | 7276/10000 [02:14<00:52, 51.88it/s]Training CobwebTree:  73%|  | 7282/10000 [02:14<00:52, 51.76it/s]Training CobwebTree:  73%|  | 7288/10000 [02:14<00:52, 51.69it/s]Training CobwebTree:  73%|  | 7294/10000 [02:15<00:54, 49.82it/s]Training CobwebTree:  73%|  | 7300/10000 [02:15<00:53, 50.51it/s]Training CobwebTree:  73%|  | 7306/10000 [02:15<00:54, 49.55it/s]Training CobwebTree:  73%|  | 7312/10000 [02:15<00:53, 50.28it/s]Training CobwebTree:  73%|  | 7318/10000 [02:15<00:52, 51.43it/s]Training CobwebTree:  73%|  | 7324/10000 [02:15<00:54, 49.22it/s]Training CobwebTree:  73%|  | 7330/10000 [02:15<00:52, 50.59it/s]Training CobwebTree:  73%|  | 7336/10000 [02:15<00:50, 52.49it/s]Training CobwebTree:  73%|  | 7342/10000 [02:16<00:51, 51.54it/s]Training CobwebTree:  73%|  | 7348/10000 [02:16<00:50, 52.38it/s]Training CobwebTree:  74%|  | 7354/10000 [02:16<00:53, 49.20it/s]Training CobwebTree:  74%|  | 7359/10000 [02:16<00:53, 49.17it/s]Training CobwebTree:  74%|  | 7366/10000 [02:16<00:49, 52.98it/s]Training CobwebTree:  74%|  | 7372/10000 [02:16<00:50, 52.22it/s]Training CobwebTree:  74%|  | 7378/10000 [02:16<00:49, 53.48it/s]Training CobwebTree:  74%|  | 7384/10000 [02:16<00:51, 51.13it/s]Training CobwebTree:  74%|  | 7390/10000 [02:16<00:53, 49.13it/s]Training CobwebTree:  74%|  | 7395/10000 [02:17<00:54, 47.74it/s]Training CobwebTree:  74%|  | 7401/10000 [02:17<00:53, 48.82it/s]Training CobwebTree:  74%|  | 7406/10000 [02:17<00:54, 47.41it/s]Training CobwebTree:  74%|  | 7411/10000 [02:17<00:54, 47.32it/s]Training CobwebTree:  74%|  | 7416/10000 [02:17<00:55, 46.93it/s]Training CobwebTree:  74%|  | 7422/10000 [02:17<00:52, 48.68it/s]Training CobwebTree:  74%|  | 7427/10000 [02:17<00:54, 47.23it/s]Training CobwebTree:  74%|  | 7432/10000 [02:17<00:54, 47.07it/s]Training CobwebTree:  74%|  | 7437/10000 [02:17<00:54, 47.36it/s]Training CobwebTree:  74%|  | 7442/10000 [02:18<00:53, 47.46it/s]Training CobwebTree:  74%|  | 7447/10000 [02:18<00:53, 47.46it/s]Training CobwebTree:  75%|  | 7452/10000 [02:18<00:55, 45.99it/s]Training CobwebTree:  75%|  | 7457/10000 [02:18<00:54, 46.40it/s]Training CobwebTree:  75%|  | 7462/10000 [02:18<00:54, 46.89it/s]Training CobwebTree:  75%|  | 7467/10000 [02:18<00:54, 46.82it/s]Training CobwebTree:  75%|  | 7472/10000 [02:18<00:54, 46.27it/s]Training CobwebTree:  75%|  | 7477/10000 [02:18<00:53, 47.24it/s]Training CobwebTree:  75%|  | 7482/10000 [02:18<00:53, 47.30it/s]Training CobwebTree:  75%|  | 7487/10000 [02:19<00:54, 46.45it/s]Training CobwebTree:  75%|  | 7492/10000 [02:19<00:55, 45.18it/s]Training CobwebTree:  75%|  | 7498/10000 [02:19<00:52, 47.75it/s]Training CobwebTree:  75%|  | 7503/10000 [02:19<00:53, 46.66it/s]Training CobwebTree:  75%|  | 7508/10000 [02:19<00:54, 45.36it/s]Training CobwebTree:  75%|  | 7514/10000 [02:19<00:51, 48.41it/s]Training CobwebTree:  75%|  | 7519/10000 [02:19<00:52, 47.52it/s]Training CobwebTree:  75%|  | 7524/10000 [02:19<00:52, 47.24it/s]Training CobwebTree:  75%|  | 7529/10000 [02:19<00:53, 46.50it/s]Training CobwebTree:  75%|  | 7534/10000 [02:20<00:52, 46.55it/s]Training CobwebTree:  75%|  | 7539/10000 [02:20<00:52, 47.22it/s]Training CobwebTree:  75%|  | 7545/10000 [02:20<00:51, 47.87it/s]Training CobwebTree:  76%|  | 7550/10000 [02:20<00:53, 45.61it/s]Training CobwebTree:  76%|  | 7555/10000 [02:20<00:54, 44.97it/s]Training CobwebTree:  76%|  | 7560/10000 [02:20<00:52, 46.18it/s]Training CobwebTree:  76%|  | 7565/10000 [02:20<00:51, 46.94it/s]Training CobwebTree:  76%|  | 7570/10000 [02:20<00:52, 46.07it/s]Training CobwebTree:  76%|  | 7575/10000 [02:20<00:53, 45.20it/s]Training CobwebTree:  76%|  | 7580/10000 [02:21<00:53, 45.32it/s]Training CobwebTree:  76%|  | 7585/10000 [02:21<00:52, 45.84it/s]Training CobwebTree:  76%|  | 7591/10000 [02:21<00:50, 47.24it/s]Training CobwebTree:  76%|  | 7596/10000 [02:21<00:51, 46.63it/s]Training CobwebTree:  76%|  | 7602/10000 [02:21<00:50, 47.87it/s]Training CobwebTree:  76%|  | 7607/10000 [02:21<00:50, 47.64it/s]Training CobwebTree:  76%|  | 7612/10000 [02:21<00:50, 47.23it/s]Training CobwebTree:  76%|  | 7617/10000 [02:21<00:51, 46.44it/s]Training CobwebTree:  76%|  | 7622/10000 [02:21<00:51, 45.84it/s]Training CobwebTree:  76%|  | 7628/10000 [02:22<00:48, 48.81it/s]Training CobwebTree:  76%|  | 7633/10000 [02:22<00:50, 46.63it/s]Training CobwebTree:  76%|  | 7638/10000 [02:22<00:53, 44.40it/s]Training CobwebTree:  76%|  | 7643/10000 [02:22<00:51, 45.58it/s]Training CobwebTree:  76%|  | 7648/10000 [02:22<00:50, 46.35it/s]Training CobwebTree:  77%|  | 7653/10000 [02:22<00:50, 46.45it/s]Training CobwebTree:  77%|  | 7658/10000 [02:22<00:50, 46.49it/s]Training CobwebTree:  77%|  | 7664/10000 [02:22<00:47, 49.45it/s]Training CobwebTree:  77%|  | 7669/10000 [02:22<00:47, 49.43it/s]Training CobwebTree:  77%|  | 7674/10000 [02:23<00:49, 47.39it/s]Training CobwebTree:  77%|  | 7679/10000 [02:23<00:49, 46.65it/s]Training CobwebTree:  77%|  | 7684/10000 [02:23<00:50, 45.69it/s]Training CobwebTree:  77%|  | 7690/10000 [02:23<00:47, 48.84it/s]Training CobwebTree:  77%|  | 7695/10000 [02:23<00:47, 48.27it/s]Training CobwebTree:  77%|  | 7700/10000 [02:23<00:47, 48.40it/s]Training CobwebTree:  77%|  | 7706/10000 [02:23<00:46, 49.77it/s]Training CobwebTree:  77%|  | 7711/10000 [02:23<00:46, 49.06it/s]Training CobwebTree:  77%|  | 7716/10000 [02:23<00:47, 47.69it/s]Training CobwebTree:  77%|  | 7722/10000 [02:24<00:44, 51.09it/s]Training CobwebTree:  77%|  | 7728/10000 [02:24<00:45, 49.42it/s]Training CobwebTree:  77%|  | 7733/10000 [02:24<00:46, 48.59it/s]Training CobwebTree:  77%|  | 7738/10000 [02:24<00:46, 48.81it/s]Training CobwebTree:  77%|  | 7743/10000 [02:24<00:46, 49.00it/s]Training CobwebTree:  77%|  | 7748/10000 [02:24<00:45, 49.00it/s]Training CobwebTree:  78%|  | 7753/10000 [02:24<00:46, 48.33it/s]Training CobwebTree:  78%|  | 7758/10000 [02:24<00:46, 48.33it/s]Training CobwebTree:  78%|  | 7765/10000 [02:24<00:43, 51.62it/s]Training CobwebTree:  78%|  | 7771/10000 [02:25<00:44, 50.14it/s]Training CobwebTree:  78%|  | 7777/10000 [02:25<00:43, 50.86it/s]Training CobwebTree:  78%|  | 7783/10000 [02:25<00:44, 50.16it/s]Training CobwebTree:  78%|  | 7789/10000 [02:25<00:44, 49.97it/s]Training CobwebTree:  78%|  | 7795/10000 [02:25<00:44, 49.66it/s]Training CobwebTree:  78%|  | 7800/10000 [02:25<00:45, 48.25it/s]Training CobwebTree:  78%|  | 7805/10000 [02:25<00:45, 48.35it/s]Training CobwebTree:  78%|  | 7810/10000 [02:25<00:45, 48.64it/s]Training CobwebTree:  78%|  | 7815/10000 [02:25<00:46, 46.96it/s]Training CobwebTree:  78%|  | 7820/10000 [02:26<00:45, 47.57it/s]Training CobwebTree:  78%|  | 7825/10000 [02:26<00:45, 47.89it/s]Training CobwebTree:  78%|  | 7830/10000 [02:26<00:45, 47.50it/s]Training CobwebTree:  78%|  | 7835/10000 [02:26<00:45, 47.93it/s]Training CobwebTree:  78%|  | 7841/10000 [02:26<00:44, 48.50it/s]Training CobwebTree:  78%|  | 7847/10000 [02:26<00:44, 48.82it/s]Training CobwebTree:  79%|  | 7852/10000 [02:26<00:44, 47.77it/s]Training CobwebTree:  79%|  | 7858/10000 [02:26<00:44, 48.29it/s]Training CobwebTree:  79%|  | 7863/10000 [02:26<00:45, 47.25it/s]Training CobwebTree:  79%|  | 7868/10000 [02:27<00:45, 46.71it/s]Training CobwebTree:  79%|  | 7873/10000 [02:27<00:46, 45.47it/s]Training CobwebTree:  79%|  | 7878/10000 [02:27<00:45, 46.59it/s]Training CobwebTree:  79%|  | 7883/10000 [02:27<00:47, 44.50it/s]Training CobwebTree:  79%|  | 7889/10000 [02:27<00:43, 48.05it/s]Training CobwebTree:  79%|  | 7894/10000 [02:27<00:44, 47.76it/s]Training CobwebTree:  79%|  | 7899/10000 [02:27<00:44, 47.52it/s]Training CobwebTree:  79%|  | 7904/10000 [02:27<00:45, 46.35it/s]Training CobwebTree:  79%|  | 7909/10000 [02:27<00:44, 46.82it/s]Training CobwebTree:  79%|  | 7914/10000 [02:28<00:45, 45.97it/s]Training CobwebTree:  79%|  | 7919/10000 [02:28<00:44, 46.44it/s]Training CobwebTree:  79%|  | 7924/10000 [02:28<00:45, 46.07it/s]Training CobwebTree:  79%|  | 7930/10000 [02:28<00:43, 48.03it/s]Training CobwebTree:  79%|  | 7935/10000 [02:28<00:44, 46.53it/s]Training CobwebTree:  79%|  | 7940/10000 [02:28<00:44, 46.26it/s]Training CobwebTree:  79%|  | 7945/10000 [02:28<00:43, 47.06it/s]Training CobwebTree:  80%|  | 7950/10000 [02:28<00:43, 46.97it/s]Training CobwebTree:  80%|  | 7955/10000 [02:28<00:43, 47.37it/s]Training CobwebTree:  80%|  | 7961/10000 [02:29<00:41, 48.92it/s]Training CobwebTree:  80%|  | 7966/10000 [02:29<00:42, 47.74it/s]Training CobwebTree:  80%|  | 7971/10000 [02:29<00:43, 46.88it/s]Training CobwebTree:  80%|  | 7976/10000 [02:29<00:42, 47.14it/s]Training CobwebTree:  80%|  | 7981/10000 [02:29<00:42, 47.48it/s]Training CobwebTree:  80%|  | 7988/10000 [02:29<00:38, 51.75it/s]Training CobwebTree:  80%|  | 7994/10000 [02:29<00:40, 49.67it/s]Training CobwebTree:  80%|  | 7999/10000 [02:29<00:40, 49.13it/s]Training CobwebTree:  80%|  | 8004/10000 [02:29<00:41, 48.28it/s]Training CobwebTree:  80%|  | 8009/10000 [02:30<00:41, 47.59it/s]Training CobwebTree:  80%|  | 8015/10000 [02:30<00:40, 48.88it/s]Training CobwebTree:  80%|  | 8021/10000 [02:30<00:39, 49.96it/s]Training CobwebTree:  80%|  | 8026/10000 [02:30<00:40, 48.74it/s]Training CobwebTree:  80%|  | 8031/10000 [02:30<00:42, 46.86it/s]Training CobwebTree:  80%|  | 8037/10000 [02:30<00:41, 47.84it/s]Training CobwebTree:  80%|  | 8042/10000 [02:30<00:41, 47.23it/s]Training CobwebTree:  80%|  | 8047/10000 [02:30<00:41, 47.58it/s]Training CobwebTree:  81%|  | 8052/10000 [02:30<00:41, 46.48it/s]Training CobwebTree:  81%|  | 8058/10000 [02:31<00:39, 48.77it/s]Training CobwebTree:  81%|  | 8064/10000 [02:31<00:39, 48.64it/s]Training CobwebTree:  81%|  | 8069/10000 [02:31<00:40, 48.15it/s]Training CobwebTree:  81%|  | 8075/10000 [02:31<00:38, 49.93it/s]Training CobwebTree:  81%|  | 8080/10000 [02:31<00:41, 46.32it/s]Training CobwebTree:  81%|  | 8085/10000 [02:31<00:41, 46.29it/s]Training CobwebTree:  81%|  | 8090/10000 [02:31<00:40, 46.93it/s]Training CobwebTree:  81%|  | 8096/10000 [02:31<00:38, 49.31it/s]Training CobwebTree:  81%|  | 8101/10000 [02:31<00:38, 49.34it/s]Training CobwebTree:  81%|  | 8106/10000 [02:32<00:39, 47.37it/s]Training CobwebTree:  81%|  | 8112/10000 [02:32<00:38, 48.50it/s]Training CobwebTree:  81%|  | 8117/10000 [02:32<00:39, 47.13it/s]Training CobwebTree:  81%|  | 8122/10000 [02:32<00:39, 47.60it/s]Training CobwebTree:  81%| | 8127/10000 [02:32<00:40, 45.96it/s]Training CobwebTree:  81%| | 8132/10000 [02:32<00:40, 45.62it/s]Training CobwebTree:  81%| | 8138/10000 [02:32<00:39, 46.63it/s]Training CobwebTree:  81%| | 8143/10000 [02:32<00:40, 46.05it/s]Training CobwebTree:  81%| | 8148/10000 [02:32<00:40, 46.30it/s]Training CobwebTree:  82%| | 8153/10000 [02:33<00:39, 47.00it/s]Training CobwebTree:  82%| | 8159/10000 [02:33<00:38, 48.02it/s]Training CobwebTree:  82%| | 8164/10000 [02:33<00:38, 48.15it/s]Training CobwebTree:  82%| | 8169/10000 [02:33<00:38, 47.52it/s]Training CobwebTree:  82%| | 8174/10000 [02:33<00:39, 46.38it/s]Training CobwebTree:  82%| | 8179/10000 [02:33<00:40, 45.17it/s]Training CobwebTree:  82%| | 8184/10000 [02:33<00:39, 46.07it/s]Training CobwebTree:  82%| | 8189/10000 [02:33<00:39, 45.73it/s]Training CobwebTree:  82%| | 8194/10000 [02:33<00:40, 44.23it/s]Training CobwebTree:  82%| | 8199/10000 [02:34<00:41, 43.55it/s]Training CobwebTree:  82%| | 8204/10000 [02:34<00:40, 44.63it/s]Training CobwebTree:  82%| | 8209/10000 [02:34<00:40, 44.25it/s]Training CobwebTree:  82%| | 8215/10000 [02:34<00:37, 47.21it/s]Training CobwebTree:  82%| | 8221/10000 [02:34<00:35, 49.86it/s]Training CobwebTree:  82%| | 8227/10000 [02:34<00:35, 49.36it/s]Training CobwebTree:  82%| | 8232/10000 [02:34<00:37, 46.99it/s]Training CobwebTree:  82%| | 8237/10000 [02:34<00:37, 47.03it/s]Training CobwebTree:  82%| | 8242/10000 [02:34<00:37, 47.19it/s]Training CobwebTree:  82%| | 8247/10000 [02:35<00:36, 47.45it/s]Training CobwebTree:  83%| | 8253/10000 [02:35<00:36, 48.42it/s]Training CobwebTree:  83%| | 8259/10000 [02:35<00:34, 50.40it/s]Training CobwebTree:  83%| | 8265/10000 [02:35<00:35, 49.01it/s]Training CobwebTree:  83%| | 8270/10000 [02:35<00:35, 48.43it/s]Training CobwebTree:  83%| | 8276/10000 [02:35<00:35, 49.25it/s]Training CobwebTree:  83%| | 8282/10000 [02:35<00:33, 51.80it/s]Training CobwebTree:  83%| | 8288/10000 [02:35<00:36, 46.77it/s]Training CobwebTree:  83%| | 8293/10000 [02:36<00:36, 47.30it/s]Training CobwebTree:  83%| | 8298/10000 [02:36<00:36, 47.11it/s]Training CobwebTree:  83%| | 8303/10000 [02:36<00:36, 46.42it/s]Training CobwebTree:  83%| | 8309/10000 [02:36<00:35, 47.40it/s]Training CobwebTree:  83%| | 8315/10000 [02:36<00:33, 49.84it/s]Training CobwebTree:  83%| | 8321/10000 [02:36<00:32, 51.35it/s]Training CobwebTree:  83%| | 8327/10000 [02:36<00:32, 51.20it/s]Training CobwebTree:  83%| | 8333/10000 [02:36<00:34, 49.01it/s]Training CobwebTree:  83%| | 8339/10000 [02:36<00:33, 50.33it/s]Training CobwebTree:  83%| | 8345/10000 [02:37<00:32, 51.44it/s]Training CobwebTree:  84%| | 8351/10000 [02:37<00:34, 48.43it/s]Training CobwebTree:  84%| | 8356/10000 [02:37<00:34, 48.33it/s]Training CobwebTree:  84%| | 8361/10000 [02:37<00:34, 47.37it/s]Training CobwebTree:  84%| | 8366/10000 [02:37<00:35, 46.31it/s]Training CobwebTree:  84%| | 8372/10000 [02:37<00:33, 48.40it/s]Training CobwebTree:  84%| | 8377/10000 [02:37<00:33, 48.71it/s]Training CobwebTree:  84%| | 8382/10000 [02:37<00:33, 48.60it/s]Training CobwebTree:  84%| | 8387/10000 [02:37<00:33, 48.58it/s]Training CobwebTree:  84%| | 8393/10000 [02:38<00:33, 48.22it/s]Training CobwebTree:  84%| | 8398/10000 [02:38<00:33, 47.39it/s]Training CobwebTree:  84%| | 8403/10000 [02:38<00:34, 46.55it/s]Training CobwebTree:  84%| | 8408/10000 [02:38<00:34, 46.60it/s]Training CobwebTree:  84%| | 8413/10000 [02:38<00:34, 46.38it/s]Training CobwebTree:  84%| | 8418/10000 [02:38<00:34, 45.28it/s]Training CobwebTree:  84%| | 8423/10000 [02:38<00:34, 46.22it/s]Training CobwebTree:  84%| | 8429/10000 [02:38<00:32, 48.21it/s]Training CobwebTree:  84%| | 8435/10000 [02:38<00:31, 49.92it/s]Training CobwebTree:  84%| | 8441/10000 [02:39<00:30, 51.19it/s]Training CobwebTree:  84%| | 8447/10000 [02:39<00:31, 49.76it/s]Training CobwebTree:  85%| | 8453/10000 [02:39<00:30, 50.13it/s]Training CobwebTree:  85%| | 8459/10000 [02:39<00:30, 49.99it/s]Training CobwebTree:  85%| | 8465/10000 [02:39<00:31, 48.83it/s]Training CobwebTree:  85%| | 8470/10000 [02:39<00:31, 48.93it/s]Training CobwebTree:  85%| | 8476/10000 [02:39<00:31, 49.09it/s]Training CobwebTree:  85%| | 8481/10000 [02:39<00:31, 48.38it/s]Training CobwebTree:  85%| | 8486/10000 [02:39<00:32, 46.28it/s]Training CobwebTree:  85%| | 8492/10000 [02:40<00:31, 47.59it/s]Training CobwebTree:  85%| | 8497/10000 [02:40<00:33, 45.24it/s]Training CobwebTree:  85%| | 8502/10000 [02:40<00:33, 45.07it/s]Training CobwebTree:  85%| | 8507/10000 [02:40<00:32, 45.79it/s]Training CobwebTree:  85%| | 8512/10000 [02:40<00:31, 46.92it/s]Training CobwebTree:  85%| | 8518/10000 [02:40<00:30, 49.12it/s]Training CobwebTree:  85%| | 8523/10000 [02:40<00:30, 48.25it/s]Training CobwebTree:  85%| | 8529/10000 [02:40<00:29, 49.14it/s]Training CobwebTree:  85%| | 8534/10000 [02:40<00:29, 49.17it/s]Training CobwebTree:  85%| | 8540/10000 [02:41<00:28, 50.84it/s]Training CobwebTree:  85%| | 8546/10000 [02:41<00:29, 49.21it/s]Training CobwebTree:  86%| | 8552/10000 [02:41<00:29, 49.73it/s]Training CobwebTree:  86%| | 8557/10000 [02:41<00:30, 47.42it/s]Training CobwebTree:  86%| | 8562/10000 [02:41<00:30, 47.76it/s]Training CobwebTree:  86%| | 8567/10000 [02:41<00:30, 47.03it/s]Training CobwebTree:  86%| | 8573/10000 [02:41<00:29, 49.03it/s]Training CobwebTree:  86%| | 8578/10000 [02:41<00:29, 48.86it/s]Training CobwebTree:  86%| | 8584/10000 [02:42<00:28, 49.86it/s]Training CobwebTree:  86%| | 8589/10000 [02:42<00:28, 49.36it/s]Training CobwebTree:  86%| | 8594/10000 [02:42<00:31, 44.56it/s]Training CobwebTree:  86%| | 8599/10000 [02:42<00:30, 45.37it/s]Training CobwebTree:  86%| | 8605/10000 [02:42<00:30, 46.42it/s]Training CobwebTree:  86%| | 8610/10000 [02:42<00:29, 46.42it/s]Training CobwebTree:  86%| | 8615/10000 [02:42<00:30, 45.49it/s]Training CobwebTree:  86%| | 8620/10000 [02:42<00:30, 45.70it/s]Training CobwebTree:  86%| | 8625/10000 [02:42<00:29, 46.87it/s]Training CobwebTree:  86%| | 8631/10000 [02:43<00:28, 48.73it/s]Training CobwebTree:  86%| | 8637/10000 [02:43<00:27, 49.55it/s]Training CobwebTree:  86%| | 8642/10000 [02:43<00:27, 48.62it/s]Training CobwebTree:  86%| | 8648/10000 [02:43<00:27, 49.73it/s]Training CobwebTree:  87%| | 8654/10000 [02:43<00:26, 51.23it/s]Training CobwebTree:  87%| | 8660/10000 [02:43<00:27, 48.24it/s]Training CobwebTree:  87%| | 8666/10000 [02:43<00:27, 48.23it/s]Training CobwebTree:  87%| | 8671/10000 [02:43<00:27, 48.66it/s]Training CobwebTree:  87%| | 8676/10000 [02:43<00:27, 47.67it/s]Training CobwebTree:  87%| | 8681/10000 [02:44<00:28, 46.69it/s]Training CobwebTree:  87%| | 8686/10000 [02:44<00:29, 44.54it/s]Training CobwebTree:  87%| | 8691/10000 [02:44<00:29, 44.86it/s]Training CobwebTree:  87%| | 8696/10000 [02:44<00:28, 45.61it/s]Training CobwebTree:  87%| | 8701/10000 [02:44<00:28, 45.70it/s]Training CobwebTree:  87%| | 8706/10000 [02:44<00:28, 44.71it/s]Training CobwebTree:  87%| | 8711/10000 [02:44<00:28, 45.78it/s]Training CobwebTree:  87%| | 8716/10000 [02:44<00:28, 44.74it/s]Training CobwebTree:  87%| | 8721/10000 [02:44<00:29, 43.96it/s]Training CobwebTree:  87%| | 8726/10000 [02:45<00:27, 45.56it/s]Training CobwebTree:  87%| | 8731/10000 [02:45<00:27, 45.88it/s]Training CobwebTree:  87%| | 8737/10000 [02:45<00:25, 48.84it/s]Training CobwebTree:  87%| | 8742/10000 [02:45<00:26, 47.90it/s]Training CobwebTree:  87%| | 8747/10000 [02:45<00:26, 46.81it/s]Training CobwebTree:  88%| | 8752/10000 [02:45<00:28, 44.06it/s]Training CobwebTree:  88%| | 8757/10000 [02:45<00:27, 44.70it/s]Training CobwebTree:  88%| | 8762/10000 [02:45<00:28, 44.06it/s]Training CobwebTree:  88%| | 8767/10000 [02:45<00:27, 44.66it/s]Training CobwebTree:  88%| | 8772/10000 [02:46<00:27, 44.05it/s]Training CobwebTree:  88%| | 8777/10000 [02:46<00:27, 45.20it/s]Training CobwebTree:  88%| | 8783/10000 [02:46<00:26, 46.62it/s]Training CobwebTree:  88%| | 8788/10000 [02:46<00:26, 46.32it/s]Training CobwebTree:  88%| | 8793/10000 [02:46<00:25, 46.50it/s]Training CobwebTree:  88%| | 8798/10000 [02:46<00:25, 47.38it/s]Training CobwebTree:  88%| | 8803/10000 [02:46<00:25, 47.52it/s]Training CobwebTree:  88%| | 8808/10000 [02:46<00:26, 44.18it/s]Training CobwebTree:  88%| | 8813/10000 [02:46<00:26, 45.19it/s]Training CobwebTree:  88%| | 8819/10000 [02:47<00:24, 48.04it/s]Training CobwebTree:  88%| | 8825/10000 [02:47<00:23, 49.87it/s]Training CobwebTree:  88%| | 8831/10000 [02:47<00:24, 47.09it/s]Training CobwebTree:  88%| | 8836/10000 [02:47<00:24, 47.18it/s]Training CobwebTree:  88%| | 8841/10000 [02:47<00:25, 45.67it/s]Training CobwebTree:  88%| | 8846/10000 [02:47<00:25, 45.15it/s]Training CobwebTree:  89%| | 8851/10000 [02:47<00:24, 46.02it/s]Training CobwebTree:  89%| | 8856/10000 [02:47<00:25, 45.04it/s]Training CobwebTree:  89%| | 8861/10000 [02:47<00:24, 46.31it/s]Training CobwebTree:  89%| | 8866/10000 [02:48<00:25, 44.24it/s]Training CobwebTree:  89%| | 8871/10000 [02:48<00:25, 44.73it/s]Training CobwebTree:  89%| | 8876/10000 [02:48<00:24, 45.89it/s]Training CobwebTree:  89%| | 8881/10000 [02:48<00:24, 45.32it/s]Training CobwebTree:  89%| | 8886/10000 [02:48<00:24, 44.57it/s]Training CobwebTree:  89%| | 8891/10000 [02:48<00:24, 45.26it/s]Training CobwebTree:  89%| | 8896/10000 [02:48<00:24, 44.25it/s]Training CobwebTree:  89%| | 8901/10000 [02:48<00:25, 43.13it/s]Training CobwebTree:  89%| | 8906/10000 [02:49<00:24, 44.16it/s]Training CobwebTree:  89%| | 8911/10000 [02:49<00:24, 43.90it/s]Training CobwebTree:  89%| | 8916/10000 [02:49<00:25, 42.74it/s]Training CobwebTree:  89%| | 8921/10000 [02:49<00:24, 44.42it/s]Training CobwebTree:  89%| | 8926/10000 [02:49<00:23, 45.49it/s]Training CobwebTree:  89%| | 8931/10000 [02:49<00:23, 45.48it/s]Training CobwebTree:  89%| | 8936/10000 [02:49<00:24, 44.14it/s]Training CobwebTree:  89%| | 8942/10000 [02:49<00:22, 46.28it/s]Training CobwebTree:  89%| | 8947/10000 [02:49<00:23, 44.86it/s]Training CobwebTree:  90%| | 8952/10000 [02:50<00:22, 46.21it/s]Training CobwebTree:  90%| | 8957/10000 [02:50<00:23, 43.77it/s]Training CobwebTree:  90%| | 8962/10000 [02:50<00:23, 44.09it/s]Training CobwebTree:  90%| | 8967/10000 [02:50<00:23, 43.78it/s]Training CobwebTree:  90%| | 8972/10000 [02:50<00:23, 44.54it/s]Training CobwebTree:  90%| | 8977/10000 [02:50<00:22, 44.82it/s]Training CobwebTree:  90%| | 8982/10000 [02:50<00:22, 46.06it/s]Training CobwebTree:  90%| | 8987/10000 [02:50<00:22, 45.23it/s]Training CobwebTree:  90%| | 8993/10000 [02:50<00:21, 46.50it/s]Training CobwebTree:  90%| | 8998/10000 [02:51<00:21, 45.67it/s]Training CobwebTree:  90%| | 9004/10000 [02:51<00:20, 47.62it/s]Training CobwebTree:  90%| | 9009/10000 [02:51<00:21, 45.83it/s]Training CobwebTree:  90%| | 9014/10000 [02:51<00:22, 44.48it/s]Training CobwebTree:  90%| | 9020/10000 [02:51<00:21, 45.81it/s]Training CobwebTree:  90%| | 9025/10000 [02:51<00:21, 44.60it/s]Training CobwebTree:  90%| | 9030/10000 [02:51<00:21, 44.57it/s]Training CobwebTree:  90%| | 9035/10000 [02:51<00:21, 45.51it/s]Training CobwebTree:  90%| | 9040/10000 [02:51<00:21, 45.48it/s]Training CobwebTree:  90%| | 9045/10000 [02:52<00:21, 44.25it/s]Training CobwebTree:  90%| | 9050/10000 [02:52<00:21, 43.62it/s]Training CobwebTree:  91%| | 9055/10000 [02:52<00:20, 45.19it/s]Training CobwebTree:  91%| | 9060/10000 [02:52<00:20, 45.37it/s]Training CobwebTree:  91%| | 9065/10000 [02:52<00:20, 44.56it/s]Training CobwebTree:  91%| | 9070/10000 [02:52<00:20, 44.75it/s]Training CobwebTree:  91%| | 9076/10000 [02:52<00:20, 46.09it/s]Training CobwebTree:  91%| | 9081/10000 [02:52<00:20, 45.72it/s]Training CobwebTree:  91%| | 9086/10000 [02:53<00:20, 45.24it/s]Training CobwebTree:  91%| | 9091/10000 [02:53<00:22, 41.04it/s]Training CobwebTree:  91%| | 9096/10000 [02:53<00:22, 41.09it/s]Training CobwebTree:  91%| | 9101/10000 [02:53<00:21, 41.60it/s]Training CobwebTree:  91%| | 9106/10000 [02:53<00:21, 42.57it/s]Training CobwebTree:  91%| | 9112/10000 [02:53<00:20, 43.81it/s]Training CobwebTree:  91%| | 9117/10000 [02:53<00:19, 44.16it/s]Training CobwebTree:  91%| | 9122/10000 [02:53<00:19, 45.10it/s]Training CobwebTree:  91%|| 9127/10000 [02:53<00:20, 43.58it/s]Training CobwebTree:  91%|| 9132/10000 [02:54<00:19, 44.34it/s]Training CobwebTree:  91%|| 9137/10000 [02:54<00:18, 45.70it/s]Training CobwebTree:  91%|| 9142/10000 [02:54<00:18, 46.61it/s]Training CobwebTree:  91%|| 9147/10000 [02:54<00:19, 44.72it/s]Training CobwebTree:  92%|| 9152/10000 [02:54<00:19, 44.48it/s]Training CobwebTree:  92%|| 9157/10000 [02:54<00:18, 45.35it/s]Training CobwebTree:  92%|| 9162/10000 [02:54<00:18, 44.27it/s]Training CobwebTree:  92%|| 9167/10000 [02:54<00:18, 45.69it/s]Training CobwebTree:  92%|| 9172/10000 [02:54<00:17, 46.72it/s]Training CobwebTree:  92%|| 9177/10000 [02:55<00:17, 46.88it/s]Training CobwebTree:  92%|| 9182/10000 [02:55<00:17, 45.97it/s]Training CobwebTree:  92%|| 9187/10000 [02:55<00:17, 45.40it/s]Training CobwebTree:  92%|| 9192/10000 [02:55<00:18, 44.59it/s]Training CobwebTree:  92%|| 9197/10000 [02:55<00:18, 44.12it/s]Training CobwebTree:  92%|| 9202/10000 [02:55<00:18, 43.58it/s]Training CobwebTree:  92%|| 9207/10000 [02:55<00:18, 43.52it/s]Training CobwebTree:  92%|| 9212/10000 [02:55<00:17, 43.81it/s]Training CobwebTree:  92%|| 9217/10000 [02:55<00:17, 44.63it/s]Training CobwebTree:  92%|| 9222/10000 [02:56<00:17, 43.66it/s]Training CobwebTree:  92%|| 9228/10000 [02:56<00:16, 46.21it/s]Training CobwebTree:  92%|| 9233/10000 [02:56<00:16, 45.44it/s]Training CobwebTree:  92%|| 9239/10000 [02:56<00:15, 48.70it/s]Training CobwebTree:  92%|| 9244/10000 [02:56<00:16, 47.03it/s]Training CobwebTree:  92%|| 9250/10000 [02:56<00:15, 48.80it/s]Training CobwebTree:  93%|| 9255/10000 [02:56<00:15, 49.06it/s]Training CobwebTree:  93%|| 9260/10000 [02:56<00:15, 47.67it/s]Training CobwebTree:  93%|| 9265/10000 [02:56<00:15, 46.72it/s]Training CobwebTree:  93%|| 9271/10000 [02:57<00:15, 47.22it/s]Training CobwebTree:  93%|| 9276/10000 [02:57<00:15, 46.89it/s]Training CobwebTree:  93%|| 9281/10000 [02:57<00:15, 47.66it/s]Training CobwebTree:  93%|| 9286/10000 [02:57<00:14, 47.94it/s]Training CobwebTree:  93%|| 9291/10000 [02:57<00:14, 48.13it/s]Training CobwebTree:  93%|| 9296/10000 [02:57<00:15, 45.87it/s]Training CobwebTree:  93%|| 9301/10000 [02:57<00:15, 44.98it/s]Training CobwebTree:  93%|| 9307/10000 [02:57<00:14, 47.74it/s]Training CobwebTree:  93%|| 9312/10000 [02:57<00:14, 47.70it/s]Training CobwebTree:  93%|| 9318/10000 [02:58<00:13, 50.26it/s]Training CobwebTree:  93%|| 9324/10000 [02:58<00:13, 50.53it/s]Training CobwebTree:  93%|| 9330/10000 [02:58<00:13, 50.13it/s]Training CobwebTree:  93%|| 9336/10000 [02:58<00:13, 48.75it/s]Training CobwebTree:  93%|| 9342/10000 [02:58<00:13, 49.46it/s]Training CobwebTree:  93%|| 9347/10000 [02:58<00:13, 46.75it/s]Training CobwebTree:  94%|| 9353/10000 [02:58<00:13, 47.94it/s]Training CobwebTree:  94%|| 9358/10000 [02:58<00:13, 47.97it/s]Training CobwebTree:  94%|| 9364/10000 [02:59<00:12, 49.95it/s]Training CobwebTree:  94%|| 9370/10000 [02:59<00:12, 50.43it/s]Training CobwebTree:  94%|| 9376/10000 [02:59<00:12, 48.55it/s]Training CobwebTree:  94%|| 9381/10000 [02:59<00:12, 48.88it/s]Training CobwebTree:  94%|| 9386/10000 [02:59<00:12, 47.68it/s]Training CobwebTree:  94%|| 9391/10000 [02:59<00:13, 46.25it/s]Training CobwebTree:  94%|| 9396/10000 [02:59<00:13, 44.39it/s]Training CobwebTree:  94%|| 9401/10000 [02:59<00:13, 45.63it/s]Training CobwebTree:  94%|| 9406/10000 [02:59<00:13, 44.26it/s]Training CobwebTree:  94%|| 9411/10000 [03:00<00:13, 43.68it/s]Training CobwebTree:  94%|| 9416/10000 [03:00<00:13, 43.54it/s]Training CobwebTree:  94%|| 9421/10000 [03:00<00:13, 43.46it/s]Training CobwebTree:  94%|| 9426/10000 [03:00<00:13, 43.67it/s]Training CobwebTree:  94%|| 9431/10000 [03:00<00:12, 45.28it/s]Training CobwebTree:  94%|| 9436/10000 [03:00<00:12, 45.83it/s]Training CobwebTree:  94%|| 9442/10000 [03:00<00:11, 47.07it/s]Training CobwebTree:  94%|| 9447/10000 [03:00<00:12, 44.85it/s]Training CobwebTree:  95%|| 9452/10000 [03:00<00:12, 42.46it/s]Training CobwebTree:  95%|| 9457/10000 [03:01<00:12, 42.53it/s]Training CobwebTree:  95%|| 9462/10000 [03:01<00:12, 43.30it/s]Training CobwebTree:  95%|| 9468/10000 [03:01<00:11, 46.83it/s]Training CobwebTree:  95%|| 9473/10000 [03:01<00:11, 45.06it/s]Training CobwebTree:  95%|| 9478/10000 [03:01<00:11, 45.11it/s]Training CobwebTree:  95%|| 9484/10000 [03:01<00:11, 46.62it/s]Training CobwebTree:  95%|| 9489/10000 [03:01<00:10, 47.10it/s]Training CobwebTree:  95%|| 9494/10000 [03:01<00:10, 47.16it/s]Training CobwebTree:  95%|| 9499/10000 [03:01<00:10, 47.74it/s]Training CobwebTree:  95%|| 9504/10000 [03:02<00:10, 47.30it/s]Training CobwebTree:  95%|| 9509/10000 [03:02<00:10, 46.46it/s]Training CobwebTree:  95%|| 9515/10000 [03:02<00:10, 47.67it/s]Training CobwebTree:  95%|| 9520/10000 [03:02<00:10, 45.55it/s]Training CobwebTree:  95%|| 9525/10000 [03:02<00:10, 45.90it/s]Training CobwebTree:  95%|| 9530/10000 [03:02<00:10, 44.71it/s]Training CobwebTree:  95%|| 9536/10000 [03:02<00:10, 46.15it/s]Training CobwebTree:  95%|| 9541/10000 [03:02<00:10, 45.42it/s]Training CobwebTree:  95%|| 9546/10000 [03:03<00:10, 43.73it/s]Training CobwebTree:  96%|| 9551/10000 [03:03<00:10, 42.94it/s]Training CobwebTree:  96%|| 9556/10000 [03:03<00:10, 44.09it/s]Training CobwebTree:  96%|| 9561/10000 [03:03<00:09, 44.75it/s]Training CobwebTree:  96%|| 9566/10000 [03:03<00:10, 43.27it/s]Training CobwebTree:  96%|| 9571/10000 [03:03<00:09, 45.00it/s]Training CobwebTree:  96%|| 9576/10000 [03:03<00:09, 44.90it/s]Training CobwebTree:  96%|| 9581/10000 [03:03<00:09, 44.82it/s]Training CobwebTree:  96%|| 9586/10000 [03:03<00:09, 44.78it/s]Training CobwebTree:  96%|| 9591/10000 [03:04<00:09, 43.49it/s]Training CobwebTree:  96%|| 9596/10000 [03:04<00:09, 44.13it/s]Training CobwebTree:  96%|| 9602/10000 [03:04<00:08, 46.76it/s]Training CobwebTree:  96%|| 9607/10000 [03:04<00:08, 46.01it/s]Training CobwebTree:  96%|| 9612/10000 [03:04<00:08, 45.21it/s]Training CobwebTree:  96%|| 9617/10000 [03:04<00:08, 44.97it/s]Training CobwebTree:  96%|| 9623/10000 [03:04<00:08, 46.66it/s]Training CobwebTree:  96%|| 9628/10000 [03:04<00:08, 46.00it/s]Training CobwebTree:  96%|| 9633/10000 [03:04<00:07, 46.56it/s]Training CobwebTree:  96%|| 9638/10000 [03:05<00:07, 45.63it/s]Training CobwebTree:  96%|| 9643/10000 [03:05<00:07, 44.87it/s]Training CobwebTree:  96%|| 9648/10000 [03:05<00:08, 42.83it/s]Training CobwebTree:  97%|| 9653/10000 [03:05<00:08, 43.32it/s]Training CobwebTree:  97%|| 9658/10000 [03:05<00:07, 43.79it/s]Training CobwebTree:  97%|| 9663/10000 [03:05<00:07, 42.34it/s]Training CobwebTree:  97%|| 9669/10000 [03:05<00:07, 44.60it/s]Training CobwebTree:  97%|| 9674/10000 [03:05<00:07, 45.06it/s]Training CobwebTree:  97%|| 9679/10000 [03:06<00:07, 44.73it/s]Training CobwebTree:  97%|| 9684/10000 [03:06<00:06, 45.86it/s]Training CobwebTree:  97%|| 9689/10000 [03:06<00:06, 46.58it/s]Training CobwebTree:  97%|| 9694/10000 [03:06<00:06, 45.96it/s]Training CobwebTree:  97%|| 9699/10000 [03:06<00:06, 46.07it/s]Training CobwebTree:  97%|| 9704/10000 [03:06<00:06, 45.17it/s]Training CobwebTree:  97%|| 9709/10000 [03:06<00:06, 45.09it/s]Training CobwebTree:  97%|| 9714/10000 [03:06<00:06, 45.84it/s]Training CobwebTree:  97%|| 9719/10000 [03:06<00:06, 45.19it/s]Training CobwebTree:  97%|| 9724/10000 [03:06<00:06, 44.48it/s]Training CobwebTree:  97%|| 9730/10000 [03:07<00:05, 46.92it/s]Training CobwebTree:  97%|| 9735/10000 [03:07<00:05, 47.18it/s]Training CobwebTree:  97%|| 9740/10000 [03:07<00:05, 47.16it/s]Training CobwebTree:  97%|| 9745/10000 [03:07<00:05, 45.85it/s]Training CobwebTree:  98%|| 9750/10000 [03:07<00:05, 45.58it/s]Training CobwebTree:  98%|| 9755/10000 [03:07<00:05, 44.22it/s]Training CobwebTree:  98%|| 9760/10000 [03:07<00:05, 45.24it/s]Training CobwebTree:  98%|| 9765/10000 [03:07<00:05, 44.28it/s]Training CobwebTree:  98%|| 9770/10000 [03:07<00:05, 45.75it/s]Training CobwebTree:  98%|| 9775/10000 [03:08<00:05, 44.95it/s]Training CobwebTree:  98%|| 9780/10000 [03:08<00:04, 45.93it/s]Training CobwebTree:  98%|| 9785/10000 [03:08<00:04, 46.50it/s]Training CobwebTree:  98%|| 9790/10000 [03:08<00:04, 44.17it/s]Training CobwebTree:  98%|| 9795/10000 [03:08<00:04, 43.30it/s]Training CobwebTree:  98%|| 9800/10000 [03:08<00:04, 43.95it/s]Training CobwebTree:  98%|| 9805/10000 [03:08<00:04, 43.85it/s]Training CobwebTree:  98%|| 9810/10000 [03:08<00:04, 44.13it/s]Training CobwebTree:  98%|| 9815/10000 [03:09<00:04, 42.52it/s]Training CobwebTree:  98%|| 9820/10000 [03:09<00:04, 41.11it/s]Training CobwebTree:  98%|| 9825/10000 [03:09<00:04, 42.83it/s]Training CobwebTree:  98%|| 9831/10000 [03:09<00:03, 45.22it/s]Training CobwebTree:  98%|| 9836/10000 [03:09<00:03, 45.37it/s]Training CobwebTree:  98%|| 9842/10000 [03:09<00:03, 46.80it/s]Training CobwebTree:  98%|| 9847/10000 [03:09<00:03, 46.00it/s]Training CobwebTree:  99%|| 9852/10000 [03:09<00:03, 44.94it/s]Training CobwebTree:  99%|| 9858/10000 [03:09<00:03, 46.59it/s]Training CobwebTree:  99%|| 9863/10000 [03:10<00:02, 46.21it/s]Training CobwebTree:  99%|| 9868/10000 [03:10<00:02, 44.42it/s]Training CobwebTree:  99%|| 9873/10000 [03:10<00:02, 43.91it/s]Training CobwebTree:  99%|| 9878/10000 [03:10<00:02, 43.77it/s]Training CobwebTree:  99%|| 9883/10000 [03:10<00:02, 42.05it/s]Training CobwebTree:  99%|| 9888/10000 [03:10<00:02, 43.06it/s]Training CobwebTree:  99%|| 9893/10000 [03:10<00:02, 44.66it/s]Training CobwebTree:  99%|| 9898/10000 [03:10<00:02, 43.77it/s]Training CobwebTree:  99%|| 9903/10000 [03:10<00:02, 44.28it/s]Training CobwebTree:  99%|| 9908/10000 [03:11<00:02, 43.21it/s]Training CobwebTree:  99%|| 9913/10000 [03:11<00:02, 42.10it/s]Training CobwebTree:  99%|| 9918/10000 [03:11<00:01, 42.79it/s]Training CobwebTree:  99%|| 9923/10000 [03:11<00:01, 42.81it/s]Training CobwebTree:  99%|| 9928/10000 [03:11<00:01, 43.17it/s]Training CobwebTree:  99%|| 9933/10000 [03:11<00:01, 44.43it/s]Training CobwebTree:  99%|| 9938/10000 [03:11<00:01, 44.86it/s]Training CobwebTree:  99%|| 9943/10000 [03:11<00:01, 43.43it/s]Training CobwebTree:  99%|| 9948/10000 [03:12<00:01, 41.83it/s]Training CobwebTree: 100%|| 9953/10000 [03:12<00:01, 42.59it/s]Training CobwebTree: 100%|| 9958/10000 [03:12<00:00, 42.85it/s]Training CobwebTree: 100%|| 9963/10000 [03:12<00:00, 42.67it/s]Training CobwebTree: 100%|| 9968/10000 [03:12<00:00, 44.42it/s]Training CobwebTree: 100%|| 9974/10000 [03:12<00:00, 46.29it/s]Training CobwebTree: 100%|| 9979/10000 [03:12<00:00, 47.23it/s]Training CobwebTree: 100%|| 9984/10000 [03:12<00:00, 46.28it/s]Training CobwebTree: 100%|| 9989/10000 [03:12<00:00, 45.57it/s]Training CobwebTree: 100%|| 9995/10000 [03:13<00:00, 47.76it/s]Training CobwebTree: 100%|| 10000/10000 [03:13<00:00, 45.25it/s]Training CobwebTree: 100%|| 10000/10000 [03:13<00:00, 51.76it/s]
2025-12-23 23:43:35,897 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:43:42,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,428 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,428 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,428 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,436 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,436 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,436 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,436 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,436 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,440 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,448 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,464 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,480 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,501 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,522 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,524 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,524 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,524 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,542 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:42,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:42,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:44,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:44,212 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:46,810 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:43:46,932 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-23 23:43:47,263 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:43:53,767 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-23 23:43:53,770 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-23 23:43:53,771 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-23 23:43:53,772 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-23 23:43:53,773 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-23 23:43:53,774 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-23 23:43:53,775 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-23 23:43:53,776 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-23 23:43:53,777 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-23 23:43:53,778 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-23 23:43:53,779 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-23 23:43:53,779 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-23 23:43:53,780 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-23 23:43:53,781 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-23 23:43:53,782 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-23 23:43:53,783 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-23 23:43:53,784 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-23 23:43:53,784 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-23 23:43:53,786 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-23 23:43:53,787 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-23 23:43:53,788 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-23 23:43:53,788 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-23 23:43:53,789 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-23 23:43:53,790 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-23 23:43:53,791 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-23 23:43:53,792 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-23 23:43:53,793 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-23 23:43:53,793 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-23 23:43:53,794 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-23 23:43:53,795 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-23 23:43:53,796 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-23 23:43:53,796 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-23 23:43:53,797 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-23 23:43:53,798 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-23 23:43:53,799 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-23 23:43:53,800 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-23 23:43:53,800 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-23 23:43:53,801 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-23 23:43:53,802 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-23 23:43:53,803 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-23 23:43:53,804 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-23 23:43:53,805 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-23 23:43:53,805 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-23 23:43:53,806 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-23 23:43:53,807 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-23 23:43:53,808 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-23 23:43:53,808 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-23 23:43:53,809 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-23 23:43:53,810 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-23 23:43:53,810 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-23 23:43:53,811 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-23 23:43:53,812 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-23 23:43:53,812 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-23 23:43:53,813 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-23 23:43:53,814 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-23 23:43:53,814 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-23 23:43:53,815 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-23 23:43:53,816 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-23 23:43:53,817 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-23 23:43:53,817 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-23 23:43:53,818 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-23 23:43:53,819 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-23 23:43:53,820 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-23 23:43:53,820 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-23 23:43:53,821 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-23 23:43:53,822 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-23 23:43:53,823 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-23 23:43:53,824 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-23 23:43:53,825 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-23 23:43:53,825 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-23 23:43:53,826 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-23 23:43:53,827 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-23 23:43:53,828 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-23 23:43:53,828 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-23 23:43:53,829 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-23 23:43:53,830 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-23 23:43:53,831 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-23 23:43:53,831 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-23 23:43:53,832 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-23 23:43:53,833 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-23 23:43:53,833 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-23 23:43:53,834 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-23 23:43:53,835 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-23 23:43:53,836 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-23 23:43:53,836 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-23 23:43:53,837 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-23 23:43:53,838 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-23 23:43:53,838 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-23 23:43:53,839 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-23 23:43:53,840 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-23 23:43:53,841 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-23 23:43:53,841 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-23 23:43:53,842 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-23 23:43:53,843 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-23 23:43:53,844 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-23 23:43:53,844 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-23 23:43:53,845 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-23 23:43:53,846 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-23 23:43:53,847 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-23 23:43:53,847 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-23 23:43:53,848 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-23 23:43:53,848 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-23 23:43:53,849 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-23 23:43:53,849 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-23 23:43:53,850 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-23 23:43:53,850 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-23 23:43:53,850 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-23 23:43:53,851 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-23 23:43:53,852 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-23 23:43:53,852 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-23 23:43:53,853 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-23 23:43:53,854 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-23 23:43:53,854 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-23 23:43:53,855 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-23 23:43:53,856 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-23 23:43:53,856 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-23 23:43:53,857 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-23 23:43:53,858 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-23 23:43:53,859 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-23 23:43:53,859 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-23 23:43:53,860 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-23 23:43:53,861 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-23 23:43:53,861 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-23 23:43:53,862 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-23 23:43:53,863 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-23 23:43:53,864 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-23 23:43:53,864 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-23 23:43:53,865 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-23 23:43:53,866 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-23 23:43:53,866 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-23 23:43:53,867 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-23 23:43:53,868 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-23 23:43:53,869 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-23 23:43:53,869 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-23 23:43:53,870 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-23 23:43:53,871 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-23 23:43:53,871 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-23 23:43:53,872 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-23 23:43:53,873 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-23 23:43:53,873 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-23 23:43:53,874 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-23 23:43:53,874 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-23 23:43:53,875 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-23 23:43:53,903 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-23 23:43:53,904 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-23 23:43:53,905 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-23 23:43:53,905 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-23 23:43:53,906 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-23 23:43:53,907 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-23 23:43:53,907 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-23 23:43:53,908 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-23 23:43:53,908 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-23 23:43:53,909 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-23 23:43:53,910 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-23 23:43:53,920 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-23 23:43:53,927 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-23 23:43:53,943 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-23 23:43:54,534 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,540 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,540 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,541 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,541 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,541 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,557 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,571 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,571 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,571 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,582 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,584 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,587 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,587 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,589 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,597 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,602 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,604 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,612 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,612 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,645 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,657 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,657 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,657 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,662 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,664 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,664 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,664 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,664 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,664 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,684 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,691 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,692 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,698 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,714 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,714 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,720 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,720 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,754 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,765 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,776 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,776 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,814 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,848 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,870 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:54,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:54,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:55,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:55,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:55,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:55,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:43:55,182 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:55,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:43:59,128 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:43:59,262 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-23 23:43:59,491 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.7373040968574228, 'coherence_npmi': 0.20928200396668298, 'topic_diversity': 0.7811563169164882, 'inter_topic_similarity': 0.11124634742736816}
2025-12-23 23:43:59,491 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6743122238016496, 'coherence_npmi': 0.16759376836111411, 'topic_diversity': 0.89, 'inter_topic_similarity': 0.20182020962238312}
2025-12-23 23:43:59,491 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6794254679623437, 'coherence_npmi': 0.16836678062028188, 'topic_diversity': 0.8475, 'inter_topic_similarity': 0.22193437814712524}
