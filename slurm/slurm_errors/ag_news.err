2025-12-22 12:04:40,856 INFO __main__: Starting benchmark for dataset=agnews
2025-12-22 12:04:42,558 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-22 12:04:42,709 INFO gensim.corpora.dictionary: built Dictionary<21572 unique tokens: ['disappointed', 'fears', 'federal', 'firm', 'mogul']...> from 7600 documents (total 189278 corpus positions)
2025-12-22 12:04:42,712 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<21572 unique tokens: ['disappointed', 'fears', 'federal', 'firm', 'mogul']...> from 7600 documents (total 189278 corpus positions)", 'datetime': '2025-12-22T12:04:42.709425', 'gensim': '4.4.0', 'python': '3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-187-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-22 12:04:43,907 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-22 12:04:43,907 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-22 12:04:46,907 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 7600 docs
2025-12-22 12:05:43,556 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:05:45,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,818 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,826 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,837 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,845 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,845 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,845 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,845 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,846 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,846 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:45,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:45,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:47,710 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:05:47,842 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 7600 virtual documents
2025-12-22 12:05:48,726 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:05:50,851 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (1222 virtual)
2025-12-22 12:05:50,853 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (2388 virtual)
2025-12-22 12:05:50,854 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3473 virtual)
2025-12-22 12:05:50,855 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4548 virtual)
2025-12-22 12:05:50,856 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5650 virtual)
2025-12-22 12:05:50,857 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6680 virtual)
2025-12-22 12:05:50,858 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7623 virtual)
2025-12-22 12:05:50,859 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8704 virtual)
2025-12-22 12:05:50,860 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9794 virtual)
2025-12-22 12:05:50,860 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10897 virtual)
2025-12-22 12:05:50,861 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (11911 virtual)
2025-12-22 12:05:50,862 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (12921 virtual)
2025-12-22 12:05:50,863 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (14032 virtual)
2025-12-22 12:05:50,864 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15005 virtual)
2025-12-22 12:05:50,865 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16044 virtual)
2025-12-22 12:05:50,865 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17093 virtual)
2025-12-22 12:05:50,866 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18108 virtual)
2025-12-22 12:05:50,867 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19117 virtual)
2025-12-22 12:05:50,868 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20085 virtual)
2025-12-22 12:05:50,869 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21140 virtual)
2025-12-22 12:05:50,869 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22139 virtual)
2025-12-22 12:05:50,870 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23227 virtual)
2025-12-22 12:05:50,871 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (24166 virtual)
2025-12-22 12:05:50,872 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (25137 virtual)
2025-12-22 12:05:50,872 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (26187 virtual)
2025-12-22 12:05:50,873 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (27113 virtual)
2025-12-22 12:05:50,874 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (28084 virtual)
2025-12-22 12:05:50,875 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (29113 virtual)
2025-12-22 12:05:50,876 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (30069 virtual)
2025-12-22 12:05:50,876 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (31017 virtual)
2025-12-22 12:05:50,877 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (31951 virtual)
2025-12-22 12:05:50,878 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (32954 virtual)
2025-12-22 12:05:50,879 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (34063 virtual)
2025-12-22 12:05:50,880 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (35021 virtual)
2025-12-22 12:05:50,880 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (36080 virtual)
2025-12-22 12:05:50,881 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (37103 virtual)
2025-12-22 12:05:50,886 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (38185 virtual)
2025-12-22 12:05:50,887 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (39172 virtual)
2025-12-22 12:05:50,887 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (40185 virtual)
2025-12-22 12:05:50,888 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (41113 virtual)
2025-12-22 12:05:50,889 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (42261 virtual)
2025-12-22 12:05:50,890 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (43352 virtual)
2025-12-22 12:05:50,890 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (44331 virtual)
2025-12-22 12:05:50,891 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (45325 virtual)
2025-12-22 12:05:50,893 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (46441 virtual)
2025-12-22 12:05:50,893 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (47389 virtual)
2025-12-22 12:05:50,894 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (48333 virtual)
2025-12-22 12:05:50,895 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (49375 virtual)
2025-12-22 12:05:50,896 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (50465 virtual)
2025-12-22 12:05:50,896 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (51456 virtual)
2025-12-22 12:05:50,897 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (52441 virtual)
2025-12-22 12:05:50,898 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (53484 virtual)
2025-12-22 12:05:50,899 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (54533 virtual)
2025-12-22 12:05:50,900 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (55592 virtual)
2025-12-22 12:05:50,901 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (56681 virtual)
2025-12-22 12:05:50,902 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (57717 virtual)
2025-12-22 12:05:50,902 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (58705 virtual)
2025-12-22 12:05:50,903 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (59654 virtual)
2025-12-22 12:05:50,904 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (60700 virtual)
2025-12-22 12:05:50,905 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (61672 virtual)
2025-12-22 12:05:50,906 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (62694 virtual)
2025-12-22 12:05:50,906 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (63757 virtual)
2025-12-22 12:05:50,907 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (64840 virtual)
2025-12-22 12:05:50,909 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (65744 virtual)
2025-12-22 12:05:50,909 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (66827 virtual)
2025-12-22 12:05:50,910 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (67785 virtual)
2025-12-22 12:05:50,911 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (68804 virtual)
2025-12-22 12:05:50,912 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (69764 virtual)
2025-12-22 12:05:50,913 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (70807 virtual)
2025-12-22 12:05:50,913 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (71846 virtual)
2025-12-22 12:05:50,914 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (72847 virtual)
2025-12-22 12:05:50,915 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (73925 virtual)
2025-12-22 12:05:50,916 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (75010 virtual)
2025-12-22 12:05:50,917 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (75968 virtual)
2025-12-22 12:05:50,918 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (76955 virtual)
2025-12-22 12:05:50,918 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (77973 virtual)
2025-12-22 12:05:50,919 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (78900 virtual)
2025-12-22 12:05:50,920 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (79915 virtual)
2025-12-22 12:05:50,921 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (80921 virtual)
2025-12-22 12:05:50,923 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (81894 virtual)
2025-12-22 12:05:50,924 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (82979 virtual)
2025-12-22 12:05:50,927 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (83937 virtual)
2025-12-22 12:05:50,928 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (84989 virtual)
2025-12-22 12:05:50,931 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (85991 virtual)
2025-12-22 12:05:50,932 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (87013 virtual)
2025-12-22 12:05:50,933 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (88050 virtual)
2025-12-22 12:05:50,947 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (88989 virtual)
2025-12-22 12:05:50,949 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (89975 virtual)
2025-12-22 12:05:50,950 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (90892 virtual)
2025-12-22 12:05:50,951 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (91944 virtual)
2025-12-22 12:05:50,951 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (93008 virtual)
2025-12-22 12:05:50,952 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (93938 virtual)
2025-12-22 12:05:50,953 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (94952 virtual)
2025-12-22 12:05:50,954 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (95989 virtual)
2025-12-22 12:05:50,954 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (97083 virtual)
2025-12-22 12:05:50,955 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (97968 virtual)
2025-12-22 12:05:50,959 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (98973 virtual)
2025-12-22 12:05:50,962 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (99956 virtual)
2025-12-22 12:05:50,963 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (101124 virtual)
2025-12-22 12:05:50,967 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (102046 virtual)
2025-12-22 12:05:50,968 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (103005 virtual)
2025-12-22 12:05:50,979 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (104049 virtual)
2025-12-22 12:05:50,979 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (105026 virtual)
2025-12-22 12:05:50,980 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (106094 virtual)
2025-12-22 12:05:50,987 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (107077 virtual)
2025-12-22 12:05:50,987 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (108131 virtual)
2025-12-22 12:05:50,991 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (109133 virtual)
2025-12-22 12:05:51,091 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (110094 virtual)
2025-12-22 12:05:51,092 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (111108 virtual)
2025-12-22 12:05:51,093 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (112228 virtual)
2025-12-22 12:05:51,093 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (113209 virtual)
2025-12-22 12:05:51,094 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (114200 virtual)
2025-12-22 12:05:51,095 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (115208 virtual)
2025-12-22 12:05:51,096 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (116189 virtual)
2025-12-22 12:05:51,096 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (117185 virtual)
2025-12-22 12:05:51,103 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (118161 virtual)
2025-12-22 12:05:51,104 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (119147 virtual)
2025-12-22 12:05:51,105 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (120162 virtual)
2025-12-22 12:05:51,105 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (120878 virtual)
2025-12-22 12:05:51,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,598 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,681 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,682 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,684 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,686 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,688 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,689 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,694 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,696 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,699 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,703 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,708 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,708 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,709 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,709 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,709 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,710 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,712 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,722 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,736 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,754 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,754 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,765 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,769 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,773 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,777 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,785 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,786 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,786 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,787 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:05:51,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,818 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,820 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:51,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:05:53,544 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:05:53,695 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 120892 virtual documents
2025-12-22 12:05:54,266 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 7600 docs
2025-12-22 12:06:27,848 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:06:30,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,341 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,348 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,348 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,349 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,358 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,358 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:30,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,392 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,392 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,402 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:30,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:32,003 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:06:32,099 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 7600 virtual documents
2025-12-22 12:06:32,595 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:06:35,125 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (1222 virtual)
2025-12-22 12:06:35,126 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (2388 virtual)
2025-12-22 12:06:35,127 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3473 virtual)
2025-12-22 12:06:35,128 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4548 virtual)
2025-12-22 12:06:35,129 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5650 virtual)
2025-12-22 12:06:35,130 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6680 virtual)
2025-12-22 12:06:35,131 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7623 virtual)
2025-12-22 12:06:35,131 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8704 virtual)
2025-12-22 12:06:35,132 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9794 virtual)
2025-12-22 12:06:35,133 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10897 virtual)
2025-12-22 12:06:35,134 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (11911 virtual)
2025-12-22 12:06:35,135 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (12921 virtual)
2025-12-22 12:06:35,136 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (14032 virtual)
2025-12-22 12:06:35,136 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15005 virtual)
2025-12-22 12:06:35,137 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16044 virtual)
2025-12-22 12:06:35,138 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17093 virtual)
2025-12-22 12:06:35,139 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18108 virtual)
2025-12-22 12:06:35,140 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19117 virtual)
2025-12-22 12:06:35,140 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20085 virtual)
2025-12-22 12:06:35,141 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21140 virtual)
2025-12-22 12:06:35,142 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22139 virtual)
2025-12-22 12:06:35,143 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23227 virtual)
2025-12-22 12:06:35,143 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (24166 virtual)
2025-12-22 12:06:35,144 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (25137 virtual)
2025-12-22 12:06:35,145 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (26187 virtual)
2025-12-22 12:06:35,146 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (27113 virtual)
2025-12-22 12:06:35,146 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (28084 virtual)
2025-12-22 12:06:35,147 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (29113 virtual)
2025-12-22 12:06:35,148 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (30069 virtual)
2025-12-22 12:06:35,149 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (31017 virtual)
2025-12-22 12:06:35,150 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (31951 virtual)
2025-12-22 12:06:35,151 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (32954 virtual)
2025-12-22 12:06:35,152 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (34063 virtual)
2025-12-22 12:06:35,152 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (35021 virtual)
2025-12-22 12:06:35,153 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (36080 virtual)
2025-12-22 12:06:35,154 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (37103 virtual)
2025-12-22 12:06:35,158 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (38185 virtual)
2025-12-22 12:06:35,159 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (39172 virtual)
2025-12-22 12:06:35,161 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (40185 virtual)
2025-12-22 12:06:35,162 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (41113 virtual)
2025-12-22 12:06:35,162 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (42261 virtual)
2025-12-22 12:06:35,163 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (43352 virtual)
2025-12-22 12:06:35,164 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (44331 virtual)
2025-12-22 12:06:35,165 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (45325 virtual)
2025-12-22 12:06:35,166 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (46441 virtual)
2025-12-22 12:06:35,166 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (47389 virtual)
2025-12-22 12:06:35,167 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (48333 virtual)
2025-12-22 12:06:35,169 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (49375 virtual)
2025-12-22 12:06:35,169 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (50465 virtual)
2025-12-22 12:06:35,170 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (51456 virtual)
2025-12-22 12:06:35,171 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (52441 virtual)
2025-12-22 12:06:35,171 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (53484 virtual)
2025-12-22 12:06:35,172 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (54533 virtual)
2025-12-22 12:06:35,173 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (55592 virtual)
2025-12-22 12:06:35,174 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (56681 virtual)
2025-12-22 12:06:35,175 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (57717 virtual)
2025-12-22 12:06:35,176 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (58705 virtual)
2025-12-22 12:06:35,176 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (59654 virtual)
2025-12-22 12:06:35,177 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (60700 virtual)
2025-12-22 12:06:35,178 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (61672 virtual)
2025-12-22 12:06:35,178 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (62694 virtual)
2025-12-22 12:06:35,180 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (63757 virtual)
2025-12-22 12:06:35,181 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (64840 virtual)
2025-12-22 12:06:35,181 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (65744 virtual)
2025-12-22 12:06:35,182 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (66827 virtual)
2025-12-22 12:06:35,183 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (67785 virtual)
2025-12-22 12:06:35,184 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (68804 virtual)
2025-12-22 12:06:35,185 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (69764 virtual)
2025-12-22 12:06:35,186 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (70807 virtual)
2025-12-22 12:06:35,187 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (71846 virtual)
2025-12-22 12:06:35,188 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (72847 virtual)
2025-12-22 12:06:35,189 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (73925 virtual)
2025-12-22 12:06:35,190 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (75010 virtual)
2025-12-22 12:06:35,191 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (75968 virtual)
2025-12-22 12:06:35,192 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (76955 virtual)
2025-12-22 12:06:35,192 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (77973 virtual)
2025-12-22 12:06:35,193 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (78900 virtual)
2025-12-22 12:06:35,194 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (79915 virtual)
2025-12-22 12:06:35,195 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (80921 virtual)
2025-12-22 12:06:35,196 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (81894 virtual)
2025-12-22 12:06:35,197 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (82979 virtual)
2025-12-22 12:06:35,198 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (83937 virtual)
2025-12-22 12:06:35,198 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (84989 virtual)
2025-12-22 12:06:35,199 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (85991 virtual)
2025-12-22 12:06:35,200 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (87013 virtual)
2025-12-22 12:06:35,201 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (88050 virtual)
2025-12-22 12:06:35,202 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (88989 virtual)
2025-12-22 12:06:35,203 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (89975 virtual)
2025-12-22 12:06:35,204 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (90892 virtual)
2025-12-22 12:06:35,205 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (91944 virtual)
2025-12-22 12:06:35,335 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (93008 virtual)
2025-12-22 12:06:35,336 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (93938 virtual)
2025-12-22 12:06:35,337 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (94952 virtual)
2025-12-22 12:06:35,337 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (95989 virtual)
2025-12-22 12:06:35,338 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (97083 virtual)
2025-12-22 12:06:35,339 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (97968 virtual)
2025-12-22 12:06:35,340 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (98973 virtual)
2025-12-22 12:06:35,363 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (99956 virtual)
2025-12-22 12:06:35,364 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (101124 virtual)
2025-12-22 12:06:35,371 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (102046 virtual)
2025-12-22 12:06:35,372 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (103005 virtual)
2025-12-22 12:06:35,379 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (104049 virtual)
2025-12-22 12:06:35,419 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (105026 virtual)
2025-12-22 12:06:35,423 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (106094 virtual)
2025-12-22 12:06:35,424 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (107077 virtual)
2025-12-22 12:06:35,425 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (108131 virtual)
2025-12-22 12:06:35,439 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (109133 virtual)
2025-12-22 12:06:35,471 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (110094 virtual)
2025-12-22 12:06:35,483 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (111108 virtual)
2025-12-22 12:06:35,487 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (112228 virtual)
2025-12-22 12:06:35,490 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (113209 virtual)
2025-12-22 12:06:35,511 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (114200 virtual)
2025-12-22 12:06:35,519 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (115208 virtual)
2025-12-22 12:06:35,531 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (116189 virtual)
2025-12-22 12:06:35,634 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (117185 virtual)
2025-12-22 12:06:35,637 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (118161 virtual)
2025-12-22 12:06:35,638 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (119147 virtual)
2025-12-22 12:06:35,646 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (120162 virtual)
2025-12-22 12:06:35,648 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (120878 virtual)
2025-12-22 12:06:35,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,700 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,849 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,852 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,852 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,856 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,857 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,861 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,865 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,905 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,907 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,914 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,915 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,921 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,924 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,928 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,953 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,962 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,870 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,964 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,973 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,974 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:35,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:35,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:36,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:06:36,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:06:37,887 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:06:37,969 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 120892 virtual documents
2025-12-22 12:06:38,305 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 7600 docs
Training CobwebTree:   0%|          | 0/7600 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 16/7600 [00:00<00:52, 143.39it/s]Training CobwebTree:   0%|          | 31/7600 [00:00<00:55, 136.27it/s]Training CobwebTree:   1%|          | 45/7600 [00:00<01:04, 117.53it/s]Training CobwebTree:   1%|          | 57/7600 [00:00<01:08, 109.96it/s]Training CobwebTree:   1%|          | 69/7600 [00:00<01:13, 101.94it/s]Training CobwebTree:   1%|          | 80/7600 [00:00<01:23, 90.42it/s] Training CobwebTree:   1%|          | 90/7600 [00:00<01:30, 82.72it/s]Training CobwebTree:   1%|         | 99/7600 [00:01<01:35, 78.39it/s]Training CobwebTree:   1%|         | 107/7600 [00:01<01:38, 75.83it/s]Training CobwebTree:   2%|         | 116/7600 [00:01<01:36, 77.42it/s]Training CobwebTree:   2%|         | 124/7600 [00:01<01:42, 73.04it/s]Training CobwebTree:   2%|         | 132/7600 [00:01<01:44, 71.39it/s]Training CobwebTree:   2%|         | 140/7600 [00:01<01:43, 72.14it/s]Training CobwebTree:   2%|         | 148/7600 [00:01<01:46, 70.14it/s]Training CobwebTree:   2%|         | 156/7600 [00:01<01:47, 69.31it/s]Training CobwebTree:   2%|         | 163/7600 [00:01<01:50, 67.35it/s]Training CobwebTree:   2%|         | 170/7600 [00:02<01:55, 64.29it/s]Training CobwebTree:   2%|         | 177/7600 [00:02<01:55, 64.32it/s]Training CobwebTree:   2%|         | 184/7600 [00:02<01:52, 65.64it/s]Training CobwebTree:   3%|         | 192/7600 [00:02<01:51, 66.45it/s]Training CobwebTree:   3%|         | 199/7600 [00:02<01:56, 63.31it/s]Training CobwebTree:   3%|         | 206/7600 [00:02<01:56, 63.30it/s]Training CobwebTree:   3%|         | 213/7600 [00:02<01:59, 62.06it/s]Training CobwebTree:   3%|         | 220/7600 [00:02<01:57, 62.68it/s]Training CobwebTree:   3%|         | 227/7600 [00:03<02:01, 60.53it/s]Training CobwebTree:   3%|         | 234/7600 [00:03<01:59, 61.50it/s]Training CobwebTree:   3%|         | 241/7600 [00:03<01:56, 63.11it/s]Training CobwebTree:   3%|         | 248/7600 [00:03<01:57, 62.40it/s]Training CobwebTree:   3%|         | 255/7600 [00:03<01:59, 61.69it/s]Training CobwebTree:   3%|         | 262/7600 [00:03<02:07, 57.42it/s]Training CobwebTree:   4%|         | 268/7600 [00:03<02:11, 55.87it/s]Training CobwebTree:   4%|         | 274/7600 [00:03<02:09, 56.43it/s]Training CobwebTree:   4%|         | 280/7600 [00:03<02:08, 56.79it/s]Training CobwebTree:   4%|         | 287/7600 [00:04<02:02, 59.66it/s]Training CobwebTree:   4%|         | 293/7600 [00:04<02:06, 57.70it/s]Training CobwebTree:   4%|         | 299/7600 [00:04<02:06, 57.50it/s]Training CobwebTree:   4%|         | 306/7600 [00:04<02:03, 59.19it/s]Training CobwebTree:   4%|         | 312/7600 [00:04<02:12, 55.09it/s]Training CobwebTree:   4%|         | 319/7600 [00:04<02:08, 56.58it/s]Training CobwebTree:   4%|         | 326/7600 [00:04<02:04, 58.22it/s]Training CobwebTree:   4%|         | 332/7600 [00:04<02:05, 58.04it/s]Training CobwebTree:   4%|         | 338/7600 [00:04<02:04, 58.28it/s]Training CobwebTree:   5%|         | 345/7600 [00:05<02:01, 59.73it/s]Training CobwebTree:   5%|         | 351/7600 [00:05<02:09, 56.08it/s]Training CobwebTree:   5%|         | 357/7600 [00:05<02:09, 55.73it/s]Training CobwebTree:   5%|         | 363/7600 [00:05<02:08, 56.32it/s]Training CobwebTree:   5%|         | 370/7600 [00:05<02:03, 58.37it/s]Training CobwebTree:   5%|         | 377/7600 [00:05<02:00, 59.98it/s]Training CobwebTree:   5%|         | 384/7600 [00:05<02:02, 58.86it/s]Training CobwebTree:   5%|         | 390/7600 [00:05<02:03, 58.52it/s]Training CobwebTree:   5%|         | 397/7600 [00:05<02:02, 59.02it/s]Training CobwebTree:   5%|         | 404/7600 [00:06<02:00, 59.56it/s]Training CobwebTree:   5%|         | 411/7600 [00:06<02:01, 59.32it/s]Training CobwebTree:   5%|         | 417/7600 [00:06<02:04, 57.63it/s]Training CobwebTree:   6%|         | 423/7600 [00:06<02:03, 58.18it/s]Training CobwebTree:   6%|         | 429/7600 [00:06<02:03, 58.29it/s]Training CobwebTree:   6%|         | 435/7600 [00:06<02:03, 57.85it/s]Training CobwebTree:   6%|         | 441/7600 [00:06<02:04, 57.62it/s]Training CobwebTree:   6%|         | 447/7600 [00:06<02:07, 55.93it/s]Training CobwebTree:   6%|         | 453/7600 [00:06<02:06, 56.69it/s]Training CobwebTree:   6%|         | 459/7600 [00:07<02:09, 54.99it/s]Training CobwebTree:   6%|         | 465/7600 [00:07<02:12, 53.70it/s]Training CobwebTree:   6%|         | 471/7600 [00:07<02:11, 54.19it/s]Training CobwebTree:   6%|         | 477/7600 [00:07<02:07, 55.73it/s]Training CobwebTree:   6%|         | 483/7600 [00:07<02:05, 56.82it/s]Training CobwebTree:   6%|         | 489/7600 [00:07<02:07, 55.56it/s]Training CobwebTree:   7%|         | 495/7600 [00:07<02:05, 56.58it/s]Training CobwebTree:   7%|         | 501/7600 [00:07<02:08, 55.36it/s]Training CobwebTree:   7%|         | 507/7600 [00:07<02:12, 53.50it/s]Training CobwebTree:   7%|         | 513/7600 [00:08<02:14, 52.57it/s]Training CobwebTree:   7%|         | 519/7600 [00:08<02:14, 52.69it/s]Training CobwebTree:   7%|         | 525/7600 [00:08<02:12, 53.28it/s]Training CobwebTree:   7%|         | 531/7600 [00:08<02:12, 53.17it/s]Training CobwebTree:   7%|         | 537/7600 [00:08<02:15, 52.06it/s]Training CobwebTree:   7%|         | 543/7600 [00:08<02:15, 52.06it/s]Training CobwebTree:   7%|         | 549/7600 [00:08<02:16, 51.80it/s]Training CobwebTree:   7%|         | 555/7600 [00:08<02:12, 53.16it/s]Training CobwebTree:   7%|         | 561/7600 [00:08<02:11, 53.37it/s]Training CobwebTree:   7%|         | 567/7600 [00:09<02:11, 53.65it/s]Training CobwebTree:   8%|         | 573/7600 [00:09<02:09, 54.07it/s]Training CobwebTree:   8%|         | 579/7600 [00:09<02:14, 52.22it/s]Training CobwebTree:   8%|         | 585/7600 [00:09<02:12, 52.87it/s]Training CobwebTree:   8%|         | 592/7600 [00:09<02:07, 55.15it/s]Training CobwebTree:   8%|         | 598/7600 [00:09<02:16, 51.29it/s]Training CobwebTree:   8%|         | 604/7600 [00:09<02:15, 51.73it/s]Training CobwebTree:   8%|         | 610/7600 [00:09<02:13, 52.23it/s]Training CobwebTree:   8%|         | 617/7600 [00:09<02:07, 54.69it/s]Training CobwebTree:   8%|         | 623/7600 [00:10<02:14, 51.80it/s]Training CobwebTree:   8%|         | 629/7600 [00:10<02:14, 51.85it/s]Training CobwebTree:   8%|         | 635/7600 [00:10<02:13, 52.12it/s]Training CobwebTree:   8%|         | 641/7600 [00:10<02:14, 51.67it/s]Training CobwebTree:   9%|         | 647/7600 [00:10<02:18, 50.24it/s]Training CobwebTree:   9%|         | 653/7600 [00:10<02:12, 52.54it/s]Training CobwebTree:   9%|         | 659/7600 [00:10<02:12, 52.22it/s]Training CobwebTree:   9%|         | 665/7600 [00:10<02:16, 50.95it/s]Training CobwebTree:   9%|         | 671/7600 [00:11<02:17, 50.28it/s]Training CobwebTree:   9%|         | 677/7600 [00:11<02:14, 51.53it/s]Training CobwebTree:   9%|         | 683/7600 [00:11<02:14, 51.39it/s]Training CobwebTree:   9%|         | 689/7600 [00:11<02:15, 50.92it/s]Training CobwebTree:   9%|         | 695/7600 [00:11<02:20, 49.23it/s]Training CobwebTree:   9%|         | 701/7600 [00:11<02:17, 50.31it/s]Training CobwebTree:   9%|         | 707/7600 [00:11<02:24, 47.77it/s]Training CobwebTree:   9%|         | 713/7600 [00:11<02:22, 48.39it/s]Training CobwebTree:   9%|         | 719/7600 [00:12<02:15, 50.80it/s]Training CobwebTree:  10%|         | 725/7600 [00:12<02:22, 48.29it/s]Training CobwebTree:  10%|         | 731/7600 [00:12<02:22, 48.23it/s]Training CobwebTree:  10%|         | 736/7600 [00:12<02:27, 46.44it/s]Training CobwebTree:  10%|         | 742/7600 [00:12<02:21, 48.53it/s]Training CobwebTree:  10%|         | 748/7600 [00:12<02:16, 50.13it/s]Training CobwebTree:  10%|         | 754/7600 [00:12<02:21, 48.23it/s]Training CobwebTree:  10%|         | 760/7600 [00:12<02:16, 50.05it/s]Training CobwebTree:  10%|         | 766/7600 [00:12<02:17, 49.66it/s]Training CobwebTree:  10%|         | 772/7600 [00:13<02:15, 50.56it/s]Training CobwebTree:  10%|         | 778/7600 [00:13<02:09, 52.79it/s]Training CobwebTree:  10%|         | 784/7600 [00:13<02:12, 51.38it/s]Training CobwebTree:  10%|         | 790/7600 [00:13<02:17, 49.37it/s]Training CobwebTree:  10%|         | 796/7600 [00:13<02:13, 50.93it/s]Training CobwebTree:  11%|         | 802/7600 [00:13<02:13, 50.87it/s]Training CobwebTree:  11%|         | 808/7600 [00:13<02:17, 49.36it/s]Training CobwebTree:  11%|         | 814/7600 [00:13<02:15, 50.17it/s]Training CobwebTree:  11%|         | 820/7600 [00:14<02:16, 49.63it/s]Training CobwebTree:  11%|         | 825/7600 [00:14<02:16, 49.58it/s]Training CobwebTree:  11%|         | 831/7600 [00:14<02:16, 49.62it/s]Training CobwebTree:  11%|         | 836/7600 [00:14<02:16, 49.40it/s]Training CobwebTree:  11%|         | 842/7600 [00:14<02:13, 50.48it/s]Training CobwebTree:  11%|         | 848/7600 [00:14<02:22, 47.24it/s]Training CobwebTree:  11%|         | 853/7600 [00:14<02:27, 45.62it/s]Training CobwebTree:  11%|        | 858/7600 [00:14<02:31, 44.48it/s]Training CobwebTree:  11%|        | 863/7600 [00:14<02:28, 45.32it/s]Training CobwebTree:  11%|        | 868/7600 [00:15<02:36, 43.07it/s]Training CobwebTree:  11%|        | 873/7600 [00:15<02:35, 43.28it/s]Training CobwebTree:  12%|        | 879/7600 [00:15<02:26, 45.77it/s]Training CobwebTree:  12%|        | 884/7600 [00:15<02:24, 46.43it/s]Training CobwebTree:  12%|        | 890/7600 [00:15<02:20, 47.80it/s]Training CobwebTree:  12%|        | 895/7600 [00:15<02:21, 47.31it/s]Training CobwebTree:  12%|        | 900/7600 [00:15<02:21, 47.32it/s]Training CobwebTree:  12%|        | 905/7600 [00:15<02:20, 47.50it/s]Training CobwebTree:  12%|        | 910/7600 [00:16<02:25, 45.82it/s]Training CobwebTree:  12%|        | 915/7600 [00:16<02:26, 45.71it/s]Training CobwebTree:  12%|        | 920/7600 [00:16<02:34, 43.18it/s]Training CobwebTree:  12%|        | 925/7600 [00:16<02:31, 43.93it/s]Training CobwebTree:  12%|        | 930/7600 [00:16<02:30, 44.25it/s]Training CobwebTree:  12%|        | 936/7600 [00:16<02:23, 46.36it/s]Training CobwebTree:  12%|        | 941/7600 [00:16<02:23, 46.34it/s]Training CobwebTree:  12%|        | 947/7600 [00:16<02:20, 47.46it/s]Training CobwebTree:  13%|        | 953/7600 [00:16<02:17, 48.40it/s]Training CobwebTree:  13%|        | 958/7600 [00:17<02:21, 46.97it/s]Training CobwebTree:  13%|        | 964/7600 [00:17<02:16, 48.63it/s]Training CobwebTree:  13%|        | 970/7600 [00:17<02:11, 50.27it/s]Training CobwebTree:  13%|        | 976/7600 [00:17<02:19, 47.64it/s]Training CobwebTree:  13%|        | 981/7600 [00:17<02:20, 47.03it/s]Training CobwebTree:  13%|        | 986/7600 [00:17<02:22, 46.49it/s]Training CobwebTree:  13%|        | 991/7600 [00:17<02:22, 46.43it/s]Training CobwebTree:  13%|        | 997/7600 [00:17<02:13, 49.34it/s]Training CobwebTree:  13%|        | 1002/7600 [00:17<02:13, 49.28it/s]Training CobwebTree:  13%|        | 1008/7600 [00:18<02:12, 49.91it/s]Training CobwebTree:  13%|        | 1014/7600 [00:18<02:10, 50.32it/s]Training CobwebTree:  13%|        | 1020/7600 [00:18<02:13, 49.28it/s]Training CobwebTree:  13%|        | 1025/7600 [00:18<02:17, 47.82it/s]Training CobwebTree:  14%|        | 1030/7600 [00:18<02:20, 46.82it/s]Training CobwebTree:  14%|        | 1035/7600 [00:18<02:27, 44.40it/s]Training CobwebTree:  14%|        | 1041/7600 [00:18<02:22, 46.01it/s]Training CobwebTree:  14%|        | 1047/7600 [00:18<02:17, 47.68it/s]Training CobwebTree:  14%|        | 1052/7600 [00:19<02:24, 45.31it/s]Training CobwebTree:  14%|        | 1058/7600 [00:19<02:16, 47.90it/s]Training CobwebTree:  14%|        | 1064/7600 [00:19<02:13, 48.95it/s]Training CobwebTree:  14%|        | 1069/7600 [00:19<02:15, 48.30it/s]Training CobwebTree:  14%|        | 1075/7600 [00:19<02:08, 50.81it/s]Training CobwebTree:  14%|        | 1081/7600 [00:19<02:07, 51.23it/s]Training CobwebTree:  14%|        | 1087/7600 [00:19<02:15, 48.24it/s]Training CobwebTree:  14%|        | 1092/7600 [00:19<02:17, 47.49it/s]Training CobwebTree:  14%|        | 1097/7600 [00:19<02:16, 47.53it/s]Training CobwebTree:  15%|        | 1103/7600 [00:20<02:14, 48.27it/s]Training CobwebTree:  15%|        | 1108/7600 [00:20<02:15, 47.87it/s]Training CobwebTree:  15%|        | 1114/7600 [00:20<02:07, 50.98it/s]Training CobwebTree:  15%|        | 1120/7600 [00:20<02:01, 53.21it/s]Training CobwebTree:  15%|        | 1126/7600 [00:20<02:13, 48.50it/s]Training CobwebTree:  15%|        | 1132/7600 [00:20<02:11, 49.29it/s]Training CobwebTree:  15%|        | 1138/7600 [00:20<02:12, 48.80it/s]Training CobwebTree:  15%|        | 1143/7600 [00:20<02:11, 49.03it/s]Training CobwebTree:  15%|        | 1148/7600 [00:20<02:10, 49.28it/s]Training CobwebTree:  15%|        | 1154/7600 [00:21<02:06, 50.86it/s]Training CobwebTree:  15%|        | 1161/7600 [00:21<02:01, 53.07it/s]Training CobwebTree:  15%|        | 1167/7600 [00:21<02:05, 51.21it/s]Training CobwebTree:  15%|        | 1173/7600 [00:21<02:08, 50.21it/s]Training CobwebTree:  16%|        | 1179/7600 [00:21<02:08, 50.01it/s]Training CobwebTree:  16%|        | 1185/7600 [00:21<02:09, 49.55it/s]Training CobwebTree:  16%|        | 1190/7600 [00:21<02:14, 47.65it/s]Training CobwebTree:  16%|        | 1195/7600 [00:21<02:17, 46.49it/s]Training CobwebTree:  16%|        | 1200/7600 [00:22<02:16, 47.04it/s]Training CobwebTree:  16%|        | 1205/7600 [00:22<02:16, 46.86it/s]Training CobwebTree:  16%|        | 1210/7600 [00:22<02:19, 45.71it/s]Training CobwebTree:  16%|        | 1215/7600 [00:22<02:16, 46.81it/s]Training CobwebTree:  16%|        | 1220/7600 [00:22<02:14, 47.39it/s]Training CobwebTree:  16%|        | 1226/7600 [00:22<02:08, 49.59it/s]Training CobwebTree:  16%|        | 1231/7600 [00:22<02:12, 48.02it/s]Training CobwebTree:  16%|        | 1237/7600 [00:22<02:11, 48.29it/s]Training CobwebTree:  16%|        | 1242/7600 [00:22<02:16, 46.56it/s]Training CobwebTree:  16%|        | 1247/7600 [00:23<02:22, 44.73it/s]Training CobwebTree:  16%|        | 1253/7600 [00:23<02:12, 47.79it/s]Training CobwebTree:  17%|        | 1258/7600 [00:23<02:19, 45.43it/s]Training CobwebTree:  17%|        | 1264/7600 [00:23<02:14, 47.02it/s]Training CobwebTree:  17%|        | 1269/7600 [00:23<02:19, 45.27it/s]Training CobwebTree:  17%|        | 1274/7600 [00:23<02:21, 44.82it/s]Training CobwebTree:  17%|        | 1279/7600 [00:23<02:25, 43.57it/s]Training CobwebTree:  17%|        | 1284/7600 [00:23<02:26, 43.07it/s]Training CobwebTree:  17%|        | 1289/7600 [00:23<02:21, 44.49it/s]Training CobwebTree:  17%|        | 1294/7600 [00:24<02:21, 44.46it/s]Training CobwebTree:  17%|        | 1300/7600 [00:24<02:15, 46.39it/s]Training CobwebTree:  17%|        | 1305/7600 [00:24<02:15, 46.43it/s]Training CobwebTree:  17%|        | 1310/7600 [00:24<02:22, 44.01it/s]Training CobwebTree:  17%|        | 1315/7600 [00:24<02:24, 43.57it/s]Training CobwebTree:  17%|        | 1320/7600 [00:24<02:20, 44.58it/s]Training CobwebTree:  17%|        | 1325/7600 [00:24<02:16, 45.91it/s]Training CobwebTree:  18%|        | 1330/7600 [00:24<02:15, 46.15it/s]Training CobwebTree:  18%|        | 1335/7600 [00:24<02:16, 45.97it/s]Training CobwebTree:  18%|        | 1340/7600 [00:25<02:21, 44.28it/s]Training CobwebTree:  18%|        | 1346/7600 [00:25<02:15, 46.28it/s]Training CobwebTree:  18%|        | 1351/7600 [00:25<02:13, 46.95it/s]Training CobwebTree:  18%|        | 1356/7600 [00:25<02:19, 44.63it/s]Training CobwebTree:  18%|        | 1361/7600 [00:25<02:19, 44.82it/s]Training CobwebTree:  18%|        | 1366/7600 [00:25<02:16, 45.74it/s]Training CobwebTree:  18%|        | 1371/7600 [00:25<02:13, 46.70it/s]Training CobwebTree:  18%|        | 1377/7600 [00:25<02:07, 48.65it/s]Training CobwebTree:  18%|        | 1383/7600 [00:25<02:04, 49.74it/s]Training CobwebTree:  18%|        | 1389/7600 [00:26<02:04, 50.00it/s]Training CobwebTree:  18%|        | 1394/7600 [00:26<02:04, 49.99it/s]Training CobwebTree:  18%|        | 1399/7600 [00:26<02:08, 48.18it/s]Training CobwebTree:  18%|        | 1404/7600 [00:26<02:08, 48.40it/s]Training CobwebTree:  19%|        | 1409/7600 [00:26<02:06, 48.76it/s]Training CobwebTree:  19%|        | 1414/7600 [00:26<02:08, 48.21it/s]Training CobwebTree:  19%|        | 1420/7600 [00:26<02:06, 49.00it/s]Training CobwebTree:  19%|        | 1425/7600 [00:26<02:08, 48.18it/s]Training CobwebTree:  19%|        | 1431/7600 [00:26<02:04, 49.48it/s]Training CobwebTree:  19%|        | 1436/7600 [00:27<02:07, 48.30it/s]Training CobwebTree:  19%|        | 1441/7600 [00:27<02:12, 46.43it/s]Training CobwebTree:  19%|        | 1446/7600 [00:27<02:15, 45.43it/s]Training CobwebTree:  19%|        | 1451/7600 [00:27<02:14, 45.82it/s]Training CobwebTree:  19%|        | 1456/7600 [00:27<02:15, 45.44it/s]Training CobwebTree:  19%|        | 1461/7600 [00:27<02:12, 46.42it/s]Training CobwebTree:  19%|        | 1466/7600 [00:27<02:09, 47.20it/s]Training CobwebTree:  19%|        | 1471/7600 [00:27<02:10, 46.85it/s]Training CobwebTree:  19%|        | 1476/7600 [00:27<02:13, 45.78it/s]Training CobwebTree:  19%|        | 1481/7600 [00:28<02:13, 46.00it/s]Training CobwebTree:  20%|        | 1487/7600 [00:28<02:05, 48.68it/s]Training CobwebTree:  20%|        | 1493/7600 [00:28<02:04, 49.06it/s]Training CobwebTree:  20%|        | 1498/7600 [00:28<02:06, 48.15it/s]Training CobwebTree:  20%|        | 1504/7600 [00:28<02:07, 47.69it/s]Training CobwebTree:  20%|        | 1509/7600 [00:28<02:11, 46.18it/s]Training CobwebTree:  20%|        | 1514/7600 [00:28<02:12, 45.95it/s]Training CobwebTree:  20%|        | 1519/7600 [00:28<02:11, 46.26it/s]Training CobwebTree:  20%|        | 1524/7600 [00:28<02:09, 46.87it/s]Training CobwebTree:  20%|        | 1529/7600 [00:29<02:13, 45.57it/s]Training CobwebTree:  20%|        | 1534/7600 [00:29<02:13, 45.34it/s]Training CobwebTree:  20%|        | 1539/7600 [00:29<02:11, 46.19it/s]Training CobwebTree:  20%|        | 1545/7600 [00:29<02:08, 47.08it/s]Training CobwebTree:  20%|        | 1550/7600 [00:29<02:08, 47.04it/s]Training CobwebTree:  20%|        | 1555/7600 [00:29<02:15, 44.75it/s]Training CobwebTree:  21%|        | 1561/7600 [00:29<02:07, 47.32it/s]Training CobwebTree:  21%|        | 1566/7600 [00:29<02:09, 46.58it/s]Training CobwebTree:  21%|        | 1571/7600 [00:29<02:07, 47.17it/s]Training CobwebTree:  21%|        | 1576/7600 [00:30<02:09, 46.57it/s]Training CobwebTree:  21%|        | 1582/7600 [00:30<02:02, 49.28it/s]Training CobwebTree:  21%|        | 1587/7600 [00:30<02:03, 48.58it/s]Training CobwebTree:  21%|        | 1592/7600 [00:30<02:10, 45.91it/s]Training CobwebTree:  21%|        | 1597/7600 [00:30<02:09, 46.47it/s]Training CobwebTree:  21%|        | 1602/7600 [00:30<02:15, 44.27it/s]Training CobwebTree:  21%|        | 1607/7600 [00:30<02:14, 44.44it/s]Training CobwebTree:  21%|        | 1612/7600 [00:30<02:16, 43.85it/s]Training CobwebTree:  21%|       | 1617/7600 [00:30<02:15, 44.01it/s]Training CobwebTree:  21%|       | 1622/7600 [00:31<02:15, 44.17it/s]Training CobwebTree:  21%|       | 1627/7600 [00:31<02:11, 45.38it/s]Training CobwebTree:  21%|       | 1632/7600 [00:31<02:09, 45.91it/s]Training CobwebTree:  22%|       | 1637/7600 [00:31<02:12, 44.89it/s]Training CobwebTree:  22%|       | 1642/7600 [00:31<02:15, 44.00it/s]Training CobwebTree:  22%|       | 1648/7600 [00:31<02:09, 46.02it/s]Training CobwebTree:  22%|       | 1653/7600 [00:31<02:07, 46.56it/s]Training CobwebTree:  22%|       | 1658/7600 [00:31<02:11, 45.26it/s]Training CobwebTree:  22%|       | 1663/7600 [00:32<02:15, 43.93it/s]Training CobwebTree:  22%|       | 1668/7600 [00:32<02:14, 44.25it/s]Training CobwebTree:  22%|       | 1674/7600 [00:32<02:04, 47.52it/s]Training CobwebTree:  22%|       | 1679/7600 [00:32<02:09, 45.84it/s]Training CobwebTree:  22%|       | 1684/7600 [00:32<02:08, 45.90it/s]Training CobwebTree:  22%|       | 1690/7600 [00:32<01:59, 49.30it/s]Training CobwebTree:  22%|       | 1695/7600 [00:32<02:12, 44.42it/s]Training CobwebTree:  22%|       | 1700/7600 [00:32<02:15, 43.42it/s]Training CobwebTree:  22%|       | 1705/7600 [00:32<02:13, 44.26it/s]Training CobwebTree:  22%|       | 1710/7600 [00:33<02:13, 43.98it/s]Training CobwebTree:  23%|       | 1715/7600 [00:33<02:11, 44.60it/s]Training CobwebTree:  23%|       | 1720/7600 [00:33<02:11, 44.69it/s]Training CobwebTree:  23%|       | 1725/7600 [00:33<02:08, 45.62it/s]Training CobwebTree:  23%|       | 1730/7600 [00:33<02:08, 45.64it/s]Training CobwebTree:  23%|       | 1736/7600 [00:33<02:07, 46.08it/s]Training CobwebTree:  23%|       | 1742/7600 [00:33<02:03, 47.30it/s]Training CobwebTree:  23%|       | 1747/7600 [00:33<02:08, 45.43it/s]Training CobwebTree:  23%|       | 1752/7600 [00:33<02:09, 45.11it/s]Training CobwebTree:  23%|       | 1758/7600 [00:34<02:03, 47.36it/s]Training CobwebTree:  23%|       | 1764/7600 [00:34<02:03, 47.16it/s]Training CobwebTree:  23%|       | 1770/7600 [00:34<02:02, 47.42it/s]Training CobwebTree:  23%|       | 1775/7600 [00:34<02:01, 47.84it/s]Training CobwebTree:  23%|       | 1780/7600 [00:34<02:01, 47.96it/s]Training CobwebTree:  23%|       | 1785/7600 [00:34<02:04, 46.72it/s]Training CobwebTree:  24%|       | 1790/7600 [00:34<02:06, 46.09it/s]Training CobwebTree:  24%|       | 1795/7600 [00:34<02:04, 46.72it/s]Training CobwebTree:  24%|       | 1800/7600 [00:34<02:04, 46.42it/s]Training CobwebTree:  24%|       | 1805/7600 [00:35<02:08, 45.24it/s]Training CobwebTree:  24%|       | 1810/7600 [00:35<02:08, 45.18it/s]Training CobwebTree:  24%|       | 1816/7600 [00:35<02:02, 47.25it/s]Training CobwebTree:  24%|       | 1821/7600 [00:35<02:04, 46.36it/s]Training CobwebTree:  24%|       | 1826/7600 [00:35<02:12, 43.54it/s]Training CobwebTree:  24%|       | 1831/7600 [00:35<02:12, 43.39it/s]Training CobwebTree:  24%|       | 1836/7600 [00:35<02:08, 44.69it/s]Training CobwebTree:  24%|       | 1841/7600 [00:35<02:05, 45.76it/s]Training CobwebTree:  24%|       | 1846/7600 [00:36<02:08, 44.88it/s]Training CobwebTree:  24%|       | 1851/7600 [00:36<02:08, 44.64it/s]Training CobwebTree:  24%|       | 1856/7600 [00:36<02:09, 44.53it/s]Training CobwebTree:  24%|       | 1861/7600 [00:36<02:08, 44.53it/s]Training CobwebTree:  25%|       | 1866/7600 [00:36<02:06, 45.26it/s]Training CobwebTree:  25%|       | 1871/7600 [00:36<02:04, 45.96it/s]Training CobwebTree:  25%|       | 1876/7600 [00:36<02:11, 43.58it/s]Training CobwebTree:  25%|       | 1882/7600 [00:36<02:04, 45.91it/s]Training CobwebTree:  25%|       | 1888/7600 [00:36<02:01, 47.11it/s]Training CobwebTree:  25%|       | 1893/7600 [00:37<02:06, 45.26it/s]Training CobwebTree:  25%|       | 1898/7600 [00:37<02:08, 44.38it/s]Training CobwebTree:  25%|       | 1903/7600 [00:37<02:09, 43.97it/s]Training CobwebTree:  25%|       | 1908/7600 [00:37<02:12, 42.99it/s]Training CobwebTree:  25%|       | 1914/7600 [00:37<02:05, 45.13it/s]Training CobwebTree:  25%|       | 1919/7600 [00:37<02:04, 45.74it/s]Training CobwebTree:  25%|       | 1924/7600 [00:37<02:09, 43.74it/s]Training CobwebTree:  25%|       | 1929/7600 [00:37<02:05, 45.22it/s]Training CobwebTree:  25%|       | 1934/7600 [00:37<02:08, 44.06it/s]Training CobwebTree:  26%|       | 1939/7600 [00:38<02:06, 44.65it/s]Training CobwebTree:  26%|       | 1944/7600 [00:38<02:13, 42.40it/s]Training CobwebTree:  26%|       | 1949/7600 [00:38<02:10, 43.34it/s]Training CobwebTree:  26%|       | 1954/7600 [00:38<02:13, 42.41it/s]Training CobwebTree:  26%|       | 1959/7600 [00:38<02:11, 42.98it/s]Training CobwebTree:  26%|       | 1964/7600 [00:38<02:09, 43.46it/s]Training CobwebTree:  26%|       | 1969/7600 [00:38<02:09, 43.63it/s]Training CobwebTree:  26%|       | 1974/7600 [00:38<02:06, 44.64it/s]Training CobwebTree:  26%|       | 1979/7600 [00:39<02:05, 44.91it/s]Training CobwebTree:  26%|       | 1984/7600 [00:39<02:04, 45.03it/s]Training CobwebTree:  26%|       | 1989/7600 [00:39<02:10, 43.02it/s]Training CobwebTree:  26%|       | 1994/7600 [00:39<02:08, 43.65it/s]Training CobwebTree:  26%|       | 1999/7600 [00:39<02:06, 44.16it/s]Training CobwebTree:  26%|       | 2004/7600 [00:39<02:02, 45.61it/s]Training CobwebTree:  26%|       | 2009/7600 [00:39<02:04, 44.91it/s]Training CobwebTree:  26%|       | 2014/7600 [00:39<02:07, 43.87it/s]Training CobwebTree:  27%|       | 2019/7600 [00:39<02:19, 40.04it/s]Training CobwebTree:  27%|       | 2024/7600 [00:40<02:11, 42.28it/s]Training CobwebTree:  27%|       | 2029/7600 [00:40<02:07, 43.70it/s]Training CobwebTree:  27%|       | 2034/7600 [00:40<02:05, 44.43it/s]Training CobwebTree:  27%|       | 2039/7600 [00:40<02:07, 43.69it/s]Training CobwebTree:  27%|       | 2044/7600 [00:40<02:06, 43.91it/s]Training CobwebTree:  27%|       | 2050/7600 [00:40<01:57, 47.39it/s]Training CobwebTree:  27%|       | 2055/7600 [00:40<01:59, 46.51it/s]Training CobwebTree:  27%|       | 2060/7600 [00:40<01:59, 46.43it/s]Training CobwebTree:  27%|       | 2065/7600 [00:40<01:58, 46.79it/s]Training CobwebTree:  27%|       | 2070/7600 [00:41<02:00, 46.07it/s]Training CobwebTree:  27%|       | 2075/7600 [00:41<01:57, 47.16it/s]Training CobwebTree:  27%|       | 2080/7600 [00:41<01:56, 47.35it/s]Training CobwebTree:  27%|       | 2085/7600 [00:41<02:00, 45.82it/s]Training CobwebTree:  28%|       | 2090/7600 [00:41<02:01, 45.29it/s]Training CobwebTree:  28%|       | 2095/7600 [00:41<02:01, 45.15it/s]Training CobwebTree:  28%|       | 2100/7600 [00:41<02:02, 45.05it/s]Training CobwebTree:  28%|       | 2105/7600 [00:41<02:04, 43.98it/s]Training CobwebTree:  28%|       | 2110/7600 [00:41<02:04, 44.07it/s]Training CobwebTree:  28%|       | 2115/7600 [00:42<02:06, 43.37it/s]Training CobwebTree:  28%|       | 2120/7600 [00:42<02:06, 43.26it/s]Training CobwebTree:  28%|       | 2125/7600 [00:42<02:03, 44.24it/s]Training CobwebTree:  28%|       | 2130/7600 [00:42<02:04, 43.81it/s]Training CobwebTree:  28%|       | 2135/7600 [00:42<02:04, 43.88it/s]Training CobwebTree:  28%|       | 2141/7600 [00:42<01:59, 45.72it/s]Training CobwebTree:  28%|       | 2146/7600 [00:42<02:00, 45.11it/s]Training CobwebTree:  28%|       | 2152/7600 [00:42<01:56, 46.90it/s]Training CobwebTree:  28%|       | 2157/7600 [00:42<01:55, 47.29it/s]Training CobwebTree:  28%|       | 2162/7600 [00:43<01:58, 46.05it/s]Training CobwebTree:  29%|       | 2167/7600 [00:43<02:01, 44.55it/s]Training CobwebTree:  29%|       | 2173/7600 [00:43<01:55, 46.90it/s]Training CobwebTree:  29%|       | 2178/7600 [00:43<01:57, 46.05it/s]Training CobwebTree:  29%|       | 2183/7600 [00:43<01:58, 45.61it/s]Training CobwebTree:  29%|       | 2188/7600 [00:43<01:59, 45.36it/s]Training CobwebTree:  29%|       | 2193/7600 [00:43<01:57, 46.02it/s]Training CobwebTree:  29%|       | 2198/7600 [00:43<02:03, 43.83it/s]Training CobwebTree:  29%|       | 2203/7600 [00:43<02:04, 43.49it/s]Training CobwebTree:  29%|       | 2208/7600 [00:44<02:10, 41.31it/s]Training CobwebTree:  29%|       | 2213/7600 [00:44<02:09, 41.69it/s]Training CobwebTree:  29%|       | 2218/7600 [00:44<02:12, 40.58it/s]Training CobwebTree:  29%|       | 2224/7600 [00:44<02:07, 42.20it/s]Training CobwebTree:  29%|       | 2229/7600 [00:44<02:05, 42.74it/s]Training CobwebTree:  29%|       | 2234/7600 [00:44<02:00, 44.59it/s]Training CobwebTree:  29%|       | 2239/7600 [00:44<02:03, 43.32it/s]Training CobwebTree:  30%|       | 2244/7600 [00:44<02:04, 42.90it/s]Training CobwebTree:  30%|       | 2249/7600 [00:45<02:06, 42.23it/s]Training CobwebTree:  30%|       | 2254/7600 [00:45<02:11, 40.61it/s]Training CobwebTree:  30%|       | 2259/7600 [00:45<02:18, 38.66it/s]Training CobwebTree:  30%|       | 2264/7600 [00:45<02:14, 39.81it/s]Training CobwebTree:  30%|       | 2270/7600 [00:45<02:04, 42.68it/s]Training CobwebTree:  30%|       | 2275/7600 [00:45<02:03, 43.10it/s]Training CobwebTree:  30%|       | 2280/7600 [00:45<02:00, 44.26it/s]Training CobwebTree:  30%|       | 2285/7600 [00:45<02:00, 44.23it/s]Training CobwebTree:  30%|       | 2290/7600 [00:46<01:59, 44.44it/s]Training CobwebTree:  30%|       | 2295/7600 [00:46<02:06, 42.08it/s]Training CobwebTree:  30%|       | 2300/7600 [00:46<02:01, 43.68it/s]Training CobwebTree:  30%|       | 2306/7600 [00:46<01:54, 46.36it/s]Training CobwebTree:  30%|       | 2311/7600 [00:46<01:55, 45.68it/s]Training CobwebTree:  30%|       | 2316/7600 [00:46<01:56, 45.27it/s]Training CobwebTree:  31%|       | 2321/7600 [00:46<01:58, 44.38it/s]Training CobwebTree:  31%|       | 2326/7600 [00:46<02:01, 43.33it/s]Training CobwebTree:  31%|       | 2331/7600 [00:46<02:02, 43.05it/s]Training CobwebTree:  31%|       | 2336/7600 [00:47<01:59, 43.89it/s]Training CobwebTree:  31%|       | 2341/7600 [00:47<01:57, 44.76it/s]Training CobwebTree:  31%|       | 2346/7600 [00:47<01:58, 44.27it/s]Training CobwebTree:  31%|       | 2351/7600 [00:47<01:57, 44.56it/s]Training CobwebTree:  31%|       | 2356/7600 [00:47<02:03, 42.59it/s]Training CobwebTree:  31%|       | 2361/7600 [00:47<02:00, 43.35it/s]Training CobwebTree:  31%|       | 2366/7600 [00:47<02:06, 41.41it/s]Training CobwebTree:  31%|       | 2371/7600 [00:47<02:02, 42.69it/s]Training CobwebTree:  31%|      | 2376/7600 [00:48<02:02, 42.81it/s]Training CobwebTree:  31%|      | 2381/7600 [00:48<01:58, 44.19it/s]Training CobwebTree:  31%|      | 2386/7600 [00:48<02:00, 43.39it/s]Training CobwebTree:  31%|      | 2391/7600 [00:48<02:01, 42.87it/s]Training CobwebTree:  32%|      | 2396/7600 [00:48<01:59, 43.39it/s]Training CobwebTree:  32%|      | 2401/7600 [00:48<02:03, 42.11it/s]Training CobwebTree:  32%|      | 2406/7600 [00:48<02:04, 41.83it/s]Training CobwebTree:  32%|      | 2411/7600 [00:48<01:58, 43.90it/s]Training CobwebTree:  32%|      | 2416/7600 [00:48<01:57, 43.98it/s]Training CobwebTree:  32%|      | 2421/7600 [00:49<02:02, 42.23it/s]Training CobwebTree:  32%|      | 2426/7600 [00:49<01:59, 43.13it/s]Training CobwebTree:  32%|      | 2431/7600 [00:49<01:59, 43.11it/s]Training CobwebTree:  32%|      | 2436/7600 [00:49<01:59, 43.26it/s]Training CobwebTree:  32%|      | 2441/7600 [00:49<02:02, 42.13it/s]Training CobwebTree:  32%|      | 2446/7600 [00:49<02:11, 39.07it/s]Training CobwebTree:  32%|      | 2451/7600 [00:49<02:10, 39.38it/s]Training CobwebTree:  32%|      | 2457/7600 [00:49<02:00, 42.76it/s]Training CobwebTree:  32%|      | 2462/7600 [00:50<02:01, 42.26it/s]Training CobwebTree:  32%|      | 2467/7600 [00:50<01:56, 43.96it/s]Training CobwebTree:  33%|      | 2472/7600 [00:50<01:55, 44.27it/s]Training CobwebTree:  33%|      | 2477/7600 [00:50<01:53, 44.94it/s]Training CobwebTree:  33%|      | 2482/7600 [00:50<01:58, 43.34it/s]Training CobwebTree:  33%|      | 2487/7600 [00:50<01:55, 44.20it/s]Training CobwebTree:  33%|      | 2492/7600 [00:50<01:58, 43.11it/s]Training CobwebTree:  33%|      | 2497/7600 [00:50<01:59, 42.65it/s]Training CobwebTree:  33%|      | 2502/7600 [00:50<02:00, 42.45it/s]Training CobwebTree:  33%|      | 2507/7600 [00:51<01:55, 44.07it/s]Training CobwebTree:  33%|      | 2512/7600 [00:51<01:58, 42.83it/s]Training CobwebTree:  33%|      | 2517/7600 [00:51<01:58, 42.79it/s]Training CobwebTree:  33%|      | 2522/7600 [00:51<01:55, 43.88it/s]Training CobwebTree:  33%|      | 2527/7600 [00:51<01:54, 44.46it/s]Training CobwebTree:  33%|      | 2532/7600 [00:51<01:54, 44.35it/s]Training CobwebTree:  33%|      | 2537/7600 [00:51<01:52, 45.18it/s]Training CobwebTree:  33%|      | 2542/7600 [00:51<01:51, 45.50it/s]Training CobwebTree:  34%|      | 2547/7600 [00:51<01:50, 45.55it/s]Training CobwebTree:  34%|      | 2552/7600 [00:52<01:51, 45.08it/s]Training CobwebTree:  34%|      | 2557/7600 [00:52<01:55, 43.75it/s]Training CobwebTree:  34%|      | 2562/7600 [00:52<02:01, 41.50it/s]Training CobwebTree:  34%|      | 2567/7600 [00:52<02:03, 40.85it/s]Training CobwebTree:  34%|      | 2572/7600 [00:52<01:56, 43.20it/s]Training CobwebTree:  34%|      | 2577/7600 [00:52<01:54, 44.04it/s]Training CobwebTree:  34%|      | 2582/7600 [00:52<01:52, 44.50it/s]Training CobwebTree:  34%|      | 2587/7600 [00:52<01:53, 44.28it/s]Training CobwebTree:  34%|      | 2592/7600 [00:53<01:52, 44.55it/s]Training CobwebTree:  34%|      | 2597/7600 [00:53<01:53, 44.06it/s]Training CobwebTree:  34%|      | 2602/7600 [00:53<01:53, 44.19it/s]Training CobwebTree:  34%|      | 2607/7600 [00:53<01:55, 43.22it/s]Training CobwebTree:  34%|      | 2612/7600 [00:53<01:58, 41.94it/s]Training CobwebTree:  34%|      | 2617/7600 [00:53<01:59, 41.76it/s]Training CobwebTree:  34%|      | 2622/7600 [00:53<02:01, 40.88it/s]Training CobwebTree:  35%|      | 2627/7600 [00:53<02:02, 40.70it/s]Training CobwebTree:  35%|      | 2632/7600 [00:53<02:06, 39.30it/s]Training CobwebTree:  35%|      | 2637/7600 [00:54<02:06, 39.36it/s]Training CobwebTree:  35%|      | 2642/7600 [00:54<02:04, 39.92it/s]Training CobwebTree:  35%|      | 2647/7600 [00:54<01:59, 41.43it/s]Training CobwebTree:  35%|      | 2652/7600 [00:54<01:58, 41.58it/s]Training CobwebTree:  35%|      | 2657/7600 [00:54<02:02, 40.25it/s]Training CobwebTree:  35%|      | 2662/7600 [00:54<02:01, 40.75it/s]Training CobwebTree:  35%|      | 2667/7600 [00:54<01:56, 42.35it/s]Training CobwebTree:  35%|      | 2672/7600 [00:54<01:55, 42.49it/s]Training CobwebTree:  35%|      | 2677/7600 [00:55<01:55, 42.80it/s]Training CobwebTree:  35%|      | 2682/7600 [00:55<01:58, 41.57it/s]Training CobwebTree:  35%|      | 2687/7600 [00:55<01:58, 41.45it/s]Training CobwebTree:  35%|      | 2692/7600 [00:55<01:56, 42.04it/s]Training CobwebTree:  35%|      | 2697/7600 [00:55<01:53, 43.06it/s]Training CobwebTree:  36%|      | 2702/7600 [00:55<01:55, 42.34it/s]Training CobwebTree:  36%|      | 2707/7600 [00:55<01:58, 41.23it/s]Training CobwebTree:  36%|      | 2712/7600 [00:55<02:03, 39.73it/s]Training CobwebTree:  36%|      | 2717/7600 [00:56<02:00, 40.46it/s]Training CobwebTree:  36%|      | 2722/7600 [00:56<01:59, 40.87it/s]Training CobwebTree:  36%|      | 2727/7600 [00:56<01:55, 42.06it/s]Training CobwebTree:  36%|      | 2732/7600 [00:56<01:53, 42.78it/s]Training CobwebTree:  36%|      | 2737/7600 [00:56<01:52, 43.16it/s]Training CobwebTree:  36%|      | 2742/7600 [00:56<01:57, 41.38it/s]Training CobwebTree:  36%|      | 2748/7600 [00:56<01:51, 43.70it/s]Training CobwebTree:  36%|      | 2753/7600 [00:56<01:48, 44.78it/s]Training CobwebTree:  36%|      | 2758/7600 [00:56<01:54, 42.31it/s]Training CobwebTree:  36%|      | 2763/7600 [00:57<01:52, 42.89it/s]Training CobwebTree:  36%|      | 2768/7600 [00:57<01:54, 42.14it/s]Training CobwebTree:  36%|      | 2773/7600 [00:57<01:56, 41.56it/s]Training CobwebTree:  37%|      | 2778/7600 [00:57<01:56, 41.45it/s]Training CobwebTree:  37%|      | 2783/7600 [00:57<01:54, 42.13it/s]Training CobwebTree:  37%|      | 2788/7600 [00:57<01:51, 43.15it/s]Training CobwebTree:  37%|      | 2793/7600 [00:57<01:50, 43.61it/s]Training CobwebTree:  37%|      | 2798/7600 [00:57<01:59, 40.16it/s]Training CobwebTree:  37%|      | 2803/7600 [00:58<01:59, 40.14it/s]Training CobwebTree:  37%|      | 2808/7600 [00:58<01:57, 40.78it/s]Training CobwebTree:  37%|      | 2813/7600 [00:58<01:54, 41.72it/s]Training CobwebTree:  37%|      | 2818/7600 [00:58<01:56, 40.88it/s]Training CobwebTree:  37%|      | 2824/7600 [00:58<01:51, 42.98it/s]Training CobwebTree:  37%|      | 2829/7600 [00:58<01:53, 42.14it/s]Training CobwebTree:  37%|      | 2834/7600 [00:58<01:57, 40.67it/s]Training CobwebTree:  37%|      | 2839/7600 [00:58<01:58, 40.21it/s]Training CobwebTree:  37%|      | 2844/7600 [00:59<01:55, 41.24it/s]Training CobwebTree:  37%|      | 2849/7600 [00:59<01:55, 41.18it/s]Training CobwebTree:  38%|      | 2854/7600 [00:59<01:56, 40.61it/s]Training CobwebTree:  38%|      | 2859/7600 [00:59<01:51, 42.65it/s]Training CobwebTree:  38%|      | 2864/7600 [00:59<01:52, 42.26it/s]Training CobwebTree:  38%|      | 2869/7600 [00:59<01:53, 41.78it/s]Training CobwebTree:  38%|      | 2874/7600 [00:59<01:50, 42.71it/s]Training CobwebTree:  38%|      | 2879/7600 [00:59<01:56, 40.69it/s]Training CobwebTree:  38%|      | 2884/7600 [01:00<01:53, 41.73it/s]Training CobwebTree:  38%|      | 2889/7600 [01:00<01:49, 42.86it/s]Training CobwebTree:  38%|      | 2894/7600 [01:00<01:50, 42.70it/s]Training CobwebTree:  38%|      | 2899/7600 [01:00<01:48, 43.43it/s]Training CobwebTree:  38%|      | 2904/7600 [01:00<01:52, 41.74it/s]Training CobwebTree:  38%|      | 2909/7600 [01:00<01:51, 42.26it/s]Training CobwebTree:  38%|      | 2914/7600 [01:00<01:54, 40.86it/s]Training CobwebTree:  38%|      | 2919/7600 [01:00<01:53, 41.19it/s]Training CobwebTree:  38%|      | 2924/7600 [01:00<01:50, 42.31it/s]Training CobwebTree:  39%|      | 2929/7600 [01:01<01:49, 42.47it/s]Training CobwebTree:  39%|      | 2934/7600 [01:01<01:56, 39.96it/s]Training CobwebTree:  39%|      | 2939/7600 [01:01<01:54, 40.62it/s]Training CobwebTree:  39%|      | 2944/7600 [01:01<01:52, 41.47it/s]Training CobwebTree:  39%|      | 2949/7600 [01:01<01:52, 41.32it/s]Training CobwebTree:  39%|      | 2954/7600 [01:01<01:51, 41.55it/s]Training CobwebTree:  39%|      | 2959/7600 [01:01<01:54, 40.38it/s]Training CobwebTree:  39%|      | 2964/7600 [01:01<01:58, 39.13it/s]Training CobwebTree:  39%|      | 2968/7600 [01:02<02:00, 38.47it/s]Training CobwebTree:  39%|      | 2973/7600 [01:02<01:56, 39.78it/s]Training CobwebTree:  39%|      | 2977/7600 [01:02<01:58, 38.97it/s]Training CobwebTree:  39%|      | 2981/7600 [01:02<01:58, 39.09it/s]Training CobwebTree:  39%|      | 2985/7600 [01:02<01:59, 38.68it/s]Training CobwebTree:  39%|      | 2989/7600 [01:02<02:04, 37.08it/s]Training CobwebTree:  39%|      | 2994/7600 [01:02<01:53, 40.50it/s]Training CobwebTree:  39%|      | 2999/7600 [01:02<01:58, 38.75it/s]Training CobwebTree:  40%|      | 3004/7600 [01:02<01:53, 40.38it/s]Training CobwebTree:  40%|      | 3009/7600 [01:03<01:52, 40.63it/s]Training CobwebTree:  40%|      | 3014/7600 [01:03<01:51, 41.25it/s]Training CobwebTree:  40%|      | 3019/7600 [01:03<01:52, 40.65it/s]Training CobwebTree:  40%|      | 3024/7600 [01:03<01:51, 41.20it/s]Training CobwebTree:  40%|      | 3029/7600 [01:03<01:48, 42.08it/s]Training CobwebTree:  40%|      | 3034/7600 [01:03<01:46, 42.70it/s]Training CobwebTree:  40%|      | 3039/7600 [01:03<01:48, 41.99it/s]Training CobwebTree:  40%|      | 3044/7600 [01:03<01:45, 43.30it/s]Training CobwebTree:  40%|      | 3049/7600 [01:04<01:45, 43.04it/s]Training CobwebTree:  40%|      | 3054/7600 [01:04<01:53, 40.14it/s]Training CobwebTree:  40%|      | 3059/7600 [01:04<01:49, 41.55it/s]Training CobwebTree:  40%|      | 3064/7600 [01:04<01:50, 41.03it/s]Training CobwebTree:  40%|      | 3069/7600 [01:04<01:46, 42.50it/s]Training CobwebTree:  40%|      | 3074/7600 [01:04<01:47, 42.22it/s]Training CobwebTree:  41%|      | 3079/7600 [01:04<01:45, 42.87it/s]Training CobwebTree:  41%|      | 3084/7600 [01:04<01:48, 41.68it/s]Training CobwebTree:  41%|      | 3089/7600 [01:05<01:46, 42.39it/s]Training CobwebTree:  41%|      | 3094/7600 [01:05<01:47, 42.05it/s]Training CobwebTree:  41%|      | 3099/7600 [01:05<01:49, 40.94it/s]Training CobwebTree:  41%|      | 3104/7600 [01:05<01:47, 41.99it/s]Training CobwebTree:  41%|      | 3109/7600 [01:05<01:47, 41.72it/s]Training CobwebTree:  41%|      | 3114/7600 [01:05<01:50, 40.56it/s]Training CobwebTree:  41%|      | 3119/7600 [01:05<01:49, 40.98it/s]Training CobwebTree:  41%|      | 3124/7600 [01:05<01:48, 41.26it/s]Training CobwebTree:  41%|      | 3129/7600 [01:05<01:47, 41.50it/s]Training CobwebTree:  41%|      | 3134/7600 [01:06<01:46, 41.94it/s]Training CobwebTree:  41%|     | 3139/7600 [01:06<01:49, 40.92it/s]Training CobwebTree:  41%|     | 3144/7600 [01:06<01:48, 41.10it/s]Training CobwebTree:  41%|     | 3149/7600 [01:06<01:51, 39.86it/s]Training CobwebTree:  42%|     | 3154/7600 [01:06<01:48, 40.96it/s]Training CobwebTree:  42%|     | 3159/7600 [01:06<01:46, 41.64it/s]Training CobwebTree:  42%|     | 3164/7600 [01:06<01:43, 42.91it/s]Training CobwebTree:  42%|     | 3169/7600 [01:06<01:43, 42.93it/s]Training CobwebTree:  42%|     | 3174/7600 [01:07<01:45, 41.95it/s]Training CobwebTree:  42%|     | 3179/7600 [01:07<01:41, 43.67it/s]Training CobwebTree:  42%|     | 3184/7600 [01:07<01:41, 43.50it/s]Training CobwebTree:  42%|     | 3189/7600 [01:07<01:43, 42.81it/s]Training CobwebTree:  42%|     | 3194/7600 [01:07<01:47, 40.93it/s]Training CobwebTree:  42%|     | 3199/7600 [01:07<01:46, 41.45it/s]Training CobwebTree:  42%|     | 3204/7600 [01:07<01:46, 41.37it/s]Training CobwebTree:  42%|     | 3209/7600 [01:07<01:44, 41.87it/s]Training CobwebTree:  42%|     | 3214/7600 [01:08<01:44, 41.96it/s]Training CobwebTree:  42%|     | 3219/7600 [01:08<01:44, 41.73it/s]Training CobwebTree:  42%|     | 3224/7600 [01:08<01:41, 43.31it/s]Training CobwebTree:  42%|     | 3229/7600 [01:08<01:42, 42.67it/s]Training CobwebTree:  43%|     | 3235/7600 [01:08<01:37, 44.96it/s]Training CobwebTree:  43%|     | 3240/7600 [01:08<01:38, 44.20it/s]Training CobwebTree:  43%|     | 3245/7600 [01:08<01:42, 42.66it/s]Training CobwebTree:  43%|     | 3250/7600 [01:08<01:43, 41.91it/s]Training CobwebTree:  43%|     | 3255/7600 [01:08<01:43, 42.06it/s]Training CobwebTree:  43%|     | 3260/7600 [01:09<01:45, 41.33it/s]Training CobwebTree:  43%|     | 3265/7600 [01:09<01:48, 39.91it/s]Training CobwebTree:  43%|     | 3270/7600 [01:09<01:48, 39.94it/s]Training CobwebTree:  43%|     | 3275/7600 [01:09<01:45, 40.98it/s]Training CobwebTree:  43%|     | 3280/7600 [01:09<01:47, 40.22it/s]Training CobwebTree:  43%|     | 3285/7600 [01:09<01:43, 41.77it/s]Training CobwebTree:  43%|     | 3290/7600 [01:09<01:46, 40.59it/s]Training CobwebTree:  43%|     | 3295/7600 [01:09<01:47, 39.97it/s]Training CobwebTree:  43%|     | 3300/7600 [01:10<01:49, 39.25it/s]Training CobwebTree:  44%|     | 3306/7600 [01:10<01:39, 43.35it/s]Training CobwebTree:  44%|     | 3311/7600 [01:10<01:41, 42.21it/s]Training CobwebTree:  44%|     | 3316/7600 [01:10<01:42, 41.95it/s]Training CobwebTree:  44%|     | 3321/7600 [01:10<01:47, 39.98it/s]Training CobwebTree:  44%|     | 3326/7600 [01:10<01:55, 37.04it/s]Training CobwebTree:  44%|     | 3331/7600 [01:10<01:49, 39.12it/s]Training CobwebTree:  44%|     | 3335/7600 [01:10<01:50, 38.60it/s]Training CobwebTree:  44%|     | 3340/7600 [01:11<01:49, 39.06it/s]Training CobwebTree:  44%|     | 3345/7600 [01:11<01:45, 40.26it/s]Training CobwebTree:  44%|     | 3350/7600 [01:11<01:44, 40.53it/s]Training CobwebTree:  44%|     | 3355/7600 [01:11<01:44, 40.46it/s]Training CobwebTree:  44%|     | 3360/7600 [01:11<01:43, 40.97it/s]Training CobwebTree:  44%|     | 3365/7600 [01:11<01:41, 41.56it/s]Training CobwebTree:  44%|     | 3370/7600 [01:11<01:42, 41.38it/s]Training CobwebTree:  44%|     | 3375/7600 [01:11<01:46, 39.62it/s]Training CobwebTree:  44%|     | 3380/7600 [01:12<01:43, 40.89it/s]Training CobwebTree:  45%|     | 3385/7600 [01:12<01:43, 40.75it/s]Training CobwebTree:  45%|     | 3390/7600 [01:12<01:42, 41.22it/s]Training CobwebTree:  45%|     | 3395/7600 [01:12<01:45, 39.71it/s]Training CobwebTree:  45%|     | 3400/7600 [01:12<01:42, 41.06it/s]Training CobwebTree:  45%|     | 3405/7600 [01:12<01:47, 39.05it/s]Training CobwebTree:  45%|     | 3409/7600 [01:12<01:48, 38.62it/s]Training CobwebTree:  45%|     | 3414/7600 [01:12<01:45, 39.79it/s]Training CobwebTree:  45%|     | 3419/7600 [01:13<01:42, 40.92it/s]Training CobwebTree:  45%|     | 3424/7600 [01:13<01:42, 40.74it/s]Training CobwebTree:  45%|     | 3429/7600 [01:13<01:37, 42.68it/s]Training CobwebTree:  45%|     | 3434/7600 [01:13<01:40, 41.37it/s]Training CobwebTree:  45%|     | 3439/7600 [01:13<01:38, 42.05it/s]Training CobwebTree:  45%|     | 3444/7600 [01:13<01:45, 39.28it/s]Training CobwebTree:  45%|     | 3449/7600 [01:13<01:44, 39.74it/s]Training CobwebTree:  45%|     | 3454/7600 [01:13<01:44, 39.55it/s]Training CobwebTree:  46%|     | 3458/7600 [01:14<01:47, 38.46it/s]Training CobwebTree:  46%|     | 3462/7600 [01:14<01:49, 37.85it/s]Training CobwebTree:  46%|     | 3467/7600 [01:14<01:46, 38.69it/s]Training CobwebTree:  46%|     | 3471/7600 [01:14<01:49, 37.79it/s]Training CobwebTree:  46%|     | 3476/7600 [01:14<01:47, 38.47it/s]Training CobwebTree:  46%|     | 3480/7600 [01:14<01:48, 38.13it/s]Training CobwebTree:  46%|     | 3485/7600 [01:14<01:43, 39.66it/s]Training CobwebTree:  46%|     | 3489/7600 [01:14<01:48, 37.79it/s]Training CobwebTree:  46%|     | 3493/7600 [01:14<01:49, 37.59it/s]Training CobwebTree:  46%|     | 3498/7600 [01:15<01:46, 38.47it/s]Training CobwebTree:  46%|     | 3503/7600 [01:15<01:42, 39.89it/s]Training CobwebTree:  46%|     | 3507/7600 [01:15<01:43, 39.63it/s]Training CobwebTree:  46%|     | 3512/7600 [01:15<01:42, 40.03it/s]Training CobwebTree:  46%|     | 3517/7600 [01:15<01:45, 38.86it/s]Training CobwebTree:  46%|     | 3522/7600 [01:15<01:45, 38.57it/s]Training CobwebTree:  46%|     | 3527/7600 [01:15<01:45, 38.60it/s]Training CobwebTree:  46%|     | 3532/7600 [01:15<01:41, 40.05it/s]Training CobwebTree:  47%|     | 3537/7600 [01:16<01:43, 39.40it/s]Training CobwebTree:  47%|     | 3542/7600 [01:16<01:36, 42.00it/s]Training CobwebTree:  47%|     | 3547/7600 [01:16<01:35, 42.35it/s]Training CobwebTree:  47%|     | 3552/7600 [01:16<01:33, 43.51it/s]Training CobwebTree:  47%|     | 3557/7600 [01:16<01:34, 42.65it/s]Training CobwebTree:  47%|     | 3562/7600 [01:16<01:38, 41.12it/s]Training CobwebTree:  47%|     | 3567/7600 [01:16<01:35, 42.25it/s]Training CobwebTree:  47%|     | 3572/7600 [01:16<01:34, 42.54it/s]Training CobwebTree:  47%|     | 3577/7600 [01:16<01:34, 42.35it/s]Training CobwebTree:  47%|     | 3582/7600 [01:17<01:37, 41.28it/s]Training CobwebTree:  47%|     | 3587/7600 [01:17<01:38, 40.76it/s]Training CobwebTree:  47%|     | 3592/7600 [01:17<01:36, 41.62it/s]Training CobwebTree:  47%|     | 3597/7600 [01:17<01:37, 41.21it/s]Training CobwebTree:  47%|     | 3602/7600 [01:17<01:37, 41.10it/s]Training CobwebTree:  47%|     | 3607/7600 [01:17<01:41, 39.27it/s]Training CobwebTree:  48%|     | 3611/7600 [01:17<01:42, 38.95it/s]Training CobwebTree:  48%|     | 3616/7600 [01:17<01:40, 39.45it/s]Training CobwebTree:  48%|     | 3620/7600 [01:18<01:42, 38.89it/s]Training CobwebTree:  48%|     | 3625/7600 [01:18<01:38, 40.28it/s]Training CobwebTree:  48%|     | 3630/7600 [01:18<01:42, 38.58it/s]Training CobwebTree:  48%|     | 3635/7600 [01:18<01:41, 39.14it/s]Training CobwebTree:  48%|     | 3640/7600 [01:18<01:36, 40.91it/s]Training CobwebTree:  48%|     | 3645/7600 [01:18<01:37, 40.60it/s]Training CobwebTree:  48%|     | 3650/7600 [01:18<01:35, 41.34it/s]Training CobwebTree:  48%|     | 3655/7600 [01:18<01:36, 41.09it/s]Training CobwebTree:  48%|     | 3660/7600 [01:19<01:40, 39.14it/s]Training CobwebTree:  48%|     | 3664/7600 [01:19<01:43, 37.94it/s]Training CobwebTree:  48%|     | 3668/7600 [01:19<01:44, 37.45it/s]Training CobwebTree:  48%|     | 3672/7600 [01:19<01:43, 37.99it/s]Training CobwebTree:  48%|     | 3677/7600 [01:19<01:40, 39.16it/s]Training CobwebTree:  48%|     | 3681/7600 [01:19<01:41, 38.55it/s]Training CobwebTree:  48%|     | 3685/7600 [01:19<01:42, 38.29it/s]Training CobwebTree:  49%|     | 3689/7600 [01:19<01:43, 37.92it/s]Training CobwebTree:  49%|     | 3694/7600 [01:19<01:37, 39.93it/s]Training CobwebTree:  49%|     | 3698/7600 [01:20<01:42, 38.07it/s]Training CobwebTree:  49%|     | 3702/7600 [01:20<01:46, 36.47it/s]Training CobwebTree:  49%|     | 3706/7600 [01:20<01:44, 37.35it/s]Training CobwebTree:  49%|     | 3711/7600 [01:20<01:38, 39.45it/s]Training CobwebTree:  49%|     | 3716/7600 [01:20<01:36, 40.26it/s]Training CobwebTree:  49%|     | 3721/7600 [01:20<01:37, 39.58it/s]Training CobwebTree:  49%|     | 3725/7600 [01:20<01:37, 39.69it/s]Training CobwebTree:  49%|     | 3730/7600 [01:20<01:35, 40.72it/s]Training CobwebTree:  49%|     | 3735/7600 [01:20<01:34, 41.08it/s]Training CobwebTree:  49%|     | 3740/7600 [01:21<01:29, 42.97it/s]Training CobwebTree:  49%|     | 3745/7600 [01:21<01:31, 42.17it/s]Training CobwebTree:  49%|     | 3750/7600 [01:21<01:31, 42.10it/s]Training CobwebTree:  49%|     | 3755/7600 [01:21<01:35, 40.34it/s]Training CobwebTree:  49%|     | 3760/7600 [01:21<01:36, 39.96it/s]Training CobwebTree:  50%|     | 3765/7600 [01:21<01:39, 38.51it/s]Training CobwebTree:  50%|     | 3769/7600 [01:21<01:40, 37.93it/s]Training CobwebTree:  50%|     | 3773/7600 [01:21<01:41, 37.66it/s]Training CobwebTree:  50%|     | 3778/7600 [01:22<01:39, 38.36it/s]Training CobwebTree:  50%|     | 3783/7600 [01:22<01:34, 40.21it/s]Training CobwebTree:  50%|     | 3788/7600 [01:22<01:31, 41.51it/s]Training CobwebTree:  50%|     | 3793/7600 [01:22<01:35, 39.69it/s]Training CobwebTree:  50%|     | 3798/7600 [01:22<01:33, 40.52it/s]Training CobwebTree:  50%|     | 3803/7600 [01:22<01:34, 40.01it/s]Training CobwebTree:  50%|     | 3808/7600 [01:22<01:34, 39.93it/s]Training CobwebTree:  50%|     | 3813/7600 [01:22<01:30, 41.75it/s]Training CobwebTree:  50%|     | 3818/7600 [01:23<01:33, 40.61it/s]Training CobwebTree:  50%|     | 3823/7600 [01:23<01:34, 40.04it/s]Training CobwebTree:  50%|     | 3828/7600 [01:23<01:37, 38.65it/s]Training CobwebTree:  50%|     | 3833/7600 [01:23<01:34, 39.70it/s]Training CobwebTree:  50%|     | 3837/7600 [01:23<01:35, 39.47it/s]Training CobwebTree:  51%|     | 3842/7600 [01:23<01:37, 38.68it/s]Training CobwebTree:  51%|     | 3847/7600 [01:23<01:35, 39.28it/s]Training CobwebTree:  51%|     | 3851/7600 [01:23<01:39, 37.55it/s]Training CobwebTree:  51%|     | 3856/7600 [01:24<01:34, 39.71it/s]Training CobwebTree:  51%|     | 3860/7600 [01:24<01:35, 39.06it/s]Training CobwebTree:  51%|     | 3865/7600 [01:24<01:31, 40.64it/s]Training CobwebTree:  51%|     | 3870/7600 [01:24<01:38, 38.04it/s]Training CobwebTree:  51%|     | 3875/7600 [01:24<01:36, 38.59it/s]Training CobwebTree:  51%|     | 3879/7600 [01:24<01:40, 37.20it/s]Training CobwebTree:  51%|     | 3883/7600 [01:24<01:38, 37.83it/s]Training CobwebTree:  51%|     | 3888/7600 [01:24<01:32, 40.08it/s]Training CobwebTree:  51%|     | 3893/7600 [01:24<01:33, 39.62it/s]Training CobwebTree:  51%|    | 3898/7600 [01:25<01:30, 41.04it/s]Training CobwebTree:  51%|    | 3903/7600 [01:25<01:28, 42.00it/s]Training CobwebTree:  51%|    | 3908/7600 [01:25<01:30, 40.86it/s]Training CobwebTree:  51%|    | 3913/7600 [01:25<01:33, 39.48it/s]Training CobwebTree:  52%|    | 3917/7600 [01:25<01:33, 39.51it/s]Training CobwebTree:  52%|    | 3921/7600 [01:25<01:35, 38.63it/s]Training CobwebTree:  52%|    | 3926/7600 [01:25<01:30, 40.60it/s]Training CobwebTree:  52%|    | 3931/7600 [01:25<01:30, 40.56it/s]Training CobwebTree:  52%|    | 3936/7600 [01:26<01:31, 39.94it/s]Training CobwebTree:  52%|    | 3941/7600 [01:26<01:31, 39.92it/s]Training CobwebTree:  52%|    | 3946/7600 [01:26<01:29, 40.64it/s]Training CobwebTree:  52%|    | 3951/7600 [01:26<01:35, 38.20it/s]Training CobwebTree:  52%|    | 3955/7600 [01:26<01:36, 37.74it/s]Training CobwebTree:  52%|    | 3960/7600 [01:26<01:34, 38.33it/s]Training CobwebTree:  52%|    | 3965/7600 [01:26<01:34, 38.42it/s]Training CobwebTree:  52%|    | 3970/7600 [01:26<01:32, 39.03it/s]Training CobwebTree:  52%|    | 3974/7600 [01:27<01:33, 38.87it/s]Training CobwebTree:  52%|    | 3978/7600 [01:27<01:33, 38.82it/s]Training CobwebTree:  52%|    | 3982/7600 [01:27<01:33, 38.89it/s]Training CobwebTree:  52%|    | 3986/7600 [01:27<01:34, 38.23it/s]Training CobwebTree:  53%|    | 3991/7600 [01:27<01:29, 40.53it/s]Training CobwebTree:  53%|    | 3996/7600 [01:27<01:32, 39.00it/s]Training CobwebTree:  53%|    | 4001/7600 [01:27<01:29, 40.30it/s]Training CobwebTree:  53%|    | 4006/7600 [01:27<01:32, 38.75it/s]Training CobwebTree:  53%|    | 4011/7600 [01:27<01:30, 39.51it/s]Training CobwebTree:  53%|    | 4015/7600 [01:28<01:31, 39.30it/s]Training CobwebTree:  53%|    | 4020/7600 [01:28<01:32, 38.78it/s]Training CobwebTree:  53%|    | 4025/7600 [01:28<01:30, 39.60it/s]Training CobwebTree:  53%|    | 4029/7600 [01:28<01:29, 39.69it/s]Training CobwebTree:  53%|    | 4033/7600 [01:28<01:31, 39.08it/s]Training CobwebTree:  53%|    | 4037/7600 [01:28<01:35, 37.34it/s]Training CobwebTree:  53%|    | 4041/7600 [01:28<01:34, 37.77it/s]Training CobwebTree:  53%|    | 4045/7600 [01:28<01:34, 37.47it/s]Training CobwebTree:  53%|    | 4050/7600 [01:28<01:30, 39.08it/s]Training CobwebTree:  53%|    | 4056/7600 [01:29<01:24, 42.13it/s]Training CobwebTree:  53%|    | 4061/7600 [01:29<01:24, 42.10it/s]Training CobwebTree:  54%|    | 4066/7600 [01:29<01:25, 41.15it/s]Training CobwebTree:  54%|    | 4071/7600 [01:29<01:24, 41.71it/s]Training CobwebTree:  54%|    | 4076/7600 [01:29<01:27, 40.26it/s]Training CobwebTree:  54%|    | 4081/7600 [01:29<01:23, 42.30it/s]Training CobwebTree:  54%|    | 4086/7600 [01:29<01:23, 42.19it/s]Training CobwebTree:  54%|    | 4091/7600 [01:29<01:26, 40.80it/s]Training CobwebTree:  54%|    | 4096/7600 [01:30<01:25, 40.99it/s]Training CobwebTree:  54%|    | 4101/7600 [01:30<01:29, 39.15it/s]Training CobwebTree:  54%|    | 4105/7600 [01:30<01:28, 39.29it/s]Training CobwebTree:  54%|    | 4110/7600 [01:30<01:27, 40.03it/s]Training CobwebTree:  54%|    | 4115/7600 [01:30<01:25, 40.82it/s]Training CobwebTree:  54%|    | 4120/7600 [01:30<01:29, 38.90it/s]Training CobwebTree:  54%|    | 4124/7600 [01:30<01:33, 37.07it/s]Training CobwebTree:  54%|    | 4129/7600 [01:30<01:29, 38.73it/s]Training CobwebTree:  54%|    | 4133/7600 [01:31<01:29, 38.52it/s]Training CobwebTree:  54%|    | 4138/7600 [01:31<01:28, 39.15it/s]Training CobwebTree:  55%|    | 4142/7600 [01:31<01:28, 38.92it/s]Training CobwebTree:  55%|    | 4147/7600 [01:31<01:25, 40.44it/s]Training CobwebTree:  55%|    | 4152/7600 [01:31<01:29, 38.34it/s]Training CobwebTree:  55%|    | 4156/7600 [01:31<01:31, 37.70it/s]Training CobwebTree:  55%|    | 4160/7600 [01:31<01:31, 37.72it/s]Training CobwebTree:  55%|    | 4165/7600 [01:31<01:32, 37.31it/s]Training CobwebTree:  55%|    | 4169/7600 [01:31<01:34, 36.24it/s]Training CobwebTree:  55%|    | 4173/7600 [01:32<01:33, 36.69it/s]Training CobwebTree:  55%|    | 4178/7600 [01:32<01:28, 38.60it/s]Training CobwebTree:  55%|    | 4182/7600 [01:32<01:32, 37.05it/s]Training CobwebTree:  55%|    | 4186/7600 [01:32<01:30, 37.55it/s]Training CobwebTree:  55%|    | 4191/7600 [01:32<01:27, 39.15it/s]Training CobwebTree:  55%|    | 4195/7600 [01:32<01:27, 39.00it/s]Training CobwebTree:  55%|    | 4200/7600 [01:32<01:21, 41.48it/s]Training CobwebTree:  55%|    | 4205/7600 [01:32<01:25, 39.69it/s]Training CobwebTree:  55%|    | 4210/7600 [01:33<01:26, 39.07it/s]Training CobwebTree:  55%|    | 4215/7600 [01:33<01:25, 39.66it/s]Training CobwebTree:  56%|    | 4219/7600 [01:33<01:26, 39.17it/s]Training CobwebTree:  56%|    | 4223/7600 [01:33<01:27, 38.81it/s]Training CobwebTree:  56%|    | 4228/7600 [01:33<01:23, 40.53it/s]Training CobwebTree:  56%|    | 4233/7600 [01:33<01:23, 40.30it/s]Training CobwebTree:  56%|    | 4238/7600 [01:33<01:20, 41.92it/s]Training CobwebTree:  56%|    | 4243/7600 [01:33<01:22, 40.56it/s]Training CobwebTree:  56%|    | 4248/7600 [01:33<01:24, 39.66it/s]Training CobwebTree:  56%|    | 4252/7600 [01:34<01:32, 36.20it/s]Training CobwebTree:  56%|    | 4256/7600 [01:34<01:30, 37.03it/s]Training CobwebTree:  56%|    | 4260/7600 [01:34<01:30, 37.07it/s]Training CobwebTree:  56%|    | 4265/7600 [01:34<01:25, 38.89it/s]Training CobwebTree:  56%|    | 4270/7600 [01:34<01:24, 39.48it/s]Training CobwebTree:  56%|    | 4274/7600 [01:34<01:25, 38.78it/s]Training CobwebTree:  56%|    | 4278/7600 [01:34<01:25, 39.05it/s]Training CobwebTree:  56%|    | 4283/7600 [01:34<01:20, 41.37it/s]Training CobwebTree:  56%|    | 4288/7600 [01:35<01:21, 40.78it/s]Training CobwebTree:  56%|    | 4293/7600 [01:35<01:17, 42.52it/s]Training CobwebTree:  57%|    | 4298/7600 [01:35<01:24, 39.25it/s]Training CobwebTree:  57%|    | 4302/7600 [01:35<01:25, 38.44it/s]Training CobwebTree:  57%|    | 4307/7600 [01:35<01:20, 40.82it/s]Training CobwebTree:  57%|    | 4312/7600 [01:35<01:20, 40.65it/s]Training CobwebTree:  57%|    | 4317/7600 [01:35<01:23, 39.48it/s]Training CobwebTree:  57%|    | 4321/7600 [01:35<01:26, 38.05it/s]Training CobwebTree:  57%|    | 4325/7600 [01:35<01:27, 37.23it/s]Training CobwebTree:  57%|    | 4330/7600 [01:36<01:21, 39.91it/s]Training CobwebTree:  57%|    | 4335/7600 [01:36<01:21, 40.02it/s]Training CobwebTree:  57%|    | 4340/7600 [01:36<01:19, 40.99it/s]Training CobwebTree:  57%|    | 4345/7600 [01:36<01:21, 39.89it/s]Training CobwebTree:  57%|    | 4350/7600 [01:36<01:24, 38.46it/s]Training CobwebTree:  57%|    | 4355/7600 [01:36<01:20, 40.22it/s]Training CobwebTree:  57%|    | 4360/7600 [01:36<01:18, 41.35it/s]Training CobwebTree:  57%|    | 4365/7600 [01:36<01:20, 40.05it/s]Training CobwebTree:  57%|    | 4370/7600 [01:37<01:18, 41.34it/s]Training CobwebTree:  58%|    | 4375/7600 [01:37<01:17, 41.49it/s]Training CobwebTree:  58%|    | 4380/7600 [01:37<01:18, 40.97it/s]Training CobwebTree:  58%|    | 4385/7600 [01:37<01:22, 39.02it/s]Training CobwebTree:  58%|    | 4389/7600 [01:37<01:21, 39.23it/s]Training CobwebTree:  58%|    | 4393/7600 [01:37<01:23, 38.64it/s]Training CobwebTree:  58%|    | 4397/7600 [01:37<01:22, 38.86it/s]Training CobwebTree:  58%|    | 4401/7600 [01:37<01:28, 36.02it/s]Training CobwebTree:  58%|    | 4405/7600 [01:37<01:28, 36.30it/s]Training CobwebTree:  58%|    | 4409/7600 [01:38<01:30, 35.26it/s]Training CobwebTree:  58%|    | 4413/7600 [01:38<01:29, 35.63it/s]Training CobwebTree:  58%|    | 4418/7600 [01:38<01:25, 37.03it/s]Training CobwebTree:  58%|    | 4423/7600 [01:38<01:22, 38.71it/s]Training CobwebTree:  58%|    | 4427/7600 [01:38<01:22, 38.64it/s]Training CobwebTree:  58%|    | 4432/7600 [01:38<01:18, 40.42it/s]Training CobwebTree:  58%|    | 4437/7600 [01:38<01:18, 40.45it/s]Training CobwebTree:  58%|    | 4442/7600 [01:38<01:19, 39.92it/s]Training CobwebTree:  59%|    | 4447/7600 [01:39<01:17, 40.79it/s]Training CobwebTree:  59%|    | 4452/7600 [01:39<01:17, 40.65it/s]Training CobwebTree:  59%|    | 4457/7600 [01:39<01:18, 40.08it/s]Training CobwebTree:  59%|    | 4462/7600 [01:39<01:20, 39.05it/s]Training CobwebTree:  59%|    | 4466/7600 [01:39<01:20, 38.96it/s]Training CobwebTree:  59%|    | 4471/7600 [01:39<01:15, 41.68it/s]Training CobwebTree:  59%|    | 4476/7600 [01:39<01:15, 41.24it/s]Training CobwebTree:  59%|    | 4481/7600 [01:39<01:16, 40.70it/s]Training CobwebTree:  59%|    | 4486/7600 [01:40<01:16, 40.76it/s]Training CobwebTree:  59%|    | 4491/7600 [01:40<01:20, 38.82it/s]Training CobwebTree:  59%|    | 4496/7600 [01:40<01:17, 40.10it/s]Training CobwebTree:  59%|    | 4501/7600 [01:40<01:16, 40.43it/s]Training CobwebTree:  59%|    | 4506/7600 [01:40<01:19, 38.96it/s]Training CobwebTree:  59%|    | 4511/7600 [01:40<01:18, 39.18it/s]Training CobwebTree:  59%|    | 4515/7600 [01:40<01:19, 38.57it/s]Training CobwebTree:  59%|    | 4519/7600 [01:40<01:20, 38.22it/s]Training CobwebTree:  60%|    | 4523/7600 [01:40<01:21, 37.97it/s]Training CobwebTree:  60%|    | 4528/7600 [01:41<01:18, 39.36it/s]Training CobwebTree:  60%|    | 4532/7600 [01:41<01:20, 38.20it/s]Training CobwebTree:  60%|    | 4537/7600 [01:41<01:17, 39.28it/s]Training CobwebTree:  60%|    | 4542/7600 [01:41<01:13, 41.60it/s]Training CobwebTree:  60%|    | 4547/7600 [01:41<01:15, 40.26it/s]Training CobwebTree:  60%|    | 4552/7600 [01:41<01:19, 38.11it/s]Training CobwebTree:  60%|    | 4557/7600 [01:41<01:17, 39.39it/s]Training CobwebTree:  60%|    | 4562/7600 [01:41<01:16, 39.85it/s]Training CobwebTree:  60%|    | 4567/7600 [01:42<01:12, 41.68it/s]Training CobwebTree:  60%|    | 4572/7600 [01:42<01:16, 39.60it/s]Training CobwebTree:  60%|    | 4577/7600 [01:42<01:13, 41.22it/s]Training CobwebTree:  60%|    | 4582/7600 [01:42<01:17, 38.72it/s]Training CobwebTree:  60%|    | 4588/7600 [01:42<01:13, 40.90it/s]Training CobwebTree:  60%|    | 4593/7600 [01:42<01:15, 39.82it/s]Training CobwebTree:  60%|    | 4598/7600 [01:42<01:17, 38.55it/s]Training CobwebTree:  61%|    | 4603/7600 [01:42<01:15, 39.60it/s]Training CobwebTree:  61%|    | 4607/7600 [01:43<01:18, 38.30it/s]Training CobwebTree:  61%|    | 4611/7600 [01:43<01:21, 36.81it/s]Training CobwebTree:  61%|    | 4615/7600 [01:43<01:20, 37.15it/s]Training CobwebTree:  61%|    | 4619/7600 [01:43<01:19, 37.58it/s]Training CobwebTree:  61%|    | 4624/7600 [01:43<01:15, 39.47it/s]Training CobwebTree:  61%|    | 4629/7600 [01:43<01:13, 40.69it/s]Training CobwebTree:  61%|    | 4634/7600 [01:43<01:12, 41.15it/s]Training CobwebTree:  61%|    | 4639/7600 [01:43<01:11, 41.30it/s]Training CobwebTree:  61%|    | 4644/7600 [01:44<01:12, 40.69it/s]Training CobwebTree:  61%|    | 4649/7600 [01:44<01:15, 39.27it/s]Training CobwebTree:  61%|    | 4653/7600 [01:44<01:15, 39.05it/s]Training CobwebTree:  61%|   | 4658/7600 [01:44<01:12, 40.82it/s]Training CobwebTree:  61%|   | 4663/7600 [01:44<01:13, 40.08it/s]Training CobwebTree:  61%|   | 4668/7600 [01:44<01:13, 39.84it/s]Training CobwebTree:  61%|   | 4672/7600 [01:44<01:13, 39.88it/s]Training CobwebTree:  62%|   | 4676/7600 [01:44<01:14, 39.03it/s]Training CobwebTree:  62%|   | 4681/7600 [01:44<01:15, 38.79it/s]Training CobwebTree:  62%|   | 4686/7600 [01:45<01:13, 39.42it/s]Training CobwebTree:  62%|   | 4690/7600 [01:45<01:14, 38.91it/s]Training CobwebTree:  62%|   | 4694/7600 [01:45<01:15, 38.27it/s]Training CobwebTree:  62%|   | 4699/7600 [01:45<01:14, 38.87it/s]Training CobwebTree:  62%|   | 4703/7600 [01:45<01:18, 36.86it/s]Training CobwebTree:  62%|   | 4707/7600 [01:45<01:18, 37.03it/s]Training CobwebTree:  62%|   | 4712/7600 [01:45<01:14, 38.81it/s]Training CobwebTree:  62%|   | 4717/7600 [01:45<01:12, 39.91it/s]Training CobwebTree:  62%|   | 4722/7600 [01:46<01:11, 40.41it/s]Training CobwebTree:  62%|   | 4727/7600 [01:46<01:09, 41.22it/s]Training CobwebTree:  62%|   | 4732/7600 [01:46<01:11, 40.04it/s]Training CobwebTree:  62%|   | 4737/7600 [01:46<01:12, 39.69it/s]Training CobwebTree:  62%|   | 4742/7600 [01:46<01:11, 40.15it/s]Training CobwebTree:  62%|   | 4747/7600 [01:46<01:14, 38.46it/s]Training CobwebTree:  63%|   | 4752/7600 [01:46<01:13, 38.64it/s]Training CobwebTree:  63%|   | 4756/7600 [01:46<01:14, 38.38it/s]Training CobwebTree:  63%|   | 4760/7600 [01:47<01:14, 38.17it/s]Training CobwebTree:  63%|   | 4765/7600 [01:47<01:13, 38.74it/s]Training CobwebTree:  63%|   | 4770/7600 [01:47<01:11, 39.37it/s]Training CobwebTree:  63%|   | 4775/7600 [01:47<01:09, 40.89it/s]Training CobwebTree:  63%|   | 4780/7600 [01:47<01:06, 42.12it/s]Training CobwebTree:  63%|   | 4785/7600 [01:47<01:08, 40.83it/s]Training CobwebTree:  63%|   | 4790/7600 [01:47<01:10, 39.95it/s]Training CobwebTree:  63%|   | 4795/7600 [01:47<01:14, 37.86it/s]Training CobwebTree:  63%|   | 4799/7600 [01:47<01:14, 37.81it/s]Training CobwebTree:  63%|   | 4803/7600 [01:48<01:14, 37.79it/s]Training CobwebTree:  63%|   | 4807/7600 [01:48<01:13, 37.98it/s]Training CobwebTree:  63%|   | 4811/7600 [01:48<01:13, 38.05it/s]Training CobwebTree:  63%|   | 4815/7600 [01:48<01:14, 37.60it/s]Training CobwebTree:  63%|   | 4820/7600 [01:48<01:13, 37.94it/s]Training CobwebTree:  63%|   | 4825/7600 [01:48<01:12, 38.29it/s]Training CobwebTree:  64%|   | 4829/7600 [01:48<01:13, 37.67it/s]Training CobwebTree:  64%|   | 4833/7600 [01:48<01:16, 36.35it/s]Training CobwebTree:  64%|   | 4837/7600 [01:49<01:14, 36.96it/s]Training CobwebTree:  64%|   | 4841/7600 [01:49<02:17, 20.09it/s]Training CobwebTree:  64%|   | 4846/7600 [01:49<01:50, 24.91it/s]Training CobwebTree:  64%|   | 4850/7600 [01:49<01:40, 27.45it/s]Training CobwebTree:  64%|   | 4854/7600 [01:49<01:32, 29.81it/s]Training CobwebTree:  64%|   | 4858/7600 [01:49<01:31, 30.04it/s]Training CobwebTree:  64%|   | 4863/7600 [01:50<01:23, 32.86it/s]Training CobwebTree:  64%|   | 4867/7600 [01:50<01:22, 33.28it/s]Training CobwebTree:  64%|   | 4871/7600 [01:50<01:22, 33.25it/s]Training CobwebTree:  64%|   | 4875/7600 [01:50<01:18, 34.66it/s]Training CobwebTree:  64%|   | 4879/7600 [01:50<01:17, 35.02it/s]Training CobwebTree:  64%|   | 4883/7600 [01:50<01:17, 35.03it/s]Training CobwebTree:  64%|   | 4887/7600 [01:50<01:14, 36.21it/s]Training CobwebTree:  64%|   | 4891/7600 [01:50<01:13, 36.72it/s]Training CobwebTree:  64%|   | 4895/7600 [01:50<01:12, 37.47it/s]Training CobwebTree:  64%|   | 4899/7600 [01:50<01:12, 37.50it/s]Training CobwebTree:  65%|   | 4904/7600 [01:51<01:09, 38.62it/s]Training CobwebTree:  65%|   | 4908/7600 [01:51<01:11, 37.52it/s]Training CobwebTree:  65%|   | 4913/7600 [01:51<01:08, 39.26it/s]Training CobwebTree:  65%|   | 4918/7600 [01:51<01:05, 40.88it/s]Training CobwebTree:  65%|   | 4923/7600 [01:51<01:08, 39.11it/s]Training CobwebTree:  65%|   | 4928/7600 [01:51<01:10, 38.16it/s]Training CobwebTree:  65%|   | 4932/7600 [01:51<01:12, 36.63it/s]Training CobwebTree:  65%|   | 4936/7600 [01:51<01:11, 37.48it/s]Training CobwebTree:  65%|   | 4940/7600 [01:52<01:10, 37.93it/s]Training CobwebTree:  65%|   | 4945/7600 [01:52<01:08, 38.83it/s]Training CobwebTree:  65%|   | 4949/7600 [01:52<01:08, 38.77it/s]Training CobwebTree:  65%|   | 4954/7600 [01:52<01:07, 38.94it/s]Training CobwebTree:  65%|   | 4958/7600 [01:52<01:07, 39.16it/s]Training CobwebTree:  65%|   | 4962/7600 [01:52<01:11, 37.14it/s]Training CobwebTree:  65%|   | 4967/7600 [01:52<01:08, 38.51it/s]Training CobwebTree:  65%|   | 4971/7600 [01:52<01:08, 38.37it/s]Training CobwebTree:  65%|   | 4975/7600 [01:52<01:08, 38.33it/s]Training CobwebTree:  66%|   | 4980/7600 [01:53<01:06, 39.41it/s]Training CobwebTree:  66%|   | 4984/7600 [01:53<01:06, 39.47it/s]Training CobwebTree:  66%|   | 4988/7600 [01:53<01:07, 38.51it/s]Training CobwebTree:  66%|   | 4993/7600 [01:53<01:06, 39.34it/s]Training CobwebTree:  66%|   | 4997/7600 [01:53<01:05, 39.48it/s]Training CobwebTree:  66%|   | 5002/7600 [01:53<01:04, 40.02it/s]Training CobwebTree:  66%|   | 5006/7600 [01:53<01:07, 38.36it/s]Training CobwebTree:  66%|   | 5010/7600 [01:53<01:06, 38.77it/s]Training CobwebTree:  66%|   | 5015/7600 [01:53<01:02, 41.55it/s]Training CobwebTree:  66%|   | 5020/7600 [01:54<01:06, 38.87it/s]Training CobwebTree:  66%|   | 5025/7600 [01:54<01:03, 40.48it/s]Training CobwebTree:  66%|   | 5030/7600 [01:54<01:03, 40.17it/s]Training CobwebTree:  66%|   | 5035/7600 [01:54<01:04, 39.53it/s]Training CobwebTree:  66%|   | 5039/7600 [01:54<01:05, 39.25it/s]Training CobwebTree:  66%|   | 5043/7600 [01:54<01:05, 38.93it/s]Training CobwebTree:  66%|   | 5047/7600 [01:54<01:06, 38.40it/s]Training CobwebTree:  66%|   | 5051/7600 [01:54<01:06, 38.13it/s]Training CobwebTree:  67%|   | 5056/7600 [01:55<01:04, 39.49it/s]Training CobwebTree:  67%|   | 5060/7600 [01:55<01:08, 37.13it/s]Training CobwebTree:  67%|   | 5064/7600 [01:55<01:08, 36.81it/s]Training CobwebTree:  67%|   | 5068/7600 [01:55<01:10, 35.76it/s]Training CobwebTree:  67%|   | 5073/7600 [01:55<01:07, 37.57it/s]Training CobwebTree:  67%|   | 5077/7600 [01:55<01:06, 37.66it/s]Training CobwebTree:  67%|   | 5082/7600 [01:55<01:02, 40.44it/s]Training CobwebTree:  67%|   | 5087/7600 [01:55<01:02, 39.93it/s]Training CobwebTree:  67%|   | 5092/7600 [01:55<01:03, 39.24it/s]Training CobwebTree:  67%|   | 5096/7600 [01:56<01:04, 38.66it/s]Training CobwebTree:  67%|   | 5101/7600 [01:56<01:01, 40.49it/s]Training CobwebTree:  67%|   | 5106/7600 [01:56<01:04, 38.41it/s]Training CobwebTree:  67%|   | 5110/7600 [01:56<01:06, 37.64it/s]Training CobwebTree:  67%|   | 5114/7600 [01:56<01:05, 37.89it/s]Training CobwebTree:  67%|   | 5119/7600 [01:56<01:03, 38.97it/s]Training CobwebTree:  67%|   | 5123/7600 [01:56<01:03, 38.96it/s]Training CobwebTree:  67%|   | 5127/7600 [01:56<01:03, 38.76it/s]Training CobwebTree:  68%|   | 5132/7600 [01:56<01:02, 39.72it/s]Training CobwebTree:  68%|   | 5136/7600 [01:57<01:02, 39.38it/s]Training CobwebTree:  68%|   | 5140/7600 [01:57<01:03, 38.95it/s]Training CobwebTree:  68%|   | 5144/7600 [01:57<01:03, 38.77it/s]Training CobwebTree:  68%|   | 5149/7600 [01:57<01:02, 39.35it/s]Training CobwebTree:  68%|   | 5153/7600 [01:57<01:02, 38.94it/s]Training CobwebTree:  68%|   | 5157/7600 [01:57<01:03, 38.36it/s]Training CobwebTree:  68%|   | 5162/7600 [01:57<01:00, 40.26it/s]Training CobwebTree:  68%|   | 5167/7600 [01:57<01:02, 39.21it/s]Training CobwebTree:  68%|   | 5171/7600 [01:57<01:01, 39.20it/s]Training CobwebTree:  68%|   | 5176/7600 [01:58<01:01, 39.74it/s]Training CobwebTree:  68%|   | 5180/7600 [01:58<01:01, 39.33it/s]Training CobwebTree:  68%|   | 5184/7600 [01:58<01:03, 38.33it/s]Training CobwebTree:  68%|   | 5188/7600 [01:58<01:02, 38.46it/s]Training CobwebTree:  68%|   | 5193/7600 [01:58<01:00, 39.51it/s]Training CobwebTree:  68%|   | 5198/7600 [01:58<01:01, 39.25it/s]Training CobwebTree:  68%|   | 5202/7600 [01:58<01:02, 38.25it/s]Training CobwebTree:  68%|   | 5206/7600 [01:58<01:02, 38.33it/s]Training CobwebTree:  69%|   | 5210/7600 [01:59<01:05, 36.51it/s]Training CobwebTree:  69%|   | 5214/7600 [01:59<01:03, 37.29it/s]Training CobwebTree:  69%|   | 5218/7600 [01:59<01:02, 38.01it/s]Training CobwebTree:  69%|   | 5223/7600 [01:59<01:00, 39.52it/s]Training CobwebTree:  69%|   | 5228/7600 [01:59<00:59, 40.00it/s]Training CobwebTree:  69%|   | 5233/7600 [01:59<00:58, 40.14it/s]Training CobwebTree:  69%|   | 5238/7600 [01:59<00:58, 40.29it/s]Training CobwebTree:  69%|   | 5243/7600 [01:59<00:56, 41.37it/s]Training CobwebTree:  69%|   | 5248/7600 [01:59<00:55, 42.46it/s]Training CobwebTree:  69%|   | 5253/7600 [02:00<00:56, 41.86it/s]Training CobwebTree:  69%|   | 5258/7600 [02:00<00:57, 40.79it/s]Training CobwebTree:  69%|   | 5263/7600 [02:00<00:56, 41.28it/s]Training CobwebTree:  69%|   | 5268/7600 [02:00<00:57, 40.57it/s]Training CobwebTree:  69%|   | 5273/7600 [02:00<00:56, 41.04it/s]Training CobwebTree:  69%|   | 5278/7600 [02:00<00:55, 42.09it/s]Training CobwebTree:  70%|   | 5283/7600 [02:00<00:59, 39.16it/s]Training CobwebTree:  70%|   | 5287/7600 [02:00<00:59, 38.71it/s]Training CobwebTree:  70%|   | 5292/7600 [02:01<00:56, 40.70it/s]Training CobwebTree:  70%|   | 5297/7600 [02:01<00:57, 40.28it/s]Training CobwebTree:  70%|   | 5302/7600 [02:01<00:58, 39.15it/s]Training CobwebTree:  70%|   | 5307/7600 [02:01<00:55, 41.12it/s]Training CobwebTree:  70%|   | 5312/7600 [02:01<00:55, 41.43it/s]Training CobwebTree:  70%|   | 5317/7600 [02:01<00:54, 41.83it/s]Training CobwebTree:  70%|   | 5322/7600 [02:01<00:56, 40.24it/s]Training CobwebTree:  70%|   | 5327/7600 [02:01<00:56, 40.05it/s]Training CobwebTree:  70%|   | 5332/7600 [02:02<00:56, 40.07it/s]Training CobwebTree:  70%|   | 5337/7600 [02:02<00:57, 39.64it/s]Training CobwebTree:  70%|   | 5342/7600 [02:02<00:55, 40.50it/s]Training CobwebTree:  70%|   | 5347/7600 [02:02<00:58, 38.69it/s]Training CobwebTree:  70%|   | 5352/7600 [02:02<00:57, 39.34it/s]Training CobwebTree:  70%|   | 5357/7600 [02:02<00:53, 41.66it/s]Training CobwebTree:  71%|   | 5362/7600 [02:02<00:55, 40.08it/s]Training CobwebTree:  71%|   | 5367/7600 [02:02<00:57, 38.90it/s]Training CobwebTree:  71%|   | 5371/7600 [02:03<00:58, 38.11it/s]Training CobwebTree:  71%|   | 5375/7600 [02:03<00:58, 38.08it/s]Training CobwebTree:  71%|   | 5379/7600 [02:03<00:59, 37.39it/s]Training CobwebTree:  71%|   | 5384/7600 [02:03<00:55, 39.60it/s]Training CobwebTree:  71%|   | 5388/7600 [02:03<00:56, 39.25it/s]Training CobwebTree:  71%|   | 5392/7600 [02:03<00:56, 39.40it/s]Training CobwebTree:  71%|   | 5396/7600 [02:03<00:56, 39.28it/s]Training CobwebTree:  71%|   | 5400/7600 [02:03<00:56, 38.98it/s]Training CobwebTree:  71%|   | 5404/7600 [02:03<00:57, 38.19it/s]Training CobwebTree:  71%|   | 5408/7600 [02:03<00:57, 37.89it/s]Training CobwebTree:  71%|   | 5412/7600 [02:04<00:59, 36.96it/s]Training CobwebTree:  71%|  | 5417/7600 [02:04<00:57, 38.22it/s]Training CobwebTree:  71%|  | 5421/7600 [02:04<00:58, 36.96it/s]Training CobwebTree:  71%|  | 5425/7600 [02:04<00:58, 37.13it/s]Training CobwebTree:  71%|  | 5429/7600 [02:04<00:58, 37.38it/s]Training CobwebTree:  71%|  | 5433/7600 [02:04<00:57, 37.95it/s]Training CobwebTree:  72%|  | 5438/7600 [02:04<00:55, 38.70it/s]Training CobwebTree:  72%|  | 5443/7600 [02:04<00:54, 39.44it/s]Training CobwebTree:  72%|  | 5447/7600 [02:04<00:57, 37.65it/s]Training CobwebTree:  72%|  | 5452/7600 [02:05<00:55, 38.66it/s]Training CobwebTree:  72%|  | 5456/7600 [02:05<00:57, 37.42it/s]Training CobwebTree:  72%|  | 5461/7600 [02:05<00:54, 38.94it/s]Training CobwebTree:  72%|  | 5465/7600 [02:05<00:54, 38.88it/s]Training CobwebTree:  72%|  | 5469/7600 [02:05<00:56, 37.76it/s]Training CobwebTree:  72%|  | 5474/7600 [02:05<00:54, 38.73it/s]Training CobwebTree:  72%|  | 5478/7600 [02:05<00:55, 38.03it/s]Training CobwebTree:  72%|  | 5483/7600 [02:05<00:53, 39.79it/s]Training CobwebTree:  72%|  | 5487/7600 [02:06<00:53, 39.23it/s]Training CobwebTree:  72%|  | 5491/7600 [02:06<00:54, 38.73it/s]Training CobwebTree:  72%|  | 5495/7600 [02:06<00:55, 38.02it/s]Training CobwebTree:  72%|  | 5499/7600 [02:06<00:56, 37.48it/s]Training CobwebTree:  72%|  | 5504/7600 [02:06<00:53, 39.00it/s]Training CobwebTree:  72%|  | 5509/7600 [02:06<00:52, 39.48it/s]Training CobwebTree:  73%|  | 5514/7600 [02:06<00:50, 40.92it/s]Training CobwebTree:  73%|  | 5519/7600 [02:06<00:52, 39.72it/s]Training CobwebTree:  73%|  | 5524/7600 [02:06<00:52, 39.58it/s]Training CobwebTree:  73%|  | 5529/7600 [02:07<00:52, 39.45it/s]Training CobwebTree:  73%|  | 5533/7600 [02:07<00:56, 36.77it/s]Training CobwebTree:  73%|  | 5538/7600 [02:07<00:54, 37.96it/s]Training CobwebTree:  73%|  | 5543/7600 [02:07<00:52, 39.30it/s]Training CobwebTree:  73%|  | 5547/7600 [02:07<00:52, 39.18it/s]Training CobwebTree:  73%|  | 5552/7600 [02:07<00:50, 40.46it/s]Training CobwebTree:  73%|  | 5557/7600 [02:07<00:50, 40.78it/s]Training CobwebTree:  73%|  | 5562/7600 [02:07<00:50, 39.98it/s]Training CobwebTree:  73%|  | 5567/7600 [02:08<00:51, 39.66it/s]Training CobwebTree:  73%|  | 5571/7600 [02:08<00:51, 39.21it/s]Training CobwebTree:  73%|  | 5576/7600 [02:08<00:50, 40.17it/s]Training CobwebTree:  73%|  | 5581/7600 [02:08<00:50, 40.13it/s]Training CobwebTree:  74%|  | 5586/7600 [02:08<00:50, 39.75it/s]Training CobwebTree:  74%|  | 5590/7600 [02:08<00:51, 38.72it/s]Training CobwebTree:  74%|  | 5595/7600 [02:08<00:50, 39.68it/s]Training CobwebTree:  74%|  | 5599/7600 [02:08<00:51, 39.11it/s]Training CobwebTree:  74%|  | 5604/7600 [02:08<00:50, 39.69it/s]Training CobwebTree:  74%|  | 5609/7600 [02:09<00:50, 39.53it/s]Training CobwebTree:  74%|  | 5614/7600 [02:09<00:49, 40.45it/s]Training CobwebTree:  74%|  | 5619/7600 [02:09<00:49, 39.95it/s]Training CobwebTree:  74%|  | 5623/7600 [02:09<00:52, 38.00it/s]Training CobwebTree:  74%|  | 5628/7600 [02:09<00:50, 38.87it/s]Training CobwebTree:  74%|  | 5632/7600 [02:09<00:50, 38.95it/s]Training CobwebTree:  74%|  | 5636/7600 [02:09<00:51, 37.77it/s]Training CobwebTree:  74%|  | 5641/7600 [02:09<00:50, 38.95it/s]Training CobwebTree:  74%|  | 5646/7600 [02:10<00:48, 40.00it/s]Training CobwebTree:  74%|  | 5650/7600 [02:10<00:49, 39.52it/s]Training CobwebTree:  74%|  | 5654/7600 [02:10<00:50, 38.47it/s]Training CobwebTree:  74%|  | 5658/7600 [02:10<00:51, 37.85it/s]Training CobwebTree:  75%|  | 5663/7600 [02:10<00:48, 40.09it/s]Training CobwebTree:  75%|  | 5668/7600 [02:10<00:51, 37.66it/s]Training CobwebTree:  75%|  | 5673/7600 [02:10<00:48, 39.56it/s]Training CobwebTree:  75%|  | 5677/7600 [02:10<00:50, 38.22it/s]Training CobwebTree:  75%|  | 5681/7600 [02:10<00:50, 38.18it/s]Training CobwebTree:  75%|  | 5685/7600 [02:11<00:51, 37.35it/s]Training CobwebTree:  75%|  | 5689/7600 [02:11<00:50, 38.01it/s]Training CobwebTree:  75%|  | 5694/7600 [02:11<00:48, 38.94it/s]Training CobwebTree:  75%|  | 5699/7600 [02:11<00:47, 39.85it/s]Training CobwebTree:  75%|  | 5704/7600 [02:11<00:46, 40.53it/s]Training CobwebTree:  75%|  | 5709/7600 [02:11<00:48, 39.23it/s]Training CobwebTree:  75%|  | 5713/7600 [02:11<00:49, 37.87it/s]Training CobwebTree:  75%|  | 5718/7600 [02:11<00:46, 40.25it/s]Training CobwebTree:  75%|  | 5723/7600 [02:12<00:46, 40.59it/s]Training CobwebTree:  75%|  | 5728/7600 [02:12<00:47, 39.79it/s]Training CobwebTree:  75%|  | 5732/7600 [02:12<00:47, 39.01it/s]Training CobwebTree:  75%|  | 5736/7600 [02:12<00:50, 37.26it/s]Training CobwebTree:  76%|  | 5740/7600 [02:12<00:49, 37.23it/s]Training CobwebTree:  76%|  | 5744/7600 [02:12<00:49, 37.72it/s]Training CobwebTree:  76%|  | 5748/7600 [02:12<00:49, 37.32it/s]Training CobwebTree:  76%|  | 5752/7600 [02:12<00:49, 37.33it/s]Training CobwebTree:  76%|  | 5757/7600 [02:12<00:47, 38.89it/s]Training CobwebTree:  76%|  | 5762/7600 [02:13<00:46, 39.59it/s]Training CobwebTree:  76%|  | 5766/7600 [02:13<00:46, 39.12it/s]Training CobwebTree:  76%|  | 5771/7600 [02:13<00:46, 39.23it/s]Training CobwebTree:  76%|  | 5775/7600 [02:13<00:47, 38.02it/s]Training CobwebTree:  76%|  | 5779/7600 [02:13<00:48, 37.80it/s]Training CobwebTree:  76%|  | 5784/7600 [02:13<00:46, 38.66it/s]Training CobwebTree:  76%|  | 5788/7600 [02:13<00:47, 38.52it/s]Training CobwebTree:  76%|  | 5792/7600 [02:13<00:47, 37.90it/s]Training CobwebTree:  76%|  | 5797/7600 [02:13<00:45, 39.21it/s]Training CobwebTree:  76%|  | 5801/7600 [02:14<00:46, 38.54it/s]Training CobwebTree:  76%|  | 5806/7600 [02:14<00:45, 39.58it/s]Training CobwebTree:  76%|  | 5810/7600 [02:14<00:45, 39.21it/s]Training CobwebTree:  76%|  | 5814/7600 [02:14<00:46, 38.38it/s]Training CobwebTree:  77%|  | 5819/7600 [02:14<00:45, 38.72it/s]Training CobwebTree:  77%|  | 5823/7600 [02:14<00:46, 38.38it/s]Training CobwebTree:  77%|  | 5827/7600 [02:14<00:45, 38.62it/s]Training CobwebTree:  77%|  | 5832/7600 [02:14<00:44, 39.36it/s]Training CobwebTree:  77%|  | 5836/7600 [02:14<00:47, 36.85it/s]Training CobwebTree:  77%|  | 5840/7600 [02:15<00:48, 36.27it/s]Training CobwebTree:  77%|  | 5844/7600 [02:15<00:49, 35.48it/s]Training CobwebTree:  77%|  | 5849/7600 [02:15<00:46, 37.61it/s]Training CobwebTree:  77%|  | 5853/7600 [02:15<00:47, 37.14it/s]Training CobwebTree:  77%|  | 5857/7600 [02:15<00:47, 36.34it/s]Training CobwebTree:  77%|  | 5862/7600 [02:15<00:46, 37.73it/s]Training CobwebTree:  77%|  | 5867/7600 [02:15<00:44, 39.19it/s]Training CobwebTree:  77%|  | 5871/7600 [02:15<00:46, 37.54it/s]Training CobwebTree:  77%|  | 5875/7600 [02:16<00:47, 36.33it/s]Training CobwebTree:  77%|  | 5879/7600 [02:16<00:48, 35.71it/s]Training CobwebTree:  77%|  | 5884/7600 [02:16<00:46, 37.28it/s]Training CobwebTree:  77%|  | 5888/7600 [02:16<00:46, 37.14it/s]Training CobwebTree:  78%|  | 5892/7600 [02:16<00:45, 37.77it/s]Training CobwebTree:  78%|  | 5896/7600 [02:16<00:46, 36.58it/s]Training CobwebTree:  78%|  | 5901/7600 [02:16<00:44, 38.38it/s]Training CobwebTree:  78%|  | 5905/7600 [02:16<00:44, 38.13it/s]Training CobwebTree:  78%|  | 5909/7600 [02:16<00:43, 38.54it/s]Training CobwebTree:  78%|  | 5913/7600 [02:17<00:44, 37.50it/s]Training CobwebTree:  78%|  | 5917/7600 [02:17<00:44, 37.66it/s]Training CobwebTree:  78%|  | 5921/7600 [02:17<00:44, 38.12it/s]Training CobwebTree:  78%|  | 5926/7600 [02:17<00:42, 39.23it/s]Training CobwebTree:  78%|  | 5930/7600 [02:17<00:43, 38.73it/s]Training CobwebTree:  78%|  | 5934/7600 [02:17<00:43, 37.99it/s]Training CobwebTree:  78%|  | 5938/7600 [02:17<00:43, 38.32it/s]Training CobwebTree:  78%|  | 5942/7600 [02:17<00:43, 38.22it/s]Training CobwebTree:  78%|  | 5947/7600 [02:17<00:42, 38.50it/s]Training CobwebTree:  78%|  | 5952/7600 [02:18<00:41, 39.88it/s]Training CobwebTree:  78%|  | 5957/7600 [02:18<00:41, 39.81it/s]Training CobwebTree:  78%|  | 5961/7600 [02:18<00:42, 38.13it/s]Training CobwebTree:  78%|  | 5965/7600 [02:18<00:42, 38.19it/s]Training CobwebTree:  79%|  | 5969/7600 [02:18<00:42, 38.40it/s]Training CobwebTree:  79%|  | 5973/7600 [02:18<00:43, 37.67it/s]Training CobwebTree:  79%|  | 5977/7600 [02:18<00:42, 38.17it/s]Training CobwebTree:  79%|  | 5982/7600 [02:18<00:42, 38.03it/s]Training CobwebTree:  79%|  | 5987/7600 [02:18<00:42, 37.95it/s]Training CobwebTree:  79%|  | 5991/7600 [02:19<00:43, 37.36it/s]Training CobwebTree:  79%|  | 5995/7600 [02:19<00:42, 37.76it/s]Training CobwebTree:  79%|  | 5999/7600 [02:19<00:42, 37.43it/s]Training CobwebTree:  79%|  | 6004/7600 [02:19<00:41, 38.33it/s]Training CobwebTree:  79%|  | 6008/7600 [02:19<00:43, 36.68it/s]Training CobwebTree:  79%|  | 6012/7600 [02:19<00:42, 37.19it/s]Training CobwebTree:  79%|  | 6016/7600 [02:19<00:42, 37.57it/s]Training CobwebTree:  79%|  | 6020/7600 [02:19<00:42, 37.06it/s]Training CobwebTree:  79%|  | 6024/7600 [02:19<00:42, 37.00it/s]Training CobwebTree:  79%|  | 6028/7600 [02:20<00:41, 37.81it/s]Training CobwebTree:  79%|  | 6032/7600 [02:20<00:43, 36.18it/s]Training CobwebTree:  79%|  | 6036/7600 [02:20<00:43, 36.36it/s]Training CobwebTree:  79%|  | 6040/7600 [02:20<00:43, 36.19it/s]Training CobwebTree:  80%|  | 6045/7600 [02:20<00:41, 37.71it/s]Training CobwebTree:  80%|  | 6049/7600 [02:20<00:42, 36.61it/s]Training CobwebTree:  80%|  | 6053/7600 [02:20<00:42, 36.49it/s]Training CobwebTree:  80%|  | 6057/7600 [02:20<00:42, 36.35it/s]Training CobwebTree:  80%|  | 6061/7600 [02:20<00:41, 36.69it/s]Training CobwebTree:  80%|  | 6065/7600 [02:21<00:40, 37.53it/s]Training CobwebTree:  80%|  | 6069/7600 [02:21<00:41, 36.50it/s]Training CobwebTree:  80%|  | 6073/7600 [02:21<00:42, 36.15it/s]Training CobwebTree:  80%|  | 6077/7600 [02:21<00:41, 36.79it/s]Training CobwebTree:  80%|  | 6081/7600 [02:21<00:40, 37.47it/s]Training CobwebTree:  80%|  | 6086/7600 [02:21<00:38, 39.47it/s]Training CobwebTree:  80%|  | 6091/7600 [02:21<00:36, 41.07it/s]Training CobwebTree:  80%|  | 6096/7600 [02:21<00:40, 37.39it/s]Training CobwebTree:  80%|  | 6100/7600 [02:22<00:41, 36.44it/s]Training CobwebTree:  80%|  | 6104/7600 [02:22<00:41, 35.67it/s]Training CobwebTree:  80%|  | 6109/7600 [02:22<00:40, 36.80it/s]Training CobwebTree:  80%|  | 6113/7600 [02:22<00:40, 36.71it/s]Training CobwebTree:  80%|  | 6117/7600 [02:22<00:39, 37.32it/s]Training CobwebTree:  81%|  | 6122/7600 [02:22<00:36, 40.25it/s]Training CobwebTree:  81%|  | 6127/7600 [02:22<00:36, 40.04it/s]Training CobwebTree:  81%|  | 6132/7600 [02:22<00:35, 41.12it/s]Training CobwebTree:  81%|  | 6137/7600 [02:22<00:36, 40.19it/s]Training CobwebTree:  81%|  | 6142/7600 [02:23<00:36, 39.64it/s]Training CobwebTree:  81%|  | 6147/7600 [02:23<00:36, 40.32it/s]Training CobwebTree:  81%|  | 6152/7600 [02:23<00:35, 40.32it/s]Training CobwebTree:  81%|  | 6157/7600 [02:23<00:35, 40.97it/s]Training CobwebTree:  81%|  | 6162/7600 [02:23<00:36, 39.06it/s]Training CobwebTree:  81%|  | 6166/7600 [02:23<00:37, 38.64it/s]Training CobwebTree:  81%|  | 6170/7600 [02:23<00:36, 38.88it/s]Training CobwebTree:  81%|  | 6174/7600 [02:23<00:39, 36.19it/s]Training CobwebTree:  81%| | 6179/7600 [02:24<00:36, 38.73it/s]Training CobwebTree:  81%| | 6183/7600 [02:24<00:37, 37.38it/s]Training CobwebTree:  81%| | 6188/7600 [02:24<00:36, 38.29it/s]Training CobwebTree:  81%| | 6192/7600 [02:24<00:36, 38.33it/s]Training CobwebTree:  82%| | 6196/7600 [02:24<00:36, 38.37it/s]Training CobwebTree:  82%| | 6200/7600 [02:24<00:37, 37.20it/s]Training CobwebTree:  82%| | 6205/7600 [02:24<00:36, 38.68it/s]Training CobwebTree:  82%| | 6209/7600 [02:24<00:35, 38.74it/s]Training CobwebTree:  82%| | 6213/7600 [02:24<00:35, 38.99it/s]Training CobwebTree:  82%| | 6217/7600 [02:25<00:35, 39.02it/s]Training CobwebTree:  82%| | 6222/7600 [02:25<00:35, 39.14it/s]Training CobwebTree:  82%| | 6226/7600 [02:25<00:37, 37.07it/s]Training CobwebTree:  82%| | 6231/7600 [02:25<00:35, 38.94it/s]Training CobwebTree:  82%| | 6235/7600 [02:25<00:35, 38.66it/s]Training CobwebTree:  82%| | 6239/7600 [02:25<00:35, 38.32it/s]Training CobwebTree:  82%| | 6243/7600 [02:25<00:36, 37.26it/s]Training CobwebTree:  82%| | 6248/7600 [02:25<00:34, 38.67it/s]Training CobwebTree:  82%| | 6252/7600 [02:25<00:35, 38.28it/s]Training CobwebTree:  82%| | 6256/7600 [02:26<00:35, 37.38it/s]Training CobwebTree:  82%| | 6260/7600 [02:26<00:37, 36.02it/s]Training CobwebTree:  82%| | 6264/7600 [02:26<00:36, 37.03it/s]Training CobwebTree:  82%| | 6268/7600 [02:26<00:36, 36.85it/s]Training CobwebTree:  83%| | 6272/7600 [02:26<00:35, 37.12it/s]Training CobwebTree:  83%| | 6276/7600 [02:26<00:35, 37.06it/s]Training CobwebTree:  83%| | 6280/7600 [02:26<00:36, 36.48it/s]Training CobwebTree:  83%| | 6284/7600 [02:26<00:35, 36.99it/s]Training CobwebTree:  83%| | 6288/7600 [02:26<00:36, 35.87it/s]Training CobwebTree:  83%| | 6292/7600 [02:27<00:38, 34.11it/s]Training CobwebTree:  83%| | 6296/7600 [02:27<00:37, 34.63it/s]Training CobwebTree:  83%| | 6300/7600 [02:27<00:37, 34.44it/s]Training CobwebTree:  83%| | 6304/7600 [02:27<00:37, 34.52it/s]Training CobwebTree:  83%| | 6308/7600 [02:27<00:37, 34.67it/s]Training CobwebTree:  83%| | 6312/7600 [02:27<00:37, 34.08it/s]Training CobwebTree:  83%| | 6317/7600 [02:27<00:35, 36.48it/s]Training CobwebTree:  83%| | 6322/7600 [02:27<00:35, 36.39it/s]Training CobwebTree:  83%| | 6326/7600 [02:28<00:34, 36.94it/s]Training CobwebTree:  83%| | 6330/7600 [02:28<00:33, 37.36it/s]Training CobwebTree:  83%| | 6334/7600 [02:28<00:34, 37.17it/s]Training CobwebTree:  83%| | 6339/7600 [02:28<00:32, 38.45it/s]Training CobwebTree:  83%| | 6343/7600 [02:28<00:33, 37.03it/s]Training CobwebTree:  84%| | 6347/7600 [02:28<00:35, 35.77it/s]Training CobwebTree:  84%| | 6351/7600 [02:28<00:34, 36.30it/s]Training CobwebTree:  84%| | 6356/7600 [02:28<00:33, 37.59it/s]Training CobwebTree:  84%| | 6360/7600 [02:28<00:33, 36.55it/s]Training CobwebTree:  84%| | 6364/7600 [02:29<00:33, 37.08it/s]Training CobwebTree:  84%| | 6369/7600 [02:29<00:32, 38.08it/s]Training CobwebTree:  84%| | 6373/7600 [02:29<00:32, 38.07it/s]Training CobwebTree:  84%| | 6377/7600 [02:29<00:31, 38.43it/s]Training CobwebTree:  84%| | 6381/7600 [02:29<00:32, 37.80it/s]Training CobwebTree:  84%| | 6385/7600 [02:29<00:31, 37.99it/s]Training CobwebTree:  84%| | 6389/7600 [02:29<00:34, 35.08it/s]Training CobwebTree:  84%| | 6393/7600 [02:29<00:33, 35.61it/s]Training CobwebTree:  84%| | 6398/7600 [02:29<00:31, 37.56it/s]Training CobwebTree:  84%| | 6402/7600 [02:30<00:32, 37.21it/s]Training CobwebTree:  84%| | 6407/7600 [02:30<00:30, 38.54it/s]Training CobwebTree:  84%| | 6411/7600 [02:30<00:31, 37.24it/s]Training CobwebTree:  84%| | 6416/7600 [02:30<00:30, 38.64it/s]Training CobwebTree:  84%| | 6421/7600 [02:30<00:30, 39.24it/s]Training CobwebTree:  85%| | 6426/7600 [02:30<00:29, 39.49it/s]Training CobwebTree:  85%| | 6431/7600 [02:30<00:29, 39.28it/s]Training CobwebTree:  85%| | 6436/7600 [02:30<00:29, 39.52it/s]Training CobwebTree:  85%| | 6441/7600 [02:31<00:28, 40.04it/s]Training CobwebTree:  85%| | 6446/7600 [02:31<00:29, 39.11it/s]Training CobwebTree:  85%| | 6450/7600 [02:31<00:29, 39.23it/s]Training CobwebTree:  85%| | 6455/7600 [02:31<00:29, 39.43it/s]Training CobwebTree:  85%| | 6460/7600 [02:31<00:28, 40.70it/s]Training CobwebTree:  85%| | 6465/7600 [02:31<00:29, 38.18it/s]Training CobwebTree:  85%| | 6469/7600 [02:31<00:30, 37.57it/s]Training CobwebTree:  85%| | 6473/7600 [02:31<00:30, 36.54it/s]Training CobwebTree:  85%| | 6477/7600 [02:32<00:31, 36.06it/s]Training CobwebTree:  85%| | 6482/7600 [02:32<00:29, 37.40it/s]Training CobwebTree:  85%| | 6486/7600 [02:32<00:29, 37.83it/s]Training CobwebTree:  85%| | 6491/7600 [02:32<00:28, 39.12it/s]Training CobwebTree:  85%| | 6495/7600 [02:32<00:29, 37.03it/s]Training CobwebTree:  86%| | 6500/7600 [02:32<00:28, 39.10it/s]Training CobwebTree:  86%| | 6504/7600 [02:32<00:29, 37.56it/s]Training CobwebTree:  86%| | 6508/7600 [02:32<00:28, 37.82it/s]Training CobwebTree:  86%| | 6513/7600 [02:32<00:27, 39.99it/s]Training CobwebTree:  86%| | 6518/7600 [02:33<00:27, 40.04it/s]Training CobwebTree:  86%| | 6523/7600 [02:33<00:27, 39.34it/s]Training CobwebTree:  86%| | 6528/7600 [02:33<00:26, 40.16it/s]Training CobwebTree:  86%| | 6533/7600 [02:33<00:27, 38.41it/s]Training CobwebTree:  86%| | 6537/7600 [02:33<00:27, 38.81it/s]Training CobwebTree:  86%| | 6541/7600 [02:33<00:28, 37.59it/s]Training CobwebTree:  86%| | 6546/7600 [02:33<00:27, 38.80it/s]Training CobwebTree:  86%| | 6550/7600 [02:33<00:27, 38.38it/s]Training CobwebTree:  86%| | 6555/7600 [02:34<00:26, 39.50it/s]Training CobwebTree:  86%| | 6559/7600 [02:34<00:27, 38.07it/s]Training CobwebTree:  86%| | 6564/7600 [02:34<00:26, 39.50it/s]Training CobwebTree:  86%| | 6568/7600 [02:34<00:27, 38.08it/s]Training CobwebTree:  86%| | 6572/7600 [02:34<00:26, 38.54it/s]Training CobwebTree:  87%| | 6576/7600 [02:34<00:27, 36.82it/s]Training CobwebTree:  87%| | 6580/7600 [02:34<00:27, 37.02it/s]Training CobwebTree:  87%| | 6585/7600 [02:34<00:26, 38.70it/s]Training CobwebTree:  87%| | 6589/7600 [02:34<00:26, 37.70it/s]Training CobwebTree:  87%| | 6593/7600 [02:35<00:26, 37.88it/s]Training CobwebTree:  87%| | 6597/7600 [02:35<00:26, 37.44it/s]Training CobwebTree:  87%| | 6601/7600 [02:35<00:27, 36.86it/s]Training CobwebTree:  87%| | 6606/7600 [02:35<00:26, 37.88it/s]Training CobwebTree:  87%| | 6610/7600 [02:35<00:25, 38.24it/s]Training CobwebTree:  87%| | 6614/7600 [02:35<00:27, 36.51it/s]Training CobwebTree:  87%| | 6618/7600 [02:35<00:26, 37.05it/s]Training CobwebTree:  87%| | 6622/7600 [02:35<00:26, 36.90it/s]Training CobwebTree:  87%| | 6626/7600 [02:35<00:26, 37.39it/s]Training CobwebTree:  87%| | 6630/7600 [02:36<00:26, 36.91it/s]Training CobwebTree:  87%| | 6634/7600 [02:36<00:26, 37.01it/s]Training CobwebTree:  87%| | 6639/7600 [02:36<00:24, 38.94it/s]Training CobwebTree:  87%| | 6643/7600 [02:36<00:24, 38.37it/s]Training CobwebTree:  87%| | 6647/7600 [02:36<00:25, 36.86it/s]Training CobwebTree:  88%| | 6652/7600 [02:36<00:24, 38.04it/s]Training CobwebTree:  88%| | 6656/7600 [02:36<00:25, 37.53it/s]Training CobwebTree:  88%| | 6660/7600 [02:36<00:24, 38.20it/s]Training CobwebTree:  88%| | 6665/7600 [02:36<00:23, 39.07it/s]Training CobwebTree:  88%| | 6669/7600 [02:37<00:24, 37.91it/s]Training CobwebTree:  88%| | 6673/7600 [02:37<00:24, 37.32it/s]Training CobwebTree:  88%| | 6677/7600 [02:37<00:25, 36.62it/s]Training CobwebTree:  88%| | 6681/7600 [02:37<00:25, 35.37it/s]Training CobwebTree:  88%| | 6686/7600 [02:37<00:24, 37.43it/s]Training CobwebTree:  88%| | 6691/7600 [02:37<00:23, 38.48it/s]Training CobwebTree:  88%| | 6696/7600 [02:37<00:22, 39.46it/s]Training CobwebTree:  88%| | 6700/7600 [02:37<00:23, 37.96it/s]Training CobwebTree:  88%| | 6705/7600 [02:37<00:22, 38.99it/s]Training CobwebTree:  88%| | 6709/7600 [02:38<00:24, 36.37it/s]Training CobwebTree:  88%| | 6714/7600 [02:38<00:23, 37.09it/s]Training CobwebTree:  88%| | 6719/7600 [02:38<00:23, 37.82it/s]Training CobwebTree:  88%| | 6723/7600 [02:38<00:23, 37.50it/s]Training CobwebTree:  89%| | 6727/7600 [02:38<00:22, 37.99it/s]Training CobwebTree:  89%| | 6731/7600 [02:38<00:24, 36.19it/s]Training CobwebTree:  89%| | 6735/7600 [02:38<00:24, 35.66it/s]Training CobwebTree:  89%| | 6739/7600 [02:38<00:23, 36.63it/s]Training CobwebTree:  89%| | 6743/7600 [02:39<00:23, 36.85it/s]Training CobwebTree:  89%| | 6747/7600 [02:39<00:23, 36.73it/s]Training CobwebTree:  89%| | 6751/7600 [02:39<00:22, 37.03it/s]Training CobwebTree:  89%| | 6755/7600 [02:39<00:22, 37.60it/s]Training CobwebTree:  89%| | 6759/7600 [02:39<00:21, 38.26it/s]Training CobwebTree:  89%| | 6763/7600 [02:39<00:22, 36.72it/s]Training CobwebTree:  89%| | 6767/7600 [02:39<00:22, 37.26it/s]Training CobwebTree:  89%| | 6771/7600 [02:39<00:22, 36.66it/s]Training CobwebTree:  89%| | 6775/7600 [02:39<00:22, 37.28it/s]Training CobwebTree:  89%| | 6780/7600 [02:40<00:21, 38.57it/s]Training CobwebTree:  89%| | 6784/7600 [02:40<00:22, 36.15it/s]Training CobwebTree:  89%| | 6789/7600 [02:40<00:21, 37.10it/s]Training CobwebTree:  89%| | 6794/7600 [02:40<00:21, 38.31it/s]Training CobwebTree:  89%| | 6798/7600 [02:40<00:20, 38.25it/s]Training CobwebTree:  90%| | 6803/7600 [02:40<00:20, 39.20it/s]Training CobwebTree:  90%| | 6807/7600 [02:40<00:20, 38.32it/s]Training CobwebTree:  90%| | 6811/7600 [02:40<00:20, 37.90it/s]Training CobwebTree:  90%| | 6815/7600 [02:40<00:20, 37.97it/s]Training CobwebTree:  90%| | 6819/7600 [02:41<00:21, 36.89it/s]Training CobwebTree:  90%| | 6823/7600 [02:41<00:20, 37.58it/s]Training CobwebTree:  90%| | 6827/7600 [02:41<00:20, 37.70it/s]Training CobwebTree:  90%| | 6832/7600 [02:41<00:20, 38.37it/s]Training CobwebTree:  90%| | 6837/7600 [02:41<00:19, 39.24it/s]Training CobwebTree:  90%| | 6841/7600 [02:41<00:20, 37.13it/s]Training CobwebTree:  90%| | 6845/7600 [02:41<00:21, 35.61it/s]Training CobwebTree:  90%| | 6849/7600 [02:41<00:20, 35.77it/s]Training CobwebTree:  90%| | 6854/7600 [02:41<00:19, 38.07it/s]Training CobwebTree:  90%| | 6858/7600 [02:42<00:19, 37.24it/s]Training CobwebTree:  90%| | 6862/7600 [02:42<00:20, 36.62it/s]Training CobwebTree:  90%| | 6866/7600 [02:42<00:20, 36.63it/s]Training CobwebTree:  90%| | 6870/7600 [02:42<00:19, 36.78it/s]Training CobwebTree:  90%| | 6874/7600 [02:42<00:20, 36.30it/s]Training CobwebTree:  90%| | 6878/7600 [02:42<00:19, 36.97it/s]Training CobwebTree:  91%| | 6882/7600 [02:42<00:19, 36.70it/s]Training CobwebTree:  91%| | 6887/7600 [02:42<00:17, 39.90it/s]Training CobwebTree:  91%| | 6892/7600 [02:42<00:18, 38.67it/s]Training CobwebTree:  91%| | 6896/7600 [02:43<00:18, 38.23it/s]Training CobwebTree:  91%| | 6900/7600 [02:43<00:18, 36.85it/s]Training CobwebTree:  91%| | 6904/7600 [02:43<00:18, 36.92it/s]Training CobwebTree:  91%| | 6908/7600 [02:43<00:18, 37.30it/s]Training CobwebTree:  91%| | 6912/7600 [02:43<00:18, 37.13it/s]Training CobwebTree:  91%| | 6917/7600 [02:43<00:18, 37.85it/s]Training CobwebTree:  91%| | 6922/7600 [02:43<00:17, 38.80it/s]Training CobwebTree:  91%| | 6926/7600 [02:43<00:17, 38.76it/s]Training CobwebTree:  91%| | 6930/7600 [02:44<00:17, 37.28it/s]Training CobwebTree:  91%| | 6934/7600 [02:44<00:17, 37.35it/s]Training CobwebTree:  91%|| 6938/7600 [02:44<00:17, 37.36it/s]Training CobwebTree:  91%|| 6942/7600 [02:44<00:17, 37.51it/s]Training CobwebTree:  91%|| 6946/7600 [02:44<00:17, 37.95it/s]Training CobwebTree:  91%|| 6950/7600 [02:44<00:17, 37.89it/s]Training CobwebTree:  92%|| 6955/7600 [02:44<00:16, 40.16it/s]Training CobwebTree:  92%|| 6960/7600 [02:44<00:16, 39.21it/s]Training CobwebTree:  92%|| 6964/7600 [02:44<00:16, 38.92it/s]Training CobwebTree:  92%|| 6968/7600 [02:44<00:16, 38.96it/s]Training CobwebTree:  92%|| 6972/7600 [02:45<00:16, 38.84it/s]Training CobwebTree:  92%|| 6976/7600 [02:45<00:17, 35.08it/s]Training CobwebTree:  92%|| 6980/7600 [02:45<00:17, 35.54it/s]Training CobwebTree:  92%|| 6985/7600 [02:45<00:15, 39.03it/s]Training CobwebTree:  92%|| 6989/7600 [02:45<00:16, 38.14it/s]Training CobwebTree:  92%|| 6993/7600 [02:45<00:15, 38.28it/s]Training CobwebTree:  92%|| 6997/7600 [02:45<00:16, 36.71it/s]Training CobwebTree:  92%|| 7002/7600 [02:45<00:16, 37.25it/s]Training CobwebTree:  92%|| 7006/7600 [02:46<00:16, 36.36it/s]Training CobwebTree:  92%|| 7010/7600 [02:46<00:16, 36.33it/s]Training CobwebTree:  92%|| 7015/7600 [02:46<00:15, 38.87it/s]Training CobwebTree:  92%|| 7020/7600 [02:46<00:14, 40.57it/s]Training CobwebTree:  92%|| 7025/7600 [02:46<00:14, 39.55it/s]Training CobwebTree:  92%|| 7030/7600 [02:46<00:14, 40.02it/s]Training CobwebTree:  93%|| 7035/7600 [02:46<00:14, 38.10it/s]Training CobwebTree:  93%|| 7039/7600 [02:46<00:14, 37.42it/s]Training CobwebTree:  93%|| 7044/7600 [02:46<00:13, 39.87it/s]Training CobwebTree:  93%|| 7049/7600 [02:47<00:13, 40.81it/s]Training CobwebTree:  93%|| 7054/7600 [02:47<00:13, 40.98it/s]Training CobwebTree:  93%|| 7059/7600 [02:47<00:13, 40.06it/s]Training CobwebTree:  93%|| 7064/7600 [02:47<00:14, 37.65it/s]Training CobwebTree:  93%|| 7069/7600 [02:47<00:14, 37.92it/s]Training CobwebTree:  93%|| 7073/7600 [02:47<00:14, 37.19it/s]Training CobwebTree:  93%|| 7077/7600 [02:47<00:13, 37.84it/s]Training CobwebTree:  93%|| 7081/7600 [02:47<00:13, 37.32it/s]Training CobwebTree:  93%|| 7086/7600 [02:48<00:13, 39.34it/s]Training CobwebTree:  93%|| 7090/7600 [02:48<00:13, 38.16it/s]Training CobwebTree:  93%|| 7094/7600 [02:48<00:13, 38.33it/s]Training CobwebTree:  93%|| 7098/7600 [02:48<00:13, 36.27it/s]Training CobwebTree:  93%|| 7103/7600 [02:48<00:13, 38.02it/s]Training CobwebTree:  94%|| 7107/7600 [02:48<00:13, 36.85it/s]Training CobwebTree:  94%|| 7111/7600 [02:48<00:13, 36.85it/s]Training CobwebTree:  94%|| 7115/7600 [02:48<00:12, 37.56it/s]Training CobwebTree:  94%|| 7120/7600 [02:48<00:12, 38.98it/s]Training CobwebTree:  94%|| 7124/7600 [02:49<00:12, 38.85it/s]Training CobwebTree:  94%|| 7128/7600 [02:49<00:12, 38.34it/s]Training CobwebTree:  94%|| 7132/7600 [02:49<00:12, 37.45it/s]Training CobwebTree:  94%|| 7136/7600 [02:49<00:12, 37.97it/s]Training CobwebTree:  94%|| 7140/7600 [02:49<00:12, 38.11it/s]Training CobwebTree:  94%|| 7144/7600 [02:49<00:12, 36.77it/s]Training CobwebTree:  94%|| 7148/7600 [02:49<00:12, 36.06it/s]Training CobwebTree:  94%|| 7152/7600 [02:49<00:12, 36.13it/s]Training CobwebTree:  94%|| 7156/7600 [02:49<00:12, 35.32it/s]Training CobwebTree:  94%|| 7160/7600 [02:50<00:12, 35.16it/s]Training CobwebTree:  94%|| 7164/7600 [02:50<00:12, 35.33it/s]Training CobwebTree:  94%|| 7169/7600 [02:50<00:11, 36.88it/s]Training CobwebTree:  94%|| 7173/7600 [02:50<00:11, 37.62it/s]Training CobwebTree:  94%|| 7177/7600 [02:50<00:11, 36.06it/s]Training CobwebTree:  94%|| 7182/7600 [02:50<00:10, 38.55it/s]Training CobwebTree:  95%|| 7186/7600 [02:50<00:10, 38.75it/s]Training CobwebTree:  95%|| 7190/7600 [02:50<00:10, 38.97it/s]Training CobwebTree:  95%|| 7194/7600 [02:50<00:10, 38.69it/s]Training CobwebTree:  95%|| 7199/7600 [02:51<00:10, 39.61it/s]Training CobwebTree:  95%|| 7204/7600 [02:51<00:10, 38.71it/s]Training CobwebTree:  95%|| 7208/7600 [02:51<00:10, 37.25it/s]Training CobwebTree:  95%|| 7212/7600 [02:51<00:10, 36.31it/s]Training CobwebTree:  95%|| 7216/7600 [02:51<00:10, 37.28it/s]Training CobwebTree:  95%|| 7221/7600 [02:51<00:09, 38.41it/s]Training CobwebTree:  95%|| 7225/7600 [02:51<00:09, 38.28it/s]Training CobwebTree:  95%|| 7230/7600 [02:51<00:09, 38.70it/s]Training CobwebTree:  95%|| 7234/7600 [02:52<00:09, 38.63it/s]Training CobwebTree:  95%|| 7238/7600 [02:52<00:09, 37.75it/s]Training CobwebTree:  95%|| 7242/7600 [02:52<00:09, 36.94it/s]Training CobwebTree:  95%|| 7246/7600 [02:52<00:09, 37.43it/s]Training CobwebTree:  95%|| 7250/7600 [02:52<00:09, 37.97it/s]Training CobwebTree:  95%|| 7254/7600 [02:52<00:09, 37.18it/s]Training CobwebTree:  96%|| 7258/7600 [02:52<00:09, 37.14it/s]Training CobwebTree:  96%|| 7262/7600 [02:52<00:08, 37.86it/s]Training CobwebTree:  96%|| 7266/7600 [02:52<00:09, 36.54it/s]Training CobwebTree:  96%|| 7270/7600 [02:52<00:08, 37.32it/s]Training CobwebTree:  96%|| 7274/7600 [02:53<00:09, 35.83it/s]Training CobwebTree:  96%|| 7278/7600 [02:53<00:08, 36.76it/s]Training CobwebTree:  96%|| 7282/7600 [02:53<00:08, 36.96it/s]Training CobwebTree:  96%|| 7286/7600 [02:53<00:08, 36.88it/s]Training CobwebTree:  96%|| 7290/7600 [02:53<00:08, 37.62it/s]Training CobwebTree:  96%|| 7295/7600 [02:53<00:07, 38.87it/s]Training CobwebTree:  96%|| 7299/7600 [02:53<00:08, 37.06it/s]Training CobwebTree:  96%|| 7303/7600 [02:53<00:08, 36.85it/s]Training CobwebTree:  96%|| 7307/7600 [02:54<00:08, 35.19it/s]Training CobwebTree:  96%|| 7311/7600 [02:54<00:07, 36.44it/s]Training CobwebTree:  96%|| 7315/7600 [02:54<00:07, 36.99it/s]Training CobwebTree:  96%|| 7320/7600 [02:54<00:07, 37.34it/s]Training CobwebTree:  96%|| 7324/7600 [02:54<00:07, 37.30it/s]Training CobwebTree:  96%|| 7329/7600 [02:54<00:07, 38.15it/s]Training CobwebTree:  96%|| 7333/7600 [02:54<00:07, 37.18it/s]Training CobwebTree:  97%|| 7337/7600 [02:54<00:07, 36.87it/s]Training CobwebTree:  97%|| 7341/7600 [02:54<00:06, 37.30it/s]Training CobwebTree:  97%|| 7345/7600 [02:55<00:06, 37.95it/s]Training CobwebTree:  97%|| 7349/7600 [02:55<00:06, 37.47it/s]Training CobwebTree:  97%|| 7353/7600 [02:55<00:06, 37.11it/s]Training CobwebTree:  97%|| 7357/7600 [02:55<00:06, 37.72it/s]Training CobwebTree:  97%|| 7361/7600 [02:55<00:06, 36.18it/s]Training CobwebTree:  97%|| 7365/7600 [02:55<00:06, 36.55it/s]Training CobwebTree:  97%|| 7369/7600 [02:55<00:06, 37.07it/s]Training CobwebTree:  97%|| 7373/7600 [02:55<00:06, 37.08it/s]Training CobwebTree:  97%|| 7377/7600 [02:55<00:06, 37.10it/s]Training CobwebTree:  97%|| 7381/7600 [02:55<00:05, 37.82it/s]Training CobwebTree:  97%|| 7385/7600 [02:56<00:05, 38.33it/s]Training CobwebTree:  97%|| 7389/7600 [02:56<00:05, 38.40it/s]Training CobwebTree:  97%|| 7393/7600 [02:56<00:05, 38.75it/s]Training CobwebTree:  97%|| 7397/7600 [02:56<00:05, 35.91it/s]Training CobwebTree:  97%|| 7401/7600 [02:56<00:05, 35.88it/s]Training CobwebTree:  97%|| 7405/7600 [02:56<00:05, 36.58it/s]Training CobwebTree:  98%|| 7410/7600 [02:56<00:05, 37.74it/s]Training CobwebTree:  98%|| 7415/7600 [02:56<00:04, 38.99it/s]Training CobwebTree:  98%|| 7419/7600 [02:56<00:04, 37.70it/s]Training CobwebTree:  98%|| 7423/7600 [02:57<00:04, 37.17it/s]Training CobwebTree:  98%|| 7427/7600 [02:57<00:04, 36.89it/s]Training CobwebTree:  98%|| 7431/7600 [02:57<00:04, 37.69it/s]Training CobwebTree:  98%|| 7435/7600 [02:57<00:04, 37.58it/s]Training CobwebTree:  98%|| 7439/7600 [02:57<00:04, 35.55it/s]Training CobwebTree:  98%|| 7443/7600 [02:57<00:04, 35.90it/s]Training CobwebTree:  98%|| 7447/7600 [02:57<00:04, 36.49it/s]Training CobwebTree:  98%|| 7451/7600 [02:57<00:04, 36.95it/s]Training CobwebTree:  98%|| 7455/7600 [02:57<00:03, 37.11it/s]Training CobwebTree:  98%|| 7459/7600 [02:58<00:03, 37.52it/s]Training CobwebTree:  98%|| 7463/7600 [02:58<00:03, 37.64it/s]Training CobwebTree:  98%|| 7467/7600 [02:58<00:03, 35.92it/s]Training CobwebTree:  98%|| 7471/7600 [02:58<00:03, 35.80it/s]Training CobwebTree:  98%|| 7476/7600 [02:58<00:03, 38.53it/s]Training CobwebTree:  98%|| 7480/7600 [02:58<00:03, 37.69it/s]Training CobwebTree:  98%|| 7485/7600 [02:58<00:02, 38.96it/s]Training CobwebTree:  99%|| 7489/7600 [02:58<00:02, 38.57it/s]Training CobwebTree:  99%|| 7493/7600 [02:58<00:02, 38.80it/s]Training CobwebTree:  99%|| 7497/7600 [02:59<00:02, 38.69it/s]Training CobwebTree:  99%|| 7501/7600 [02:59<00:02, 38.44it/s]Training CobwebTree:  99%|| 7505/7600 [02:59<00:02, 35.93it/s]Training CobwebTree:  99%|| 7509/7600 [02:59<00:02, 36.02it/s]Training CobwebTree:  99%|| 7513/7600 [02:59<00:02, 35.09it/s]Training CobwebTree:  99%|| 7517/7600 [02:59<00:02, 35.35it/s]Training CobwebTree:  99%|| 7521/7600 [02:59<00:02, 35.34it/s]Training CobwebTree:  99%|| 7525/7600 [02:59<00:02, 34.96it/s]Training CobwebTree:  99%|| 7529/7600 [02:59<00:02, 34.92it/s]Training CobwebTree:  99%|| 7533/7600 [03:00<00:01, 35.00it/s]Training CobwebTree:  99%|| 7538/7600 [03:00<00:01, 37.08it/s]Training CobwebTree:  99%|| 7542/7600 [03:00<00:01, 36.36it/s]Training CobwebTree:  99%|| 7547/7600 [03:00<00:01, 37.80it/s]Training CobwebTree:  99%|| 7551/7600 [03:00<00:01, 37.50it/s]Training CobwebTree:  99%|| 7555/7600 [03:00<00:01, 37.65it/s]Training CobwebTree:  99%|| 7559/7600 [03:00<00:01, 36.14it/s]Training CobwebTree: 100%|| 7563/7600 [03:00<00:01, 36.20it/s]Training CobwebTree: 100%|| 7567/7600 [03:01<00:00, 36.45it/s]Training CobwebTree: 100%|| 7571/7600 [03:01<00:00, 35.49it/s]Training CobwebTree: 100%|| 7575/7600 [03:01<00:00, 36.44it/s]Training CobwebTree: 100%|| 7580/7600 [03:01<00:00, 36.77it/s]Training CobwebTree: 100%|| 7585/7600 [03:01<00:00, 38.55it/s]Training CobwebTree: 100%|| 7590/7600 [03:01<00:00, 40.37it/s]Training CobwebTree: 100%|| 7595/7600 [03:01<00:00, 39.79it/s]Training CobwebTree: 100%|| 7599/7600 [03:01<00:00, 39.29it/s]Training CobwebTree: 100%|| 7600/7600 [03:01<00:00, 41.79it/s]
2025-12-22 12:10:14,185 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:10:16,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,890 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,897 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,901 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,901 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,902 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,904 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,904 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,905 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,908 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,908 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,909 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,909 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,909 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:16,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:16,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:17,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:17,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:17,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:18,932 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:10:19,142 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 7600 virtual documents
2025-12-22 12:10:20,746 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:10:23,530 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (1222 virtual)
2025-12-22 12:10:23,532 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (2388 virtual)
2025-12-22 12:10:23,534 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3473 virtual)
2025-12-22 12:10:23,535 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4548 virtual)
2025-12-22 12:10:23,536 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5650 virtual)
2025-12-22 12:10:23,538 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6680 virtual)
2025-12-22 12:10:23,538 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7623 virtual)
2025-12-22 12:10:23,539 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8704 virtual)
2025-12-22 12:10:23,540 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9794 virtual)
2025-12-22 12:10:23,541 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10897 virtual)
2025-12-22 12:10:23,542 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (11911 virtual)
2025-12-22 12:10:23,543 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (12921 virtual)
2025-12-22 12:10:23,544 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (14032 virtual)
2025-12-22 12:10:23,545 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15005 virtual)
2025-12-22 12:10:23,546 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16044 virtual)
2025-12-22 12:10:23,547 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17093 virtual)
2025-12-22 12:10:23,548 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18108 virtual)
2025-12-22 12:10:23,549 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19117 virtual)
2025-12-22 12:10:23,550 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20085 virtual)
2025-12-22 12:10:23,551 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21140 virtual)
2025-12-22 12:10:23,552 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22139 virtual)
2025-12-22 12:10:23,553 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23227 virtual)
2025-12-22 12:10:23,554 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (24166 virtual)
2025-12-22 12:10:23,555 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (25137 virtual)
2025-12-22 12:10:23,556 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (26187 virtual)
2025-12-22 12:10:23,557 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (27113 virtual)
2025-12-22 12:10:23,558 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (28084 virtual)
2025-12-22 12:10:23,559 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (29113 virtual)
2025-12-22 12:10:23,560 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (30069 virtual)
2025-12-22 12:10:23,561 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (31017 virtual)
2025-12-22 12:10:23,561 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (31951 virtual)
2025-12-22 12:10:23,562 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (32954 virtual)
2025-12-22 12:10:23,563 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (34063 virtual)
2025-12-22 12:10:23,564 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (35021 virtual)
2025-12-22 12:10:23,565 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (36080 virtual)
2025-12-22 12:10:23,566 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (37103 virtual)
2025-12-22 12:10:23,567 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (38185 virtual)
2025-12-22 12:10:23,568 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (39172 virtual)
2025-12-22 12:10:23,569 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (40185 virtual)
2025-12-22 12:10:23,570 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (41113 virtual)
2025-12-22 12:10:23,571 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (42261 virtual)
2025-12-22 12:10:23,572 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (43352 virtual)
2025-12-22 12:10:23,573 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (44331 virtual)
2025-12-22 12:10:23,573 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (45325 virtual)
2025-12-22 12:10:23,574 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (46441 virtual)
2025-12-22 12:10:23,575 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (47389 virtual)
2025-12-22 12:10:23,576 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (48333 virtual)
2025-12-22 12:10:23,577 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (49375 virtual)
2025-12-22 12:10:23,578 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (50465 virtual)
2025-12-22 12:10:23,579 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (51456 virtual)
2025-12-22 12:10:23,580 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (52441 virtual)
2025-12-22 12:10:23,580 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (53484 virtual)
2025-12-22 12:10:23,581 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (54533 virtual)
2025-12-22 12:10:23,582 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (55592 virtual)
2025-12-22 12:10:23,583 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (56681 virtual)
2025-12-22 12:10:23,584 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (57717 virtual)
2025-12-22 12:10:23,585 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (58705 virtual)
2025-12-22 12:10:23,586 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (59654 virtual)
2025-12-22 12:10:23,586 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (60700 virtual)
2025-12-22 12:10:23,587 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (61672 virtual)
2025-12-22 12:10:23,588 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (62694 virtual)
2025-12-22 12:10:23,589 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (63757 virtual)
2025-12-22 12:10:23,590 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (64840 virtual)
2025-12-22 12:10:23,591 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (65744 virtual)
2025-12-22 12:10:23,592 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (66827 virtual)
2025-12-22 12:10:23,593 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (67785 virtual)
2025-12-22 12:10:23,594 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (68804 virtual)
2025-12-22 12:10:23,594 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (69764 virtual)
2025-12-22 12:10:23,595 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (70807 virtual)
2025-12-22 12:10:23,596 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (71846 virtual)
2025-12-22 12:10:23,597 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (72847 virtual)
2025-12-22 12:10:23,598 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (73925 virtual)
2025-12-22 12:10:23,619 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (75010 virtual)
2025-12-22 12:10:23,620 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (75968 virtual)
2025-12-22 12:10:23,627 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (76955 virtual)
2025-12-22 12:10:23,634 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (77973 virtual)
2025-12-22 12:10:23,636 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (78900 virtual)
2025-12-22 12:10:23,637 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (79915 virtual)
2025-12-22 12:10:23,638 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (80921 virtual)
2025-12-22 12:10:23,647 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (81894 virtual)
2025-12-22 12:10:23,647 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (82979 virtual)
2025-12-22 12:10:23,659 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (83937 virtual)
2025-12-22 12:10:23,671 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (84989 virtual)
2025-12-22 12:10:23,672 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (85991 virtual)
2025-12-22 12:10:23,683 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (87013 virtual)
2025-12-22 12:10:23,711 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (88050 virtual)
2025-12-22 12:10:23,719 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (88989 virtual)
2025-12-22 12:10:23,721 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (89975 virtual)
2025-12-22 12:10:23,722 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (90892 virtual)
2025-12-22 12:10:23,722 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (91944 virtual)
2025-12-22 12:10:23,731 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (93008 virtual)
2025-12-22 12:10:23,731 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (93938 virtual)
2025-12-22 12:10:23,755 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (94952 virtual)
2025-12-22 12:10:23,777 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (95989 virtual)
2025-12-22 12:10:23,778 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (97083 virtual)
2025-12-22 12:10:23,778 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (97968 virtual)
2025-12-22 12:10:23,803 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (98973 virtual)
2025-12-22 12:10:23,804 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (99956 virtual)
2025-12-22 12:10:23,811 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (101124 virtual)
2025-12-22 12:10:23,812 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (102046 virtual)
2025-12-22 12:10:23,813 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (103005 virtual)
2025-12-22 12:10:23,827 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (104049 virtual)
2025-12-22 12:10:23,828 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (105026 virtual)
2025-12-22 12:10:23,835 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (106094 virtual)
2025-12-22 12:10:23,851 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (107077 virtual)
2025-12-22 12:10:23,987 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (108131 virtual)
2025-12-22 12:10:23,989 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (109133 virtual)
2025-12-22 12:10:23,990 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (110094 virtual)
2025-12-22 12:10:23,991 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (111108 virtual)
2025-12-22 12:10:23,992 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (112228 virtual)
2025-12-22 12:10:24,011 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (113209 virtual)
2025-12-22 12:10:24,023 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (114200 virtual)
2025-12-22 12:10:24,039 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (115208 virtual)
2025-12-22 12:10:24,047 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (116189 virtual)
2025-12-22 12:10:24,071 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (117185 virtual)
2025-12-22 12:10:24,073 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (118161 virtual)
2025-12-22 12:10:24,078 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (119147 virtual)
2025-12-22 12:10:24,112 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (120162 virtual)
2025-12-22 12:10:24,119 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (120878 virtual)
2025-12-22 12:10:24,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,261 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,261 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,263 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,263 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,265 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,267 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,267 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,267 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,272 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,277 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,277 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,278 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,278 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,287 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,304 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,406 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,504 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,527 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:10:24,593 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:24,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:10:26,193 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:10:26,349 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 120892 virtual documents
2025-12-22 12:10:27,107 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.6752738178426324, 'coherence_npmi': 0.1438841257406559, 'topic_diversity': 0.8541176470588235, 'inter_topic_similarity': 0.17000654339790344}
2025-12-22 12:10:27,107 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.610506949646373, 'coherence_npmi': 0.09546436527485504, 'topic_diversity': 0.858, 'inter_topic_similarity': 0.25397083163261414}
2025-12-22 12:10:27,107 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.5738339121857832, 'coherence_npmi': 0.059445152038873586, 'topic_diversity': 0.8216, 'inter_topic_similarity': 0.2010844200849533}
