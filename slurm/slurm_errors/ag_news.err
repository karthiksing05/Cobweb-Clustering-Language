2025-12-20 23:48:12,610 INFO __main__: Starting benchmark for dataset=agnews
2025-12-20 23:48:15,720 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-20 23:48:15,898 INFO gensim.corpora.dictionary: built Dictionary<21775 unique tokens: ['band', 'bears', 'black', 'claw', 'cynics']...> from 10000 documents (total 256241 corpus positions)
2025-12-20 23:48:15,905 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<21775 unique tokens: ['band', 'bears', 'black', 'claw', 'cynics']...> from 10000 documents (total 256241 corpus positions)", 'datetime': '2025-12-20T23:48:15.898454', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-200-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-20 23:48:16,203 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-20 23:48:16,203 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-20 23:48:18,333 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10000 docs
2025-12-20 23:49:15,909 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:49:20,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,295 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,299 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,302 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,302 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,302 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,311 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,311 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,311 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,311 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,311 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,313 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,313 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,314 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,314 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,314 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,314 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,314 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,314 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,315 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,316 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,316 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,316 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,317 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,317 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,317 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,316 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,317 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,319 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,319 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,319 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,320 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,320 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,322 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,322 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,341 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,362 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,382 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,465 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:20,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,525 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,530 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,530 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:20,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:22,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:22,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:24,620 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:49:25,064 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-20 23:49:27,587 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:49:32,257 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-20 23:49:32,259 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-20 23:49:32,260 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-20 23:49:32,261 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-20 23:49:32,262 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-20 23:49:32,262 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-20 23:49:32,263 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-20 23:49:32,264 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-20 23:49:32,265 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-20 23:49:32,266 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-20 23:49:32,267 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-20 23:49:32,268 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-20 23:49:32,269 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-20 23:49:32,270 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-20 23:49:32,271 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-20 23:49:32,271 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-20 23:49:32,272 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-20 23:49:32,273 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-20 23:49:32,274 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-20 23:49:32,275 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-20 23:49:32,276 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-20 23:49:32,277 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-20 23:49:32,277 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-20 23:49:32,278 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-20 23:49:32,279 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-20 23:49:32,280 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-20 23:49:32,281 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-20 23:49:32,282 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-20 23:49:32,283 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-20 23:49:32,284 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-20 23:49:32,285 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-20 23:49:32,286 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-20 23:49:32,287 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-20 23:49:32,287 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-20 23:49:32,288 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-20 23:49:32,289 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-20 23:49:32,290 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-20 23:49:32,291 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-20 23:49:32,292 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-20 23:49:32,292 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-20 23:49:32,293 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-20 23:49:32,294 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-20 23:49:32,295 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-20 23:49:32,296 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-20 23:49:32,297 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-20 23:49:32,297 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-20 23:49:32,298 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-20 23:49:32,299 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-20 23:49:32,300 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-20 23:49:32,300 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-20 23:49:32,301 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-20 23:49:32,302 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-20 23:49:32,303 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-20 23:49:32,304 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-20 23:49:32,305 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-20 23:49:32,306 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-20 23:49:32,307 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-20 23:49:32,307 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-20 23:49:32,308 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-20 23:49:32,309 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-20 23:49:32,310 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-20 23:49:32,311 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-20 23:49:32,312 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-20 23:49:32,312 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-20 23:49:32,313 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-20 23:49:32,314 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-20 23:49:32,315 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-20 23:49:32,316 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-20 23:49:32,316 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-20 23:49:32,317 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-20 23:49:32,317 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-20 23:49:32,318 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-20 23:49:32,318 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-20 23:49:32,318 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-20 23:49:32,319 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-20 23:49:32,320 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-20 23:49:32,320 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-20 23:49:32,321 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-20 23:49:32,321 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-20 23:49:32,322 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-20 23:49:32,322 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-20 23:49:32,323 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-20 23:49:32,324 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-20 23:49:32,325 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-20 23:49:32,326 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-20 23:49:32,326 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-20 23:49:32,327 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-20 23:49:32,328 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-20 23:49:32,329 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-20 23:49:32,330 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-20 23:49:32,331 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-20 23:49:32,332 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-20 23:49:32,332 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-20 23:49:32,333 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-20 23:49:32,334 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-20 23:49:32,335 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-20 23:49:32,335 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-20 23:49:32,336 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-20 23:49:32,337 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-20 23:49:32,338 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-20 23:49:32,339 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-20 23:49:32,339 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-20 23:49:32,340 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-20 23:49:32,341 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-20 23:49:32,342 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-20 23:49:32,343 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-20 23:49:32,343 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-20 23:49:32,344 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-20 23:49:32,345 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-20 23:49:32,346 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-20 23:49:32,346 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-20 23:49:32,347 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-20 23:49:32,348 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-20 23:49:32,348 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-20 23:49:32,349 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-20 23:49:32,350 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-20 23:49:32,351 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-20 23:49:32,352 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-20 23:49:32,352 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-20 23:49:32,353 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-20 23:49:32,353 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-20 23:49:32,354 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-20 23:49:32,355 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-20 23:49:32,739 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-20 23:49:32,741 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-20 23:49:32,742 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-20 23:49:32,743 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-20 23:49:32,932 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-20 23:49:32,933 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-20 23:49:32,934 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-20 23:49:32,935 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-20 23:49:33,083 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-20 23:49:33,100 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-20 23:49:33,101 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-20 23:49:33,102 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-20 23:49:33,103 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-20 23:49:33,104 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-20 23:49:33,105 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-20 23:49:33,226 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-20 23:49:33,240 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-20 23:49:33,313 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-20 23:49:33,314 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-20 23:49:33,315 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-20 23:49:33,317 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-20 23:49:33,318 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-20 23:49:33,319 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-20 23:49:33,367 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-20 23:49:33,381 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-20 23:49:33,401 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-20 23:49:33,403 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-20 23:49:33,409 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-20 23:49:33,410 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-20 23:49:33,416 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-20 23:49:33,429 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-20 23:49:33,444 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-20 23:49:33,445 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-20 23:49:33,450 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-20 23:49:33,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,472 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,473 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,473 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,473 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,475 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,475 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,476 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,476 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,476 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,488 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,492 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,498 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,501 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,540 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,704 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,709 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,828 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:33,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:33,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:34,126 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:34,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:35,448 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:49:35,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:49:37,772 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:49:38,586 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-20 23:49:39,452 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10000 docs
2025-12-20 23:50:16,992 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:50:21,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,438 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,448 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,448 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,448 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,448 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,449 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,449 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,452 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,460 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,460 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,460 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,460 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,460 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,462 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,462 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,465 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,466 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,466 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,468 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,469 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,472 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,472 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,473 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,473 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,534 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,538 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,549 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,566 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,566 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,581 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,586 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,590 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:21,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:21,678 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:25,090 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:50:25,244 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-20 23:50:26,134 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:50:30,520 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-20 23:50:30,522 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-20 23:50:30,524 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-20 23:50:30,524 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-20 23:50:30,525 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-20 23:50:30,526 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-20 23:50:30,527 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-20 23:50:30,528 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-20 23:50:30,528 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-20 23:50:30,529 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-20 23:50:30,530 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-20 23:50:30,531 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-20 23:50:30,532 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-20 23:50:30,533 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-20 23:50:30,534 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-20 23:50:30,535 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-20 23:50:30,536 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-20 23:50:30,537 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-20 23:50:30,538 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-20 23:50:30,539 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-20 23:50:30,539 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-20 23:50:30,540 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-20 23:50:30,541 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-20 23:50:30,543 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-20 23:50:30,544 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-20 23:50:30,544 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-20 23:50:30,545 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-20 23:50:30,546 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-20 23:50:30,547 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-20 23:50:30,548 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-20 23:50:30,549 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-20 23:50:30,549 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-20 23:50:30,550 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-20 23:50:30,551 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-20 23:50:30,552 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-20 23:50:30,552 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-20 23:50:30,553 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-20 23:50:30,553 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-20 23:50:30,554 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-20 23:50:30,555 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-20 23:50:30,556 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-20 23:50:30,557 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-20 23:50:30,557 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-20 23:50:30,558 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-20 23:50:30,559 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-20 23:50:30,560 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-20 23:50:30,561 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-20 23:50:30,561 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-20 23:50:30,562 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-20 23:50:30,563 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-20 23:50:30,564 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-20 23:50:30,565 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-20 23:50:30,566 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-20 23:50:30,567 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-20 23:50:30,567 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-20 23:50:30,568 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-20 23:50:30,569 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-20 23:50:30,569 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-20 23:50:30,570 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-20 23:50:30,571 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-20 23:50:30,572 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-20 23:50:30,573 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-20 23:50:30,573 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-20 23:50:30,574 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-20 23:50:30,575 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-20 23:50:30,576 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-20 23:50:30,577 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-20 23:50:30,577 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-20 23:50:30,578 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-20 23:50:30,579 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-20 23:50:30,580 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-20 23:50:30,581 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-20 23:50:30,581 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-20 23:50:30,582 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-20 23:50:30,583 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-20 23:50:30,584 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-20 23:50:30,596 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-20 23:50:30,597 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-20 23:50:30,615 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-20 23:50:30,616 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-20 23:50:30,617 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-20 23:50:30,617 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-20 23:50:30,618 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-20 23:50:30,619 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-20 23:50:30,619 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-20 23:50:30,620 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-20 23:50:30,621 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-20 23:50:30,621 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-20 23:50:30,622 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-20 23:50:30,622 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-20 23:50:30,623 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-20 23:50:30,624 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-20 23:50:30,624 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-20 23:50:30,625 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-20 23:50:30,626 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-20 23:50:30,626 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-20 23:50:30,627 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-20 23:50:30,627 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-20 23:50:30,628 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-20 23:50:30,629 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-20 23:50:30,629 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-20 23:50:30,630 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-20 23:50:30,646 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-20 23:50:30,647 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-20 23:50:30,648 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-20 23:50:30,648 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-20 23:50:30,649 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-20 23:50:30,650 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-20 23:50:30,651 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-20 23:50:30,651 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-20 23:50:30,652 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-20 23:50:30,653 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-20 23:50:30,653 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-20 23:50:30,654 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-20 23:50:30,654 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-20 23:50:30,655 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-20 23:50:30,656 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-20 23:50:30,657 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-20 23:50:30,657 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-20 23:50:30,658 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-20 23:50:30,659 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-20 23:50:30,660 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-20 23:50:30,661 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-20 23:50:30,661 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-20 23:50:30,662 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-20 23:50:30,663 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-20 23:50:30,664 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-20 23:50:30,664 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-20 23:50:30,665 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-20 23:50:30,666 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-20 23:50:30,667 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-20 23:50:30,668 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-20 23:50:30,669 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-20 23:50:30,669 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-20 23:50:30,670 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-20 23:50:30,671 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-20 23:50:30,672 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-20 23:50:30,673 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-20 23:50:30,673 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-20 23:50:30,674 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-20 23:50:30,675 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-20 23:50:30,676 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-20 23:50:30,677 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-20 23:50:30,678 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-20 23:50:30,678 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-20 23:50:30,680 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-20 23:50:30,680 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-20 23:50:30,681 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-20 23:50:30,682 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-20 23:50:30,682 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-20 23:50:30,684 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-20 23:50:30,684 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-20 23:50:30,685 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-20 23:50:30,686 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-20 23:50:30,686 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-20 23:50:30,687 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-20 23:50:30,688 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-20 23:50:31,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,406 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,429 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,440 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,533 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,540 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,546 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,556 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,692 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,697 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,700 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,701 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,764 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,764 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,767 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,942 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:31,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:31,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:32,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:50:32,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:32,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:32,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:32,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:50:35,377 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:50:35,525 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-20 23:50:35,877 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10000 docs
Training CobwebTree:   0%|          | 0/10000 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 17/10000 [00:00<00:59, 168.13it/s]Training CobwebTree:   0%|          | 34/10000 [00:00<01:11, 139.34it/s]Training CobwebTree:   0%|          | 49/10000 [00:00<01:22, 120.74it/s]Training CobwebTree:   1%|          | 62/10000 [00:00<01:40, 99.27it/s] Training CobwebTree:   1%|          | 73/10000 [00:00<01:46, 93.64it/s]Training CobwebTree:   1%|          | 83/10000 [00:00<01:56, 85.03it/s]Training CobwebTree:   1%|          | 92/10000 [00:00<02:05, 79.14it/s]Training CobwebTree:   1%|          | 101/10000 [00:01<02:07, 77.77it/s]Training CobwebTree:   1%|          | 109/10000 [00:01<02:11, 75.02it/s]Training CobwebTree:   1%|          | 117/10000 [00:01<02:13, 73.94it/s]Training CobwebTree:   1%|         | 125/10000 [00:01<02:13, 74.18it/s]Training CobwebTree:   1%|         | 133/10000 [00:01<02:19, 70.60it/s]Training CobwebTree:   1%|         | 141/10000 [00:01<02:18, 71.11it/s]Training CobwebTree:   1%|         | 149/10000 [00:01<02:23, 68.58it/s]Training CobwebTree:   2%|         | 156/10000 [00:01<02:41, 61.03it/s]Training CobwebTree:   2%|         | 163/10000 [00:02<02:36, 62.97it/s]Training CobwebTree:   2%|         | 170/10000 [00:02<02:33, 64.03it/s]Training CobwebTree:   2%|         | 177/10000 [00:02<02:33, 63.82it/s]Training CobwebTree:   2%|         | 184/10000 [00:02<02:41, 60.66it/s]Training CobwebTree:   2%|         | 191/10000 [00:02<02:39, 61.50it/s]Training CobwebTree:   2%|         | 198/10000 [00:02<02:39, 61.50it/s]Training CobwebTree:   2%|         | 205/10000 [00:02<02:44, 59.70it/s]Training CobwebTree:   2%|         | 212/10000 [00:02<02:53, 56.53it/s]Training CobwebTree:   2%|         | 219/10000 [00:02<02:46, 58.83it/s]Training CobwebTree:   2%|         | 225/10000 [00:03<02:45, 59.13it/s]Training CobwebTree:   2%|         | 231/10000 [00:03<02:47, 58.43it/s]Training CobwebTree:   2%|         | 237/10000 [00:03<02:49, 57.54it/s]Training CobwebTree:   2%|         | 243/10000 [00:03<02:52, 56.43it/s]Training CobwebTree:   2%|         | 249/10000 [00:03<02:51, 56.71it/s]Training CobwebTree:   3%|         | 255/10000 [00:03<02:49, 57.47it/s]Training CobwebTree:   3%|         | 262/10000 [00:03<02:46, 58.34it/s]Training CobwebTree:   3%|         | 268/10000 [00:03<02:49, 57.39it/s]Training CobwebTree:   3%|         | 274/10000 [00:03<02:53, 55.97it/s]Training CobwebTree:   3%|         | 280/10000 [00:04<03:01, 53.55it/s]Training CobwebTree:   3%|         | 286/10000 [00:04<02:59, 54.12it/s]Training CobwebTree:   3%|         | 292/10000 [00:04<02:56, 55.02it/s]Training CobwebTree:   3%|         | 298/10000 [00:04<02:53, 55.81it/s]Training CobwebTree:   3%|         | 304/10000 [00:04<02:55, 55.13it/s]Training CobwebTree:   3%|         | 310/10000 [00:04<02:52, 56.14it/s]Training CobwebTree:   3%|         | 316/10000 [00:04<02:54, 55.48it/s]Training CobwebTree:   3%|         | 322/10000 [00:04<02:55, 55.18it/s]Training CobwebTree:   3%|         | 328/10000 [00:04<03:02, 52.90it/s]Training CobwebTree:   3%|         | 334/10000 [00:05<02:58, 54.09it/s]Training CobwebTree:   3%|         | 340/10000 [00:05<02:59, 53.89it/s]Training CobwebTree:   3%|         | 346/10000 [00:05<02:59, 53.82it/s]Training CobwebTree:   4%|         | 352/10000 [00:05<03:00, 53.33it/s]Training CobwebTree:   4%|         | 358/10000 [00:05<03:13, 49.71it/s]Training CobwebTree:   4%|         | 364/10000 [00:05<03:16, 49.16it/s]Training CobwebTree:   4%|         | 369/10000 [00:05<03:19, 48.31it/s]Training CobwebTree:   4%|         | 374/10000 [00:05<03:18, 48.62it/s]Training CobwebTree:   4%|         | 379/10000 [00:05<03:19, 48.27it/s]Training CobwebTree:   4%|         | 384/10000 [00:06<03:29, 45.88it/s]Training CobwebTree:   4%|         | 389/10000 [00:06<03:37, 44.29it/s]Training CobwebTree:   4%|         | 394/10000 [00:06<03:32, 45.25it/s]Training CobwebTree:   4%|         | 399/10000 [00:06<03:33, 45.05it/s]Training CobwebTree:   4%|         | 404/10000 [00:06<03:28, 46.12it/s]Training CobwebTree:   4%|         | 409/10000 [00:06<03:23, 47.16it/s]Training CobwebTree:   4%|         | 415/10000 [00:06<03:14, 49.25it/s]Training CobwebTree:   4%|         | 421/10000 [00:06<03:11, 50.06it/s]Training CobwebTree:   4%|         | 427/10000 [00:06<03:12, 49.62it/s]Training CobwebTree:   4%|         | 432/10000 [00:07<03:13, 49.52it/s]Training CobwebTree:   4%|         | 437/10000 [00:07<03:21, 47.50it/s]Training CobwebTree:   4%|         | 442/10000 [00:07<03:19, 47.86it/s]Training CobwebTree:   4%|         | 447/10000 [00:07<03:25, 46.53it/s]Training CobwebTree:   5%|         | 455/10000 [00:07<02:59, 53.17it/s]Training CobwebTree:   5%|         | 461/10000 [00:07<02:55, 54.27it/s]Training CobwebTree:   5%|         | 467/10000 [00:07<02:55, 54.33it/s]Training CobwebTree:   5%|         | 473/10000 [00:07<02:53, 54.83it/s]Training CobwebTree:   5%|         | 479/10000 [00:07<02:52, 55.05it/s]Training CobwebTree:   5%|         | 485/10000 [00:08<02:57, 53.56it/s]Training CobwebTree:   5%|         | 491/10000 [00:08<02:57, 53.49it/s]Training CobwebTree:   5%|         | 497/10000 [00:08<02:53, 54.72it/s]Training CobwebTree:   5%|         | 503/10000 [00:08<02:51, 55.32it/s]Training CobwebTree:   5%|         | 509/10000 [00:08<02:57, 53.44it/s]Training CobwebTree:   5%|         | 515/10000 [00:08<03:03, 51.82it/s]Training CobwebTree:   5%|         | 521/10000 [00:08<02:56, 53.64it/s]Training CobwebTree:   5%|         | 527/10000 [00:08<03:01, 52.24it/s]Training CobwebTree:   5%|         | 533/10000 [00:09<03:09, 49.99it/s]Training CobwebTree:   5%|         | 539/10000 [00:09<03:09, 49.87it/s]Training CobwebTree:   5%|         | 545/10000 [00:09<03:07, 50.50it/s]Training CobwebTree:   6%|         | 551/10000 [00:09<03:10, 49.68it/s]Training CobwebTree:   6%|         | 557/10000 [00:09<03:13, 48.89it/s]Training CobwebTree:   6%|         | 564/10000 [00:09<02:59, 52.64it/s]Training CobwebTree:   6%|         | 570/10000 [00:09<02:57, 53.00it/s]Training CobwebTree:   6%|         | 576/10000 [00:09<02:58, 52.83it/s]Training CobwebTree:   6%|         | 582/10000 [00:09<02:52, 54.52it/s]Training CobwebTree:   6%|         | 589/10000 [00:10<02:47, 56.28it/s]Training CobwebTree:   6%|         | 595/10000 [00:10<02:44, 57.10it/s]Training CobwebTree:   6%|         | 601/10000 [00:10<02:44, 57.19it/s]Training CobwebTree:   6%|         | 607/10000 [00:10<02:49, 55.54it/s]Training CobwebTree:   6%|         | 613/10000 [00:10<02:51, 54.80it/s]Training CobwebTree:   6%|         | 619/10000 [00:10<02:51, 54.71it/s]Training CobwebTree:   6%|         | 625/10000 [00:10<02:50, 54.98it/s]Training CobwebTree:   6%|         | 631/10000 [00:10<02:50, 55.04it/s]Training CobwebTree:   6%|         | 637/10000 [00:10<02:56, 52.95it/s]Training CobwebTree:   6%|         | 643/10000 [00:11<03:05, 50.37it/s]Training CobwebTree:   6%|         | 649/10000 [00:11<03:03, 50.92it/s]Training CobwebTree:   7%|         | 655/10000 [00:11<03:06, 50.07it/s]Training CobwebTree:   7%|         | 661/10000 [00:11<03:07, 49.88it/s]Training CobwebTree:   7%|         | 667/10000 [00:11<03:03, 50.84it/s]Training CobwebTree:   7%|         | 673/10000 [00:11<02:57, 52.66it/s]Training CobwebTree:   7%|         | 679/10000 [00:11<02:57, 52.59it/s]Training CobwebTree:   7%|         | 685/10000 [00:11<03:09, 49.21it/s]Training CobwebTree:   7%|         | 691/10000 [00:12<03:06, 49.90it/s]Training CobwebTree:   7%|         | 697/10000 [00:12<03:02, 50.85it/s]Training CobwebTree:   7%|         | 703/10000 [00:12<03:03, 50.79it/s]Training CobwebTree:   7%|         | 709/10000 [00:12<03:00, 51.40it/s]Training CobwebTree:   7%|         | 715/10000 [00:12<03:02, 50.93it/s]Training CobwebTree:   7%|         | 721/10000 [00:12<03:13, 48.07it/s]Training CobwebTree:   7%|         | 726/10000 [00:12<03:12, 48.11it/s]Training CobwebTree:   7%|         | 731/10000 [00:12<03:11, 48.30it/s]Training CobwebTree:   7%|         | 737/10000 [00:12<03:01, 51.04it/s]Training CobwebTree:   7%|         | 743/10000 [00:13<03:04, 50.14it/s]Training CobwebTree:   7%|         | 749/10000 [00:13<03:07, 49.21it/s]Training CobwebTree:   8%|         | 754/10000 [00:13<03:09, 48.89it/s]Training CobwebTree:   8%|         | 760/10000 [00:13<03:03, 50.46it/s]Training CobwebTree:   8%|         | 766/10000 [00:13<03:04, 49.94it/s]Training CobwebTree:   8%|         | 772/10000 [00:13<02:57, 52.12it/s]Training CobwebTree:   8%|         | 778/10000 [00:13<03:11, 48.24it/s]Training CobwebTree:   8%|         | 783/10000 [00:13<03:13, 47.55it/s]Training CobwebTree:   8%|         | 788/10000 [00:13<03:11, 48.12it/s]Training CobwebTree:   8%|         | 793/10000 [00:14<03:11, 48.10it/s]Training CobwebTree:   8%|         | 799/10000 [00:14<03:07, 49.15it/s]Training CobwebTree:   8%|         | 806/10000 [00:14<02:56, 52.18it/s]Training CobwebTree:   8%|         | 812/10000 [00:14<02:57, 51.67it/s]Training CobwebTree:   8%|         | 818/10000 [00:14<03:02, 50.43it/s]Training CobwebTree:   8%|         | 825/10000 [00:14<02:51, 53.38it/s]Training CobwebTree:   8%|         | 831/10000 [00:14<02:52, 53.21it/s]Training CobwebTree:   8%|         | 837/10000 [00:14<02:58, 51.42it/s]Training CobwebTree:   8%|         | 843/10000 [00:15<02:58, 51.16it/s]Training CobwebTree:   8%|         | 849/10000 [00:15<03:05, 49.45it/s]Training CobwebTree:   9%|         | 854/10000 [00:15<03:04, 49.50it/s]Training CobwebTree:   9%|         | 860/10000 [00:15<02:58, 51.28it/s]Training CobwebTree:   9%|         | 866/10000 [00:15<03:04, 49.42it/s]Training CobwebTree:   9%|         | 871/10000 [00:15<03:06, 48.85it/s]Training CobwebTree:   9%|         | 877/10000 [00:15<03:03, 49.75it/s]Training CobwebTree:   9%|         | 882/10000 [00:15<03:08, 48.41it/s]Training CobwebTree:   9%|         | 887/10000 [00:15<03:09, 48.12it/s]Training CobwebTree:   9%|         | 892/10000 [00:16<03:10, 47.81it/s]Training CobwebTree:   9%|         | 897/10000 [00:16<03:16, 46.36it/s]Training CobwebTree:   9%|         | 902/10000 [00:16<03:23, 44.74it/s]Training CobwebTree:   9%|         | 908/10000 [00:16<03:07, 48.55it/s]Training CobwebTree:   9%|         | 913/10000 [00:16<03:16, 46.31it/s]Training CobwebTree:   9%|         | 919/10000 [00:16<03:07, 48.56it/s]Training CobwebTree:   9%|         | 924/10000 [00:16<03:06, 48.67it/s]Training CobwebTree:   9%|         | 929/10000 [00:16<03:09, 47.91it/s]Training CobwebTree:   9%|         | 935/10000 [00:16<03:02, 49.58it/s]Training CobwebTree:   9%|         | 941/10000 [00:17<03:03, 49.25it/s]Training CobwebTree:   9%|         | 946/10000 [00:17<03:03, 49.22it/s]Training CobwebTree:  10%|         | 951/10000 [00:17<03:08, 47.93it/s]Training CobwebTree:  10%|         | 956/10000 [00:17<03:11, 47.34it/s]Training CobwebTree:  10%|         | 961/10000 [00:17<03:15, 46.27it/s]Training CobwebTree:  10%|         | 966/10000 [00:17<03:13, 46.59it/s]Training CobwebTree:  10%|         | 971/10000 [00:17<03:13, 46.78it/s]Training CobwebTree:  10%|         | 976/10000 [00:17<03:18, 45.51it/s]Training CobwebTree:  10%|         | 981/10000 [00:17<03:18, 45.33it/s]Training CobwebTree:  10%|         | 986/10000 [00:18<03:22, 44.52it/s]Training CobwebTree:  10%|         | 991/10000 [00:18<03:23, 44.20it/s]Training CobwebTree:  10%|         | 996/10000 [00:18<03:27, 43.39it/s]Training CobwebTree:  10%|         | 1003/10000 [00:18<03:06, 48.22it/s]Training CobwebTree:  10%|         | 1008/10000 [00:18<03:09, 47.44it/s]Training CobwebTree:  10%|         | 1014/10000 [00:18<03:02, 49.15it/s]Training CobwebTree:  10%|         | 1019/10000 [00:18<03:04, 48.56it/s]Training CobwebTree:  10%|         | 1024/10000 [00:18<03:08, 47.59it/s]Training CobwebTree:  10%|         | 1029/10000 [00:18<03:14, 46.10it/s]Training CobwebTree:  10%|         | 1034/10000 [00:19<03:17, 45.50it/s]Training CobwebTree:  10%|         | 1039/10000 [00:19<03:15, 45.89it/s]Training CobwebTree:  10%|         | 1044/10000 [00:19<03:13, 46.22it/s]Training CobwebTree:  10%|         | 1049/10000 [00:19<03:13, 46.32it/s]Training CobwebTree:  11%|         | 1054/10000 [00:19<03:13, 46.26it/s]Training CobwebTree:  11%|         | 1059/10000 [00:19<03:19, 44.87it/s]Training CobwebTree:  11%|         | 1064/10000 [00:19<03:17, 45.19it/s]Training CobwebTree:  11%|         | 1069/10000 [00:19<03:22, 44.03it/s]Training CobwebTree:  11%|         | 1074/10000 [00:19<03:24, 43.67it/s]Training CobwebTree:  11%|         | 1080/10000 [00:20<03:15, 45.70it/s]Training CobwebTree:  11%|         | 1085/10000 [00:20<03:21, 44.23it/s]Training CobwebTree:  11%|         | 1090/10000 [00:20<03:23, 43.87it/s]Training CobwebTree:  11%|         | 1096/10000 [00:20<03:11, 46.62it/s]Training CobwebTree:  11%|         | 1101/10000 [00:20<03:13, 46.09it/s]Training CobwebTree:  11%|         | 1106/10000 [00:20<03:19, 44.55it/s]Training CobwebTree:  11%|         | 1112/10000 [00:20<03:10, 46.67it/s]Training CobwebTree:  11%|         | 1117/10000 [00:20<03:08, 47.25it/s]Training CobwebTree:  11%|         | 1122/10000 [00:21<03:13, 45.96it/s]Training CobwebTree:  11%|        | 1127/10000 [00:21<03:15, 45.27it/s]Training CobwebTree:  11%|        | 1132/10000 [00:21<03:15, 45.36it/s]Training CobwebTree:  11%|        | 1138/10000 [00:21<03:07, 47.21it/s]Training CobwebTree:  11%|        | 1143/10000 [00:21<03:04, 47.90it/s]Training CobwebTree:  11%|        | 1148/10000 [00:21<03:12, 45.99it/s]Training CobwebTree:  12%|        | 1153/10000 [00:21<03:14, 45.40it/s]Training CobwebTree:  12%|        | 1158/10000 [00:21<03:11, 46.21it/s]Training CobwebTree:  12%|        | 1163/10000 [00:21<03:09, 46.65it/s]Training CobwebTree:  12%|        | 1168/10000 [00:22<03:10, 46.33it/s]Training CobwebTree:  12%|        | 1173/10000 [00:22<03:15, 45.08it/s]Training CobwebTree:  12%|        | 1178/10000 [00:22<03:13, 45.69it/s]Training CobwebTree:  12%|        | 1183/10000 [00:22<03:14, 45.33it/s]Training CobwebTree:  12%|        | 1188/10000 [00:22<03:17, 44.61it/s]Training CobwebTree:  12%|        | 1193/10000 [00:22<03:16, 44.89it/s]Training CobwebTree:  12%|        | 1198/10000 [00:22<03:13, 45.40it/s]Training CobwebTree:  12%|        | 1203/10000 [00:22<03:17, 44.48it/s]Training CobwebTree:  12%|        | 1208/10000 [00:22<03:16, 44.84it/s]Training CobwebTree:  12%|        | 1213/10000 [00:23<03:28, 42.11it/s]Training CobwebTree:  12%|        | 1218/10000 [00:23<03:18, 44.14it/s]Training CobwebTree:  12%|        | 1223/10000 [00:23<03:13, 45.27it/s]Training CobwebTree:  12%|        | 1228/10000 [00:23<03:26, 42.45it/s]Training CobwebTree:  12%|        | 1233/10000 [00:23<03:20, 43.72it/s]Training CobwebTree:  12%|        | 1238/10000 [00:23<03:19, 44.01it/s]Training CobwebTree:  12%|        | 1243/10000 [00:23<03:15, 44.90it/s]Training CobwebTree:  12%|        | 1248/10000 [00:23<03:13, 45.20it/s]Training CobwebTree:  13%|        | 1253/10000 [00:23<03:15, 44.80it/s]Training CobwebTree:  13%|        | 1258/10000 [00:24<03:23, 42.96it/s]Training CobwebTree:  13%|        | 1263/10000 [00:24<03:25, 42.43it/s]Training CobwebTree:  13%|        | 1268/10000 [00:24<03:24, 42.80it/s]Training CobwebTree:  13%|        | 1273/10000 [00:24<03:23, 42.93it/s]Training CobwebTree:  13%|        | 1278/10000 [00:24<03:17, 44.13it/s]Training CobwebTree:  13%|        | 1283/10000 [00:24<03:12, 45.17it/s]Training CobwebTree:  13%|        | 1288/10000 [00:24<03:10, 45.62it/s]Training CobwebTree:  13%|        | 1293/10000 [00:24<03:15, 44.43it/s]Training CobwebTree:  13%|        | 1298/10000 [00:24<03:17, 44.03it/s]Training CobwebTree:  13%|        | 1304/10000 [00:25<03:07, 46.44it/s]Training CobwebTree:  13%|        | 1309/10000 [00:25<03:09, 45.88it/s]Training CobwebTree:  13%|        | 1315/10000 [00:25<03:03, 47.29it/s]Training CobwebTree:  13%|        | 1320/10000 [00:25<03:12, 45.10it/s]Training CobwebTree:  13%|        | 1325/10000 [00:25<03:08, 46.01it/s]Training CobwebTree:  13%|        | 1330/10000 [00:25<03:09, 45.81it/s]Training CobwebTree:  13%|        | 1335/10000 [00:25<03:12, 44.91it/s]Training CobwebTree:  13%|        | 1340/10000 [00:25<03:07, 46.14it/s]Training CobwebTree:  13%|        | 1345/10000 [00:26<03:14, 44.57it/s]Training CobwebTree:  14%|        | 1350/10000 [00:26<03:11, 45.28it/s]Training CobwebTree:  14%|        | 1355/10000 [00:26<03:19, 43.32it/s]Training CobwebTree:  14%|        | 1360/10000 [00:26<03:22, 42.61it/s]Training CobwebTree:  14%|        | 1365/10000 [00:26<03:22, 42.63it/s]Training CobwebTree:  14%|        | 1370/10000 [00:26<03:16, 43.94it/s]Training CobwebTree:  14%|        | 1375/10000 [00:26<03:12, 44.73it/s]Training CobwebTree:  14%|        | 1380/10000 [00:26<03:14, 44.35it/s]Training CobwebTree:  14%|        | 1385/10000 [00:26<03:12, 44.67it/s]Training CobwebTree:  14%|        | 1391/10000 [00:27<03:05, 46.45it/s]Training CobwebTree:  14%|        | 1396/10000 [00:27<03:04, 46.65it/s]Training CobwebTree:  14%|        | 1401/10000 [00:27<03:03, 46.83it/s]Training CobwebTree:  14%|        | 1407/10000 [00:27<02:56, 48.65it/s]Training CobwebTree:  14%|        | 1413/10000 [00:27<02:56, 48.72it/s]Training CobwebTree:  14%|        | 1419/10000 [00:27<02:57, 48.31it/s]Training CobwebTree:  14%|        | 1424/10000 [00:27<03:00, 47.52it/s]Training CobwebTree:  14%|        | 1429/10000 [00:27<03:15, 43.76it/s]Training CobwebTree:  14%|        | 1434/10000 [00:27<03:14, 44.01it/s]Training CobwebTree:  14%|        | 1439/10000 [00:28<03:09, 45.21it/s]Training CobwebTree:  14%|        | 1444/10000 [00:28<03:16, 43.47it/s]Training CobwebTree:  14%|        | 1449/10000 [00:28<03:15, 43.78it/s]Training CobwebTree:  15%|        | 1454/10000 [00:28<03:19, 42.81it/s]Training CobwebTree:  15%|        | 1459/10000 [00:28<03:20, 42.68it/s]Training CobwebTree:  15%|        | 1464/10000 [00:28<03:18, 42.90it/s]Training CobwebTree:  15%|        | 1469/10000 [00:28<03:11, 44.58it/s]Training CobwebTree:  15%|        | 1474/10000 [00:28<03:15, 43.71it/s]Training CobwebTree:  15%|        | 1479/10000 [00:29<03:20, 42.52it/s]Training CobwebTree:  15%|        | 1484/10000 [00:29<03:21, 42.32it/s]Training CobwebTree:  15%|        | 1489/10000 [00:29<03:13, 44.08it/s]Training CobwebTree:  15%|        | 1494/10000 [00:29<03:16, 43.29it/s]Training CobwebTree:  15%|        | 1500/10000 [00:29<03:02, 46.55it/s]Training CobwebTree:  15%|        | 1505/10000 [00:29<03:17, 43.10it/s]Training CobwebTree:  15%|        | 1510/10000 [00:29<03:13, 43.89it/s]Training CobwebTree:  15%|        | 1515/10000 [00:29<03:21, 42.20it/s]Training CobwebTree:  15%|        | 1520/10000 [00:29<03:29, 40.56it/s]Training CobwebTree:  15%|        | 1525/10000 [00:30<03:18, 42.73it/s]Training CobwebTree:  15%|        | 1530/10000 [00:30<03:28, 40.69it/s]Training CobwebTree:  15%|        | 1535/10000 [00:30<03:20, 42.13it/s]Training CobwebTree:  15%|        | 1540/10000 [00:30<03:18, 42.67it/s]Training CobwebTree:  15%|        | 1545/10000 [00:30<03:23, 41.47it/s]Training CobwebTree:  16%|        | 1551/10000 [00:30<03:13, 43.76it/s]Training CobwebTree:  16%|        | 1556/10000 [00:30<03:12, 43.93it/s]Training CobwebTree:  16%|        | 1561/10000 [00:30<03:16, 43.04it/s]Training CobwebTree:  16%|        | 1566/10000 [00:31<03:26, 40.89it/s]Training CobwebTree:  16%|        | 1571/10000 [00:31<03:31, 39.86it/s]Training CobwebTree:  16%|        | 1576/10000 [00:31<03:24, 41.14it/s]Training CobwebTree:  16%|        | 1581/10000 [00:31<03:24, 41.21it/s]Training CobwebTree:  16%|        | 1586/10000 [00:31<03:26, 40.76it/s]Training CobwebTree:  16%|        | 1591/10000 [00:31<03:24, 41.18it/s]Training CobwebTree:  16%|        | 1596/10000 [00:31<03:17, 42.63it/s]Training CobwebTree:  16%|        | 1601/10000 [00:31<03:18, 42.24it/s]Training CobwebTree:  16%|        | 1606/10000 [00:32<03:12, 43.58it/s]Training CobwebTree:  16%|        | 1612/10000 [00:32<03:01, 46.25it/s]Training CobwebTree:  16%|        | 1617/10000 [00:32<02:58, 46.84it/s]Training CobwebTree:  16%|        | 1622/10000 [00:32<03:05, 45.23it/s]Training CobwebTree:  16%|        | 1627/10000 [00:32<03:11, 43.76it/s]Training CobwebTree:  16%|        | 1632/10000 [00:32<03:13, 43.31it/s]Training CobwebTree:  16%|        | 1637/10000 [00:32<03:24, 40.81it/s]Training CobwebTree:  16%|        | 1642/10000 [00:32<03:19, 41.89it/s]Training CobwebTree:  16%|        | 1647/10000 [00:32<03:12, 43.31it/s]Training CobwebTree:  17%|        | 1652/10000 [00:33<03:13, 43.06it/s]Training CobwebTree:  17%|        | 1657/10000 [00:33<03:07, 44.55it/s]Training CobwebTree:  17%|        | 1662/10000 [00:33<03:22, 41.22it/s]Training CobwebTree:  17%|        | 1667/10000 [00:33<03:19, 41.69it/s]Training CobwebTree:  17%|        | 1672/10000 [00:33<03:20, 41.61it/s]Training CobwebTree:  17%|        | 1677/10000 [00:33<03:23, 40.93it/s]Training CobwebTree:  17%|        | 1682/10000 [00:33<03:17, 42.08it/s]Training CobwebTree:  17%|        | 1688/10000 [00:33<03:06, 44.57it/s]Training CobwebTree:  17%|        | 1693/10000 [00:34<03:22, 41.08it/s]Training CobwebTree:  17%|        | 1698/10000 [00:34<03:19, 41.68it/s]Training CobwebTree:  17%|        | 1703/10000 [00:34<03:24, 40.59it/s]Training CobwebTree:  17%|        | 1708/10000 [00:34<03:13, 42.86it/s]Training CobwebTree:  17%|        | 1713/10000 [00:34<03:09, 43.84it/s]Training CobwebTree:  17%|        | 1718/10000 [00:34<03:16, 42.22it/s]Training CobwebTree:  17%|        | 1723/10000 [00:34<03:18, 41.68it/s]Training CobwebTree:  17%|        | 1728/10000 [00:34<03:18, 41.65it/s]Training CobwebTree:  17%|        | 1733/10000 [00:34<03:10, 43.49it/s]Training CobwebTree:  17%|        | 1738/10000 [00:35<03:17, 41.93it/s]Training CobwebTree:  17%|        | 1743/10000 [00:35<03:08, 43.82it/s]Training CobwebTree:  17%|        | 1748/10000 [00:35<03:16, 41.97it/s]Training CobwebTree:  18%|        | 1753/10000 [00:35<03:15, 42.24it/s]Training CobwebTree:  18%|        | 1758/10000 [00:35<03:22, 40.73it/s]Training CobwebTree:  18%|        | 1763/10000 [00:35<03:21, 40.96it/s]Training CobwebTree:  18%|        | 1768/10000 [00:35<03:21, 40.95it/s]Training CobwebTree:  18%|        | 1773/10000 [00:35<03:27, 39.67it/s]Training CobwebTree:  18%|        | 1778/10000 [00:36<03:16, 41.76it/s]Training CobwebTree:  18%|        | 1783/10000 [00:36<03:19, 41.12it/s]Training CobwebTree:  18%|        | 1788/10000 [00:36<03:23, 40.28it/s]Training CobwebTree:  18%|        | 1793/10000 [00:36<03:18, 41.41it/s]Training CobwebTree:  18%|        | 1798/10000 [00:36<03:29, 39.21it/s]Training CobwebTree:  18%|        | 1802/10000 [00:36<03:38, 37.60it/s]Training CobwebTree:  18%|        | 1806/10000 [00:36<03:35, 37.98it/s]Training CobwebTree:  18%|        | 1811/10000 [00:36<03:29, 39.03it/s]Training CobwebTree:  18%|        | 1816/10000 [00:37<03:26, 39.61it/s]Training CobwebTree:  18%|        | 1821/10000 [00:37<03:20, 40.72it/s]Training CobwebTree:  18%|        | 1826/10000 [00:37<03:25, 39.76it/s]Training CobwebTree:  18%|        | 1830/10000 [00:37<03:28, 39.16it/s]Training CobwebTree:  18%|        | 1834/10000 [00:37<03:40, 37.02it/s]Training CobwebTree:  18%|        | 1839/10000 [00:37<03:21, 40.40it/s]Training CobwebTree:  18%|        | 1844/10000 [00:37<03:26, 39.44it/s]Training CobwebTree:  18%|        | 1848/10000 [00:37<03:32, 38.33it/s]Training CobwebTree:  19%|        | 1852/10000 [00:37<03:31, 38.59it/s]Training CobwebTree:  19%|        | 1857/10000 [00:38<03:26, 39.44it/s]Training CobwebTree:  19%|        | 1861/10000 [00:38<03:33, 38.11it/s]Training CobwebTree:  19%|        | 1866/10000 [00:38<03:26, 39.39it/s]Training CobwebTree:  19%|        | 1870/10000 [00:38<03:29, 38.84it/s]Training CobwebTree:  19%|        | 1874/10000 [00:38<03:27, 39.12it/s]Training CobwebTree:  19%|        | 1879/10000 [00:38<03:23, 39.91it/s]Training CobwebTree:  19%|        | 1884/10000 [00:38<03:20, 40.49it/s]Training CobwebTree:  19%|        | 1889/10000 [00:38<03:14, 41.71it/s]Training CobwebTree:  19%|        | 1894/10000 [00:39<03:18, 40.94it/s]Training CobwebTree:  19%|        | 1899/10000 [00:39<03:26, 39.16it/s]Training CobwebTree:  19%|        | 1903/10000 [00:39<03:28, 38.88it/s]Training CobwebTree:  19%|        | 1908/10000 [00:39<03:22, 39.93it/s]Training CobwebTree:  19%|        | 1913/10000 [00:39<03:15, 41.44it/s]Training CobwebTree:  19%|        | 1918/10000 [00:39<03:17, 40.93it/s]Training CobwebTree:  19%|        | 1923/10000 [00:39<03:12, 41.91it/s]Training CobwebTree:  19%|        | 1928/10000 [00:39<03:24, 39.45it/s]Training CobwebTree:  19%|        | 1933/10000 [00:39<03:19, 40.48it/s]Training CobwebTree:  19%|        | 1938/10000 [00:40<03:15, 41.20it/s]Training CobwebTree:  19%|        | 1943/10000 [00:40<03:11, 42.10it/s]Training CobwebTree:  19%|        | 1948/10000 [00:40<03:06, 43.11it/s]Training CobwebTree:  20%|        | 1953/10000 [00:40<02:58, 44.96it/s]Training CobwebTree:  20%|        | 1958/10000 [00:40<03:00, 44.52it/s]Training CobwebTree:  20%|        | 1963/10000 [00:40<03:15, 41.17it/s]Training CobwebTree:  20%|        | 1968/10000 [00:40<03:18, 40.48it/s]Training CobwebTree:  20%|        | 1973/10000 [00:40<03:18, 40.43it/s]Training CobwebTree:  20%|        | 1978/10000 [00:41<03:20, 40.07it/s]Training CobwebTree:  20%|        | 1983/10000 [00:41<03:10, 42.02it/s]Training CobwebTree:  20%|        | 1988/10000 [00:41<03:12, 41.69it/s]Training CobwebTree:  20%|        | 1993/10000 [00:41<03:20, 39.96it/s]Training CobwebTree:  20%|        | 1998/10000 [00:41<03:16, 40.68it/s]Training CobwebTree:  20%|        | 2003/10000 [00:41<03:13, 41.39it/s]Training CobwebTree:  20%|        | 2008/10000 [00:41<03:28, 38.30it/s]Training CobwebTree:  20%|        | 2012/10000 [00:41<03:31, 37.83it/s]Training CobwebTree:  20%|        | 2017/10000 [00:42<03:22, 39.37it/s]Training CobwebTree:  20%|        | 2021/10000 [00:42<03:26, 38.69it/s]Training CobwebTree:  20%|        | 2026/10000 [00:42<03:16, 40.54it/s]Training CobwebTree:  20%|        | 2031/10000 [00:42<03:19, 39.92it/s]Training CobwebTree:  20%|        | 2036/10000 [00:42<03:18, 40.20it/s]Training CobwebTree:  20%|        | 2041/10000 [00:42<03:15, 40.72it/s]Training CobwebTree:  20%|        | 2046/10000 [00:42<03:18, 40.02it/s]Training CobwebTree:  21%|        | 2051/10000 [00:42<03:13, 41.07it/s]Training CobwebTree:  21%|        | 2056/10000 [00:42<03:11, 41.51it/s]Training CobwebTree:  21%|        | 2061/10000 [00:43<03:09, 41.98it/s]Training CobwebTree:  21%|        | 2066/10000 [00:43<03:13, 40.93it/s]Training CobwebTree:  21%|        | 2071/10000 [00:43<03:06, 42.54it/s]Training CobwebTree:  21%|        | 2076/10000 [00:43<03:12, 41.08it/s]Training CobwebTree:  21%|        | 2081/10000 [00:43<03:19, 39.78it/s]Training CobwebTree:  21%|        | 2086/10000 [00:43<03:15, 40.46it/s]Training CobwebTree:  21%|        | 2091/10000 [00:43<03:17, 39.96it/s]Training CobwebTree:  21%|        | 2096/10000 [00:43<03:13, 40.80it/s]Training CobwebTree:  21%|        | 2101/10000 [00:44<03:19, 39.61it/s]Training CobwebTree:  21%|        | 2105/10000 [00:44<03:26, 38.18it/s]Training CobwebTree:  21%|        | 2110/10000 [00:44<03:14, 40.52it/s]Training CobwebTree:  21%|        | 2115/10000 [00:44<03:06, 42.22it/s]Training CobwebTree:  21%|        | 2120/10000 [00:44<03:03, 42.89it/s]Training CobwebTree:  21%|       | 2125/10000 [00:44<03:08, 41.82it/s]Training CobwebTree:  21%|       | 2130/10000 [00:44<03:10, 41.33it/s]Training CobwebTree:  21%|       | 2135/10000 [00:44<03:21, 39.04it/s]Training CobwebTree:  21%|       | 2139/10000 [00:45<03:26, 38.10it/s]Training CobwebTree:  21%|       | 2143/10000 [00:45<03:30, 37.38it/s]Training CobwebTree:  21%|       | 2147/10000 [00:45<03:26, 37.96it/s]Training CobwebTree:  22%|       | 2151/10000 [00:45<03:28, 37.61it/s]Training CobwebTree:  22%|       | 2156/10000 [00:45<03:24, 38.37it/s]Training CobwebTree:  22%|       | 2160/10000 [00:45<03:26, 37.95it/s]Training CobwebTree:  22%|       | 2165/10000 [00:45<03:19, 39.31it/s]Training CobwebTree:  22%|       | 2171/10000 [00:45<03:00, 43.45it/s]Training CobwebTree:  22%|       | 2176/10000 [00:45<03:04, 42.33it/s]Training CobwebTree:  22%|       | 2181/10000 [00:46<03:11, 40.80it/s]Training CobwebTree:  22%|       | 2186/10000 [00:46<03:07, 41.63it/s]Training CobwebTree:  22%|       | 2191/10000 [00:46<03:07, 41.74it/s]Training CobwebTree:  22%|       | 2196/10000 [00:46<03:10, 40.95it/s]Training CobwebTree:  22%|       | 2201/10000 [00:46<03:05, 42.00it/s]Training CobwebTree:  22%|       | 2206/10000 [00:46<03:04, 42.23it/s]Training CobwebTree:  22%|       | 2211/10000 [00:46<03:05, 41.96it/s]Training CobwebTree:  22%|       | 2216/10000 [00:46<03:09, 41.14it/s]Training CobwebTree:  22%|       | 2221/10000 [00:47<03:12, 40.34it/s]Training CobwebTree:  22%|       | 2226/10000 [00:47<03:12, 40.43it/s]Training CobwebTree:  22%|       | 2231/10000 [00:47<03:08, 41.23it/s]Training CobwebTree:  22%|       | 2237/10000 [00:47<02:56, 44.10it/s]Training CobwebTree:  22%|       | 2242/10000 [00:47<03:06, 41.60it/s]Training CobwebTree:  22%|       | 2247/10000 [00:47<03:06, 41.63it/s]Training CobwebTree:  23%|       | 2252/10000 [00:47<03:24, 37.94it/s]Training CobwebTree:  23%|       | 2256/10000 [00:47<03:24, 37.84it/s]Training CobwebTree:  23%|       | 2260/10000 [00:48<03:27, 37.29it/s]Training CobwebTree:  23%|       | 2264/10000 [00:48<03:24, 37.81it/s]Training CobwebTree:  23%|       | 2269/10000 [00:48<03:15, 39.62it/s]Training CobwebTree:  23%|       | 2274/10000 [00:48<03:15, 39.48it/s]Training CobwebTree:  23%|       | 2279/10000 [00:48<03:15, 39.46it/s]Training CobwebTree:  23%|       | 2283/10000 [00:48<03:17, 39.13it/s]Training CobwebTree:  23%|       | 2288/10000 [00:48<03:14, 39.58it/s]Training CobwebTree:  23%|       | 2293/10000 [00:48<03:11, 40.20it/s]Training CobwebTree:  23%|       | 2298/10000 [00:49<03:22, 38.00it/s]Training CobwebTree:  23%|       | 2302/10000 [00:49<03:32, 36.31it/s]Training CobwebTree:  23%|       | 2307/10000 [00:49<03:25, 37.37it/s]Training CobwebTree:  23%|       | 2311/10000 [00:49<03:24, 37.62it/s]Training CobwebTree:  23%|       | 2315/10000 [00:49<03:23, 37.74it/s]Training CobwebTree:  23%|       | 2319/10000 [00:49<03:30, 36.43it/s]Training CobwebTree:  23%|       | 2323/10000 [00:49<03:31, 36.34it/s]Training CobwebTree:  23%|       | 2328/10000 [00:49<03:23, 37.73it/s]Training CobwebTree:  23%|       | 2332/10000 [00:49<03:26, 37.07it/s]Training CobwebTree:  23%|       | 2336/10000 [00:50<03:32, 36.13it/s]Training CobwebTree:  23%|       | 2340/10000 [00:50<03:35, 35.56it/s]Training CobwebTree:  23%|       | 2344/10000 [00:50<03:33, 35.84it/s]Training CobwebTree:  23%|       | 2349/10000 [00:50<03:27, 36.86it/s]Training CobwebTree:  24%|       | 2353/10000 [00:50<03:33, 35.75it/s]Training CobwebTree:  24%|       | 2357/10000 [00:50<03:32, 36.02it/s]Training CobwebTree:  24%|       | 2361/10000 [00:50<03:30, 36.34it/s]Training CobwebTree:  24%|       | 2365/10000 [00:50<03:25, 37.16it/s]Training CobwebTree:  24%|       | 2370/10000 [00:50<03:19, 38.15it/s]Training CobwebTree:  24%|       | 2375/10000 [00:51<03:10, 39.96it/s]Training CobwebTree:  24%|       | 2379/10000 [00:51<03:13, 39.42it/s]Training CobwebTree:  24%|       | 2383/10000 [00:51<03:15, 38.97it/s]Training CobwebTree:  24%|       | 2387/10000 [00:51<03:18, 38.30it/s]Training CobwebTree:  24%|       | 2391/10000 [00:51<03:23, 37.31it/s]Training CobwebTree:  24%|       | 2395/10000 [00:51<03:22, 37.57it/s]Training CobwebTree:  24%|       | 2400/10000 [00:51<03:12, 39.46it/s]Training CobwebTree:  24%|       | 2404/10000 [00:51<03:12, 39.47it/s]Training CobwebTree:  24%|       | 2408/10000 [00:51<03:22, 37.49it/s]Training CobwebTree:  24%|       | 2412/10000 [00:52<03:19, 37.94it/s]Training CobwebTree:  24%|       | 2417/10000 [00:52<03:09, 39.91it/s]Training CobwebTree:  24%|       | 2421/10000 [00:52<03:19, 38.01it/s]Training CobwebTree:  24%|       | 2426/10000 [00:52<03:12, 39.43it/s]Training CobwebTree:  24%|       | 2431/10000 [00:52<03:10, 39.73it/s]Training CobwebTree:  24%|       | 2435/10000 [00:52<03:16, 38.53it/s]Training CobwebTree:  24%|       | 2440/10000 [00:52<03:10, 39.69it/s]Training CobwebTree:  24%|       | 2444/10000 [00:52<03:10, 39.73it/s]Training CobwebTree:  24%|       | 2448/10000 [00:52<03:13, 38.94it/s]Training CobwebTree:  25%|       | 2453/10000 [00:53<03:08, 39.99it/s]Training CobwebTree:  25%|       | 2457/10000 [00:53<03:09, 39.76it/s]Training CobwebTree:  25%|       | 2461/10000 [00:53<03:11, 39.38it/s]Training CobwebTree:  25%|       | 2466/10000 [00:53<03:08, 39.96it/s]Training CobwebTree:  25%|       | 2470/10000 [00:53<03:15, 38.53it/s]Training CobwebTree:  25%|       | 2475/10000 [00:53<03:12, 39.18it/s]Training CobwebTree:  25%|       | 2480/10000 [00:53<03:03, 41.03it/s]Training CobwebTree:  25%|       | 2485/10000 [00:53<03:11, 39.15it/s]Training CobwebTree:  25%|       | 2489/10000 [00:54<03:21, 37.23it/s]Training CobwebTree:  25%|       | 2493/10000 [00:54<03:18, 37.74it/s]Training CobwebTree:  25%|       | 2498/10000 [00:54<03:15, 38.38it/s]Training CobwebTree:  25%|       | 2502/10000 [00:54<03:24, 36.66it/s]Training CobwebTree:  25%|       | 2506/10000 [00:54<03:25, 36.50it/s]Training CobwebTree:  25%|       | 2510/10000 [00:54<03:28, 35.95it/s]Training CobwebTree:  25%|       | 2514/10000 [00:54<03:33, 35.03it/s]Training CobwebTree:  25%|       | 2518/10000 [00:54<03:46, 32.98it/s]Training CobwebTree:  25%|       | 2522/10000 [00:54<03:46, 33.05it/s]Training CobwebTree:  25%|       | 2527/10000 [00:55<03:29, 35.60it/s]Training CobwebTree:  25%|       | 2531/10000 [00:55<03:24, 36.46it/s]Training CobwebTree:  25%|       | 2535/10000 [00:55<03:30, 35.40it/s]Training CobwebTree:  25%|       | 2539/10000 [00:55<03:31, 35.21it/s]Training CobwebTree:  25%|       | 2543/10000 [00:55<03:26, 36.16it/s]Training CobwebTree:  25%|       | 2547/10000 [00:55<03:33, 34.89it/s]Training CobwebTree:  26%|       | 2551/10000 [00:55<03:35, 34.62it/s]Training CobwebTree:  26%|       | 2556/10000 [00:55<03:26, 36.09it/s]Training CobwebTree:  26%|       | 2561/10000 [00:56<03:15, 38.03it/s]Training CobwebTree:  26%|       | 2566/10000 [00:56<03:10, 39.07it/s]Training CobwebTree:  26%|       | 2570/10000 [00:56<03:11, 38.82it/s]Training CobwebTree:  26%|       | 2574/10000 [00:56<03:12, 38.64it/s]Training CobwebTree:  26%|       | 2578/10000 [00:56<03:17, 37.60it/s]Training CobwebTree:  26%|       | 2583/10000 [00:56<03:13, 38.33it/s]Training CobwebTree:  26%|       | 2588/10000 [00:56<03:07, 39.52it/s]Training CobwebTree:  26%|       | 2593/10000 [00:56<03:10, 38.82it/s]Training CobwebTree:  26%|       | 2597/10000 [00:56<03:15, 37.96it/s]Training CobwebTree:  26%|       | 2602/10000 [00:57<03:08, 39.30it/s]Training CobwebTree:  26%|       | 2607/10000 [00:57<03:05, 39.86it/s]Training CobwebTree:  26%|       | 2612/10000 [00:57<02:55, 42.14it/s]Training CobwebTree:  26%|       | 2617/10000 [00:57<03:05, 39.88it/s]Training CobwebTree:  26%|       | 2622/10000 [00:57<03:12, 38.36it/s]Training CobwebTree:  26%|       | 2627/10000 [00:57<03:07, 39.23it/s]Training CobwebTree:  26%|       | 2631/10000 [00:57<03:08, 39.10it/s]Training CobwebTree:  26%|       | 2636/10000 [00:57<02:56, 41.76it/s]Training CobwebTree:  26%|       | 2641/10000 [00:58<02:55, 41.92it/s]Training CobwebTree:  26%|       | 2646/10000 [00:58<03:10, 38.58it/s]Training CobwebTree:  26%|       | 2650/10000 [00:58<03:14, 37.83it/s]Training CobwebTree:  27%|       | 2654/10000 [00:58<03:14, 37.68it/s]Training CobwebTree:  27%|       | 2658/10000 [00:58<03:17, 37.24it/s]Training CobwebTree:  27%|       | 2663/10000 [00:58<03:13, 37.93it/s]Training CobwebTree:  27%|       | 2667/10000 [00:58<03:12, 38.01it/s]Training CobwebTree:  27%|       | 2671/10000 [00:58<03:18, 36.98it/s]Training CobwebTree:  27%|       | 2676/10000 [00:58<03:10, 38.46it/s]Training CobwebTree:  27%|       | 2681/10000 [00:59<03:04, 39.59it/s]Training CobwebTree:  27%|       | 2685/10000 [00:59<03:07, 39.04it/s]Training CobwebTree:  27%|       | 2689/10000 [00:59<03:06, 39.24it/s]Training CobwebTree:  27%|       | 2693/10000 [00:59<03:10, 38.33it/s]Training CobwebTree:  27%|       | 2698/10000 [00:59<03:09, 38.56it/s]Training CobwebTree:  27%|       | 2703/10000 [00:59<03:05, 39.30it/s]Training CobwebTree:  27%|       | 2707/10000 [00:59<03:17, 36.99it/s]Training CobwebTree:  27%|       | 2712/10000 [00:59<03:09, 38.50it/s]Training CobwebTree:  27%|       | 2717/10000 [01:00<03:07, 38.88it/s]Training CobwebTree:  27%|       | 2721/10000 [01:00<03:14, 37.49it/s]Training CobwebTree:  27%|       | 2726/10000 [01:00<03:13, 37.62it/s]Training CobwebTree:  27%|       | 2731/10000 [01:00<03:04, 39.30it/s]Training CobwebTree:  27%|       | 2735/10000 [01:00<03:05, 39.09it/s]Training CobwebTree:  27%|       | 2739/10000 [01:00<03:08, 38.44it/s]Training CobwebTree:  27%|       | 2743/10000 [01:00<03:09, 38.21it/s]Training CobwebTree:  27%|       | 2747/10000 [01:00<03:09, 38.31it/s]Training CobwebTree:  28%|       | 2752/10000 [01:00<03:08, 38.49it/s]Training CobwebTree:  28%|       | 2756/10000 [01:01<03:16, 36.78it/s]Training CobwebTree:  28%|       | 2760/10000 [01:01<03:14, 37.24it/s]Training CobwebTree:  28%|       | 2764/10000 [01:01<03:17, 36.56it/s]Training CobwebTree:  28%|       | 2768/10000 [01:01<03:15, 36.96it/s]Training CobwebTree:  28%|       | 2773/10000 [01:01<03:17, 36.55it/s]Training CobwebTree:  28%|       | 2777/10000 [01:01<03:21, 35.84it/s]Training CobwebTree:  28%|       | 2781/10000 [01:01<03:22, 35.71it/s]Training CobwebTree:  28%|       | 2785/10000 [01:01<03:16, 36.63it/s]Training CobwebTree:  28%|       | 2789/10000 [01:01<03:14, 37.01it/s]Training CobwebTree:  28%|       | 2794/10000 [01:02<03:12, 37.43it/s]Training CobwebTree:  28%|       | 2798/10000 [01:02<03:09, 37.95it/s]Training CobwebTree:  28%|       | 2803/10000 [01:02<02:58, 40.28it/s]Training CobwebTree:  28%|       | 2808/10000 [01:02<02:50, 42.15it/s]Training CobwebTree:  28%|       | 2813/10000 [01:02<02:50, 42.04it/s]Training CobwebTree:  28%|       | 2818/10000 [01:02<02:52, 41.57it/s]Training CobwebTree:  28%|       | 2823/10000 [01:02<02:51, 41.84it/s]Training CobwebTree:  28%|       | 2828/10000 [01:02<02:51, 41.78it/s]Training CobwebTree:  28%|       | 2833/10000 [01:03<02:50, 42.12it/s]Training CobwebTree:  28%|       | 2838/10000 [01:03<03:02, 39.33it/s]Training CobwebTree:  28%|       | 2843/10000 [01:03<02:56, 40.46it/s]Training CobwebTree:  28%|       | 2848/10000 [01:03<02:47, 42.69it/s]Training CobwebTree:  29%|       | 2853/10000 [01:03<02:52, 41.39it/s]Training CobwebTree:  29%|       | 2858/10000 [01:03<02:59, 39.86it/s]Training CobwebTree:  29%|       | 2863/10000 [01:03<02:57, 40.14it/s]Training CobwebTree:  29%|       | 2868/10000 [01:03<02:59, 39.80it/s]Training CobwebTree:  29%|       | 2873/10000 [01:04<02:53, 41.05it/s]Training CobwebTree:  29%|       | 2878/10000 [01:04<02:54, 40.92it/s]Training CobwebTree:  29%|       | 2883/10000 [01:04<02:52, 41.33it/s]Training CobwebTree:  29%|       | 2888/10000 [01:04<02:54, 40.81it/s]Training CobwebTree:  29%|       | 2893/10000 [01:04<02:50, 41.65it/s]Training CobwebTree:  29%|       | 2898/10000 [01:04<02:58, 39.86it/s]Training CobwebTree:  29%|       | 2903/10000 [01:04<03:08, 37.70it/s]Training CobwebTree:  29%|       | 2909/10000 [01:04<02:52, 41.02it/s]Training CobwebTree:  29%|       | 2914/10000 [01:05<02:53, 40.74it/s]Training CobwebTree:  29%|       | 2919/10000 [01:05<03:05, 38.12it/s]Training CobwebTree:  29%|       | 2923/10000 [01:05<03:04, 38.33it/s]Training CobwebTree:  29%|       | 2928/10000 [01:05<03:01, 38.98it/s]Training CobwebTree:  29%|       | 2932/10000 [01:05<03:00, 39.23it/s]Training CobwebTree:  29%|       | 2936/10000 [01:05<03:02, 38.64it/s]Training CobwebTree:  29%|       | 2940/10000 [01:05<03:03, 38.38it/s]Training CobwebTree:  29%|       | 2944/10000 [01:05<03:07, 37.62it/s]Training CobwebTree:  29%|       | 2949/10000 [01:05<03:01, 38.78it/s]Training CobwebTree:  30%|       | 2954/10000 [01:06<02:57, 39.74it/s]Training CobwebTree:  30%|       | 2958/10000 [01:06<03:03, 38.31it/s]Training CobwebTree:  30%|       | 2962/10000 [01:06<03:09, 37.18it/s]Training CobwebTree:  30%|       | 2966/10000 [01:06<03:07, 37.48it/s]Training CobwebTree:  30%|       | 2971/10000 [01:06<03:02, 38.62it/s]Training CobwebTree:  30%|       | 2975/10000 [01:06<03:03, 38.33it/s]Training CobwebTree:  30%|       | 2979/10000 [01:06<03:05, 37.78it/s]Training CobwebTree:  30%|       | 2983/10000 [01:06<03:17, 35.61it/s]Training CobwebTree:  30%|       | 2988/10000 [01:07<03:05, 37.80it/s]Training CobwebTree:  30%|       | 2992/10000 [01:07<03:03, 38.22it/s]Training CobwebTree:  30%|       | 2996/10000 [01:07<03:01, 38.63it/s]Training CobwebTree:  30%|       | 3000/10000 [01:07<03:02, 38.29it/s]Training CobwebTree:  30%|       | 3005/10000 [01:07<02:56, 39.71it/s]Training CobwebTree:  30%|       | 3009/10000 [01:07<03:07, 37.23it/s]Training CobwebTree:  30%|       | 3014/10000 [01:07<02:56, 39.64it/s]Training CobwebTree:  30%|       | 3019/10000 [01:07<02:45, 42.20it/s]Training CobwebTree:  30%|       | 3024/10000 [01:07<02:52, 40.34it/s]Training CobwebTree:  30%|       | 3029/10000 [01:08<02:50, 40.98it/s]Training CobwebTree:  30%|       | 3034/10000 [01:08<02:49, 41.04it/s]Training CobwebTree:  30%|       | 3039/10000 [01:08<03:01, 38.44it/s]Training CobwebTree:  30%|       | 3043/10000 [01:08<03:10, 36.58it/s]Training CobwebTree:  30%|       | 3047/10000 [01:08<03:11, 36.34it/s]Training CobwebTree:  31%|       | 3051/10000 [01:08<03:09, 36.72it/s]Training CobwebTree:  31%|       | 3055/10000 [01:08<03:12, 36.13it/s]Training CobwebTree:  31%|       | 3059/10000 [01:08<03:08, 36.86it/s]Training CobwebTree:  31%|       | 3064/10000 [01:08<02:59, 38.60it/s]Training CobwebTree:  31%|       | 3068/10000 [01:09<02:59, 38.68it/s]Training CobwebTree:  31%|       | 3072/10000 [01:09<03:07, 36.98it/s]Training CobwebTree:  31%|       | 3076/10000 [01:09<03:17, 35.05it/s]Training CobwebTree:  31%|       | 3080/10000 [01:09<03:14, 35.65it/s]Training CobwebTree:  31%|       | 3084/10000 [01:09<03:15, 35.45it/s]Training CobwebTree:  31%|       | 3088/10000 [01:09<03:09, 36.42it/s]Training CobwebTree:  31%|       | 3093/10000 [01:09<03:00, 38.25it/s]Training CobwebTree:  31%|       | 3097/10000 [01:09<03:12, 35.81it/s]Training CobwebTree:  31%|       | 3101/10000 [01:10<03:12, 35.83it/s]Training CobwebTree:  31%|       | 3106/10000 [01:10<03:06, 36.87it/s]Training CobwebTree:  31%|       | 3110/10000 [01:10<03:06, 36.86it/s]Training CobwebTree:  31%|       | 3115/10000 [01:10<03:04, 37.30it/s]Training CobwebTree:  31%|       | 3119/10000 [01:10<03:08, 36.53it/s]Training CobwebTree:  31%|       | 3124/10000 [01:10<02:57, 38.65it/s]Training CobwebTree:  31%|      | 3129/10000 [01:10<02:51, 40.06it/s]Training CobwebTree:  31%|      | 3134/10000 [01:10<02:58, 38.42it/s]Training CobwebTree:  31%|      | 3138/10000 [01:10<03:01, 37.90it/s]Training CobwebTree:  31%|      | 3143/10000 [01:11<02:54, 39.22it/s]Training CobwebTree:  31%|      | 3147/10000 [01:11<02:56, 38.80it/s]Training CobwebTree:  32%|      | 3151/10000 [01:11<03:08, 36.33it/s]Training CobwebTree:  32%|      | 3155/10000 [01:11<03:05, 36.85it/s]Training CobwebTree:  32%|      | 3159/10000 [01:11<03:10, 35.93it/s]Training CobwebTree:  32%|      | 3163/10000 [01:11<03:10, 35.88it/s]Training CobwebTree:  32%|      | 3167/10000 [01:11<03:09, 35.97it/s]Training CobwebTree:  32%|      | 3172/10000 [01:11<03:06, 36.67it/s]Training CobwebTree:  32%|      | 3176/10000 [01:12<03:05, 36.74it/s]Training CobwebTree:  32%|      | 3180/10000 [01:12<03:23, 33.58it/s]Training CobwebTree:  32%|      | 3184/10000 [01:12<03:16, 34.72it/s]Training CobwebTree:  32%|      | 3189/10000 [01:12<03:01, 37.45it/s]Training CobwebTree:  32%|      | 3194/10000 [01:12<02:51, 39.63it/s]Training CobwebTree:  32%|      | 3199/10000 [01:12<02:47, 40.50it/s]Training CobwebTree:  32%|      | 3204/10000 [01:12<02:52, 39.43it/s]Training CobwebTree:  32%|      | 3208/10000 [01:12<02:53, 39.10it/s]Training CobwebTree:  32%|      | 3212/10000 [01:12<02:56, 38.42it/s]Training CobwebTree:  32%|      | 3216/10000 [01:13<03:06, 36.33it/s]Training CobwebTree:  32%|      | 3221/10000 [01:13<02:59, 37.75it/s]Training CobwebTree:  32%|      | 3225/10000 [01:13<03:04, 36.67it/s]Training CobwebTree:  32%|      | 3229/10000 [01:13<03:03, 36.80it/s]Training CobwebTree:  32%|      | 3233/10000 [01:13<03:07, 36.16it/s]Training CobwebTree:  32%|      | 3237/10000 [01:13<03:10, 35.49it/s]Training CobwebTree:  32%|      | 3242/10000 [01:13<03:06, 36.20it/s]Training CobwebTree:  32%|      | 3247/10000 [01:13<02:59, 37.61it/s]Training CobwebTree:  33%|      | 3251/10000 [01:14<03:02, 37.03it/s]Training CobwebTree:  33%|      | 3255/10000 [01:14<03:01, 37.25it/s]Training CobwebTree:  33%|      | 3260/10000 [01:14<02:53, 38.84it/s]Training CobwebTree:  33%|      | 3264/10000 [01:14<02:54, 38.52it/s]Training CobwebTree:  33%|      | 3268/10000 [01:14<03:04, 36.49it/s]Training CobwebTree:  33%|      | 3272/10000 [01:14<03:09, 35.45it/s]Training CobwebTree:  33%|      | 3277/10000 [01:14<03:00, 37.29it/s]Training CobwebTree:  33%|      | 3281/10000 [01:14<03:00, 37.30it/s]Training CobwebTree:  33%|      | 3285/10000 [01:14<03:08, 35.54it/s]Training CobwebTree:  33%|      | 3289/10000 [01:15<03:14, 34.49it/s]Training CobwebTree:  33%|      | 3293/10000 [01:15<03:13, 34.67it/s]Training CobwebTree:  33%|      | 3297/10000 [01:15<03:10, 35.18it/s]Training CobwebTree:  33%|      | 3302/10000 [01:15<03:04, 36.21it/s]Training CobwebTree:  33%|      | 3306/10000 [01:15<03:13, 34.56it/s]Training CobwebTree:  33%|      | 3310/10000 [01:15<03:14, 34.33it/s]Training CobwebTree:  33%|      | 3314/10000 [01:15<03:16, 34.01it/s]Training CobwebTree:  33%|      | 3318/10000 [01:15<03:10, 35.15it/s]Training CobwebTree:  33%|      | 3323/10000 [01:16<02:58, 37.49it/s]Training CobwebTree:  33%|      | 3327/10000 [01:16<03:00, 36.99it/s]Training CobwebTree:  33%|      | 3331/10000 [01:16<03:04, 36.23it/s]Training CobwebTree:  33%|      | 3336/10000 [01:16<02:58, 37.31it/s]Training CobwebTree:  33%|      | 3340/10000 [01:16<03:09, 35.10it/s]Training CobwebTree:  33%|      | 3344/10000 [01:16<03:13, 34.36it/s]Training CobwebTree:  33%|      | 3348/10000 [01:16<03:16, 33.83it/s]Training CobwebTree:  34%|      | 3352/10000 [01:16<03:17, 33.74it/s]Training CobwebTree:  34%|      | 3357/10000 [01:16<02:59, 36.95it/s]Training CobwebTree:  34%|      | 3362/10000 [01:17<03:00, 36.84it/s]Training CobwebTree:  34%|      | 3366/10000 [01:17<03:02, 36.38it/s]Training CobwebTree:  34%|      | 3370/10000 [01:17<03:04, 36.01it/s]Training CobwebTree:  34%|      | 3374/10000 [01:17<02:59, 36.95it/s]Training CobwebTree:  34%|      | 3379/10000 [01:17<02:49, 39.05it/s]Training CobwebTree:  34%|      | 3383/10000 [01:17<02:49, 39.15it/s]Training CobwebTree:  34%|      | 3387/10000 [01:17<02:51, 38.64it/s]Training CobwebTree:  34%|      | 3391/10000 [01:17<02:56, 37.55it/s]Training CobwebTree:  34%|      | 3395/10000 [01:17<02:57, 37.20it/s]Training CobwebTree:  34%|      | 3400/10000 [01:18<02:52, 38.26it/s]Training CobwebTree:  34%|      | 3405/10000 [01:18<02:52, 38.17it/s]Training CobwebTree:  34%|      | 3409/10000 [01:18<02:57, 37.12it/s]Training CobwebTree:  34%|      | 3413/10000 [01:18<02:58, 36.85it/s]Training CobwebTree:  34%|      | 3417/10000 [01:18<02:59, 36.58it/s]Training CobwebTree:  34%|      | 3421/10000 [01:18<03:00, 36.39it/s]Training CobwebTree:  34%|      | 3425/10000 [01:18<03:05, 35.47it/s]Training CobwebTree:  34%|      | 3429/10000 [01:18<03:02, 36.04it/s]Training CobwebTree:  34%|      | 3433/10000 [01:19<02:57, 37.01it/s]Training CobwebTree:  34%|      | 3437/10000 [01:19<03:04, 35.63it/s]Training CobwebTree:  34%|      | 3441/10000 [01:19<03:05, 35.37it/s]Training CobwebTree:  34%|      | 3446/10000 [01:19<02:56, 37.10it/s]Training CobwebTree:  34%|      | 3450/10000 [01:19<02:54, 37.56it/s]Training CobwebTree:  35%|      | 3455/10000 [01:19<02:49, 38.51it/s]Training CobwebTree:  35%|      | 3459/10000 [01:19<03:02, 35.87it/s]Training CobwebTree:  35%|      | 3463/10000 [01:19<03:10, 34.31it/s]Training CobwebTree:  35%|      | 3468/10000 [01:19<02:58, 36.64it/s]Training CobwebTree:  35%|      | 3472/10000 [01:20<02:55, 37.12it/s]Training CobwebTree:  35%|      | 3476/10000 [01:20<02:58, 36.61it/s]Training CobwebTree:  35%|      | 3480/10000 [01:20<02:58, 36.43it/s]Training CobwebTree:  35%|      | 3484/10000 [01:20<03:00, 36.00it/s]Training CobwebTree:  35%|      | 3489/10000 [01:20<02:52, 37.70it/s]Training CobwebTree:  35%|      | 3493/10000 [01:20<02:59, 36.19it/s]Training CobwebTree:  35%|      | 3497/10000 [01:20<03:03, 35.45it/s]Training CobwebTree:  35%|      | 3502/10000 [01:20<02:50, 38.11it/s]Training CobwebTree:  35%|      | 3506/10000 [01:21<02:56, 36.76it/s]Training CobwebTree:  35%|      | 3510/10000 [01:21<03:04, 35.17it/s]Training CobwebTree:  35%|      | 3515/10000 [01:21<02:55, 36.90it/s]Training CobwebTree:  35%|      | 3519/10000 [01:21<02:57, 36.48it/s]Training CobwebTree:  35%|      | 3523/10000 [01:21<02:57, 36.54it/s]Training CobwebTree:  35%|      | 3527/10000 [01:21<02:53, 37.37it/s]Training CobwebTree:  35%|      | 3531/10000 [01:21<02:53, 37.26it/s]Training CobwebTree:  35%|      | 3535/10000 [01:21<02:54, 37.05it/s]Training CobwebTree:  35%|      | 3539/10000 [01:21<02:50, 37.79it/s]Training CobwebTree:  35%|      | 3543/10000 [01:22<02:55, 36.85it/s]Training CobwebTree:  35%|      | 3547/10000 [01:22<02:52, 37.40it/s]Training CobwebTree:  36%|      | 3551/10000 [01:22<02:57, 36.42it/s]Training CobwebTree:  36%|      | 3555/10000 [01:22<02:58, 36.15it/s]Training CobwebTree:  36%|      | 3559/10000 [01:22<02:55, 36.69it/s]Training CobwebTree:  36%|      | 3563/10000 [01:22<02:59, 35.86it/s]Training CobwebTree:  36%|      | 3567/10000 [01:22<02:57, 36.29it/s]Training CobwebTree:  36%|      | 3572/10000 [01:22<02:47, 38.28it/s]Training CobwebTree:  36%|      | 3577/10000 [01:22<02:39, 40.18it/s]Training CobwebTree:  36%|      | 3582/10000 [01:23<02:43, 39.27it/s]Training CobwebTree:  36%|      | 3586/10000 [01:23<02:50, 37.61it/s]Training CobwebTree:  36%|      | 3591/10000 [01:23<02:47, 38.27it/s]Training CobwebTree:  36%|      | 3595/10000 [01:23<02:57, 36.14it/s]Training CobwebTree:  36%|      | 3599/10000 [01:23<02:57, 35.98it/s]Training CobwebTree:  36%|      | 3603/10000 [01:23<02:56, 36.20it/s]Training CobwebTree:  36%|      | 3607/10000 [01:23<02:57, 36.12it/s]Training CobwebTree:  36%|      | 3612/10000 [01:23<02:43, 39.05it/s]Training CobwebTree:  36%|      | 3616/10000 [01:23<02:47, 38.06it/s]Training CobwebTree:  36%|      | 3621/10000 [01:24<02:46, 38.35it/s]Training CobwebTree:  36%|      | 3626/10000 [01:24<02:48, 37.90it/s]Training CobwebTree:  36%|      | 3630/10000 [01:24<02:53, 36.74it/s]Training CobwebTree:  36%|      | 3634/10000 [01:24<02:54, 36.53it/s]Training CobwebTree:  36%|      | 3638/10000 [01:24<02:58, 35.61it/s]Training CobwebTree:  36%|      | 3643/10000 [01:24<02:50, 37.18it/s]Training CobwebTree:  36%|      | 3648/10000 [01:24<02:44, 38.58it/s]Training CobwebTree:  37%|      | 3653/10000 [01:24<02:38, 40.15it/s]Training CobwebTree:  37%|      | 3658/10000 [01:25<02:40, 39.51it/s]Training CobwebTree:  37%|      | 3663/10000 [01:25<02:40, 39.55it/s]Training CobwebTree:  37%|      | 3667/10000 [01:25<02:46, 38.13it/s]Training CobwebTree:  37%|      | 3672/10000 [01:25<02:40, 39.34it/s]Training CobwebTree:  37%|      | 3676/10000 [01:25<02:40, 39.34it/s]Training CobwebTree:  37%|      | 3680/10000 [01:25<02:44, 38.54it/s]Training CobwebTree:  37%|      | 3684/10000 [01:25<02:57, 35.65it/s]Training CobwebTree:  37%|      | 3688/10000 [01:25<02:57, 35.65it/s]Training CobwebTree:  37%|      | 3692/10000 [01:25<02:54, 36.22it/s]Training CobwebTree:  37%|      | 3696/10000 [01:26<02:59, 35.11it/s]Training CobwebTree:  37%|      | 3700/10000 [01:26<03:02, 34.61it/s]Training CobwebTree:  37%|      | 3704/10000 [01:26<03:01, 34.78it/s]Training CobwebTree:  37%|      | 3708/10000 [01:26<02:55, 35.81it/s]Training CobwebTree:  37%|      | 3712/10000 [01:26<02:52, 36.43it/s]Training CobwebTree:  37%|      | 3717/10000 [01:26<02:46, 37.70it/s]Training CobwebTree:  37%|      | 3721/10000 [01:26<02:46, 37.76it/s]Training CobwebTree:  37%|      | 3726/10000 [01:26<02:41, 38.76it/s]Training CobwebTree:  37%|      | 3731/10000 [01:27<02:36, 40.14it/s]Training CobwebTree:  37%|      | 3736/10000 [01:27<02:29, 41.83it/s]Training CobwebTree:  37%|      | 3741/10000 [01:27<02:36, 40.00it/s]Training CobwebTree:  37%|      | 3746/10000 [01:27<02:45, 37.88it/s]Training CobwebTree:  38%|      | 3750/10000 [01:27<02:53, 36.03it/s]Training CobwebTree:  38%|      | 3754/10000 [01:27<02:54, 35.80it/s]Training CobwebTree:  38%|      | 3758/10000 [01:27<02:59, 34.78it/s]Training CobwebTree:  38%|      | 3763/10000 [01:27<02:49, 36.71it/s]Training CobwebTree:  38%|      | 3768/10000 [01:28<02:47, 37.24it/s]Training CobwebTree:  38%|      | 3772/10000 [01:28<02:47, 37.22it/s]Training CobwebTree:  38%|      | 3776/10000 [01:28<03:02, 34.10it/s]Training CobwebTree:  38%|      | 3780/10000 [01:28<03:00, 34.43it/s]Training CobwebTree:  38%|      | 3784/10000 [01:28<02:58, 34.83it/s]Training CobwebTree:  38%|      | 3789/10000 [01:28<02:43, 38.06it/s]Training CobwebTree:  38%|      | 3793/10000 [01:28<02:41, 38.40it/s]Training CobwebTree:  38%|      | 3797/10000 [01:28<02:42, 38.15it/s]Training CobwebTree:  38%|      | 3801/10000 [01:28<02:54, 35.63it/s]Training CobwebTree:  38%|      | 3806/10000 [01:29<02:46, 37.15it/s]Training CobwebTree:  38%|      | 3811/10000 [01:29<02:39, 38.90it/s]Training CobwebTree:  38%|      | 3815/10000 [01:29<02:38, 38.93it/s]Training CobwebTree:  38%|      | 3820/10000 [01:29<02:33, 40.27it/s]Training CobwebTree:  38%|      | 3825/10000 [01:29<02:34, 39.96it/s]Training CobwebTree:  38%|      | 3830/10000 [01:29<02:41, 38.20it/s]Training CobwebTree:  38%|      | 3835/10000 [01:29<02:34, 39.99it/s]Training CobwebTree:  38%|      | 3840/10000 [01:29<02:45, 37.12it/s]Training CobwebTree:  38%|      | 3844/10000 [01:30<02:44, 37.46it/s]Training CobwebTree:  38%|      | 3849/10000 [01:30<02:39, 38.49it/s]Training CobwebTree:  39%|      | 3853/10000 [01:30<02:46, 36.99it/s]Training CobwebTree:  39%|      | 3858/10000 [01:30<02:42, 37.87it/s]Training CobwebTree:  39%|      | 3863/10000 [01:30<02:37, 39.03it/s]Training CobwebTree:  39%|      | 3867/10000 [01:30<02:43, 37.54it/s]Training CobwebTree:  39%|      | 3871/10000 [01:30<02:44, 37.15it/s]Training CobwebTree:  39%|      | 3875/10000 [01:30<02:44, 37.29it/s]Training CobwebTree:  39%|      | 3879/10000 [01:30<02:47, 36.60it/s]Training CobwebTree:  39%|      | 3883/10000 [01:31<02:45, 36.96it/s]Training CobwebTree:  39%|      | 3888/10000 [01:31<02:41, 37.94it/s]Training CobwebTree:  39%|      | 3892/10000 [01:31<02:49, 36.00it/s]Training CobwebTree:  39%|      | 3896/10000 [01:31<02:54, 34.92it/s]Training CobwebTree:  39%|      | 3900/10000 [01:31<02:52, 35.41it/s]Training CobwebTree:  39%|      | 3904/10000 [01:31<02:52, 35.43it/s]Training CobwebTree:  39%|      | 3908/10000 [01:31<03:00, 33.67it/s]Training CobwebTree:  39%|      | 3912/10000 [01:31<03:00, 33.64it/s]Training CobwebTree:  39%|      | 3916/10000 [01:32<02:53, 35.01it/s]Training CobwebTree:  39%|      | 3920/10000 [01:32<02:48, 36.00it/s]Training CobwebTree:  39%|      | 3925/10000 [01:32<02:41, 37.58it/s]Training CobwebTree:  39%|      | 3930/10000 [01:32<02:39, 38.01it/s]Training CobwebTree:  39%|      | 3934/10000 [01:32<02:41, 37.63it/s]Training CobwebTree:  39%|      | 3938/10000 [01:32<02:42, 37.29it/s]Training CobwebTree:  39%|      | 3942/10000 [01:32<02:50, 35.52it/s]Training CobwebTree:  39%|      | 3947/10000 [01:32<02:41, 37.46it/s]Training CobwebTree:  40%|      | 3951/10000 [01:32<02:49, 35.74it/s]Training CobwebTree:  40%|      | 3955/10000 [01:33<02:51, 35.32it/s]Training CobwebTree:  40%|      | 3960/10000 [01:33<02:44, 36.77it/s]Training CobwebTree:  40%|      | 3964/10000 [01:33<02:44, 36.79it/s]Training CobwebTree:  40%|      | 3969/10000 [01:33<02:40, 37.57it/s]Training CobwebTree:  40%|      | 3973/10000 [01:33<02:39, 37.77it/s]Training CobwebTree:  40%|      | 3977/10000 [01:33<02:38, 38.03it/s]Training CobwebTree:  40%|      | 3981/10000 [01:33<02:46, 36.16it/s]Training CobwebTree:  40%|      | 3985/10000 [01:33<02:45, 36.38it/s]Training CobwebTree:  40%|      | 3990/10000 [01:34<02:35, 38.64it/s]Training CobwebTree:  40%|      | 3994/10000 [01:34<02:40, 37.50it/s]Training CobwebTree:  40%|      | 3999/10000 [01:34<02:31, 39.74it/s]Training CobwebTree:  40%|      | 4003/10000 [01:34<02:33, 39.13it/s]Training CobwebTree:  40%|      | 4007/10000 [01:34<02:35, 38.65it/s]Training CobwebTree:  40%|      | 4012/10000 [01:34<02:32, 39.26it/s]Training CobwebTree:  40%|      | 4016/10000 [01:34<02:31, 39.39it/s]Training CobwebTree:  40%|      | 4020/10000 [01:34<02:38, 37.63it/s]Training CobwebTree:  40%|      | 4024/10000 [01:34<02:43, 36.62it/s]Training CobwebTree:  40%|      | 4028/10000 [01:35<02:44, 36.39it/s]Training CobwebTree:  40%|      | 4032/10000 [01:35<02:51, 34.88it/s]Training CobwebTree:  40%|      | 4037/10000 [01:35<02:43, 36.44it/s]Training CobwebTree:  40%|      | 4041/10000 [01:35<02:41, 36.84it/s]Training CobwebTree:  40%|      | 4046/10000 [01:35<02:37, 37.86it/s]Training CobwebTree:  40%|      | 4050/10000 [01:35<02:42, 36.51it/s]Training CobwebTree:  41%|      | 4054/10000 [01:35<02:43, 36.42it/s]Training CobwebTree:  41%|      | 4058/10000 [01:35<02:40, 37.04it/s]Training CobwebTree:  41%|      | 4062/10000 [01:35<02:37, 37.74it/s]Training CobwebTree:  41%|      | 4066/10000 [01:36<02:38, 37.40it/s]Training CobwebTree:  41%|      | 4070/10000 [01:36<02:49, 35.03it/s]Training CobwebTree:  41%|      | 4075/10000 [01:36<02:37, 37.60it/s]Training CobwebTree:  41%|      | 4080/10000 [01:36<02:33, 38.54it/s]Training CobwebTree:  41%|      | 4084/10000 [01:36<02:32, 38.88it/s]Training CobwebTree:  41%|      | 4088/10000 [01:36<02:37, 37.52it/s]Training CobwebTree:  41%|      | 4093/10000 [01:36<02:31, 38.95it/s]Training CobwebTree:  41%|      | 4097/10000 [01:36<02:39, 36.99it/s]Training CobwebTree:  41%|      | 4101/10000 [01:36<02:37, 37.38it/s]Training CobwebTree:  41%|      | 4105/10000 [01:37<02:39, 36.91it/s]Training CobwebTree:  41%|      | 4109/10000 [01:37<02:38, 37.17it/s]Training CobwebTree:  41%|      | 4113/10000 [01:37<02:36, 37.66it/s]Training CobwebTree:  41%|      | 4118/10000 [01:37<02:30, 39.07it/s]Training CobwebTree:  41%|      | 4123/10000 [01:37<02:31, 38.90it/s]Training CobwebTree:  41%|     | 4128/10000 [01:37<02:28, 39.66it/s]Training CobwebTree:  41%|     | 4133/10000 [01:37<02:27, 39.76it/s]Training CobwebTree:  41%|     | 4138/10000 [01:37<02:26, 39.96it/s]Training CobwebTree:  41%|     | 4143/10000 [01:38<02:26, 40.00it/s]Training CobwebTree:  41%|     | 4147/10000 [01:38<02:34, 37.88it/s]Training CobwebTree:  42%|     | 4151/10000 [01:38<02:43, 35.81it/s]Training CobwebTree:  42%|     | 4155/10000 [01:38<02:44, 35.56it/s]Training CobwebTree:  42%|     | 4159/10000 [01:38<02:43, 35.62it/s]Training CobwebTree:  42%|     | 4163/10000 [01:38<02:40, 36.44it/s]Training CobwebTree:  42%|     | 4168/10000 [01:38<02:35, 37.63it/s]Training CobwebTree:  42%|     | 4172/10000 [01:38<02:35, 37.44it/s]Training CobwebTree:  42%|     | 4176/10000 [01:38<02:39, 36.51it/s]Training CobwebTree:  42%|     | 4180/10000 [01:39<02:38, 36.72it/s]Training CobwebTree:  42%|     | 4184/10000 [01:39<02:43, 35.60it/s]Training CobwebTree:  42%|     | 4188/10000 [01:39<02:41, 36.00it/s]Training CobwebTree:  42%|     | 4192/10000 [01:39<02:43, 35.56it/s]Training CobwebTree:  42%|     | 4196/10000 [01:39<02:37, 36.77it/s]Training CobwebTree:  42%|     | 4200/10000 [01:39<02:39, 36.38it/s]Training CobwebTree:  42%|     | 4204/10000 [01:39<02:41, 35.89it/s]Training CobwebTree:  42%|     | 4208/10000 [01:39<02:37, 36.88it/s]Training CobwebTree:  42%|     | 4213/10000 [01:39<02:31, 38.11it/s]Training CobwebTree:  42%|     | 4217/10000 [01:40<02:38, 36.54it/s]Training CobwebTree:  42%|     | 4222/10000 [01:40<02:35, 37.13it/s]Training CobwebTree:  42%|     | 4227/10000 [01:40<02:32, 37.80it/s]Training CobwebTree:  42%|     | 4231/10000 [01:40<02:34, 37.36it/s]Training CobwebTree:  42%|     | 4235/10000 [01:40<02:38, 36.42it/s]Training CobwebTree:  42%|     | 4239/10000 [01:40<02:40, 35.84it/s]Training CobwebTree:  42%|     | 4243/10000 [01:40<02:38, 36.29it/s]Training CobwebTree:  42%|     | 4247/10000 [01:40<02:39, 36.02it/s]Training CobwebTree:  43%|     | 4251/10000 [01:41<02:46, 34.46it/s]Training CobwebTree:  43%|     | 4255/10000 [01:41<02:41, 35.49it/s]Training CobwebTree:  43%|     | 4260/10000 [01:41<02:37, 36.34it/s]Training CobwebTree:  43%|     | 4264/10000 [01:41<02:43, 35.16it/s]Training CobwebTree:  43%|     | 4269/10000 [01:41<02:30, 38.04it/s]Training CobwebTree:  43%|     | 4273/10000 [01:41<02:30, 38.04it/s]Training CobwebTree:  43%|     | 4277/10000 [01:41<02:31, 37.86it/s]Training CobwebTree:  43%|     | 4281/10000 [01:41<02:32, 37.45it/s]Training CobwebTree:  43%|     | 4286/10000 [01:41<02:35, 36.72it/s]Training CobwebTree:  43%|     | 4291/10000 [01:42<02:32, 37.37it/s]Training CobwebTree:  43%|     | 4296/10000 [01:42<02:24, 39.61it/s]Training CobwebTree:  43%|     | 4300/10000 [01:42<02:26, 39.00it/s]Training CobwebTree:  43%|     | 4304/10000 [01:42<02:25, 39.10it/s]Training CobwebTree:  43%|     | 4308/10000 [01:42<02:27, 38.61it/s]Training CobwebTree:  43%|     | 4312/10000 [01:42<02:34, 36.87it/s]Training CobwebTree:  43%|     | 4316/10000 [01:42<02:36, 36.32it/s]Training CobwebTree:  43%|     | 4320/10000 [01:42<02:35, 36.42it/s]Training CobwebTree:  43%|     | 4324/10000 [01:42<02:32, 37.25it/s]Training CobwebTree:  43%|     | 4329/10000 [01:43<02:26, 38.59it/s]Training CobwebTree:  43%|     | 4333/10000 [01:43<02:31, 37.35it/s]Training CobwebTree:  43%|     | 4338/10000 [01:43<02:29, 37.97it/s]Training CobwebTree:  43%|     | 4342/10000 [01:43<02:32, 36.98it/s]Training CobwebTree:  43%|     | 4346/10000 [01:43<02:35, 36.43it/s]Training CobwebTree:  44%|     | 4350/10000 [01:43<02:33, 36.78it/s]Training CobwebTree:  44%|     | 4354/10000 [01:43<02:33, 36.69it/s]Training CobwebTree:  44%|     | 4358/10000 [01:43<02:37, 35.79it/s]Training CobwebTree:  44%|     | 4362/10000 [01:44<02:38, 35.47it/s]Training CobwebTree:  44%|     | 4366/10000 [01:44<02:42, 34.72it/s]Training CobwebTree:  44%|     | 4370/10000 [01:44<02:40, 35.15it/s]Training CobwebTree:  44%|     | 4374/10000 [01:44<02:37, 35.78it/s]Training CobwebTree:  44%|     | 4378/10000 [01:44<02:35, 36.25it/s]Training CobwebTree:  44%|     | 4383/10000 [01:44<02:29, 37.59it/s]Training CobwebTree:  44%|     | 4387/10000 [01:44<02:30, 37.30it/s]Training CobwebTree:  44%|     | 4391/10000 [01:44<02:32, 36.87it/s]Training CobwebTree:  44%|     | 4395/10000 [01:44<02:30, 37.12it/s]Training CobwebTree:  44%|     | 4399/10000 [01:45<02:31, 37.07it/s]Training CobwebTree:  44%|     | 4403/10000 [01:45<02:35, 36.00it/s]Training CobwebTree:  44%|     | 4407/10000 [01:45<02:36, 35.79it/s]Training CobwebTree:  44%|     | 4411/10000 [01:45<02:34, 36.13it/s]Training CobwebTree:  44%|     | 4416/10000 [01:45<02:25, 38.41it/s]Training CobwebTree:  44%|     | 4420/10000 [01:45<02:24, 38.61it/s]Training CobwebTree:  44%|     | 4424/10000 [01:45<02:31, 36.83it/s]Training CobwebTree:  44%|     | 4428/10000 [01:45<02:31, 36.78it/s]Training CobwebTree:  44%|     | 4432/10000 [01:45<02:35, 35.75it/s]Training CobwebTree:  44%|     | 4436/10000 [01:46<02:35, 35.79it/s]Training CobwebTree:  44%|     | 4440/10000 [01:46<02:34, 36.05it/s]Training CobwebTree:  44%|     | 4444/10000 [01:46<02:30, 36.96it/s]Training CobwebTree:  44%|     | 4448/10000 [01:46<02:35, 35.81it/s]Training CobwebTree:  45%|     | 4452/10000 [01:46<02:39, 34.78it/s]Training CobwebTree:  45%|     | 4456/10000 [01:46<02:49, 32.74it/s]Training CobwebTree:  45%|     | 4460/10000 [01:46<02:48, 32.86it/s]Training CobwebTree:  45%|     | 4465/10000 [01:46<02:39, 34.77it/s]Training CobwebTree:  45%|     | 4469/10000 [01:47<02:36, 35.32it/s]Training CobwebTree:  45%|     | 4473/10000 [01:47<02:42, 34.01it/s]Training CobwebTree:  45%|     | 4477/10000 [01:47<02:41, 34.15it/s]Training CobwebTree:  45%|     | 4481/10000 [01:47<02:34, 35.68it/s]Training CobwebTree:  45%|     | 4485/10000 [01:47<02:31, 36.33it/s]Training CobwebTree:  45%|     | 4489/10000 [01:47<02:33, 35.92it/s]Training CobwebTree:  45%|     | 4493/10000 [01:47<02:28, 37.01it/s]Training CobwebTree:  45%|     | 4497/10000 [01:47<02:36, 35.10it/s]Training CobwebTree:  45%|     | 4501/10000 [01:47<02:32, 36.17it/s]Training CobwebTree:  45%|     | 4505/10000 [01:48<02:37, 34.93it/s]Training CobwebTree:  45%|     | 4509/10000 [01:48<02:44, 33.46it/s]Training CobwebTree:  45%|     | 4514/10000 [01:48<02:34, 35.61it/s]Training CobwebTree:  45%|     | 4519/10000 [01:48<02:26, 37.49it/s]Training CobwebTree:  45%|     | 4523/10000 [01:48<02:24, 37.83it/s]Training CobwebTree:  45%|     | 4527/10000 [01:48<02:27, 37.05it/s]Training CobwebTree:  45%|     | 4531/10000 [01:48<02:27, 37.08it/s]Training CobwebTree:  45%|     | 4535/10000 [01:48<02:25, 37.64it/s]Training CobwebTree:  45%|     | 4539/10000 [01:48<02:31, 36.10it/s]Training CobwebTree:  45%|     | 4543/10000 [01:49<02:31, 35.98it/s]Training CobwebTree:  45%|     | 4547/10000 [01:49<02:30, 36.13it/s]Training CobwebTree:  46%|     | 4551/10000 [01:49<02:38, 34.43it/s]Training CobwebTree:  46%|     | 4555/10000 [01:49<02:35, 34.95it/s]Training CobwebTree:  46%|     | 4560/10000 [01:49<02:26, 37.13it/s]Training CobwebTree:  46%|     | 4564/10000 [01:49<02:25, 37.23it/s]Training CobwebTree:  46%|     | 4568/10000 [01:49<02:28, 36.70it/s]Training CobwebTree:  46%|     | 4572/10000 [01:49<02:24, 37.47it/s]Training CobwebTree:  46%|     | 4576/10000 [01:49<02:26, 36.97it/s]Training CobwebTree:  46%|     | 4580/10000 [01:50<02:26, 36.90it/s]Training CobwebTree:  46%|     | 4584/10000 [01:50<02:28, 36.46it/s]Training CobwebTree:  46%|     | 4588/10000 [01:50<02:32, 35.46it/s]Training CobwebTree:  46%|     | 4592/10000 [01:50<02:30, 35.97it/s]Training CobwebTree:  46%|     | 4596/10000 [01:50<02:38, 34.01it/s]Training CobwebTree:  46%|     | 4601/10000 [01:50<02:32, 35.48it/s]Training CobwebTree:  46%|     | 4606/10000 [01:50<02:25, 37.11it/s]Training CobwebTree:  46%|     | 4610/10000 [01:50<02:24, 37.36it/s]Training CobwebTree:  46%|     | 4615/10000 [01:51<02:23, 37.63it/s]Training CobwebTree:  46%|     | 4620/10000 [01:51<02:23, 37.45it/s]Training CobwebTree:  46%|     | 4624/10000 [01:51<02:23, 37.42it/s]Training CobwebTree:  46%|     | 4629/10000 [01:51<02:23, 37.45it/s]Training CobwebTree:  46%|     | 4633/10000 [01:51<02:20, 38.08it/s]Training CobwebTree:  46%|     | 4637/10000 [01:51<02:19, 38.48it/s]Training CobwebTree:  46%|     | 4641/10000 [01:51<02:23, 37.26it/s]Training CobwebTree:  46%|     | 4645/10000 [01:51<02:22, 37.68it/s]Training CobwebTree:  46%|     | 4649/10000 [01:51<02:21, 37.69it/s]Training CobwebTree:  47%|     | 4653/10000 [01:52<02:30, 35.64it/s]Training CobwebTree:  47%|     | 4657/10000 [01:52<02:30, 35.53it/s]Training CobwebTree:  47%|     | 4661/10000 [01:52<02:35, 34.42it/s]Training CobwebTree:  47%|     | 4666/10000 [01:52<02:24, 37.03it/s]Training CobwebTree:  47%|     | 4670/10000 [01:52<02:23, 37.20it/s]Training CobwebTree:  47%|     | 4674/10000 [01:52<02:20, 37.95it/s]Training CobwebTree:  47%|     | 4678/10000 [01:52<02:23, 37.08it/s]Training CobwebTree:  47%|     | 4683/10000 [01:52<02:24, 36.88it/s]Training CobwebTree:  47%|     | 4687/10000 [01:52<02:21, 37.44it/s]Training CobwebTree:  47%|     | 4691/10000 [01:53<02:26, 36.35it/s]Training CobwebTree:  47%|     | 4695/10000 [01:53<02:25, 36.56it/s]Training CobwebTree:  47%|     | 4699/10000 [01:53<02:26, 36.16it/s]Training CobwebTree:  47%|     | 4703/10000 [01:53<02:27, 36.00it/s]Training CobwebTree:  47%|     | 4707/10000 [01:53<02:26, 36.11it/s]Training CobwebTree:  47%|     | 4711/10000 [01:53<02:33, 34.45it/s]Training CobwebTree:  47%|     | 4715/10000 [01:53<02:32, 34.77it/s]Training CobwebTree:  47%|     | 4719/10000 [01:53<02:26, 36.01it/s]Training CobwebTree:  47%|     | 4723/10000 [01:53<02:23, 36.89it/s]Training CobwebTree:  47%|     | 4728/10000 [01:54<02:15, 39.02it/s]Training CobwebTree:  47%|     | 4732/10000 [01:54<02:15, 38.88it/s]Training CobwebTree:  47%|     | 4736/10000 [01:54<02:14, 39.12it/s]Training CobwebTree:  47%|     | 4740/10000 [01:54<02:18, 37.85it/s]Training CobwebTree:  47%|     | 4745/10000 [01:54<02:17, 38.32it/s]Training CobwebTree:  48%|     | 4750/10000 [01:54<02:13, 39.18it/s]Training CobwebTree:  48%|     | 4754/10000 [01:54<02:17, 38.28it/s]Training CobwebTree:  48%|     | 4759/10000 [01:54<02:13, 39.38it/s]Training CobwebTree:  48%|     | 4763/10000 [01:54<02:15, 38.77it/s]Training CobwebTree:  48%|     | 4767/10000 [01:55<02:16, 38.23it/s]Training CobwebTree:  48%|     | 4771/10000 [01:55<02:22, 36.70it/s]Training CobwebTree:  48%|     | 4775/10000 [01:55<02:20, 37.16it/s]Training CobwebTree:  48%|     | 4779/10000 [01:55<02:22, 36.70it/s]Training CobwebTree:  48%|     | 4783/10000 [01:55<02:27, 35.40it/s]Training CobwebTree:  48%|     | 4787/10000 [01:55<02:29, 34.86it/s]Training CobwebTree:  48%|     | 4791/10000 [01:55<02:26, 35.53it/s]Training CobwebTree:  48%|     | 4795/10000 [01:55<02:26, 35.47it/s]Training CobwebTree:  48%|     | 4799/10000 [01:56<02:22, 36.48it/s]Training CobwebTree:  48%|     | 4803/10000 [01:56<02:22, 36.40it/s]Training CobwebTree:  48%|     | 4807/10000 [01:56<02:27, 35.33it/s]Training CobwebTree:  48%|     | 4811/10000 [01:56<02:31, 34.29it/s]Training CobwebTree:  48%|     | 4815/10000 [01:56<02:26, 35.42it/s]Training CobwebTree:  48%|     | 4819/10000 [01:56<02:24, 35.74it/s]Training CobwebTree:  48%|     | 4823/10000 [01:56<02:31, 34.28it/s]Training CobwebTree:  48%|     | 4827/10000 [01:56<02:24, 35.70it/s]Training CobwebTree:  48%|     | 4831/10000 [01:56<02:28, 34.81it/s]Training CobwebTree:  48%|     | 4835/10000 [01:57<02:29, 34.63it/s]Training CobwebTree:  48%|     | 4839/10000 [01:57<02:29, 34.48it/s]Training CobwebTree:  48%|     | 4844/10000 [01:57<02:18, 37.10it/s]Training CobwebTree:  48%|     | 4848/10000 [01:57<02:20, 36.62it/s]Training CobwebTree:  49%|     | 4852/10000 [01:57<02:22, 36.05it/s]Training CobwebTree:  49%|     | 4856/10000 [01:57<02:24, 35.55it/s]Training CobwebTree:  49%|     | 4860/10000 [01:57<02:27, 34.89it/s]Training CobwebTree:  49%|     | 4864/10000 [01:57<02:32, 33.71it/s]Training CobwebTree:  49%|     | 4869/10000 [01:57<02:22, 36.13it/s]Training CobwebTree:  49%|     | 4873/10000 [01:58<02:22, 36.09it/s]Training CobwebTree:  49%|     | 4878/10000 [01:58<02:14, 38.19it/s]Training CobwebTree:  49%|     | 4882/10000 [01:58<02:18, 37.00it/s]Training CobwebTree:  49%|     | 4886/10000 [01:58<02:18, 36.94it/s]Training CobwebTree:  49%|     | 4890/10000 [01:58<02:18, 37.01it/s]Training CobwebTree:  49%|     | 4895/10000 [01:58<02:13, 38.22it/s]Training CobwebTree:  49%|     | 4900/10000 [01:58<02:10, 39.13it/s]Training CobwebTree:  49%|     | 4904/10000 [01:58<02:13, 38.06it/s]Training CobwebTree:  49%|     | 4909/10000 [01:59<02:11, 38.62it/s]Training CobwebTree:  49%|     | 4913/10000 [01:59<02:21, 35.98it/s]Training CobwebTree:  49%|     | 4917/10000 [01:59<02:23, 35.45it/s]Training CobwebTree:  49%|     | 4922/10000 [01:59<02:14, 37.77it/s]Training CobwebTree:  49%|     | 4926/10000 [01:59<02:16, 37.09it/s]Training CobwebTree:  49%|     | 4930/10000 [01:59<02:20, 36.10it/s]Training CobwebTree:  49%|     | 4934/10000 [01:59<02:20, 36.18it/s]Training CobwebTree:  49%|     | 4938/10000 [01:59<02:17, 36.87it/s]Training CobwebTree:  49%|     | 4942/10000 [01:59<02:16, 37.10it/s]Training CobwebTree:  49%|     | 4946/10000 [02:00<02:19, 36.25it/s]Training CobwebTree:  50%|     | 4950/10000 [02:00<02:17, 36.61it/s]Training CobwebTree:  50%|     | 4954/10000 [02:00<02:18, 36.36it/s]Training CobwebTree:  50%|     | 4959/10000 [02:00<02:10, 38.67it/s]Training CobwebTree:  50%|     | 4964/10000 [02:00<02:10, 38.51it/s]Training CobwebTree:  50%|     | 4969/10000 [02:00<02:12, 38.10it/s]Training CobwebTree:  50%|     | 4973/10000 [02:00<02:11, 38.25it/s]Training CobwebTree:  50%|     | 4977/10000 [02:00<02:10, 38.44it/s]Training CobwebTree:  50%|     | 4981/10000 [02:00<02:09, 38.73it/s]Training CobwebTree:  50%|     | 4985/10000 [02:01<02:14, 37.31it/s]Training CobwebTree:  50%|     | 4989/10000 [02:01<02:13, 37.46it/s]Training CobwebTree:  50%|     | 4994/10000 [02:01<02:16, 36.81it/s]Training CobwebTree:  50%|     | 4998/10000 [02:01<02:15, 36.87it/s]Training CobwebTree:  50%|     | 5002/10000 [02:01<02:14, 37.09it/s]Training CobwebTree:  50%|     | 5006/10000 [02:01<02:16, 36.70it/s]Training CobwebTree:  50%|     | 5010/10000 [02:01<02:13, 37.27it/s]Training CobwebTree:  50%|     | 5014/10000 [02:01<02:16, 36.40it/s]Training CobwebTree:  50%|     | 5018/10000 [02:01<02:14, 37.03it/s]Training CobwebTree:  50%|     | 5022/10000 [02:02<02:15, 36.62it/s]Training CobwebTree:  50%|     | 5027/10000 [02:02<02:09, 38.45it/s]Training CobwebTree:  50%|     | 5031/10000 [02:02<02:14, 36.82it/s]Training CobwebTree:  50%|     | 5035/10000 [02:02<02:13, 37.22it/s]Training CobwebTree:  50%|     | 5040/10000 [02:02<02:09, 38.16it/s]Training CobwebTree:  50%|     | 5044/10000 [02:02<02:14, 36.76it/s]Training CobwebTree:  50%|     | 5048/10000 [02:02<02:15, 36.64it/s]Training CobwebTree:  51%|     | 5052/10000 [02:02<02:13, 36.98it/s]Training CobwebTree:  51%|     | 5056/10000 [02:02<02:11, 37.49it/s]Training CobwebTree:  51%|     | 5060/10000 [02:03<02:17, 35.93it/s]Training CobwebTree:  51%|     | 5064/10000 [02:03<02:18, 35.67it/s]Training CobwebTree:  51%|     | 5068/10000 [02:03<02:22, 34.72it/s]Training CobwebTree:  51%|     | 5072/10000 [02:03<02:26, 33.66it/s]Training CobwebTree:  51%|     | 5076/10000 [02:03<02:25, 33.80it/s]Training CobwebTree:  51%|     | 5080/10000 [02:03<02:21, 34.69it/s]Training CobwebTree:  51%|     | 5084/10000 [02:03<02:21, 34.81it/s]Training CobwebTree:  51%|     | 5088/10000 [02:03<02:16, 35.92it/s]Training CobwebTree:  51%|     | 5092/10000 [02:04<02:19, 35.21it/s]Training CobwebTree:  51%|     | 5097/10000 [02:04<02:14, 36.33it/s]Training CobwebTree:  51%|     | 5101/10000 [02:04<02:16, 35.77it/s]Training CobwebTree:  51%|     | 5105/10000 [02:04<02:16, 35.93it/s]Training CobwebTree:  51%|     | 5110/10000 [02:04<02:10, 37.54it/s]Training CobwebTree:  51%|     | 5114/10000 [02:04<02:13, 36.55it/s]Training CobwebTree:  51%|     | 5118/10000 [02:04<02:11, 37.15it/s]Training CobwebTree:  51%|     | 5122/10000 [02:04<02:16, 35.80it/s]Training CobwebTree:  51%|    | 5127/10000 [02:04<02:13, 36.49it/s]Training CobwebTree:  51%|    | 5131/10000 [02:05<02:12, 36.76it/s]Training CobwebTree:  51%|    | 5135/10000 [02:05<02:16, 35.74it/s]Training CobwebTree:  51%|    | 5139/10000 [02:05<02:12, 36.61it/s]Training CobwebTree:  51%|    | 5143/10000 [02:05<02:15, 35.76it/s]Training CobwebTree:  51%|    | 5147/10000 [02:05<02:11, 36.82it/s]Training CobwebTree:  52%|    | 5152/10000 [02:05<02:04, 39.07it/s]Training CobwebTree:  52%|    | 5156/10000 [02:05<02:09, 37.51it/s]Training CobwebTree:  52%|    | 5160/10000 [02:05<02:16, 35.35it/s]Training CobwebTree:  52%|    | 5164/10000 [02:06<02:18, 35.04it/s]Training CobwebTree:  52%|    | 5168/10000 [02:06<02:12, 36.34it/s]Training CobwebTree:  52%|    | 5173/10000 [02:06<02:09, 37.34it/s]Training CobwebTree:  52%|    | 5177/10000 [02:06<02:13, 36.13it/s]Training CobwebTree:  52%|    | 5182/10000 [02:06<02:09, 37.22it/s]Training CobwebTree:  52%|    | 5186/10000 [02:06<02:21, 34.08it/s]Training CobwebTree:  52%|    | 5190/10000 [02:06<02:16, 35.18it/s]Training CobwebTree:  52%|    | 5194/10000 [02:06<02:20, 34.33it/s]Training CobwebTree:  52%|    | 5198/10000 [02:06<02:18, 34.61it/s]Training CobwebTree:  52%|    | 5203/10000 [02:07<02:11, 36.52it/s]Training CobwebTree:  52%|    | 5207/10000 [02:07<02:15, 35.44it/s]Training CobwebTree:  52%|    | 5211/10000 [02:07<02:12, 36.16it/s]Training CobwebTree:  52%|    | 5215/10000 [02:07<02:10, 36.80it/s]Training CobwebTree:  52%|    | 5219/10000 [02:07<02:15, 35.39it/s]Training CobwebTree:  52%|    | 5224/10000 [02:07<02:08, 37.15it/s]Training CobwebTree:  52%|    | 5228/10000 [02:07<02:11, 36.25it/s]Training CobwebTree:  52%|    | 5232/10000 [02:07<02:16, 34.82it/s]Training CobwebTree:  52%|    | 5236/10000 [02:08<02:27, 32.40it/s]Training CobwebTree:  52%|    | 5240/10000 [02:08<02:24, 32.98it/s]Training CobwebTree:  52%|    | 5244/10000 [02:08<02:18, 34.38it/s]Training CobwebTree:  52%|    | 5248/10000 [02:08<02:18, 34.32it/s]Training CobwebTree:  53%|    | 5252/10000 [02:08<02:12, 35.78it/s]Training CobwebTree:  53%|    | 5256/10000 [02:08<02:14, 35.37it/s]Training CobwebTree:  53%|    | 5261/10000 [02:08<02:08, 36.91it/s]Training CobwebTree:  53%|    | 5265/10000 [02:08<02:06, 37.54it/s]Training CobwebTree:  53%|    | 5269/10000 [02:08<02:10, 36.35it/s]Training CobwebTree:  53%|    | 5273/10000 [02:09<02:06, 37.25it/s]Training CobwebTree:  53%|    | 5277/10000 [02:09<02:04, 37.88it/s]Training CobwebTree:  53%|    | 5281/10000 [02:09<02:11, 35.99it/s]Training CobwebTree:  53%|    | 5285/10000 [02:09<02:10, 36.07it/s]Training CobwebTree:  53%|    | 5290/10000 [02:09<02:06, 37.29it/s]Training CobwebTree:  53%|    | 5294/10000 [02:09<02:13, 35.18it/s]Training CobwebTree:  53%|    | 5298/10000 [02:09<02:17, 34.27it/s]Training CobwebTree:  53%|    | 5302/10000 [02:09<02:17, 34.13it/s]Training CobwebTree:  53%|    | 5307/10000 [02:10<02:13, 35.20it/s]Training CobwebTree:  53%|    | 5312/10000 [02:10<02:06, 37.06it/s]Training CobwebTree:  53%|    | 5316/10000 [02:10<02:14, 34.85it/s]Training CobwebTree:  53%|    | 5320/10000 [02:10<02:14, 34.68it/s]Training CobwebTree:  53%|    | 5324/10000 [02:10<02:16, 34.30it/s]Training CobwebTree:  53%|    | 5328/10000 [02:10<02:12, 35.39it/s]Training CobwebTree:  53%|    | 5332/10000 [02:10<02:14, 34.70it/s]Training CobwebTree:  53%|    | 5336/10000 [02:10<02:24, 32.30it/s]Training CobwebTree:  53%|    | 5340/10000 [02:11<02:19, 33.42it/s]Training CobwebTree:  53%|    | 5344/10000 [02:11<02:15, 34.33it/s]Training CobwebTree:  53%|    | 5348/10000 [02:11<02:17, 33.75it/s]Training CobwebTree:  54%|    | 5352/10000 [02:11<02:13, 34.82it/s]Training CobwebTree:  54%|    | 5356/10000 [02:11<02:08, 36.14it/s]Training CobwebTree:  54%|    | 5360/10000 [02:11<02:08, 36.12it/s]Training CobwebTree:  54%|    | 5364/10000 [02:11<02:05, 37.00it/s]Training CobwebTree:  54%|    | 5369/10000 [02:11<01:59, 38.61it/s]Training CobwebTree:  54%|    | 5373/10000 [02:11<02:03, 37.51it/s]Training CobwebTree:  54%|    | 5377/10000 [02:12<02:05, 36.85it/s]Training CobwebTree:  54%|    | 5381/10000 [02:12<02:09, 35.56it/s]Training CobwebTree:  54%|    | 5386/10000 [02:12<02:01, 37.87it/s]Training CobwebTree:  54%|    | 5390/10000 [02:12<02:03, 37.43it/s]Training CobwebTree:  54%|    | 5394/10000 [02:12<02:07, 36.02it/s]Training CobwebTree:  54%|    | 5398/10000 [02:12<02:11, 35.07it/s]Training CobwebTree:  54%|    | 5402/10000 [02:12<02:19, 33.00it/s]Training CobwebTree:  54%|    | 5407/10000 [02:12<02:06, 36.30it/s]Training CobwebTree:  54%|    | 5411/10000 [02:12<02:10, 35.29it/s]Training CobwebTree:  54%|    | 5415/10000 [02:13<02:14, 34.07it/s]Training CobwebTree:  54%|    | 5420/10000 [02:13<02:07, 35.87it/s]Training CobwebTree:  54%|    | 5424/10000 [02:13<02:11, 34.72it/s]Training CobwebTree:  54%|    | 5428/10000 [02:13<02:17, 33.16it/s]Training CobwebTree:  54%|    | 5432/10000 [02:13<02:18, 33.05it/s]Training CobwebTree:  54%|    | 5436/10000 [02:13<02:15, 33.62it/s]Training CobwebTree:  54%|    | 5441/10000 [02:13<02:07, 35.62it/s]Training CobwebTree:  54%|    | 5445/10000 [02:13<02:07, 35.61it/s]Training CobwebTree:  54%|    | 5449/10000 [02:14<02:03, 36.71it/s]Training CobwebTree:  55%|    | 5453/10000 [02:14<02:06, 35.96it/s]Training CobwebTree:  55%|    | 5457/10000 [02:14<02:10, 34.82it/s]Training CobwebTree:  55%|    | 5462/10000 [02:14<02:03, 36.79it/s]Training CobwebTree:  55%|    | 5466/10000 [02:14<02:03, 36.69it/s]Training CobwebTree:  55%|    | 5471/10000 [02:14<01:59, 37.80it/s]Training CobwebTree:  55%|    | 5476/10000 [02:14<01:58, 38.02it/s]Training CobwebTree:  55%|    | 5480/10000 [02:14<02:00, 37.48it/s]Training CobwebTree:  55%|    | 5484/10000 [02:14<02:02, 36.86it/s]Training CobwebTree:  55%|    | 5488/10000 [02:15<02:01, 37.09it/s]Training CobwebTree:  55%|    | 5492/10000 [02:15<02:02, 36.89it/s]Training CobwebTree:  55%|    | 5496/10000 [02:15<01:59, 37.65it/s]Training CobwebTree:  55%|    | 5500/10000 [02:15<02:06, 35.62it/s]Training CobwebTree:  55%|    | 5504/10000 [02:15<02:07, 35.16it/s]Training CobwebTree:  55%|    | 5508/10000 [02:15<02:08, 35.00it/s]Training CobwebTree:  55%|    | 5512/10000 [02:15<02:15, 33.03it/s]Training CobwebTree:  55%|    | 5516/10000 [02:15<02:10, 34.34it/s]Training CobwebTree:  55%|    | 5520/10000 [02:16<02:07, 35.16it/s]Training CobwebTree:  55%|    | 5524/10000 [02:16<02:09, 34.61it/s]Training CobwebTree:  55%|    | 5528/10000 [02:16<02:06, 35.48it/s]Training CobwebTree:  55%|    | 5533/10000 [02:16<01:56, 38.48it/s]Training CobwebTree:  55%|    | 5537/10000 [02:16<01:59, 37.39it/s]Training CobwebTree:  55%|    | 5541/10000 [02:16<02:03, 36.19it/s]Training CobwebTree:  55%|    | 5545/10000 [02:16<02:07, 34.92it/s]Training CobwebTree:  56%|    | 5550/10000 [02:16<01:59, 37.10it/s]Training CobwebTree:  56%|    | 5554/10000 [02:16<02:01, 36.56it/s]Training CobwebTree:  56%|    | 5558/10000 [02:17<02:03, 35.84it/s]Training CobwebTree:  56%|    | 5563/10000 [02:17<01:56, 37.95it/s]Training CobwebTree:  56%|    | 5567/10000 [02:17<02:02, 36.13it/s]Training CobwebTree:  56%|    | 5571/10000 [02:17<02:00, 36.68it/s]Training CobwebTree:  56%|    | 5575/10000 [02:17<02:03, 35.83it/s]Training CobwebTree:  56%|    | 5579/10000 [02:17<02:06, 34.84it/s]Training CobwebTree:  56%|    | 5583/10000 [02:17<02:03, 35.74it/s]Training CobwebTree:  56%|    | 5587/10000 [02:17<02:01, 36.34it/s]Training CobwebTree:  56%|    | 5591/10000 [02:17<02:02, 35.90it/s]Training CobwebTree:  56%|    | 5595/10000 [02:18<02:02, 35.85it/s]Training CobwebTree:  56%|    | 5600/10000 [02:18<01:58, 37.07it/s]Training CobwebTree:  56%|    | 5604/10000 [02:18<02:03, 35.60it/s]Training CobwebTree:  56%|    | 5608/10000 [02:18<02:01, 36.08it/s]Training CobwebTree:  56%|    | 5612/10000 [02:18<02:00, 36.52it/s]Training CobwebTree:  56%|    | 5617/10000 [02:18<01:56, 37.48it/s]Training CobwebTree:  56%|    | 5621/10000 [02:18<02:02, 35.74it/s]Training CobwebTree:  56%|    | 5625/10000 [02:18<02:08, 33.97it/s]Training CobwebTree:  56%|    | 5629/10000 [02:19<02:08, 33.93it/s]Training CobwebTree:  56%|    | 5633/10000 [02:19<02:08, 34.01it/s]Training CobwebTree:  56%|    | 5638/10000 [02:19<02:03, 35.37it/s]Training CobwebTree:  56%|    | 5642/10000 [02:19<02:00, 36.10it/s]Training CobwebTree:  56%|    | 5646/10000 [02:19<02:03, 35.24it/s]Training CobwebTree:  56%|    | 5650/10000 [02:19<02:05, 34.67it/s]Training CobwebTree:  57%|    | 5654/10000 [02:19<02:06, 34.44it/s]Training CobwebTree:  57%|    | 5658/10000 [02:19<02:08, 33.67it/s]Training CobwebTree:  57%|    | 5662/10000 [02:19<02:05, 34.56it/s]Training CobwebTree:  57%|    | 5666/10000 [02:20<02:02, 35.38it/s]Training CobwebTree:  57%|    | 5670/10000 [02:20<02:00, 35.97it/s]Training CobwebTree:  57%|    | 5674/10000 [02:20<01:59, 36.15it/s]Training CobwebTree:  57%|    | 5678/10000 [02:20<02:00, 35.78it/s]Training CobwebTree:  57%|    | 5682/10000 [02:20<01:59, 36.21it/s]Training CobwebTree:  57%|    | 5686/10000 [02:20<02:03, 34.87it/s]Training CobwebTree:  57%|    | 5690/10000 [02:20<02:00, 35.83it/s]Training CobwebTree:  57%|    | 5694/10000 [02:20<01:57, 36.70it/s]Training CobwebTree:  57%|    | 5698/10000 [02:20<01:59, 36.08it/s]Training CobwebTree:  57%|    | 5702/10000 [02:21<01:56, 37.03it/s]Training CobwebTree:  57%|    | 5706/10000 [02:21<01:58, 36.22it/s]Training CobwebTree:  57%|    | 5710/10000 [02:21<02:00, 35.50it/s]Training CobwebTree:  57%|    | 5714/10000 [02:21<02:01, 35.36it/s]Training CobwebTree:  57%|    | 5718/10000 [02:21<02:04, 34.40it/s]Training CobwebTree:  57%|    | 5722/10000 [02:21<02:03, 34.75it/s]Training CobwebTree:  57%|    | 5727/10000 [02:21<01:57, 36.50it/s]Training CobwebTree:  57%|    | 5731/10000 [02:21<02:00, 35.44it/s]Training CobwebTree:  57%|    | 5735/10000 [02:22<02:04, 34.22it/s]Training CobwebTree:  57%|    | 5739/10000 [02:22<02:04, 34.34it/s]Training CobwebTree:  57%|    | 5744/10000 [02:22<01:50, 38.36it/s]Training CobwebTree:  57%|    | 5748/10000 [02:22<01:50, 38.45it/s]Training CobwebTree:  58%|    | 5752/10000 [02:22<01:51, 38.20it/s]Training CobwebTree:  58%|    | 5756/10000 [02:22<01:52, 37.56it/s]Training CobwebTree:  58%|    | 5760/10000 [02:22<02:00, 35.11it/s]Training CobwebTree:  58%|    | 5764/10000 [02:22<01:59, 35.52it/s]Training CobwebTree:  58%|    | 5768/10000 [02:22<01:55, 36.55it/s]Training CobwebTree:  58%|    | 5772/10000 [02:23<01:57, 36.10it/s]Training CobwebTree:  58%|    | 5776/10000 [02:23<01:59, 35.29it/s]Training CobwebTree:  58%|    | 5780/10000 [02:23<01:56, 36.18it/s]Training CobwebTree:  58%|    | 5784/10000 [02:23<01:58, 35.58it/s]Training CobwebTree:  58%|    | 5788/10000 [02:23<01:58, 35.56it/s]Training CobwebTree:  58%|    | 5792/10000 [02:23<02:02, 34.22it/s]Training CobwebTree:  58%|    | 5796/10000 [02:23<01:59, 35.07it/s]Training CobwebTree:  58%|    | 5800/10000 [02:23<01:59, 35.04it/s]Training CobwebTree:  58%|    | 5804/10000 [02:23<01:59, 34.98it/s]Training CobwebTree:  58%|    | 5808/10000 [02:24<01:59, 35.14it/s]Training CobwebTree:  58%|    | 5812/10000 [02:24<02:03, 33.92it/s]Training CobwebTree:  58%|    | 5817/10000 [02:24<01:55, 36.31it/s]Training CobwebTree:  58%|    | 5821/10000 [02:24<01:53, 36.88it/s]Training CobwebTree:  58%|    | 5826/10000 [02:24<01:51, 37.52it/s]Training CobwebTree:  58%|    | 5830/10000 [02:24<01:50, 37.62it/s]Training CobwebTree:  58%|    | 5834/10000 [02:24<01:54, 36.33it/s]Training CobwebTree:  58%|    | 5838/10000 [02:24<01:57, 35.45it/s]Training CobwebTree:  58%|    | 5842/10000 [02:25<01:56, 35.72it/s]Training CobwebTree:  58%|    | 5846/10000 [02:25<01:59, 34.83it/s]Training CobwebTree:  58%|    | 5850/10000 [02:25<01:55, 35.87it/s]Training CobwebTree:  59%|    | 5855/10000 [02:25<01:55, 35.96it/s]Training CobwebTree:  59%|    | 5859/10000 [02:25<01:53, 36.49it/s]Training CobwebTree:  59%|    | 5863/10000 [02:25<01:52, 36.65it/s]Training CobwebTree:  59%|    | 5867/10000 [02:25<01:55, 35.71it/s]Training CobwebTree:  59%|    | 5871/10000 [02:25<02:00, 34.37it/s]Training CobwebTree:  59%|    | 5875/10000 [02:25<01:59, 34.43it/s]Training CobwebTree:  59%|    | 5879/10000 [02:26<02:00, 34.09it/s]Training CobwebTree:  59%|    | 5883/10000 [02:26<02:04, 33.19it/s]Training CobwebTree:  59%|    | 5887/10000 [02:26<02:01, 33.93it/s]Training CobwebTree:  59%|    | 5891/10000 [02:26<01:57, 35.09it/s]Training CobwebTree:  59%|    | 5895/10000 [02:26<01:57, 34.89it/s]Training CobwebTree:  59%|    | 5899/10000 [02:26<01:56, 35.15it/s]Training CobwebTree:  59%|    | 5903/10000 [02:26<01:57, 34.79it/s]Training CobwebTree:  59%|    | 5907/10000 [02:26<01:57, 34.86it/s]Training CobwebTree:  59%|    | 5911/10000 [02:26<01:59, 34.36it/s]Training CobwebTree:  59%|    | 5915/10000 [02:27<02:02, 33.34it/s]Training CobwebTree:  59%|    | 5919/10000 [02:27<02:09, 31.47it/s]Training CobwebTree:  59%|    | 5923/10000 [02:27<02:11, 30.98it/s]Training CobwebTree:  59%|    | 5927/10000 [02:27<02:10, 31.33it/s]Training CobwebTree:  59%|    | 5931/10000 [02:27<02:10, 31.22it/s]Training CobwebTree:  59%|    | 5935/10000 [02:27<02:12, 30.71it/s]Training CobwebTree:  59%|    | 5939/10000 [02:27<02:04, 32.63it/s]Training CobwebTree:  59%|    | 5943/10000 [02:27<02:00, 33.74it/s]Training CobwebTree:  59%|    | 5947/10000 [02:28<01:54, 35.29it/s]Training CobwebTree:  60%|    | 5951/10000 [02:28<01:56, 34.75it/s]Training CobwebTree:  60%|    | 5955/10000 [02:28<01:59, 33.75it/s]Training CobwebTree:  60%|    | 5959/10000 [02:28<01:56, 34.80it/s]Training CobwebTree:  60%|    | 5963/10000 [02:28<02:02, 32.98it/s]Training CobwebTree:  60%|    | 5967/10000 [02:28<01:55, 34.80it/s]Training CobwebTree:  60%|    | 5972/10000 [02:28<01:46, 37.83it/s]Training CobwebTree:  60%|    | 5976/10000 [02:28<01:51, 36.05it/s]Training CobwebTree:  60%|    | 5980/10000 [02:29<01:55, 34.80it/s]Training CobwebTree:  60%|    | 5984/10000 [02:29<01:52, 35.70it/s]Training CobwebTree:  60%|    | 5988/10000 [02:29<01:54, 35.10it/s]Training CobwebTree:  60%|    | 5992/10000 [02:29<01:54, 35.12it/s]Training CobwebTree:  60%|    | 5996/10000 [02:29<01:51, 35.77it/s]Training CobwebTree:  60%|    | 6001/10000 [02:29<01:49, 36.38it/s]Training CobwebTree:  60%|    | 6006/10000 [02:29<01:44, 38.15it/s]Training CobwebTree:  60%|    | 6010/10000 [02:29<01:50, 36.12it/s]Training CobwebTree:  60%|    | 6014/10000 [02:29<01:50, 36.07it/s]Training CobwebTree:  60%|    | 6018/10000 [02:30<02:00, 33.01it/s]Training CobwebTree:  60%|    | 6022/10000 [02:30<02:00, 33.13it/s]Training CobwebTree:  60%|    | 6026/10000 [02:30<01:54, 34.76it/s]Training CobwebTree:  60%|    | 6030/10000 [02:30<01:53, 34.88it/s]Training CobwebTree:  60%|    | 6035/10000 [02:30<01:48, 36.38it/s]Training CobwebTree:  60%|    | 6039/10000 [02:30<01:52, 35.19it/s]Training CobwebTree:  60%|    | 6043/10000 [02:30<01:51, 35.47it/s]Training CobwebTree:  60%|    | 6047/10000 [02:30<01:58, 33.45it/s]Training CobwebTree:  61%|    | 6051/10000 [02:31<01:57, 33.73it/s]Training CobwebTree:  61%|    | 6056/10000 [02:31<01:49, 36.04it/s]Training CobwebTree:  61%|    | 6060/10000 [02:31<01:59, 33.08it/s]Training CobwebTree:  61%|    | 6064/10000 [02:31<01:55, 34.10it/s]Training CobwebTree:  61%|    | 6068/10000 [02:31<01:52, 34.90it/s]Training CobwebTree:  61%|    | 6072/10000 [02:31<01:51, 35.31it/s]Training CobwebTree:  61%|    | 6076/10000 [02:31<01:50, 35.42it/s]Training CobwebTree:  61%|    | 6080/10000 [02:31<01:53, 34.45it/s]Training CobwebTree:  61%|    | 6084/10000 [02:32<01:56, 33.64it/s]Training CobwebTree:  61%|    | 6088/10000 [02:32<01:58, 32.88it/s]Training CobwebTree:  61%|    | 6092/10000 [02:32<01:57, 33.28it/s]Training CobwebTree:  61%|    | 6096/10000 [02:32<02:01, 32.14it/s]Training CobwebTree:  61%|    | 6100/10000 [02:32<01:57, 33.15it/s]Training CobwebTree:  61%|    | 6104/10000 [02:32<01:55, 33.66it/s]Training CobwebTree:  61%|    | 6109/10000 [02:32<01:49, 35.48it/s]Training CobwebTree:  61%|    | 6113/10000 [02:32<01:51, 34.75it/s]Training CobwebTree:  61%|    | 6117/10000 [02:33<01:53, 34.15it/s]Training CobwebTree:  61%|    | 6121/10000 [02:33<01:51, 34.79it/s]Training CobwebTree:  61%|   | 6125/10000 [02:33<01:51, 34.75it/s]Training CobwebTree:  61%|   | 6129/10000 [02:33<01:48, 35.67it/s]Training CobwebTree:  61%|   | 6133/10000 [02:33<01:54, 33.83it/s]Training CobwebTree:  61%|   | 6137/10000 [02:33<01:50, 34.93it/s]Training CobwebTree:  61%|   | 6141/10000 [02:33<01:50, 35.01it/s]Training CobwebTree:  61%|   | 6145/10000 [02:33<01:50, 34.84it/s]Training CobwebTree:  61%|   | 6149/10000 [02:33<01:52, 34.20it/s]Training CobwebTree:  62%|   | 6153/10000 [02:34<01:52, 34.20it/s]Training CobwebTree:  62%|   | 6157/10000 [02:34<01:51, 34.42it/s]Training CobwebTree:  62%|   | 6161/10000 [02:34<01:52, 34.17it/s]Training CobwebTree:  62%|   | 6166/10000 [02:34<01:47, 35.74it/s]Training CobwebTree:  62%|   | 6171/10000 [02:34<01:42, 37.23it/s]Training CobwebTree:  62%|   | 6175/10000 [02:34<01:51, 34.21it/s]Training CobwebTree:  62%|   | 6179/10000 [02:34<01:47, 35.55it/s]Training CobwebTree:  62%|   | 6183/10000 [02:34<01:46, 35.78it/s]Training CobwebTree:  62%|   | 6187/10000 [02:34<01:46, 35.73it/s]Training CobwebTree:  62%|   | 6191/10000 [02:35<01:46, 35.84it/s]Training CobwebTree:  62%|   | 6195/10000 [02:35<01:46, 35.73it/s]Training CobwebTree:  62%|   | 6199/10000 [02:35<01:49, 34.67it/s]Training CobwebTree:  62%|   | 6203/10000 [02:35<01:48, 35.06it/s]Training CobwebTree:  62%|   | 6208/10000 [02:35<01:41, 37.53it/s]Training CobwebTree:  62%|   | 6212/10000 [02:35<01:47, 35.10it/s]Training CobwebTree:  62%|   | 6217/10000 [02:35<01:43, 36.49it/s]Training CobwebTree:  62%|   | 6221/10000 [02:35<01:42, 36.71it/s]Training CobwebTree:  62%|   | 6225/10000 [02:36<01:46, 35.38it/s]Training CobwebTree:  62%|   | 6230/10000 [02:36<01:41, 36.97it/s]Training CobwebTree:  62%|   | 6234/10000 [02:36<01:48, 34.86it/s]Training CobwebTree:  62%|   | 6238/10000 [02:36<01:45, 35.51it/s]Training CobwebTree:  62%|   | 6243/10000 [02:36<01:40, 37.24it/s]Training CobwebTree:  62%|   | 6247/10000 [02:36<01:40, 37.45it/s]Training CobwebTree:  63%|   | 6251/10000 [02:36<01:38, 38.05it/s]Training CobwebTree:  63%|   | 6255/10000 [02:36<01:42, 36.41it/s]Training CobwebTree:  63%|   | 6259/10000 [02:36<01:44, 35.70it/s]Training CobwebTree:  63%|   | 6263/10000 [02:37<01:47, 34.77it/s]Training CobwebTree:  63%|   | 6267/10000 [02:37<01:44, 35.88it/s]Training CobwebTree:  63%|   | 6272/10000 [02:37<01:41, 36.74it/s]Training CobwebTree:  63%|   | 6276/10000 [02:37<01:41, 36.61it/s]Training CobwebTree:  63%|   | 6280/10000 [02:37<01:46, 35.01it/s]Training CobwebTree:  63%|   | 6284/10000 [02:37<01:49, 33.88it/s]Training CobwebTree:  63%|   | 6288/10000 [02:37<01:50, 33.52it/s]Training CobwebTree:  63%|   | 6292/10000 [02:37<01:48, 34.20it/s]Training CobwebTree:  63%|   | 6296/10000 [02:38<01:46, 34.84it/s]Training CobwebTree:  63%|   | 6300/10000 [02:38<01:47, 34.29it/s]Training CobwebTree:  63%|   | 6304/10000 [02:38<01:43, 35.75it/s]Training CobwebTree:  63%|   | 6308/10000 [02:38<01:41, 36.45it/s]Training CobwebTree:  63%|   | 6312/10000 [02:38<01:43, 35.48it/s]Training CobwebTree:  63%|   | 6316/10000 [02:38<01:46, 34.49it/s]Training CobwebTree:  63%|   | 6320/10000 [02:38<01:49, 33.63it/s]Training CobwebTree:  63%|   | 6325/10000 [02:38<01:41, 36.24it/s]Training CobwebTree:  63%|   | 6329/10000 [02:38<01:41, 36.22it/s]Training CobwebTree:  63%|   | 6333/10000 [02:39<01:47, 34.11it/s]Training CobwebTree:  63%|   | 6338/10000 [02:39<01:42, 35.62it/s]Training CobwebTree:  63%|   | 6342/10000 [02:39<01:43, 35.50it/s]Training CobwebTree:  63%|   | 6347/10000 [02:39<01:36, 38.02it/s]Training CobwebTree:  64%|   | 6351/10000 [02:39<01:44, 34.85it/s]Training CobwebTree:  64%|   | 6355/10000 [02:39<01:43, 35.23it/s]Training CobwebTree:  64%|   | 6359/10000 [02:39<01:43, 35.24it/s]Training CobwebTree:  64%|   | 6363/10000 [02:39<01:45, 34.48it/s]Training CobwebTree:  64%|   | 6367/10000 [02:40<01:45, 34.41it/s]Training CobwebTree:  64%|   | 6371/10000 [02:40<01:48, 33.48it/s]Training CobwebTree:  64%|   | 6375/10000 [02:40<01:44, 34.64it/s]Training CobwebTree:  64%|   | 6380/10000 [02:40<01:40, 36.07it/s]Training CobwebTree:  64%|   | 6384/10000 [02:40<01:43, 34.99it/s]Training CobwebTree:  64%|   | 6388/10000 [02:40<01:40, 36.05it/s]Training CobwebTree:  64%|   | 6392/10000 [02:40<01:43, 34.79it/s]Training CobwebTree:  64%|   | 6397/10000 [02:40<01:38, 36.61it/s]Training CobwebTree:  64%|   | 6401/10000 [02:41<01:38, 36.64it/s]Training CobwebTree:  64%|   | 6405/10000 [02:41<01:42, 34.91it/s]Training CobwebTree:  64%|   | 6409/10000 [02:41<01:39, 36.02it/s]Training CobwebTree:  64%|   | 6414/10000 [02:41<01:35, 37.74it/s]Training CobwebTree:  64%|   | 6419/10000 [02:41<01:30, 39.55it/s]Training CobwebTree:  64%|   | 6424/10000 [02:41<01:32, 38.77it/s]Training CobwebTree:  64%|   | 6429/10000 [02:41<01:27, 40.94it/s]Training CobwebTree:  64%|   | 6434/10000 [02:41<01:29, 39.99it/s]Training CobwebTree:  64%|   | 6439/10000 [02:41<01:30, 39.51it/s]Training CobwebTree:  64%|   | 6444/10000 [02:42<01:27, 40.49it/s]Training CobwebTree:  64%|   | 6449/10000 [02:42<01:36, 36.92it/s]Training CobwebTree:  65%|   | 6453/10000 [02:42<01:36, 36.76it/s]Training CobwebTree:  65%|   | 6457/10000 [02:42<01:37, 36.16it/s]Training CobwebTree:  65%|   | 6461/10000 [02:42<01:37, 36.48it/s]Training CobwebTree:  65%|   | 6465/10000 [02:42<01:36, 36.77it/s]Training CobwebTree:  65%|   | 6469/10000 [02:42<01:40, 35.29it/s]Training CobwebTree:  65%|   | 6473/10000 [02:42<01:37, 36.13it/s]Training CobwebTree:  65%|   | 6477/10000 [02:43<01:36, 36.61it/s]Training CobwebTree:  65%|   | 6481/10000 [02:43<01:39, 35.40it/s]Training CobwebTree:  65%|   | 6485/10000 [02:43<01:42, 34.30it/s]Training CobwebTree:  65%|   | 6490/10000 [02:43<01:35, 36.74it/s]Training CobwebTree:  65%|   | 6494/10000 [02:43<01:34, 37.04it/s]Training CobwebTree:  65%|   | 6498/10000 [02:43<01:36, 36.18it/s]Training CobwebTree:  65%|   | 6502/10000 [02:43<01:34, 36.91it/s]Training CobwebTree:  65%|   | 6506/10000 [02:43<01:34, 36.93it/s]Training CobwebTree:  65%|   | 6510/10000 [02:43<01:40, 34.74it/s]Training CobwebTree:  65%|   | 6514/10000 [02:44<01:42, 34.16it/s]Training CobwebTree:  65%|   | 6519/10000 [02:44<01:34, 36.91it/s]Training CobwebTree:  65%|   | 6523/10000 [02:44<01:39, 35.01it/s]Training CobwebTree:  65%|   | 6527/10000 [02:44<01:37, 35.72it/s]Training CobwebTree:  65%|   | 6531/10000 [02:44<01:36, 35.82it/s]Training CobwebTree:  65%|   | 6535/10000 [02:44<01:37, 35.64it/s]Training CobwebTree:  65%|   | 6539/10000 [02:44<01:40, 34.52it/s]Training CobwebTree:  65%|   | 6543/10000 [02:44<01:43, 33.54it/s]Training CobwebTree:  65%|   | 6547/10000 [02:45<01:39, 34.61it/s]Training CobwebTree:  66%|   | 6551/10000 [02:45<01:39, 34.78it/s]Training CobwebTree:  66%|   | 6555/10000 [02:45<01:43, 33.26it/s]Training CobwebTree:  66%|   | 6559/10000 [02:45<01:41, 34.05it/s]Training CobwebTree:  66%|   | 6564/10000 [02:45<01:35, 35.90it/s]Training CobwebTree:  66%|   | 6569/10000 [02:45<01:28, 38.67it/s]Training CobwebTree:  66%|   | 6573/10000 [02:45<01:28, 38.55it/s]Training CobwebTree:  66%|   | 6577/10000 [02:45<01:31, 37.28it/s]Training CobwebTree:  66%|   | 6581/10000 [02:45<01:32, 36.91it/s]Training CobwebTree:  66%|   | 6585/10000 [02:46<01:30, 37.68it/s]Training CobwebTree:  66%|   | 6589/10000 [02:46<01:30, 37.87it/s]Training CobwebTree:  66%|   | 6593/10000 [02:46<01:29, 38.01it/s]Training CobwebTree:  66%|   | 6597/10000 [02:46<01:28, 38.29it/s]Training CobwebTree:  66%|   | 6601/10000 [02:46<01:28, 38.20it/s]Training CobwebTree:  66%|   | 6605/10000 [02:46<01:30, 37.40it/s]Training CobwebTree:  66%|   | 6609/10000 [02:46<01:30, 37.47it/s]Training CobwebTree:  66%|   | 6613/10000 [02:46<01:31, 37.08it/s]Training CobwebTree:  66%|   | 6617/10000 [02:46<01:34, 35.87it/s]Training CobwebTree:  66%|   | 6621/10000 [02:47<01:33, 36.10it/s]Training CobwebTree:  66%|   | 6625/10000 [02:47<01:34, 35.88it/s]Training CobwebTree:  66%|   | 6629/10000 [02:47<01:35, 35.25it/s]Training CobwebTree:  66%|   | 6633/10000 [02:47<01:35, 35.29it/s]Training CobwebTree:  66%|   | 6637/10000 [02:47<01:44, 32.22it/s]Training CobwebTree:  66%|   | 6642/10000 [02:47<01:36, 34.69it/s]Training CobwebTree:  66%|   | 6646/10000 [02:47<01:35, 35.15it/s]Training CobwebTree:  66%|   | 6650/10000 [02:47<01:34, 35.53it/s]Training CobwebTree:  67%|   | 6654/10000 [02:47<01:33, 35.89it/s]Training CobwebTree:  67%|   | 6658/10000 [02:48<01:32, 36.01it/s]Training CobwebTree:  67%|   | 6662/10000 [02:48<01:34, 35.44it/s]Training CobwebTree:  67%|   | 6666/10000 [02:48<01:38, 33.77it/s]Training CobwebTree:  67%|   | 6670/10000 [02:48<01:35, 34.87it/s]Training CobwebTree:  67%|   | 6674/10000 [02:48<01:34, 35.03it/s]Training CobwebTree:  67%|   | 6678/10000 [02:48<01:36, 34.59it/s]Training CobwebTree:  67%|   | 6682/10000 [02:48<01:37, 33.98it/s]Training CobwebTree:  67%|   | 6686/10000 [02:48<01:33, 35.47it/s]Training CobwebTree:  67%|   | 6690/10000 [02:48<01:32, 35.66it/s]Training CobwebTree:  67%|   | 6694/10000 [02:49<01:35, 34.56it/s]Training CobwebTree:  67%|   | 6698/10000 [02:49<01:32, 35.84it/s]Training CobwebTree:  67%|   | 6702/10000 [02:49<01:37, 33.85it/s]Training CobwebTree:  67%|   | 6706/10000 [02:49<01:36, 34.25it/s]Training CobwebTree:  67%|   | 6710/10000 [02:49<01:34, 34.99it/s]Training CobwebTree:  67%|   | 6715/10000 [02:49<01:29, 36.60it/s]Training CobwebTree:  67%|   | 6719/10000 [02:49<01:34, 34.56it/s]Training CobwebTree:  67%|   | 6723/10000 [02:49<01:40, 32.45it/s]Training CobwebTree:  67%|   | 6727/10000 [02:50<01:38, 33.22it/s]Training CobwebTree:  67%|   | 6731/10000 [02:50<01:39, 32.86it/s]Training CobwebTree:  67%|   | 6735/10000 [02:50<01:36, 33.95it/s]Training CobwebTree:  67%|   | 6739/10000 [02:50<01:35, 34.24it/s]Training CobwebTree:  67%|   | 6744/10000 [02:50<01:31, 35.44it/s]Training CobwebTree:  67%|   | 6748/10000 [02:50<01:33, 34.94it/s]Training CobwebTree:  68%|   | 6752/10000 [02:50<01:30, 35.75it/s]Training CobwebTree:  68%|   | 6756/10000 [02:50<01:30, 35.86it/s]Training CobwebTree:  68%|   | 6760/10000 [02:51<01:28, 36.66it/s]Training CobwebTree:  68%|   | 6765/10000 [02:51<01:25, 37.64it/s]Training CobwebTree:  68%|   | 6769/10000 [02:51<01:32, 34.96it/s]Training CobwebTree:  68%|   | 6773/10000 [02:51<01:32, 34.78it/s]Training CobwebTree:  68%|   | 6777/10000 [02:51<01:33, 34.40it/s]Training CobwebTree:  68%|   | 6781/10000 [02:51<01:34, 33.92it/s]Training CobwebTree:  68%|   | 6785/10000 [02:51<01:36, 33.29it/s]Training CobwebTree:  68%|   | 6789/10000 [02:51<01:32, 34.84it/s]Training CobwebTree:  68%|   | 6794/10000 [02:51<01:31, 35.13it/s]Training CobwebTree:  68%|   | 6798/10000 [02:52<01:34, 33.78it/s]Training CobwebTree:  68%|   | 6802/10000 [02:52<01:34, 33.89it/s]Training CobwebTree:  68%|   | 6806/10000 [02:52<01:32, 34.60it/s]Training CobwebTree:  68%|   | 6811/10000 [02:52<01:26, 37.02it/s]Training CobwebTree:  68%|   | 6815/10000 [02:52<01:29, 35.69it/s]Training CobwebTree:  68%|   | 6819/10000 [02:52<01:28, 36.03it/s]Training CobwebTree:  68%|   | 6823/10000 [02:52<01:29, 35.66it/s]Training CobwebTree:  68%|   | 6827/10000 [02:52<01:29, 35.54it/s]Training CobwebTree:  68%|   | 6832/10000 [02:53<01:26, 36.51it/s]Training CobwebTree:  68%|   | 6837/10000 [02:53<01:22, 38.28it/s]Training CobwebTree:  68%|   | 6842/10000 [02:53<01:23, 37.80it/s]Training CobwebTree:  68%|   | 6846/10000 [02:53<01:23, 37.77it/s]Training CobwebTree:  68%|   | 6850/10000 [02:53<01:23, 37.65it/s]Training CobwebTree:  69%|   | 6854/10000 [02:53<01:23, 37.71it/s]Training CobwebTree:  69%|   | 6858/10000 [02:53<01:28, 35.52it/s]Training CobwebTree:  69%|   | 6862/10000 [02:53<01:31, 34.42it/s]Training CobwebTree:  69%|   | 6866/10000 [02:53<01:28, 35.31it/s]Training CobwebTree:  69%|   | 6870/10000 [02:54<01:29, 34.87it/s]Training CobwebTree:  69%|   | 6874/10000 [02:54<01:28, 35.39it/s]Training CobwebTree:  69%|   | 6878/10000 [02:54<01:26, 36.27it/s]Training CobwebTree:  69%|   | 6882/10000 [02:54<01:31, 34.25it/s]Training CobwebTree:  69%|   | 6886/10000 [02:54<01:29, 34.71it/s]Training CobwebTree:  69%|   | 6890/10000 [02:54<01:26, 36.13it/s]Training CobwebTree:  69%|   | 6894/10000 [02:54<01:28, 35.16it/s]Training CobwebTree:  69%|   | 6899/10000 [02:54<01:24, 36.76it/s]Training CobwebTree:  69%|   | 6903/10000 [02:55<01:22, 37.58it/s]Training CobwebTree:  69%|   | 6907/10000 [02:55<01:22, 37.41it/s]Training CobwebTree:  69%|   | 6911/10000 [02:55<01:25, 36.06it/s]Training CobwebTree:  69%|   | 6916/10000 [02:55<01:22, 37.42it/s]Training CobwebTree:  69%|   | 6921/10000 [02:55<01:22, 37.37it/s]Training CobwebTree:  69%|   | 6925/10000 [02:55<01:22, 37.10it/s]Training CobwebTree:  69%|   | 6929/10000 [02:55<01:24, 36.14it/s]Training CobwebTree:  69%|   | 6933/10000 [02:55<01:24, 36.11it/s]Training CobwebTree:  69%|   | 6937/10000 [02:55<01:26, 35.38it/s]Training CobwebTree:  69%|   | 6941/10000 [02:56<01:28, 34.47it/s]Training CobwebTree:  69%|   | 6945/10000 [02:56<01:25, 35.90it/s]Training CobwebTree:  69%|   | 6949/10000 [02:56<01:26, 35.22it/s]Training CobwebTree:  70%|   | 6953/10000 [02:56<01:25, 35.47it/s]Training CobwebTree:  70%|   | 6957/10000 [02:56<01:28, 34.55it/s]Training CobwebTree:  70%|   | 6961/10000 [02:56<01:31, 33.14it/s]Training CobwebTree:  70%|   | 6965/10000 [02:56<01:27, 34.66it/s]Training CobwebTree:  70%|   | 6969/10000 [02:56<01:26, 35.16it/s]Training CobwebTree:  70%|   | 6973/10000 [02:56<01:27, 34.76it/s]Training CobwebTree:  70%|   | 6977/10000 [02:57<01:29, 33.92it/s]Training CobwebTree:  70%|   | 6981/10000 [02:57<01:31, 32.92it/s]Training CobwebTree:  70%|   | 6985/10000 [02:57<01:30, 33.16it/s]Training CobwebTree:  70%|   | 6989/10000 [02:57<01:36, 31.22it/s]Training CobwebTree:  70%|   | 6993/10000 [02:57<01:32, 32.48it/s]Training CobwebTree:  70%|   | 6997/10000 [02:57<01:28, 34.08it/s]Training CobwebTree:  70%|   | 7002/10000 [02:57<01:22, 36.42it/s]Training CobwebTree:  70%|   | 7006/10000 [02:57<01:26, 34.55it/s]Training CobwebTree:  70%|   | 7010/10000 [02:58<01:28, 33.62it/s]Training CobwebTree:  70%|   | 7014/10000 [02:58<01:28, 33.67it/s]Training CobwebTree:  70%|   | 7018/10000 [02:58<01:27, 34.23it/s]Training CobwebTree:  70%|   | 7023/10000 [02:58<01:23, 35.56it/s]Training CobwebTree:  70%|   | 7027/10000 [02:58<01:26, 34.49it/s]Training CobwebTree:  70%|   | 7031/10000 [02:58<01:23, 35.66it/s]Training CobwebTree:  70%|   | 7035/10000 [02:58<01:27, 33.81it/s]Training CobwebTree:  70%|   | 7039/10000 [02:58<01:26, 34.23it/s]Training CobwebTree:  70%|   | 7043/10000 [02:59<01:30, 32.80it/s]Training CobwebTree:  70%|   | 7047/10000 [02:59<01:29, 33.16it/s]Training CobwebTree:  71%|   | 7051/10000 [02:59<01:24, 34.74it/s]Training CobwebTree:  71%|   | 7055/10000 [02:59<01:25, 34.34it/s]Training CobwebTree:  71%|   | 7059/10000 [02:59<01:26, 34.09it/s]Training CobwebTree:  71%|   | 7063/10000 [02:59<01:26, 33.86it/s]Training CobwebTree:  71%|   | 7067/10000 [02:59<01:27, 33.57it/s]Training CobwebTree:  71%|   | 7071/10000 [02:59<01:31, 32.18it/s]Training CobwebTree:  71%|   | 7075/10000 [03:00<01:28, 33.21it/s]Training CobwebTree:  71%|   | 7079/10000 [03:00<01:30, 32.45it/s]Training CobwebTree:  71%|   | 7083/10000 [03:00<01:26, 33.88it/s]Training CobwebTree:  71%|   | 7087/10000 [03:00<01:22, 35.22it/s]Training CobwebTree:  71%|   | 7091/10000 [03:00<01:23, 35.03it/s]Training CobwebTree:  71%|   | 7096/10000 [03:00<01:20, 36.00it/s]Training CobwebTree:  71%|   | 7100/10000 [03:00<01:24, 34.27it/s]Training CobwebTree:  71%|   | 7104/10000 [03:00<01:24, 34.23it/s]Training CobwebTree:  71%|   | 7108/10000 [03:00<01:22, 35.00it/s]Training CobwebTree:  71%|   | 7112/10000 [03:01<01:22, 34.93it/s]Training CobwebTree:  71%|   | 7116/10000 [03:01<01:24, 34.12it/s]Training CobwebTree:  71%|   | 7120/10000 [03:01<01:23, 34.49it/s]Training CobwebTree:  71%|   | 7124/10000 [03:01<01:20, 35.62it/s]Training CobwebTree:  71%|  | 7128/10000 [03:01<01:20, 35.83it/s]Training CobwebTree:  71%|  | 7132/10000 [03:01<01:22, 34.71it/s]Training CobwebTree:  71%|  | 7136/10000 [03:01<01:23, 34.37it/s]Training CobwebTree:  71%|  | 7140/10000 [03:01<01:22, 34.69it/s]Training CobwebTree:  71%|  | 7144/10000 [03:01<01:20, 35.28it/s]Training CobwebTree:  71%|  | 7148/10000 [03:02<01:23, 34.00it/s]Training CobwebTree:  72%|  | 7152/10000 [03:02<01:20, 35.22it/s]Training CobwebTree:  72%|  | 7157/10000 [03:02<01:16, 37.36it/s]Training CobwebTree:  72%|  | 7161/10000 [03:02<01:20, 35.48it/s]Training CobwebTree:  72%|  | 7165/10000 [03:02<01:20, 35.18it/s]Training CobwebTree:  72%|  | 7169/10000 [03:02<01:22, 34.39it/s]Training CobwebTree:  72%|  | 7173/10000 [03:02<01:28, 31.99it/s]Training CobwebTree:  72%|  | 7177/10000 [03:02<01:27, 32.09it/s]Training CobwebTree:  72%|  | 7181/10000 [03:03<01:25, 32.95it/s]Training CobwebTree:  72%|  | 7185/10000 [03:03<01:22, 34.17it/s]Training CobwebTree:  72%|  | 7189/10000 [03:03<01:22, 34.27it/s]Training CobwebTree:  72%|  | 7193/10000 [03:03<01:19, 35.11it/s]Training CobwebTree:  72%|  | 7197/10000 [03:03<01:23, 33.41it/s]Training CobwebTree:  72%|  | 7201/10000 [03:03<01:25, 32.76it/s]Training CobwebTree:  72%|  | 7205/10000 [03:03<01:22, 34.08it/s]Training CobwebTree:  72%|  | 7209/10000 [03:03<01:22, 33.68it/s]Training CobwebTree:  72%|  | 7213/10000 [03:04<01:21, 34.07it/s]Training CobwebTree:  72%|  | 7217/10000 [03:04<01:24, 33.07it/s]Training CobwebTree:  72%|  | 7221/10000 [03:04<01:22, 33.54it/s]Training CobwebTree:  72%|  | 7225/10000 [03:04<01:22, 33.54it/s]Training CobwebTree:  72%|  | 7230/10000 [03:04<01:16, 36.28it/s]Training CobwebTree:  72%|  | 7234/10000 [03:04<01:16, 36.10it/s]Training CobwebTree:  72%|  | 7238/10000 [03:04<01:19, 34.89it/s]Training CobwebTree:  72%|  | 7242/10000 [03:04<01:19, 34.73it/s]Training CobwebTree:  72%|  | 7246/10000 [03:04<01:17, 35.54it/s]Training CobwebTree:  72%|  | 7250/10000 [03:05<01:18, 35.10it/s]Training CobwebTree:  73%|  | 7254/10000 [03:05<01:18, 34.90it/s]Training CobwebTree:  73%|  | 7258/10000 [03:05<01:18, 35.11it/s]Training CobwebTree:  73%|  | 7262/10000 [03:05<01:18, 34.69it/s]Training CobwebTree:  73%|  | 7266/10000 [03:05<01:18, 34.72it/s]Training CobwebTree:  73%|  | 7270/10000 [03:05<01:19, 34.47it/s]Training CobwebTree:  73%|  | 7274/10000 [03:05<01:16, 35.75it/s]Training CobwebTree:  73%|  | 7278/10000 [03:05<01:16, 35.80it/s]Training CobwebTree:  73%|  | 7282/10000 [03:05<01:16, 35.72it/s]Training CobwebTree:  73%|  | 7286/10000 [03:06<01:15, 36.11it/s]Training CobwebTree:  73%|  | 7290/10000 [03:06<01:17, 34.98it/s]Training CobwebTree:  73%|  | 7294/10000 [03:06<01:16, 35.32it/s]Training CobwebTree:  73%|  | 7298/10000 [03:06<01:18, 34.54it/s]Training CobwebTree:  73%|  | 7302/10000 [03:06<01:17, 34.85it/s]Training CobwebTree:  73%|  | 7306/10000 [03:06<01:15, 35.62it/s]Training CobwebTree:  73%|  | 7310/10000 [03:06<01:14, 36.16it/s]Training CobwebTree:  73%|  | 7314/10000 [03:06<01:16, 35.24it/s]Training CobwebTree:  73%|  | 7318/10000 [03:07<01:16, 35.17it/s]Training CobwebTree:  73%|  | 7322/10000 [03:07<01:18, 34.22it/s]Training CobwebTree:  73%|  | 7326/10000 [03:07<01:17, 34.43it/s]Training CobwebTree:  73%|  | 7330/10000 [03:07<01:15, 35.41it/s]Training CobwebTree:  73%|  | 7334/10000 [03:07<01:13, 36.11it/s]Training CobwebTree:  73%|  | 7338/10000 [03:07<01:11, 37.07it/s]Training CobwebTree:  73%|  | 7342/10000 [03:07<01:11, 37.09it/s]Training CobwebTree:  73%|  | 7347/10000 [03:07<01:10, 37.89it/s]Training CobwebTree:  74%|  | 7351/10000 [03:07<01:10, 37.49it/s]Training CobwebTree:  74%|  | 7355/10000 [03:08<01:15, 35.16it/s]Training CobwebTree:  74%|  | 7359/10000 [03:08<01:16, 34.38it/s]Training CobwebTree:  74%|  | 7363/10000 [03:08<01:15, 35.03it/s]Training CobwebTree:  74%|  | 7368/10000 [03:08<01:09, 37.69it/s]Training CobwebTree:  74%|  | 7372/10000 [03:08<01:09, 37.65it/s]Training CobwebTree:  74%|  | 7376/10000 [03:08<01:12, 36.23it/s]Training CobwebTree:  74%|  | 7380/10000 [03:08<01:11, 36.57it/s]Training CobwebTree:  74%|  | 7384/10000 [03:08<01:11, 36.38it/s]Training CobwebTree:  74%|  | 7388/10000 [03:08<01:15, 34.79it/s]Training CobwebTree:  74%|  | 7392/10000 [03:09<01:17, 33.72it/s]Training CobwebTree:  74%|  | 7396/10000 [03:09<01:16, 34.13it/s]Training CobwebTree:  74%|  | 7400/10000 [03:09<01:17, 33.67it/s]Training CobwebTree:  74%|  | 7404/10000 [03:09<01:21, 31.98it/s]Training CobwebTree:  74%|  | 7408/10000 [03:09<01:24, 30.74it/s]Training CobwebTree:  74%|  | 7412/10000 [03:09<01:20, 32.24it/s]Training CobwebTree:  74%|  | 7416/10000 [03:09<01:17, 33.14it/s]Training CobwebTree:  74%|  | 7420/10000 [03:09<01:14, 34.41it/s]Training CobwebTree:  74%|  | 7424/10000 [03:10<01:13, 35.13it/s]Training CobwebTree:  74%|  | 7428/10000 [03:10<01:13, 35.10it/s]Training CobwebTree:  74%|  | 7432/10000 [03:10<01:11, 36.04it/s]Training CobwebTree:  74%|  | 7436/10000 [03:10<01:10, 36.42it/s]Training CobwebTree:  74%|  | 7440/10000 [03:10<01:12, 35.43it/s]Training CobwebTree:  74%|  | 7444/10000 [03:10<01:14, 34.29it/s]Training CobwebTree:  74%|  | 7448/10000 [03:10<01:14, 34.36it/s]Training CobwebTree:  75%|  | 7452/10000 [03:10<01:13, 34.85it/s]Training CobwebTree:  75%|  | 7456/10000 [03:10<01:11, 35.62it/s]Training CobwebTree:  75%|  | 7460/10000 [03:11<01:09, 36.42it/s]Training CobwebTree:  75%|  | 7464/10000 [03:11<01:12, 35.08it/s]Training CobwebTree:  75%|  | 7468/10000 [03:11<01:11, 35.30it/s]Training CobwebTree:  75%|  | 7472/10000 [03:11<01:13, 34.39it/s]Training CobwebTree:  75%|  | 7476/10000 [03:11<01:13, 34.16it/s]Training CobwebTree:  75%|  | 7480/10000 [03:11<01:13, 34.16it/s]Training CobwebTree:  75%|  | 7485/10000 [03:11<01:12, 34.56it/s]Training CobwebTree:  75%|  | 7489/10000 [03:11<01:13, 34.21it/s]Training CobwebTree:  75%|  | 7493/10000 [03:12<01:15, 33.37it/s]Training CobwebTree:  75%|  | 7497/10000 [03:12<01:12, 34.29it/s]Training CobwebTree:  75%|  | 7501/10000 [03:12<01:17, 32.33it/s]Training CobwebTree:  75%|  | 7505/10000 [03:12<01:14, 33.28it/s]Training CobwebTree:  75%|  | 7509/10000 [03:12<01:15, 33.06it/s]Training CobwebTree:  75%|  | 7513/10000 [03:12<01:14, 33.35it/s]Training CobwebTree:  75%|  | 7517/10000 [03:12<01:15, 32.89it/s]Training CobwebTree:  75%|  | 7521/10000 [03:12<01:12, 34.01it/s]Training CobwebTree:  75%|  | 7525/10000 [03:12<01:13, 33.55it/s]Training CobwebTree:  75%|  | 7529/10000 [03:13<01:13, 33.44it/s]Training CobwebTree:  75%|  | 7533/10000 [03:13<01:11, 34.42it/s]Training CobwebTree:  75%|  | 7537/10000 [03:13<01:11, 34.55it/s]Training CobwebTree:  75%|  | 7541/10000 [03:13<01:11, 34.19it/s]Training CobwebTree:  75%|  | 7545/10000 [03:13<01:15, 32.38it/s]Training CobwebTree:  75%|  | 7549/10000 [03:13<01:13, 33.32it/s]Training CobwebTree:  76%|  | 7553/10000 [03:13<01:12, 33.87it/s]Training CobwebTree:  76%|  | 7557/10000 [03:13<01:14, 32.77it/s]Training CobwebTree:  76%|  | 7561/10000 [03:14<01:15, 32.31it/s]Training CobwebTree:  76%|  | 7565/10000 [03:14<01:13, 32.99it/s]Training CobwebTree:  76%|  | 7569/10000 [03:14<01:10, 34.43it/s]Training CobwebTree:  76%|  | 7573/10000 [03:14<01:15, 32.26it/s]Training CobwebTree:  76%|  | 7577/10000 [03:14<01:13, 33.08it/s]Training CobwebTree:  76%|  | 7581/10000 [03:14<01:13, 33.00it/s]Training CobwebTree:  76%|  | 7585/10000 [03:14<01:11, 33.89it/s]Training CobwebTree:  76%|  | 7589/10000 [03:14<01:08, 35.00it/s]Training CobwebTree:  76%|  | 7593/10000 [03:14<01:07, 35.67it/s]Training CobwebTree:  76%|  | 7597/10000 [03:15<01:06, 35.99it/s]Training CobwebTree:  76%|  | 7601/10000 [03:15<01:11, 33.57it/s]Training CobwebTree:  76%|  | 7605/10000 [03:15<01:12, 33.07it/s]Training CobwebTree:  76%|  | 7610/10000 [03:15<01:09, 34.54it/s]Training CobwebTree:  76%|  | 7614/10000 [03:15<01:07, 35.17it/s]Training CobwebTree:  76%|  | 7618/10000 [03:15<01:09, 34.44it/s]Training CobwebTree:  76%|  | 7622/10000 [03:15<01:13, 32.42it/s]Training CobwebTree:  76%|  | 7626/10000 [03:15<01:10, 33.67it/s]Training CobwebTree:  76%|  | 7630/10000 [03:16<01:08, 34.65it/s]Training CobwebTree:  76%|  | 7634/10000 [03:16<01:09, 34.03it/s]Training CobwebTree:  76%|  | 7638/10000 [03:16<01:11, 32.99it/s]Training CobwebTree:  76%|  | 7642/10000 [03:16<01:11, 33.04it/s]Training CobwebTree:  76%|  | 7646/10000 [03:16<01:09, 33.86it/s]Training CobwebTree:  77%|  | 7651/10000 [03:16<01:06, 35.45it/s]Training CobwebTree:  77%|  | 7655/10000 [03:16<01:07, 34.85it/s]Training CobwebTree:  77%|  | 7659/10000 [03:16<01:09, 33.85it/s]Training CobwebTree:  77%|  | 7663/10000 [03:17<01:09, 33.46it/s]Training CobwebTree:  77%|  | 7667/10000 [03:17<01:08, 34.20it/s]Training CobwebTree:  77%|  | 7671/10000 [03:17<01:05, 35.30it/s]Training CobwebTree:  77%|  | 7675/10000 [03:17<01:07, 34.21it/s]Training CobwebTree:  77%|  | 7679/10000 [03:17<01:08, 33.72it/s]Training CobwebTree:  77%|  | 7683/10000 [03:17<01:10, 33.06it/s]Training CobwebTree:  77%|  | 7687/10000 [03:17<01:08, 33.65it/s]Training CobwebTree:  77%|  | 7692/10000 [03:17<01:05, 35.26it/s]Training CobwebTree:  77%|  | 7696/10000 [03:18<01:05, 35.30it/s]Training CobwebTree:  77%|  | 7700/10000 [03:18<01:08, 33.57it/s]Training CobwebTree:  77%|  | 7704/10000 [03:18<01:06, 34.29it/s]Training CobwebTree:  77%|  | 7708/10000 [03:18<01:09, 33.00it/s]Training CobwebTree:  77%|  | 7712/10000 [03:18<01:08, 33.55it/s]Training CobwebTree:  77%|  | 7716/10000 [03:18<01:08, 33.41it/s]Training CobwebTree:  77%|  | 7720/10000 [03:18<01:06, 34.09it/s]Training CobwebTree:  77%|  | 7724/10000 [03:18<01:07, 33.65it/s]Training CobwebTree:  77%|  | 7728/10000 [03:18<01:07, 33.83it/s]Training CobwebTree:  77%|  | 7732/10000 [03:19<01:07, 33.48it/s]Training CobwebTree:  77%|  | 7736/10000 [03:19<01:08, 32.90it/s]Training CobwebTree:  77%|  | 7740/10000 [03:19<01:12, 31.38it/s]Training CobwebTree:  77%|  | 7744/10000 [03:19<01:10, 31.79it/s]Training CobwebTree:  77%|  | 7748/10000 [03:19<01:08, 32.79it/s]Training CobwebTree:  78%|  | 7752/10000 [03:19<01:08, 32.78it/s]Training CobwebTree:  78%|  | 7756/10000 [03:19<01:11, 31.50it/s]Training CobwebTree:  78%|  | 7760/10000 [03:19<01:08, 32.48it/s]Training CobwebTree:  78%|  | 7764/10000 [03:20<01:06, 33.87it/s]Training CobwebTree:  78%|  | 7768/10000 [03:20<01:06, 33.38it/s]Training CobwebTree:  78%|  | 7772/10000 [03:20<01:04, 34.72it/s]Training CobwebTree:  78%|  | 7776/10000 [03:20<01:03, 34.81it/s]Training CobwebTree:  78%|  | 7780/10000 [03:20<01:04, 34.41it/s]Training CobwebTree:  78%|  | 7784/10000 [03:20<01:06, 33.46it/s]Training CobwebTree:  78%|  | 7788/10000 [03:20<01:05, 33.90it/s]Training CobwebTree:  78%|  | 7792/10000 [03:20<01:04, 33.98it/s]Training CobwebTree:  78%|  | 7796/10000 [03:21<01:02, 35.19it/s]Training CobwebTree:  78%|  | 7800/10000 [03:21<01:02, 34.94it/s]Training CobwebTree:  78%|  | 7804/10000 [03:21<01:07, 32.42it/s]Training CobwebTree:  78%|  | 7808/10000 [03:21<01:06, 32.79it/s]Training CobwebTree:  78%|  | 7812/10000 [03:21<01:06, 32.80it/s]Training CobwebTree:  78%|  | 7816/10000 [03:21<01:08, 31.96it/s]Training CobwebTree:  78%|  | 7820/10000 [03:21<01:05, 33.10it/s]Training CobwebTree:  78%|  | 7824/10000 [03:21<01:05, 33.30it/s]Training CobwebTree:  78%|  | 7828/10000 [03:21<01:06, 32.58it/s]Training CobwebTree:  78%|  | 7832/10000 [03:22<01:05, 33.29it/s]Training CobwebTree:  78%|  | 7836/10000 [03:22<01:04, 33.72it/s]Training CobwebTree:  78%|  | 7840/10000 [03:22<01:02, 34.67it/s]Training CobwebTree:  78%|  | 7844/10000 [03:22<01:02, 34.26it/s]Training CobwebTree:  78%|  | 7848/10000 [03:22<01:03, 34.02it/s]Training CobwebTree:  79%|  | 7852/10000 [03:22<01:02, 34.55it/s]Training CobwebTree:  79%|  | 7856/10000 [03:22<01:03, 33.76it/s]Training CobwebTree:  79%|  | 7860/10000 [03:22<01:02, 34.02it/s]Training CobwebTree:  79%|  | 7864/10000 [03:23<01:02, 34.01it/s]Training CobwebTree:  79%|  | 7868/10000 [03:23<01:02, 34.25it/s]Training CobwebTree:  79%|  | 7872/10000 [03:23<01:01, 34.55it/s]Training CobwebTree:  79%|  | 7876/10000 [03:23<01:02, 33.79it/s]Training CobwebTree:  79%|  | 7880/10000 [03:23<01:03, 33.51it/s]Training CobwebTree:  79%|  | 7884/10000 [03:23<01:01, 34.18it/s]Training CobwebTree:  79%|  | 7888/10000 [03:23<01:01, 34.35it/s]Training CobwebTree:  79%|  | 7892/10000 [03:23<00:59, 35.20it/s]Training CobwebTree:  79%|  | 7896/10000 [03:23<01:01, 34.01it/s]Training CobwebTree:  79%|  | 7900/10000 [03:24<01:00, 34.76it/s]Training CobwebTree:  79%|  | 7904/10000 [03:24<01:00, 34.45it/s]Training CobwebTree:  79%|  | 7908/10000 [03:24<00:58, 35.67it/s]Training CobwebTree:  79%|  | 7912/10000 [03:24<01:01, 34.03it/s]Training CobwebTree:  79%|  | 7916/10000 [03:24<01:01, 34.06it/s]Training CobwebTree:  79%|  | 7920/10000 [03:24<01:01, 33.82it/s]Training CobwebTree:  79%|  | 7924/10000 [03:24<01:00, 34.52it/s]Training CobwebTree:  79%|  | 7928/10000 [03:24<01:00, 34.38it/s]Training CobwebTree:  79%|  | 7932/10000 [03:25<01:00, 33.98it/s]Training CobwebTree:  79%|  | 7936/10000 [03:25<01:01, 33.66it/s]Training CobwebTree:  79%|  | 7940/10000 [03:25<01:00, 33.88it/s]Training CobwebTree:  79%|  | 7944/10000 [03:25<01:00, 34.08it/s]Training CobwebTree:  79%|  | 7948/10000 [03:25<01:01, 33.40it/s]Training CobwebTree:  80%|  | 7952/10000 [03:25<01:00, 34.10it/s]Training CobwebTree:  80%|  | 7956/10000 [03:25<00:59, 34.49it/s]Training CobwebTree:  80%|  | 7960/10000 [03:25<00:59, 34.36it/s]Training CobwebTree:  80%|  | 7964/10000 [03:25<01:00, 33.83it/s]Training CobwebTree:  80%|  | 7968/10000 [03:26<00:58, 34.68it/s]Training CobwebTree:  80%|  | 7972/10000 [03:26<01:01, 33.11it/s]Training CobwebTree:  80%|  | 7976/10000 [03:26<01:00, 33.65it/s]Training CobwebTree:  80%|  | 7980/10000 [03:26<01:01, 32.83it/s]Training CobwebTree:  80%|  | 7984/10000 [03:26<01:04, 31.22it/s]Training CobwebTree:  80%|  | 7988/10000 [03:26<01:00, 32.98it/s]Training CobwebTree:  80%|  | 7992/10000 [03:26<01:00, 33.37it/s]Training CobwebTree:  80%|  | 7996/10000 [03:26<00:58, 33.97it/s]Training CobwebTree:  80%|  | 8000/10000 [03:27<00:58, 34.04it/s]Training CobwebTree:  80%|  | 8004/10000 [03:27<00:56, 35.43it/s]Training CobwebTree:  80%|  | 8008/10000 [03:27<00:58, 34.26it/s]Training CobwebTree:  80%|  | 8012/10000 [03:27<01:00, 33.04it/s]Training CobwebTree:  80%|  | 8017/10000 [03:27<00:55, 35.53it/s]Training CobwebTree:  80%|  | 8021/10000 [03:27<00:55, 35.49it/s]Training CobwebTree:  80%|  | 8025/10000 [03:27<00:57, 34.22it/s]Training CobwebTree:  80%|  | 8029/10000 [03:27<01:00, 32.52it/s]Training CobwebTree:  80%|  | 8033/10000 [03:28<01:01, 31.76it/s]Training CobwebTree:  80%|  | 8037/10000 [03:28<01:02, 31.50it/s]Training CobwebTree:  80%|  | 8041/10000 [03:28<01:02, 31.42it/s]Training CobwebTree:  80%|  | 8045/10000 [03:28<01:01, 31.78it/s]Training CobwebTree:  80%|  | 8049/10000 [03:28<01:02, 31.37it/s]Training CobwebTree:  81%|  | 8053/10000 [03:28<01:02, 31.19it/s]Training CobwebTree:  81%|  | 8058/10000 [03:28<00:57, 33.75it/s]Training CobwebTree:  81%|  | 8062/10000 [03:28<00:57, 33.72it/s]Training CobwebTree:  81%|  | 8066/10000 [03:29<00:56, 34.16it/s]Training CobwebTree:  81%|  | 8070/10000 [03:29<00:57, 33.36it/s]Training CobwebTree:  81%|  | 8074/10000 [03:29<00:56, 33.83it/s]Training CobwebTree:  81%|  | 8078/10000 [03:29<00:55, 34.74it/s]Training CobwebTree:  81%|  | 8082/10000 [03:29<00:59, 32.35it/s]Training CobwebTree:  81%|  | 8086/10000 [03:29<00:57, 33.50it/s]Training CobwebTree:  81%|  | 8090/10000 [03:29<00:59, 32.29it/s]Training CobwebTree:  81%|  | 8095/10000 [03:29<00:54, 34.68it/s]Training CobwebTree:  81%|  | 8099/10000 [03:30<00:55, 34.39it/s]Training CobwebTree:  81%|  | 8103/10000 [03:30<00:59, 31.90it/s]Training CobwebTree:  81%|  | 8107/10000 [03:30<00:57, 32.87it/s]Training CobwebTree:  81%|  | 8111/10000 [03:30<00:59, 31.57it/s]Training CobwebTree:  81%|  | 8115/10000 [03:30<00:58, 32.10it/s]Training CobwebTree:  81%|  | 8119/10000 [03:30<00:57, 32.74it/s]Training CobwebTree:  81%|  | 8123/10000 [03:30<00:59, 31.61it/s]Training CobwebTree:  81%| | 8127/10000 [03:30<01:02, 29.82it/s]Training CobwebTree:  81%| | 8131/10000 [03:31<01:01, 30.22it/s]Training CobwebTree:  81%| | 8135/10000 [03:31<00:59, 31.41it/s]Training CobwebTree:  81%| | 8139/10000 [03:31<00:57, 32.31it/s]Training CobwebTree:  81%| | 8143/10000 [03:31<00:57, 32.10it/s]Training CobwebTree:  81%| | 8147/10000 [03:31<00:56, 32.54it/s]Training CobwebTree:  82%| | 8151/10000 [03:31<00:57, 32.23it/s]Training CobwebTree:  82%| | 8155/10000 [03:31<00:56, 32.76it/s]Training CobwebTree:  82%| | 8159/10000 [03:31<00:55, 33.11it/s]Training CobwebTree:  82%| | 8163/10000 [03:32<00:56, 32.55it/s]Training CobwebTree:  82%| | 8167/10000 [03:32<00:54, 33.58it/s]Training CobwebTree:  82%| | 8172/10000 [03:32<00:51, 35.64it/s]Training CobwebTree:  82%| | 8176/10000 [03:32<00:51, 35.64it/s]Training CobwebTree:  82%| | 8180/10000 [03:32<00:51, 35.69it/s]Training CobwebTree:  82%| | 8184/10000 [03:32<00:52, 34.37it/s]Training CobwebTree:  82%| | 8188/10000 [03:32<00:52, 34.41it/s]Training CobwebTree:  82%| | 8192/10000 [03:32<00:53, 33.88it/s]Training CobwebTree:  82%| | 8196/10000 [03:32<00:54, 33.20it/s]Training CobwebTree:  82%| | 8200/10000 [03:33<00:52, 34.55it/s]Training CobwebTree:  82%| | 8204/10000 [03:33<00:50, 35.81it/s]Training CobwebTree:  82%| | 8208/10000 [03:33<00:52, 34.39it/s]Training CobwebTree:  82%| | 8212/10000 [03:33<00:52, 34.01it/s]Training CobwebTree:  82%| | 8216/10000 [03:33<00:50, 35.60it/s]Training CobwebTree:  82%| | 8220/10000 [03:33<00:48, 36.64it/s]Training CobwebTree:  82%| | 8224/10000 [03:33<00:49, 35.65it/s]Training CobwebTree:  82%| | 8228/10000 [03:33<00:51, 34.72it/s]Training CobwebTree:  82%| | 8232/10000 [03:34<00:51, 34.39it/s]Training CobwebTree:  82%| | 8237/10000 [03:34<00:49, 35.49it/s]Training CobwebTree:  82%| | 8241/10000 [03:34<00:49, 35.35it/s]Training CobwebTree:  82%| | 8245/10000 [03:34<00:50, 34.87it/s]Training CobwebTree:  82%| | 8249/10000 [03:34<00:51, 33.92it/s]Training CobwebTree:  83%| | 8253/10000 [03:34<00:50, 34.92it/s]Training CobwebTree:  83%| | 8257/10000 [03:34<00:48, 36.20it/s]Training CobwebTree:  83%| | 8261/10000 [03:34<00:47, 36.81it/s]Training CobwebTree:  83%| | 8265/10000 [03:34<00:49, 35.12it/s]Training CobwebTree:  83%| | 8269/10000 [03:35<00:49, 34.96it/s]Training CobwebTree:  83%| | 8273/10000 [03:35<00:50, 33.88it/s]Training CobwebTree:  83%| | 8277/10000 [03:35<00:49, 34.73it/s]Training CobwebTree:  83%| | 8281/10000 [03:35<00:47, 36.06it/s]Training CobwebTree:  83%| | 8285/10000 [03:35<00:48, 35.40it/s]Training CobwebTree:  83%| | 8289/10000 [03:35<00:51, 33.12it/s]Training CobwebTree:  83%| | 8293/10000 [03:35<00:49, 34.67it/s]Training CobwebTree:  83%| | 8297/10000 [03:35<00:49, 34.37it/s]Training CobwebTree:  83%| | 8301/10000 [03:35<00:48, 34.86it/s]Training CobwebTree:  83%| | 8305/10000 [03:36<00:49, 34.22it/s]Training CobwebTree:  83%| | 8309/10000 [03:36<00:49, 34.04it/s]Training CobwebTree:  83%| | 8313/10000 [03:36<00:47, 35.16it/s]Training CobwebTree:  83%|