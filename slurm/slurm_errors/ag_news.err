2025-12-21 13:41:31,973 INFO __main__: Starting benchmark for dataset=agnews
2025-12-21 13:41:34,596 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-21 13:41:34,738 INFO gensim.corpora.dictionary: built Dictionary<21775 unique tokens: ['band', 'bears', 'black', 'claw', 'cynics']...> from 10000 documents (total 256241 corpus positions)
2025-12-21 13:41:34,741 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<21775 unique tokens: ['band', 'bears', 'black', 'claw', 'cynics']...> from 10000 documents (total 256241 corpus positions)", 'datetime': '2025-12-21T13:41:34.739099', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-153-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-21 13:41:35,535 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-21 13:41:35,535 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-21 13:41:38,061 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10000 docs
2025-12-21 13:43:32,506 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:43:36,711 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,711 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,714 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,715 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,715 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,716 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,716 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,716 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,722 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,722 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,728 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,728 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,728 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,728 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,732 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,739 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,739 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,739 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,752 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,752 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,752 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:36,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:36,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:38,082 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:38,082 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:40,433 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:43:40,764 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-21 13:43:42,414 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:43:46,912 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-21 13:43:46,913 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-21 13:43:46,914 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-21 13:43:46,915 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-21 13:43:46,915 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-21 13:43:46,916 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-21 13:43:46,917 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-21 13:43:46,918 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-21 13:43:46,919 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-21 13:43:46,920 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-21 13:43:46,920 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-21 13:43:46,921 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-21 13:43:46,922 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-21 13:43:46,923 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-21 13:43:46,923 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-21 13:43:46,924 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-21 13:43:46,925 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-21 13:43:46,925 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-21 13:43:46,926 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-21 13:43:46,927 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-21 13:43:46,928 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-21 13:43:46,928 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-21 13:43:46,929 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-21 13:43:46,930 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-21 13:43:46,931 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-21 13:43:46,932 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-21 13:43:46,932 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-21 13:43:46,933 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-21 13:43:46,934 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-21 13:43:46,935 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-21 13:43:46,936 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-21 13:43:46,936 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-21 13:43:46,937 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-21 13:43:46,938 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-21 13:43:46,939 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-21 13:43:46,939 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-21 13:43:46,940 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-21 13:43:46,941 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-21 13:43:46,941 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-21 13:43:46,942 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-21 13:43:46,943 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-21 13:43:46,943 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-21 13:43:46,944 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-21 13:43:46,945 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-21 13:43:46,946 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-21 13:43:46,946 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-21 13:43:46,947 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-21 13:43:46,948 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-21 13:43:46,948 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-21 13:43:46,949 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-21 13:43:46,949 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-21 13:43:46,950 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-21 13:43:46,951 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-21 13:43:46,952 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-21 13:43:46,952 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-21 13:43:46,953 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-21 13:43:46,954 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-21 13:43:46,955 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-21 13:43:46,955 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-21 13:43:46,956 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-21 13:43:46,957 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-21 13:43:46,958 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-21 13:43:46,958 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-21 13:43:46,959 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-21 13:43:46,960 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-21 13:43:46,960 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-21 13:43:46,961 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-21 13:43:46,962 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-21 13:43:46,962 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-21 13:43:46,963 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-21 13:43:46,964 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-21 13:43:46,964 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-21 13:43:46,965 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-21 13:43:46,966 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-21 13:43:46,967 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-21 13:43:46,968 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-21 13:43:46,968 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-21 13:43:46,969 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-21 13:43:46,969 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-21 13:43:46,970 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-21 13:43:46,971 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-21 13:43:46,971 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-21 13:43:46,972 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-21 13:43:46,973 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-21 13:43:46,974 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-21 13:43:46,974 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-21 13:43:46,975 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-21 13:43:46,976 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-21 13:43:46,976 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-21 13:43:46,977 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-21 13:43:46,978 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-21 13:43:46,978 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-21 13:43:46,979 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-21 13:43:46,980 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-21 13:43:46,980 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-21 13:43:46,981 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-21 13:43:46,982 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-21 13:43:46,983 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-21 13:43:46,983 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-21 13:43:46,984 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-21 13:43:46,985 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-21 13:43:46,985 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-21 13:43:46,986 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-21 13:43:46,987 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-21 13:43:46,987 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-21 13:43:46,988 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-21 13:43:46,989 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-21 13:43:46,990 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-21 13:43:46,990 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-21 13:43:46,991 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-21 13:43:46,992 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-21 13:43:46,993 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-21 13:43:46,993 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-21 13:43:46,994 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-21 13:43:46,994 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-21 13:43:46,995 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-21 13:43:46,996 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-21 13:43:46,996 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-21 13:43:46,997 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-21 13:43:46,997 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-21 13:43:46,998 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-21 13:43:46,999 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-21 13:43:46,999 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-21 13:43:47,000 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-21 13:43:47,001 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-21 13:43:47,002 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-21 13:43:47,002 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-21 13:43:47,003 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-21 13:43:47,004 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-21 13:43:47,004 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-21 13:43:47,005 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-21 13:43:47,006 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-21 13:43:47,006 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-21 13:43:47,007 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-21 13:43:47,008 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-21 13:43:47,008 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-21 13:43:47,009 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-21 13:43:47,010 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-21 13:43:47,011 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-21 13:43:47,011 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-21 13:43:47,018 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-21 13:43:47,019 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-21 13:43:47,019 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-21 13:43:47,020 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-21 13:43:47,031 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-21 13:43:47,031 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-21 13:43:47,032 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-21 13:43:47,035 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-21 13:43:47,035 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-21 13:43:47,036 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-21 13:43:47,037 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-21 13:43:47,037 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-21 13:43:47,038 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-21 13:43:47,039 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-21 13:43:47,040 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-21 13:43:47,063 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-21 13:43:47,063 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-21 13:43:47,919 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,919 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,919 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,923 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,923 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,925 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,930 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,930 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,931 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,931 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,932 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,932 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,932 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,932 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,933 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,935 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,935 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,935 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,935 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,938 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,938 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,938 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,942 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,942 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,944 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,944 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,945 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,945 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,945 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,948 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,948 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,948 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,948 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,949 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,951 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,973 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,994 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:47,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:47,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,024 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,060 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,068 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,088 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,204 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,207 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,208 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,249 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:48,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:48,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:50,050 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:50,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:50,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:43:50,234 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:43:52,206 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:43:52,507 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-21 13:43:53,107 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10000 docs
2025-12-21 13:45:04,536 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:45:08,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,641 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,645 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,646 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,648 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,708 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:08,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:08,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:12,257 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:45:12,374 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-21 13:45:12,770 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:45:16,776 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-21 13:45:16,777 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-21 13:45:16,778 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-21 13:45:16,779 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-21 13:45:16,780 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-21 13:45:16,780 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-21 13:45:16,781 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-21 13:45:16,782 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-21 13:45:16,783 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-21 13:45:16,784 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-21 13:45:16,785 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-21 13:45:16,785 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-21 13:45:16,786 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-21 13:45:16,787 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-21 13:45:16,787 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-21 13:45:16,788 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-21 13:45:16,789 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-21 13:45:16,789 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-21 13:45:16,790 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-21 13:45:16,791 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-21 13:45:16,792 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-21 13:45:16,792 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-21 13:45:16,793 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-21 13:45:16,793 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-21 13:45:16,795 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-21 13:45:16,795 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-21 13:45:16,796 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-21 13:45:16,797 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-21 13:45:16,797 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-21 13:45:16,798 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-21 13:45:16,800 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-21 13:45:16,801 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-21 13:45:16,801 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-21 13:45:16,802 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-21 13:45:16,803 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-21 13:45:16,803 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-21 13:45:16,804 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-21 13:45:16,804 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-21 13:45:16,804 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-21 13:45:16,805 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-21 13:45:16,805 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-21 13:45:16,805 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-21 13:45:16,806 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-21 13:45:16,806 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-21 13:45:16,806 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-21 13:45:16,807 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-21 13:45:16,807 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-21 13:45:16,807 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-21 13:45:16,808 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-21 13:45:16,808 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-21 13:45:16,808 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-21 13:45:16,809 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-21 13:45:16,809 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-21 13:45:16,810 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-21 13:45:16,810 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-21 13:45:16,810 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-21 13:45:16,810 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-21 13:45:16,811 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-21 13:45:16,811 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-21 13:45:16,812 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-21 13:45:16,812 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-21 13:45:16,812 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-21 13:45:16,813 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-21 13:45:16,813 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-21 13:45:16,813 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-21 13:45:16,814 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-21 13:45:16,814 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-21 13:45:16,814 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-21 13:45:16,815 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-21 13:45:16,815 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-21 13:45:16,815 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-21 13:45:16,816 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-21 13:45:16,816 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-21 13:45:16,816 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-21 13:45:16,817 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-21 13:45:16,817 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-21 13:45:16,818 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-21 13:45:16,818 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-21 13:45:16,818 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-21 13:45:16,819 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-21 13:45:16,819 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-21 13:45:16,819 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-21 13:45:16,820 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-21 13:45:16,820 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-21 13:45:16,820 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-21 13:45:16,821 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-21 13:45:16,821 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-21 13:45:16,821 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-21 13:45:16,822 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-21 13:45:16,822 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-21 13:45:16,822 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-21 13:45:16,822 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-21 13:45:16,823 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-21 13:45:16,823 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-21 13:45:16,823 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-21 13:45:16,824 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-21 13:45:16,824 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-21 13:45:16,824 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-21 13:45:16,825 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-21 13:45:16,826 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-21 13:45:16,826 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-21 13:45:16,827 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-21 13:45:16,827 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-21 13:45:16,828 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-21 13:45:16,829 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-21 13:45:16,829 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-21 13:45:16,830 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-21 13:45:16,831 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-21 13:45:16,831 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-21 13:45:16,832 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-21 13:45:16,832 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-21 13:45:16,833 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-21 13:45:16,833 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-21 13:45:16,834 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-21 13:45:16,835 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-21 13:45:16,836 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-21 13:45:16,836 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-21 13:45:16,837 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-21 13:45:16,838 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-21 13:45:16,838 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-21 13:45:16,839 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-21 13:45:16,839 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-21 13:45:16,840 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-21 13:45:16,841 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-21 13:45:16,841 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-21 13:45:16,842 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-21 13:45:16,842 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-21 13:45:16,843 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-21 13:45:16,844 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-21 13:45:16,844 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-21 13:45:16,845 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-21 13:45:16,846 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-21 13:45:16,847 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-21 13:45:16,847 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-21 13:45:16,848 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-21 13:45:16,848 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-21 13:45:16,849 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-21 13:45:16,850 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-21 13:45:16,859 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-21 13:45:16,859 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-21 13:45:16,923 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-21 13:45:16,924 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-21 13:45:16,924 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-21 13:45:16,925 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-21 13:45:16,926 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-21 13:45:16,926 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-21 13:45:16,927 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-21 13:45:16,927 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-21 13:45:16,929 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-21 13:45:16,930 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-21 13:45:16,955 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-21 13:45:16,956 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-21 13:45:16,957 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-21 13:45:16,957 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-21 13:45:16,958 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-21 13:45:16,958 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-21 13:45:16,959 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-21 13:45:17,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,624 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,636 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,644 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,644 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,649 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,649 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,659 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,660 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,676 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,676 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,682 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,692 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,702 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,712 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,744 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,781 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:17,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:17,973 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:45:18,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:45:21,173 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:45:21,319 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-21 13:45:21,577 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10000 docs
Training CobwebTree:   0%|          | 0/10000 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 23/10000 [00:00<00:46, 216.29it/s]Training CobwebTree:   0%|          | 45/10000 [00:00<01:00, 164.33it/s]Training CobwebTree:   1%|          | 63/10000 [00:00<01:13, 135.94it/s]Training CobwebTree:   1%|          | 78/10000 [00:00<01:21, 121.19it/s]Training CobwebTree:   1%|          | 91/10000 [00:00<01:36, 103.11it/s]Training CobwebTree:   1%|          | 102/10000 [00:00<01:41, 97.09it/s]Training CobwebTree:   1%|          | 112/10000 [00:01<01:49, 90.40it/s]Training CobwebTree:   1%|          | 122/10000 [00:01<01:53, 87.30it/s]Training CobwebTree:   1%|         | 131/10000 [00:01<01:57, 83.85it/s]Training CobwebTree:   1%|         | 140/10000 [00:01<02:03, 79.81it/s]Training CobwebTree:   1%|         | 148/10000 [00:01<02:09, 76.11it/s]Training CobwebTree:   2%|         | 156/10000 [00:01<02:14, 73.19it/s]Training CobwebTree:   2%|         | 164/10000 [00:01<02:15, 72.59it/s]Training CobwebTree:   2%|         | 173/10000 [00:01<02:11, 74.93it/s]Training CobwebTree:   2%|         | 181/10000 [00:01<02:17, 71.46it/s]Training CobwebTree:   2%|         | 190/10000 [00:02<02:12, 73.79it/s]Training CobwebTree:   2%|         | 200/10000 [00:02<02:04, 78.91it/s]Training CobwebTree:   2%|         | 208/10000 [00:02<02:07, 76.92it/s]Training CobwebTree:   2%|         | 217/10000 [00:02<02:06, 77.18it/s]Training CobwebTree:   2%|         | 226/10000 [00:02<02:03, 79.45it/s]Training CobwebTree:   2%|         | 235/10000 [00:02<01:59, 81.45it/s]Training CobwebTree:   2%|         | 244/10000 [00:02<02:09, 75.29it/s]Training CobwebTree:   3%|         | 253/10000 [00:02<02:05, 77.73it/s]Training CobwebTree:   3%|         | 262/10000 [00:02<02:02, 79.39it/s]Training CobwebTree:   3%|         | 271/10000 [00:03<02:00, 80.49it/s]Training CobwebTree:   3%|         | 280/10000 [00:03<02:12, 73.37it/s]Training CobwebTree:   3%|         | 288/10000 [00:03<02:13, 72.61it/s]Training CobwebTree:   3%|         | 298/10000 [00:03<02:04, 78.17it/s]Training CobwebTree:   3%|         | 306/10000 [00:03<02:05, 77.16it/s]Training CobwebTree:   3%|         | 315/10000 [00:03<02:01, 79.65it/s]Training CobwebTree:   3%|         | 324/10000 [00:03<02:03, 78.04it/s]Training CobwebTree:   3%|         | 332/10000 [00:03<02:08, 75.38it/s]Training CobwebTree:   3%|         | 340/10000 [00:04<02:09, 74.45it/s]Training CobwebTree:   3%|         | 348/10000 [00:04<02:15, 71.36it/s]Training CobwebTree:   4%|         | 356/10000 [00:04<02:24, 66.92it/s]Training CobwebTree:   4%|         | 363/10000 [00:04<02:22, 67.45it/s]Training CobwebTree:   4%|         | 371/10000 [00:04<02:17, 69.79it/s]Training CobwebTree:   4%|         | 380/10000 [00:04<02:12, 72.63it/s]Training CobwebTree:   4%|         | 388/10000 [00:04<02:09, 74.02it/s]Training CobwebTree:   4%|         | 396/10000 [00:04<02:08, 74.98it/s]Training CobwebTree:   4%|         | 404/10000 [00:04<02:08, 74.64it/s]Training CobwebTree:   4%|         | 412/10000 [00:05<02:09, 73.76it/s]Training CobwebTree:   4%|         | 422/10000 [00:05<02:01, 78.96it/s]Training CobwebTree:   4%|         | 430/10000 [00:05<02:02, 78.12it/s]Training CobwebTree:   4%|         | 438/10000 [00:05<02:07, 75.23it/s]Training CobwebTree:   4%|         | 446/10000 [00:05<02:09, 73.90it/s]Training CobwebTree:   5%|         | 456/10000 [00:05<01:59, 80.04it/s]Training CobwebTree:   5%|         | 466/10000 [00:05<01:53, 83.68it/s]Training CobwebTree:   5%|         | 475/10000 [00:05<01:52, 85.03it/s]Training CobwebTree:   5%|         | 484/10000 [00:05<01:52, 84.45it/s]Training CobwebTree:   5%|         | 493/10000 [00:06<01:52, 84.33it/s]Training CobwebTree:   5%|         | 502/10000 [00:06<01:53, 83.49it/s]Training CobwebTree:   5%|         | 511/10000 [00:06<01:58, 79.89it/s]Training CobwebTree:   5%|         | 520/10000 [00:06<01:59, 79.59it/s]Training CobwebTree:   5%|         | 528/10000 [00:06<02:00, 78.69it/s]Training CobwebTree:   5%|         | 536/10000 [00:06<02:02, 77.05it/s]Training CobwebTree:   5%|         | 544/10000 [00:06<02:04, 75.94it/s]Training CobwebTree:   6%|         | 552/10000 [00:06<02:06, 74.51it/s]Training CobwebTree:   6%|         | 560/10000 [00:06<02:04, 75.83it/s]Training CobwebTree:   6%|         | 570/10000 [00:07<01:55, 81.36it/s]Training CobwebTree:   6%|         | 579/10000 [00:07<01:59, 78.56it/s]Training CobwebTree:   6%|         | 587/10000 [00:07<02:00, 78.26it/s]Training CobwebTree:   6%|         | 596/10000 [00:07<01:57, 79.77it/s]Training CobwebTree:   6%|         | 605/10000 [00:07<01:55, 81.21it/s]Training CobwebTree:   6%|         | 614/10000 [00:07<01:57, 80.03it/s]Training CobwebTree:   6%|         | 623/10000 [00:07<01:58, 78.90it/s]Training CobwebTree:   6%|         | 631/10000 [00:07<02:04, 75.39it/s]Training CobwebTree:   6%|         | 639/10000 [00:07<02:09, 72.11it/s]Training CobwebTree:   6%|         | 647/10000 [00:08<02:14, 69.59it/s]Training CobwebTree:   7%|         | 655/10000 [00:08<02:13, 70.19it/s]Training CobwebTree:   7%|         | 663/10000 [00:08<02:12, 70.50it/s]Training CobwebTree:   7%|         | 671/10000 [00:08<02:10, 71.46it/s]Training CobwebTree:   7%|         | 679/10000 [00:08<02:07, 73.04it/s]Training CobwebTree:   7%|         | 687/10000 [00:08<02:09, 72.06it/s]Training CobwebTree:   7%|         | 695/10000 [00:08<02:10, 71.58it/s]Training CobwebTree:   7%|         | 703/10000 [00:08<02:11, 70.60it/s]Training CobwebTree:   7%|         | 711/10000 [00:08<02:10, 71.15it/s]Training CobwebTree:   7%|         | 719/10000 [00:09<02:13, 69.62it/s]Training CobwebTree:   7%|         | 726/10000 [00:09<02:15, 68.27it/s]Training CobwebTree:   7%|         | 734/10000 [00:09<02:11, 70.50it/s]Training CobwebTree:   7%|         | 742/10000 [00:09<02:07, 72.48it/s]Training CobwebTree:   8%|         | 752/10000 [00:09<01:58, 78.34it/s]Training CobwebTree:   8%|         | 760/10000 [00:09<02:01, 76.11it/s]Training CobwebTree:   8%|         | 769/10000 [00:09<01:58, 78.06it/s]Training CobwebTree:   8%|         | 777/10000 [00:09<01:58, 77.96it/s]Training CobwebTree:   8%|         | 785/10000 [00:09<02:03, 74.56it/s]Training CobwebTree:   8%|         | 793/10000 [00:10<02:11, 70.28it/s]Training CobwebTree:   8%|         | 801/10000 [00:10<02:09, 71.15it/s]Training CobwebTree:   8%|         | 809/10000 [00:10<02:08, 71.56it/s]Training CobwebTree:   8%|         | 817/10000 [00:10<02:09, 71.08it/s]Training CobwebTree:   8%|         | 825/10000 [00:10<02:15, 67.93it/s]Training CobwebTree:   8%|         | 832/10000 [00:10<02:16, 67.12it/s]Training CobwebTree:   8%|         | 840/10000 [00:10<02:13, 68.38it/s]Training CobwebTree:   8%|         | 847/10000 [00:10<02:17, 66.62it/s]Training CobwebTree:   9%|         | 854/10000 [00:10<02:19, 65.60it/s]Training CobwebTree:   9%|         | 862/10000 [00:11<02:14, 67.92it/s]Training CobwebTree:   9%|         | 870/10000 [00:11<02:15, 67.47it/s]Training CobwebTree:   9%|         | 878/10000 [00:11<02:12, 68.86it/s]Training CobwebTree:   9%|         | 885/10000 [00:11<02:16, 66.90it/s]Training CobwebTree:   9%|         | 892/10000 [00:11<02:17, 66.17it/s]Training CobwebTree:   9%|         | 899/10000 [00:11<02:21, 64.53it/s]Training CobwebTree:   9%|         | 907/10000 [00:11<02:16, 66.85it/s]Training CobwebTree:   9%|         | 914/10000 [00:11<02:16, 66.47it/s]Training CobwebTree:   9%|         | 921/10000 [00:11<02:17, 66.27it/s]Training CobwebTree:   9%|         | 928/10000 [00:12<02:22, 63.52it/s]Training CobwebTree:   9%|         | 935/10000 [00:12<02:24, 62.77it/s]Training CobwebTree:   9%|         | 942/10000 [00:12<02:28, 61.04it/s]Training CobwebTree:   9%|         | 949/10000 [00:12<02:23, 63.00it/s]Training CobwebTree:  10%|         | 956/10000 [00:12<02:25, 62.26it/s]Training CobwebTree:  10%|         | 963/10000 [00:12<02:23, 62.88it/s]Training CobwebTree:  10%|         | 970/10000 [00:12<02:22, 63.16it/s]Training CobwebTree:  10%|         | 977/10000 [00:12<02:22, 63.45it/s]Training CobwebTree:  10%|         | 985/10000 [00:12<02:17, 65.49it/s]Training CobwebTree:  10%|         | 992/10000 [00:13<02:21, 63.88it/s]Training CobwebTree:  10%|         | 999/10000 [00:13<02:25, 61.75it/s]Training CobwebTree:  10%|         | 1006/10000 [00:13<02:22, 63.03it/s]Training CobwebTree:  10%|         | 1013/10000 [00:13<02:22, 63.04it/s]Training CobwebTree:  10%|         | 1020/10000 [00:13<02:24, 61.94it/s]Training CobwebTree:  10%|         | 1027/10000 [00:13<02:25, 61.63it/s]Training CobwebTree:  10%|         | 1034/10000 [00:13<02:21, 63.52it/s]Training CobwebTree:  10%|         | 1041/10000 [00:13<02:19, 64.32it/s]Training CobwebTree:  10%|         | 1048/10000 [00:13<02:18, 64.45it/s]Training CobwebTree:  11%|         | 1055/10000 [00:14<02:21, 63.23it/s]Training CobwebTree:  11%|         | 1063/10000 [00:14<02:18, 64.63it/s]Training CobwebTree:  11%|         | 1070/10000 [00:14<02:23, 62.04it/s]Training CobwebTree:  11%|         | 1077/10000 [00:14<02:24, 61.92it/s]Training CobwebTree:  11%|         | 1084/10000 [00:14<02:24, 61.64it/s]Training CobwebTree:  11%|         | 1091/10000 [00:14<02:26, 60.68it/s]Training CobwebTree:  11%|         | 1099/10000 [00:14<02:19, 63.58it/s]Training CobwebTree:  11%|         | 1106/10000 [00:14<02:22, 62.62it/s]Training CobwebTree:  11%|         | 1114/10000 [00:15<02:15, 65.81it/s]Training CobwebTree:  11%|         | 1121/10000 [00:15<02:21, 62.76it/s]Training CobwebTree:  11%|        | 1128/10000 [00:15<02:21, 62.71it/s]Training CobwebTree:  11%|        | 1135/10000 [00:15<02:26, 60.69it/s]Training CobwebTree:  11%|        | 1142/10000 [00:15<02:23, 61.92it/s]Training CobwebTree:  11%|        | 1149/10000 [00:15<02:22, 62.12it/s]Training CobwebTree:  12%|        | 1156/10000 [00:15<02:18, 63.80it/s]Training CobwebTree:  12%|        | 1163/10000 [00:15<02:16, 64.65it/s]Training CobwebTree:  12%|        | 1170/10000 [00:15<02:13, 66.02it/s]Training CobwebTree:  12%|        | 1177/10000 [00:16<02:14, 65.69it/s]Training CobwebTree:  12%|        | 1184/10000 [00:16<02:16, 64.69it/s]Training CobwebTree:  12%|        | 1191/10000 [00:16<02:13, 65.88it/s]Training CobwebTree:  12%|        | 1198/10000 [00:16<02:11, 66.79it/s]Training CobwebTree:  12%|        | 1206/10000 [00:16<02:07, 69.13it/s]Training CobwebTree:  12%|        | 1213/10000 [00:16<02:18, 63.52it/s]Training CobwebTree:  12%|        | 1220/10000 [00:16<02:15, 64.95it/s]Training CobwebTree:  12%|        | 1228/10000 [00:16<02:12, 66.09it/s]Training CobwebTree:  12%|        | 1235/10000 [00:16<02:12, 66.23it/s]Training CobwebTree:  12%|        | 1242/10000 [00:17<02:11, 66.39it/s]Training CobwebTree:  12%|        | 1249/10000 [00:17<02:16, 64.28it/s]Training CobwebTree:  13%|        | 1256/10000 [00:17<02:17, 63.74it/s]Training CobwebTree:  13%|        | 1263/10000 [00:17<02:16, 63.94it/s]Training CobwebTree:  13%|        | 1270/10000 [00:17<02:16, 63.85it/s]Training CobwebTree:  13%|        | 1277/10000 [00:17<02:19, 62.61it/s]Training CobwebTree:  13%|        | 1284/10000 [00:17<02:20, 61.88it/s]Training CobwebTree:  13%|        | 1291/10000 [00:17<02:25, 60.04it/s]Training CobwebTree:  13%|        | 1298/10000 [00:17<02:24, 60.18it/s]Training CobwebTree:  13%|        | 1305/10000 [00:18<02:19, 62.19it/s]Training CobwebTree:  13%|        | 1313/10000 [00:18<02:09, 66.84it/s]Training CobwebTree:  13%|        | 1320/10000 [00:18<02:15, 64.28it/s]Training CobwebTree:  13%|        | 1327/10000 [00:18<02:16, 63.63it/s]Training CobwebTree:  13%|        | 1334/10000 [00:18<02:16, 63.58it/s]Training CobwebTree:  13%|        | 1341/10000 [00:18<02:15, 63.80it/s]Training CobwebTree:  13%|        | 1348/10000 [00:18<02:22, 60.89it/s]Training CobwebTree:  14%|        | 1355/10000 [00:18<02:23, 60.18it/s]Training CobwebTree:  14%|        | 1362/10000 [00:18<02:24, 59.58it/s]Training CobwebTree:  14%|        | 1369/10000 [00:19<02:24, 59.63it/s]Training CobwebTree:  14%|        | 1375/10000 [00:19<02:25, 59.36it/s]Training CobwebTree:  14%|        | 1381/10000 [00:19<02:26, 58.80it/s]Training CobwebTree:  14%|        | 1388/10000 [00:19<02:24, 59.46it/s]Training CobwebTree:  14%|        | 1395/10000 [00:19<02:21, 60.76it/s]Training CobwebTree:  14%|        | 1402/10000 [00:19<02:17, 62.68it/s]Training CobwebTree:  14%|        | 1409/10000 [00:19<02:23, 59.96it/s]Training CobwebTree:  14%|        | 1416/10000 [00:19<02:18, 62.02it/s]Training CobwebTree:  14%|        | 1423/10000 [00:19<02:24, 59.54it/s]Training CobwebTree:  14%|        | 1430/10000 [00:20<02:25, 59.01it/s]Training CobwebTree:  14%|        | 1437/10000 [00:20<02:23, 59.83it/s]Training CobwebTree:  14%|        | 1444/10000 [00:20<02:22, 60.16it/s]Training CobwebTree:  15%|        | 1451/10000 [00:20<02:24, 59.08it/s]Training CobwebTree:  15%|        | 1458/10000 [00:20<02:24, 59.20it/s]Training CobwebTree:  15%|        | 1464/10000 [00:20<02:24, 59.15it/s]Training CobwebTree:  15%|        | 1471/10000 [00:20<02:26, 58.34it/s]Training CobwebTree:  15%|        | 1477/10000 [00:20<02:30, 56.69it/s]Training CobwebTree:  15%|        | 1483/10000 [00:20<02:28, 57.51it/s]Training CobwebTree:  15%|        | 1490/10000 [00:21<02:24, 59.10it/s]Training CobwebTree:  15%|        | 1497/10000 [00:21<02:20, 60.70it/s]Training CobwebTree:  15%|        | 1504/10000 [00:21<02:22, 59.59it/s]Training CobwebTree:  15%|        | 1511/10000 [00:21<02:22, 59.67it/s]Training CobwebTree:  15%|        | 1517/10000 [00:21<02:24, 58.52it/s]Training CobwebTree:  15%|        | 1526/10000 [00:21<02:10, 64.69it/s]Training CobwebTree:  15%|        | 1533/10000 [00:21<02:17, 61.47it/s]Training CobwebTree:  15%|        | 1540/10000 [00:21<02:19, 60.57it/s]Training CobwebTree:  15%|        | 1547/10000 [00:22<02:19, 60.54it/s]Training CobwebTree:  16%|        | 1554/10000 [00:22<02:17, 61.26it/s]Training CobwebTree:  16%|        | 1561/10000 [00:22<02:23, 58.72it/s]Training CobwebTree:  16%|        | 1568/10000 [00:22<02:22, 59.00it/s]Training CobwebTree:  16%|        | 1574/10000 [00:22<02:24, 58.49it/s]Training CobwebTree:  16%|        | 1581/10000 [00:22<02:21, 59.66it/s]Training CobwebTree:  16%|        | 1587/10000 [00:22<02:23, 58.68it/s]Training CobwebTree:  16%|        | 1594/10000 [00:22<02:20, 59.79it/s]Training CobwebTree:  16%|        | 1601/10000 [00:22<02:18, 60.57it/s]Training CobwebTree:  16%|        | 1609/10000 [00:23<02:12, 63.53it/s]Training CobwebTree:  16%|        | 1616/10000 [00:23<02:13, 62.75it/s]Training CobwebTree:  16%|        | 1624/10000 [00:23<02:11, 63.89it/s]Training CobwebTree:  16%|        | 1631/10000 [00:23<02:17, 60.82it/s]Training CobwebTree:  16%|        | 1638/10000 [00:23<02:19, 59.92it/s]Training CobwebTree:  16%|        | 1645/10000 [00:23<02:20, 59.38it/s]Training CobwebTree:  17%|        | 1651/10000 [00:23<02:20, 59.25it/s]Training CobwebTree:  17%|        | 1657/10000 [00:23<02:22, 58.75it/s]Training CobwebTree:  17%|        | 1663/10000 [00:23<02:25, 57.39it/s]Training CobwebTree:  17%|        | 1670/10000 [00:24<02:23, 58.07it/s]Training CobwebTree:  17%|        | 1676/10000 [00:24<02:23, 57.97it/s]Training CobwebTree:  17%|        | 1683/10000 [00:24<02:20, 59.26it/s]Training CobwebTree:  17%|        | 1690/10000 [00:24<02:21, 58.81it/s]Training CobwebTree:  17%|        | 1697/10000 [00:24<02:16, 60.81it/s]Training CobwebTree:  17%|        | 1704/10000 [00:24<02:16, 60.63it/s]Training CobwebTree:  17%|        | 1711/10000 [00:24<02:11, 62.93it/s]Training CobwebTree:  17%|        | 1718/10000 [00:24<02:12, 62.44it/s]Training CobwebTree:  17%|        | 1725/10000 [00:24<02:12, 62.64it/s]Training CobwebTree:  17%|        | 1732/10000 [00:25<02:10, 63.41it/s]Training CobwebTree:  17%|        | 1739/10000 [00:25<02:11, 63.03it/s]Training CobwebTree:  17%|        | 1746/10000 [00:25<02:10, 63.24it/s]Training CobwebTree:  18%|        | 1753/10000 [00:25<02:13, 61.96it/s]Training CobwebTree:  18%|        | 1760/10000 [00:25<02:12, 62.36it/s]Training CobwebTree:  18%|        | 1767/10000 [00:25<02:11, 62.75it/s]Training CobwebTree:  18%|        | 1774/10000 [00:25<02:13, 61.42it/s]Training CobwebTree:  18%|        | 1781/10000 [00:25<02:11, 62.66it/s]Training CobwebTree:  18%|        | 1788/10000 [00:26<02:20, 58.38it/s]Training CobwebTree:  18%|        | 1795/10000 [00:26<02:17, 59.61it/s]Training CobwebTree:  18%|        | 1802/10000 [00:26<02:21, 57.83it/s]Training CobwebTree:  18%|        | 1808/10000 [00:26<02:20, 58.36it/s]Training CobwebTree:  18%|        | 1815/10000 [00:26<02:15, 60.28it/s]Training CobwebTree:  18%|        | 1822/10000 [00:26<02:20, 58.39it/s]Training CobwebTree:  18%|        | 1828/10000 [00:26<02:22, 57.26it/s]Training CobwebTree:  18%|        | 1835/10000 [00:26<02:17, 59.40it/s]Training CobwebTree:  18%|        | 1842/10000 [00:26<02:15, 60.08it/s]Training CobwebTree:  18%|        | 1849/10000 [00:27<02:16, 59.79it/s]Training CobwebTree:  19%|        | 1855/10000 [00:27<02:18, 58.64it/s]Training CobwebTree:  19%|        | 1861/10000 [00:27<02:18, 58.71it/s]Training CobwebTree:  19%|        | 1868/10000 [00:27<02:13, 60.81it/s]Training CobwebTree:  19%|        | 1875/10000 [00:27<02:17, 59.24it/s]Training CobwebTree:  19%|        | 1881/10000 [00:27<02:17, 58.84it/s]Training CobwebTree:  19%|        | 1889/10000 [00:27<02:10, 62.05it/s]Training CobwebTree:  19%|        | 1896/10000 [00:27<02:10, 62.29it/s]Training CobwebTree:  19%|        | 1903/10000 [00:27<02:12, 60.97it/s]Training CobwebTree:  19%|        | 1910/10000 [00:28<02:12, 61.11it/s]Training CobwebTree:  19%|        | 1917/10000 [00:28<02:16, 59.31it/s]Training CobwebTree:  19%|        | 1923/10000 [00:28<02:17, 58.86it/s]Training CobwebTree:  19%|        | 1929/10000 [00:28<02:20, 57.38it/s]Training CobwebTree:  19%|        | 1936/10000 [00:28<02:14, 60.09it/s]Training CobwebTree:  19%|        | 1943/10000 [00:28<02:16, 58.92it/s]Training CobwebTree:  20%|        | 1950/10000 [00:28<02:12, 60.75it/s]Training CobwebTree:  20%|        | 1957/10000 [00:28<02:12, 60.77it/s]Training CobwebTree:  20%|        | 1964/10000 [00:28<02:19, 57.50it/s]Training CobwebTree:  20%|        | 1970/10000 [00:29<02:19, 57.63it/s]Training CobwebTree:  20%|        | 1976/10000 [00:29<02:22, 56.26it/s]Training CobwebTree:  20%|        | 1983/10000 [00:29<02:14, 59.47it/s]Training CobwebTree:  20%|        | 1989/10000 [00:29<02:18, 57.90it/s]Training CobwebTree:  20%|        | 1996/10000 [00:29<02:16, 58.44it/s]Training CobwebTree:  20%|        | 2002/10000 [00:29<02:19, 57.24it/s]Training CobwebTree:  20%|        | 2008/10000 [00:29<02:21, 56.63it/s]Training CobwebTree:  20%|        | 2014/10000 [00:29<02:23, 55.72it/s]Training CobwebTree:  20%|        | 2021/10000 [00:29<02:16, 58.58it/s]Training CobwebTree:  20%|        | 2027/10000 [00:30<02:16, 58.45it/s]Training CobwebTree:  20%|        | 2033/10000 [00:30<02:19, 57.10it/s]Training CobwebTree:  20%|        | 2039/10000 [00:30<02:23, 55.43it/s]Training CobwebTree:  20%|        | 2046/10000 [00:30<02:19, 56.97it/s]Training CobwebTree:  21%|        | 2053/10000 [00:30<02:14, 59.02it/s]Training CobwebTree:  21%|        | 2060/10000 [00:30<02:14, 59.20it/s]Training CobwebTree:  21%|        | 2066/10000 [00:30<02:19, 57.03it/s]Training CobwebTree:  21%|        | 2072/10000 [00:30<02:19, 56.92it/s]Training CobwebTree:  21%|        | 2078/10000 [00:30<02:21, 56.09it/s]Training CobwebTree:  21%|        | 2084/10000 [00:31<02:22, 55.73it/s]Training CobwebTree:  21%|        | 2090/10000 [00:31<02:22, 55.57it/s]Training CobwebTree:  21%|        | 2096/10000 [00:31<02:20, 56.31it/s]Training CobwebTree:  21%|        | 2102/10000 [00:31<02:19, 56.49it/s]Training CobwebTree:  21%|        | 2108/10000 [00:31<02:27, 53.45it/s]Training CobwebTree:  21%|        | 2115/10000 [00:31<02:19, 56.64it/s]Training CobwebTree:  21%|        | 2121/10000 [00:31<02:19, 56.68it/s]Training CobwebTree:  21%|       | 2127/10000 [00:31<02:18, 56.80it/s]Training CobwebTree:  21%|       | 2133/10000 [00:31<02:16, 57.43it/s]Training CobwebTree:  21%|       | 2139/10000 [00:32<02:21, 55.65it/s]Training CobwebTree:  21%|       | 2145/10000 [00:32<02:18, 56.52it/s]Training CobwebTree:  22%|       | 2151/10000 [00:32<02:23, 54.82it/s]Training CobwebTree:  22%|       | 2157/10000 [00:32<02:28, 52.89it/s]Training CobwebTree:  22%|       | 2164/10000 [00:32<02:19, 56.03it/s]Training CobwebTree:  22%|       | 2171/10000 [00:32<02:14, 58.16it/s]Training CobwebTree:  22%|       | 2177/10000 [00:32<02:13, 58.62it/s]Training CobwebTree:  22%|       | 2184/10000 [00:32<02:13, 58.65it/s]Training CobwebTree:  22%|       | 2191/10000 [00:32<02:12, 58.81it/s]Training CobwebTree:  22%|       | 2197/10000 [00:33<02:15, 57.42it/s]Training CobwebTree:  22%|       | 2203/10000 [00:33<02:16, 57.20it/s]Training CobwebTree:  22%|       | 2210/10000 [00:33<02:13, 58.45it/s]Training CobwebTree:  22%|       | 2216/10000 [00:33<02:14, 57.69it/s]Training CobwebTree:  22%|       | 2222/10000 [00:33<02:13, 58.15it/s]Training CobwebTree:  22%|       | 2228/10000 [00:33<02:13, 58.03it/s]Training CobwebTree:  22%|       | 2234/10000 [00:33<02:16, 57.01it/s]Training CobwebTree:  22%|       | 2240/10000 [00:33<02:14, 57.65it/s]Training CobwebTree:  22%|       | 2246/10000 [00:33<02:15, 57.35it/s]Training CobwebTree:  23%|       | 2253/10000 [00:34<02:12, 58.35it/s]Training CobwebTree:  23%|       | 2260/10000 [00:34<02:10, 59.34it/s]Training CobwebTree:  23%|       | 2266/10000 [00:34<02:10, 59.19it/s]Training CobwebTree:  23%|       | 2272/10000 [00:34<02:10, 59.19it/s]Training CobwebTree:  23%|       | 2279/10000 [00:34<02:10, 59.19it/s]Training CobwebTree:  23%|       | 2285/10000 [00:34<02:14, 57.50it/s]Training CobwebTree:  23%|       | 2292/10000 [00:34<02:09, 59.70it/s]Training CobwebTree:  23%|       | 2298/10000 [00:34<02:10, 59.02it/s]Training CobwebTree:  23%|       | 2304/10000 [00:34<02:11, 58.56it/s]Training CobwebTree:  23%|       | 2310/10000 [00:35<02:18, 55.64it/s]Training CobwebTree:  23%|       | 2317/10000 [00:35<02:14, 57.29it/s]Training CobwebTree:  23%|       | 2323/10000 [00:35<02:20, 54.49it/s]Training CobwebTree:  23%|       | 2329/10000 [00:35<02:18, 55.27it/s]Training CobwebTree:  23%|       | 2335/10000 [00:35<02:15, 56.42it/s]Training CobwebTree:  23%|       | 2342/10000 [00:35<02:13, 57.28it/s]Training CobwebTree:  23%|       | 2348/10000 [00:35<02:13, 57.41it/s]Training CobwebTree:  24%|       | 2354/10000 [00:35<02:14, 56.71it/s]Training CobwebTree:  24%|       | 2360/10000 [00:35<02:19, 54.82it/s]Training CobwebTree:  24%|       | 2366/10000 [00:36<02:19, 54.87it/s]Training CobwebTree:  24%|       | 2372/10000 [00:36<02:17, 55.55it/s]Training CobwebTree:  24%|       | 2379/10000 [00:36<02:11, 58.17it/s]Training CobwebTree:  24%|       | 2385/10000 [00:36<02:11, 57.79it/s]Training CobwebTree:  24%|       | 2391/10000 [00:36<02:15, 56.29it/s]Training CobwebTree:  24%|       | 2397/10000 [00:36<02:14, 56.64it/s]Training CobwebTree:  24%|       | 2404/10000 [00:36<02:10, 58.11it/s]Training CobwebTree:  24%|       | 2410/10000 [00:36<02:22, 53.33it/s]Training CobwebTree:  24%|       | 2417/10000 [00:36<02:15, 55.93it/s]Training CobwebTree:  24%|       | 2423/10000 [00:37<02:14, 56.26it/s]Training CobwebTree:  24%|       | 2430/10000 [00:37<02:11, 57.67it/s]Training CobwebTree:  24%|       | 2436/10000 [00:37<02:19, 54.11it/s]Training CobwebTree:  24%|       | 2443/10000 [00:37<02:12, 56.85it/s]Training CobwebTree:  24%|       | 2450/10000 [00:37<02:08, 58.58it/s]Training CobwebTree:  25%|       | 2457/10000 [00:37<02:04, 60.59it/s]Training CobwebTree:  25%|       | 2464/10000 [00:37<02:04, 60.45it/s]Training CobwebTree:  25%|       | 2471/10000 [00:37<02:05, 59.87it/s]Training CobwebTree:  25%|       | 2478/10000 [00:37<02:07, 58.90it/s]Training CobwebTree:  25%|       | 2484/10000 [00:38<02:11, 57.30it/s]Training CobwebTree:  25%|       | 2491/10000 [00:38<02:09, 57.83it/s]Training CobwebTree:  25%|       | 2497/10000 [00:38<02:10, 57.50it/s]Training CobwebTree:  25%|       | 2503/10000 [00:38<02:15, 55.49it/s]Training CobwebTree:  25%|       | 2509/10000 [00:38<02:14, 55.79it/s]Training CobwebTree:  25%|       | 2515/10000 [00:38<02:15, 55.17it/s]Training CobwebTree:  25%|       | 2521/10000 [00:38<02:21, 52.93it/s]Training CobwebTree:  25%|       | 2527/10000 [00:38<02:19, 53.52it/s]Training CobwebTree:  25%|       | 2533/10000 [00:38<02:19, 53.41it/s]Training CobwebTree:  25%|       | 2539/10000 [00:39<02:21, 52.57it/s]Training CobwebTree:  25%|       | 2545/10000 [00:39<02:23, 51.78it/s]Training CobwebTree:  26%|       | 2551/10000 [00:39<02:20, 52.83it/s]Training CobwebTree:  26%|       | 2557/10000 [00:39<02:19, 53.30it/s]Training CobwebTree:  26%|       | 2563/10000 [00:39<02:16, 54.33it/s]Training CobwebTree:  26%|       | 2569/10000 [00:39<02:16, 54.34it/s]Training CobwebTree:  26%|       | 2575/10000 [00:39<02:20, 52.96it/s]Training CobwebTree:  26%|       | 2581/10000 [00:39<02:20, 52.94it/s]Training CobwebTree:  26%|       | 2587/10000 [00:39<02:17, 53.89it/s]Training CobwebTree:  26%|       | 2593/10000 [00:40<02:24, 51.18it/s]Training CobwebTree:  26%|       | 2599/10000 [00:40<02:24, 51.19it/s]Training CobwebTree:  26%|       | 2605/10000 [00:40<02:22, 51.73it/s]Training CobwebTree:  26%|       | 2611/10000 [00:40<02:21, 52.34it/s]Training CobwebTree:  26%|       | 2617/10000 [00:40<02:17, 53.84it/s]Training CobwebTree:  26%|       | 2623/10000 [00:40<02:19, 52.77it/s]Training CobwebTree:  26%|       | 2629/10000 [00:40<02:15, 54.34it/s]Training CobwebTree:  26%|       | 2635/10000 [00:40<02:13, 54.99it/s]Training CobwebTree:  26%|       | 2641/10000 [00:41<02:15, 54.12it/s]Training CobwebTree:  26%|       | 2647/10000 [00:41<02:22, 51.49it/s]Training CobwebTree:  27%|       | 2653/10000 [00:41<02:28, 49.47it/s]Training CobwebTree:  27%|       | 2659/10000 [00:41<02:24, 50.78it/s]Training CobwebTree:  27%|       | 2665/10000 [00:41<02:22, 51.43it/s]Training CobwebTree:  27%|       | 2671/10000 [00:41<02:24, 50.63it/s]Training CobwebTree:  27%|       | 2677/10000 [00:41<02:19, 52.50it/s]Training CobwebTree:  27%|       | 2683/10000 [00:41<02:17, 53.15it/s]Training CobwebTree:  27%|       | 2689/10000 [00:41<02:15, 53.85it/s]Training CobwebTree:  27%|       | 2695/10000 [00:42<02:14, 54.50it/s]Training CobwebTree:  27%|       | 2701/10000 [00:42<02:14, 54.16it/s]Training CobwebTree:  27%|       | 2707/10000 [00:42<02:18, 52.79it/s]Training CobwebTree:  27%|       | 2714/10000 [00:42<02:14, 54.11it/s]Training CobwebTree:  27%|       | 2720/10000 [00:42<02:13, 54.68it/s]Training CobwebTree:  27%|       | 2726/10000 [00:42<02:15, 53.76it/s]Training CobwebTree:  27%|       | 2732/10000 [00:42<02:12, 54.82it/s]Training CobwebTree:  27%|       | 2739/10000 [00:42<02:09, 56.16it/s]Training CobwebTree:  27%|       | 2745/10000 [00:42<02:13, 54.35it/s]Training CobwebTree:  28%|       | 2752/10000 [00:43<02:07, 56.75it/s]Training CobwebTree:  28%|       | 2758/10000 [00:43<02:08, 56.36it/s]Training CobwebTree:  28%|       | 2765/10000 [00:43<02:05, 57.51it/s]Training CobwebTree:  28%|       | 2772/10000 [00:43<02:04, 58.07it/s]Training CobwebTree:  28%|       | 2778/10000 [00:43<02:11, 55.05it/s]Training CobwebTree:  28%|       | 2784/10000 [00:43<02:12, 54.26it/s]Training CobwebTree:  28%|       | 2790/10000 [00:43<02:15, 53.21it/s]Training CobwebTree:  28%|       | 2796/10000 [00:43<02:11, 54.63it/s]Training CobwebTree:  28%|       | 2802/10000 [00:43<02:10, 55.25it/s]Training CobwebTree:  28%|       | 2809/10000 [00:44<02:03, 58.21it/s]Training CobwebTree:  28%|       | 2815/10000 [00:44<02:06, 56.85it/s]Training CobwebTree:  28%|       | 2821/10000 [00:44<02:06, 56.97it/s]Training CobwebTree:  28%|       | 2827/10000 [00:44<02:10, 55.10it/s]Training CobwebTree:  28%|       | 2833/10000 [00:44<02:08, 55.92it/s]Training CobwebTree:  28%|       | 2839/10000 [00:44<02:07, 56.07it/s]Training CobwebTree:  28%|       | 2846/10000 [00:44<02:00, 59.60it/s]Training CobwebTree:  29%|       | 2852/10000 [00:44<02:03, 57.78it/s]Training CobwebTree:  29%|       | 2858/10000 [00:44<02:06, 56.24it/s]Training CobwebTree:  29%|       | 2864/10000 [00:45<02:05, 56.95it/s]Training CobwebTree:  29%|       | 2870/10000 [00:45<02:04, 57.36it/s]Training CobwebTree:  29%|       | 2876/10000 [00:45<02:04, 57.37it/s]Training CobwebTree:  29%|       | 2884/10000 [00:45<01:56, 61.09it/s]Training CobwebTree:  29%|       | 2891/10000 [00:45<01:57, 60.38it/s]Training CobwebTree:  29%|       | 2898/10000 [00:45<01:58, 59.93it/s]Training CobwebTree:  29%|       | 2904/10000 [00:45<02:03, 57.59it/s]Training CobwebTree:  29%|       | 2911/10000 [00:45<02:01, 58.29it/s]Training CobwebTree:  29%|       | 2918/10000 [00:45<01:56, 60.77it/s]Training CobwebTree:  29%|       | 2925/10000 [00:46<01:56, 60.47it/s]Training CobwebTree:  29%|       | 2932/10000 [00:46<01:59, 59.28it/s]Training CobwebTree:  29%|       | 2938/10000 [00:46<02:02, 57.59it/s]Training CobwebTree:  29%|       | 2944/10000 [00:46<02:01, 58.02it/s]Training CobwebTree:  30%|       | 2951/10000 [00:46<02:03, 57.12it/s]Training CobwebTree:  30%|       | 2957/10000 [00:46<02:06, 55.50it/s]Training CobwebTree:  30%|       | 2963/10000 [00:46<02:09, 54.41it/s]Training CobwebTree:  30%|       | 2969/10000 [00:46<02:09, 54.39it/s]Training CobwebTree:  30%|       | 2975/10000 [00:47<02:13, 52.65it/s]Training CobwebTree:  30%|       | 2981/10000 [00:47<02:09, 54.39it/s]Training CobwebTree:  30%|       | 2987/10000 [00:47<02:08, 54.41it/s]Training CobwebTree:  30%|       | 2993/10000 [00:47<02:07, 55.13it/s]Training CobwebTree:  30%|       | 3000/10000 [00:47<02:02, 56.94it/s]Training CobwebTree:  30%|       | 3006/10000 [00:47<02:01, 57.69it/s]Training CobwebTree:  30%|       | 3012/10000 [00:47<02:05, 55.72it/s]Training CobwebTree:  30%|       | 3019/10000 [00:47<01:59, 58.37it/s]Training CobwebTree:  30%|       | 3026/10000 [00:47<01:54, 60.84it/s]Training CobwebTree:  30%|       | 3033/10000 [00:47<01:53, 61.60it/s]Training CobwebTree:  30%|       | 3040/10000 [00:48<02:06, 55.17it/s]Training CobwebTree:  30%|       | 3046/10000 [00:48<02:06, 55.13it/s]Training CobwebTree:  31%|       | 3052/10000 [00:48<02:08, 54.18it/s]Training CobwebTree:  31%|       | 3058/10000 [00:48<02:14, 51.55it/s]Training CobwebTree:  31%|       | 3064/10000 [00:48<02:09, 53.60it/s]Training CobwebTree:  31%|       | 3070/10000 [00:48<02:08, 53.97it/s]Training CobwebTree:  31%|       | 3076/10000 [00:48<02:15, 51.01it/s]Training CobwebTree:  31%|       | 3082/10000 [00:48<02:17, 50.44it/s]Training CobwebTree:  31%|       | 3088/10000 [00:49<02:17, 50.33it/s]Training CobwebTree:  31%|       | 3094/10000 [00:49<02:18, 49.73it/s]Training CobwebTree:  31%|       | 3099/10000 [00:49<02:20, 49.28it/s]Training CobwebTree:  31%|       | 3104/10000 [00:49<02:22, 48.49it/s]Training CobwebTree:  31%|       | 3110/10000 [00:49<02:21, 48.82it/s]Training CobwebTree:  31%|       | 3116/10000 [00:49<02:14, 51.19it/s]Training CobwebTree:  31%|       | 3122/10000 [00:49<02:13, 51.66it/s]Training CobwebTree:  31%|      | 3128/10000 [00:49<02:08, 53.48it/s]Training CobwebTree:  31%|      | 3134/10000 [00:49<02:05, 54.64it/s]Training CobwebTree:  31%|      | 3140/10000 [00:50<02:06, 54.22it/s]Training CobwebTree:  31%|      | 3146/10000 [00:50<02:08, 53.33it/s]Training CobwebTree:  32%|      | 3152/10000 [00:50<02:09, 53.02it/s]Training CobwebTree:  32%|      | 3158/10000 [00:50<02:06, 53.89it/s]Training CobwebTree:  32%|      | 3164/10000 [00:50<02:09, 52.98it/s]Training CobwebTree:  32%|      | 3170/10000 [00:50<02:10, 52.32it/s]Training CobwebTree:  32%|      | 3176/10000 [00:50<02:12, 51.56it/s]Training CobwebTree:  32%|      | 3182/10000 [00:50<02:13, 50.94it/s]Training CobwebTree:  32%|      | 3188/10000 [00:51<02:11, 51.97it/s]Training CobwebTree:  32%|      | 3194/10000 [00:51<02:12, 51.40it/s]Training CobwebTree:  32%|      | 3200/10000 [00:51<02:10, 52.15it/s]Training CobwebTree:  32%|      | 3207/10000 [00:51<02:08, 52.88it/s]Training CobwebTree:  32%|      | 3213/10000 [00:51<02:06, 53.58it/s]Training CobwebTree:  32%|      | 3219/10000 [00:51<02:08, 52.90it/s]Training CobwebTree:  32%|      | 3225/10000 [00:51<02:03, 54.80it/s]Training CobwebTree:  32%|      | 3231/10000 [00:51<02:08, 52.54it/s]Training CobwebTree:  32%|      | 3237/10000 [00:51<02:15, 49.79it/s]Training CobwebTree:  32%|      | 3243/10000 [00:52<02:15, 49.71it/s]Training CobwebTree:  32%|      | 3249/10000 [00:52<02:14, 50.28it/s]Training CobwebTree:  33%|      | 3256/10000 [00:52<02:03, 54.56it/s]Training CobwebTree:  33%|      | 3263/10000 [00:52<01:58, 56.64it/s]Training CobwebTree:  33%|      | 3269/10000 [00:52<02:01, 55.62it/s]Training CobwebTree:  33%|      | 3275/10000 [00:52<02:07, 52.78it/s]Training CobwebTree:  33%|      | 3282/10000 [00:52<02:03, 54.53it/s]Training CobwebTree:  33%|      | 3288/10000 [00:52<02:08, 52.28it/s]Training CobwebTree:  33%|      | 3294/10000 [00:53<02:08, 52.23it/s]Training CobwebTree:  33%|      | 3300/10000 [00:53<02:10, 51.38it/s]Training CobwebTree:  33%|      | 3306/10000 [00:53<02:06, 52.89it/s]Training CobwebTree:  33%|      | 3312/10000 [00:53<02:08, 51.88it/s]Training CobwebTree:  33%|      | 3319/10000 [00:53<02:01, 54.89it/s]Training CobwebTree:  33%|      | 3326/10000 [00:53<02:00, 55.37it/s]Training CobwebTree:  33%|      | 3332/10000 [00:53<02:02, 54.42it/s]Training CobwebTree:  33%|      | 3338/10000 [00:53<02:01, 54.69it/s]Training CobwebTree:  33%|      | 3344/10000 [00:53<02:07, 52.27it/s]Training CobwebTree:  34%|      | 3350/10000 [00:54<02:05, 52.94it/s]Training CobwebTree:  34%|      | 3356/10000 [00:54<02:02, 54.14it/s]Training CobwebTree:  34%|      | 3362/10000 [00:54<02:01, 54.44it/s]Training CobwebTree:  34%|      | 3368/10000 [00:54<02:05, 52.70it/s]Training CobwebTree:  34%|      | 3374/10000 [00:54<02:02, 53.89it/s]Training CobwebTree:  34%|      | 3380/10000 [00:54<02:00, 54.71it/s]Training CobwebTree:  34%|      | 3386/10000 [00:54<02:05, 52.73it/s]Training CobwebTree:  34%|      | 3392/10000 [00:54<02:09, 51.20it/s]Training CobwebTree:  34%|      | 3398/10000 [00:54<02:08, 51.44it/s]Training CobwebTree:  34%|      | 3405/10000 [00:55<02:03, 53.19it/s]Training CobwebTree:  34%|      | 3411/10000 [00:55<02:06, 51.88it/s]Training CobwebTree:  34%|      | 3417/10000 [00:55<02:07, 51.64it/s]Training CobwebTree:  34%|      | 3423/10000 [00:55<02:08, 51.00it/s]Training CobwebTree:  34%|      | 3429/10000 [00:55<02:09, 50.80it/s]Training CobwebTree:  34%|      | 3436/10000 [00:55<01:59, 55.05it/s]Training CobwebTree:  34%|      | 3442/10000 [00:55<02:02, 53.62it/s]Training CobwebTree:  34%|      | 3448/10000 [00:55<02:02, 53.64it/s]Training CobwebTree:  35%|      | 3454/10000 [00:56<02:01, 53.92it/s]Training CobwebTree:  35%|      | 3460/10000 [00:56<02:01, 53.85it/s]Training CobwebTree:  35%|      | 3466/10000 [00:56<02:01, 53.75it/s]Training CobwebTree:  35%|      | 3473/10000 [00:56<01:58, 55.29it/s]Training CobwebTree:  35%|      | 3479/10000 [00:56<02:04, 52.38it/s]Training CobwebTree:  35%|      | 3485/10000 [00:56<02:07, 51.21it/s]Training CobwebTree:  35%|      | 3491/10000 [00:56<02:09, 50.11it/s]Training CobwebTree:  35%|      | 3497/10000 [00:56<02:07, 50.90it/s]Training CobwebTree:  35%|      | 3503/10000 [00:56<02:09, 50.14it/s]Training CobwebTree:  35%|      | 3509/10000 [00:57<02:06, 51.45it/s]Training CobwebTree:  35%|      | 3515/10000 [00:57<02:06, 51.14it/s]Training CobwebTree:  35%|      | 3521/10000 [00:57<02:03, 52.32it/s]Training CobwebTree:  35%|      | 3528/10000 [00:57<01:57, 55.31it/s]Training CobwebTree:  35%|      | 3534/10000 [00:57<01:57, 55.09it/s]Training CobwebTree:  35%|      | 3540/10000 [00:57<02:01, 53.34it/s]Training CobwebTree:  35%|      | 3546/10000 [00:57<02:03, 52.08it/s]Training CobwebTree:  36%|      | 3552/10000 [00:57<02:07, 50.61it/s]Training CobwebTree:  36%|      | 3558/10000 [00:58<02:06, 50.97it/s]Training CobwebTree:  36%|      | 3564/10000 [00:58<02:08, 50.17it/s]Training CobwebTree:  36%|      | 3570/10000 [00:58<02:02, 52.41it/s]Training CobwebTree:  36%|      | 3576/10000 [00:58<02:04, 51.50it/s]Training CobwebTree:  36%|      | 3582/10000 [00:58<02:03, 51.86it/s]Training CobwebTree:  36%|      | 3588/10000 [00:58<02:04, 51.31it/s]Training CobwebTree:  36%|      | 3594/10000 [00:58<02:06, 50.71it/s]Training CobwebTree:  36%|      | 3600/10000 [00:58<02:05, 50.94it/s]Training CobwebTree:  36%|      | 3606/10000 [00:58<02:09, 49.45it/s]Training CobwebTree:  36%|      | 3613/10000 [00:59<02:00, 53.03it/s]Training CobwebTree:  36%|      | 3619/10000 [00:59<02:06, 50.54it/s]Training CobwebTree:  36%|      | 3625/10000 [00:59<02:02, 52.12it/s]Training CobwebTree:  36%|      | 3631/10000 [00:59<02:01, 52.26it/s]Training CobwebTree:  36%|      | 3637/10000 [00:59<02:03, 51.34it/s]Training CobwebTree:  36%|      | 3643/10000 [00:59<02:05, 50.73it/s]Training CobwebTree:  36%|      | 3650/10000 [00:59<01:55, 54.94it/s]Training CobwebTree:  37%|      | 3656/10000 [00:59<01:59, 53.23it/s]Training CobwebTree:  37%|      | 3662/10000 [01:00<01:56, 54.50it/s]Training CobwebTree:  37%|      | 3669/10000 [01:00<01:52, 56.35it/s]Training CobwebTree:  37%|      | 3675/10000 [01:00<01:51, 56.49it/s]Training CobwebTree:  37%|      | 3681/10000 [01:00<01:55, 54.80it/s]Training CobwebTree:  37%|      | 3687/10000 [01:00<01:59, 52.84it/s]Training CobwebTree:  37%|      | 3693/10000 [01:00<02:00, 52.21it/s]Training CobwebTree:  37%|      | 3699/10000 [01:00<02:00, 52.30it/s]Training CobwebTree:  37%|      | 3705/10000 [01:00<02:01, 51.80it/s]Training CobwebTree:  37%|      | 3711/10000 [01:00<02:03, 50.96it/s]Training CobwebTree:  37%|      | 3717/10000 [01:01<02:06, 49.86it/s]Training CobwebTree:  37%|      | 3723/10000 [01:01<02:03, 50.91it/s]Training CobwebTree:  37%|      | 3729/10000 [01:01<01:58, 53.09it/s]Training CobwebTree:  37%|      | 3735/10000 [01:01<01:54, 54.50it/s]Training CobwebTree:  37%|      | 3741/10000 [01:01<01:54, 54.64it/s]Training CobwebTree:  37%|      | 3748/10000 [01:01<01:51, 56.01it/s]Training CobwebTree:  38%|      | 3754/10000 [01:01<01:53, 54.85it/s]Training CobwebTree:  38%|      | 3760/10000 [01:01<01:53, 55.10it/s]Training CobwebTree:  38%|      | 3767/10000 [01:01<01:48, 57.23it/s]Training CobwebTree:  38%|      | 3773/10000 [01:02<01:54, 54.58it/s]Training CobwebTree:  38%|      | 3779/10000 [01:02<01:57, 52.85it/s]Training CobwebTree:  38%|      | 3785/10000 [01:02<01:58, 52.36it/s]Training CobwebTree:  38%|      | 3791/10000 [01:02<01:56, 53.47it/s]Training CobwebTree:  38%|      | 3797/10000 [01:02<02:01, 51.02it/s]Training CobwebTree:  38%|      | 3803/10000 [01:02<01:59, 51.70it/s]Training CobwebTree:  38%|      | 3809/10000 [01:02<02:00, 51.59it/s]Training CobwebTree:  38%|      | 3815/10000 [01:02<01:56, 53.09it/s]Training CobwebTree:  38%|      | 3821/10000 [01:02<01:54, 53.83it/s]Training CobwebTree:  38%|      | 3827/10000 [01:03<01:56, 53.18it/s]Training CobwebTree:  38%|      | 3833/10000 [01:03<01:59, 51.62it/s]Training CobwebTree:  38%|      | 3839/10000 [01:03<01:56, 52.81it/s]Training CobwebTree:  38%|      | 3845/10000 [01:03<01:57, 52.24it/s]Training CobwebTree:  39%|      | 3851/10000 [01:03<01:57, 52.44it/s]Training CobwebTree:  39%|      | 3857/10000 [01:03<01:54, 53.77it/s]Training CobwebTree:  39%|      | 3863/10000 [01:03<01:54, 53.48it/s]Training CobwebTree:  39%|      | 3869/10000 [01:03<01:55, 53.19it/s]Training CobwebTree:  39%|      | 3875/10000 [01:04<01:57, 52.20it/s]Training CobwebTree:  39%|      | 3881/10000 [01:04<02:00, 50.75it/s]Training CobwebTree:  39%|      | 3888/10000 [01:04<01:53, 53.83it/s]Training CobwebTree:  39%|      | 3894/10000 [01:04<02:01, 50.08it/s]Training CobwebTree:  39%|      | 3900/10000 [01:04<02:00, 50.60it/s]Training CobwebTree:  39%|      | 3906/10000 [01:04<02:06, 48.16it/s]Training CobwebTree:  39%|      | 3911/10000 [01:04<02:08, 47.25it/s]Training CobwebTree:  39%|      | 3918/10000 [01:04<01:58, 51.23it/s]Training CobwebTree:  39%|      | 3924/10000 [01:04<01:53, 53.46it/s]Training CobwebTree:  39%|      | 3930/10000 [01:05<01:52, 53.85it/s]Training CobwebTree:  39%|      | 3936/10000 [01:05<01:56, 52.10it/s]Training CobwebTree:  39%|      | 3942/10000 [01:05<01:58, 51.02it/s]Training CobwebTree:  39%|      | 3948/10000 [01:05<02:00, 50.07it/s]Training CobwebTree:  40%|      | 3954/10000 [01:05<01:59, 50.62it/s]Training CobwebTree:  40%|      | 3960/10000 [01:05<01:54, 52.81it/s]Training CobwebTree:  40%|      | 3966/10000 [01:05<01:50, 54.75it/s]Training CobwebTree:  40%|      | 3972/10000 [01:05<01:52, 53.82it/s]Training CobwebTree:  40%|      | 3978/10000 [01:06<01:55, 52.36it/s]Training CobwebTree:  40%|      | 3984/10000 [01:06<01:58, 50.91it/s]Training CobwebTree:  40%|      | 3990/10000 [01:06<01:53, 53.09it/s]Training CobwebTree:  40%|      | 3996/10000 [01:06<01:54, 52.48it/s]Training CobwebTree:  40%|      | 4002/10000 [01:06<01:51, 53.67it/s]Training CobwebTree:  40%|      | 4008/10000 [01:06<01:52, 53.03it/s]Training CobwebTree:  40%|      | 4015/10000 [01:06<01:46, 56.06it/s]Training CobwebTree:  40%|      | 4021/10000 [01:06<01:48, 55.22it/s]Training CobwebTree:  40%|      | 4027/10000 [01:06<01:48, 55.26it/s]Training CobwebTree:  40%|      | 4033/10000 [01:07<01:50, 53.86it/s]Training CobwebTree:  40%|      | 4039/10000 [01:07<01:50, 54.03it/s]Training CobwebTree:  40%|      | 4045/10000 [01:07<01:49, 54.60it/s]Training CobwebTree:  41%|      | 4051/10000 [01:07<01:51, 53.34it/s]Training CobwebTree:  41%|      | 4057/10000 [01:07<01:50, 53.62it/s]Training CobwebTree:  41%|      | 4063/10000 [01:07<01:48, 54.75it/s]Training CobwebTree:  41%|      | 4070/10000 [01:07<01:42, 57.61it/s]Training CobwebTree:  41%|      | 4078/10000 [01:07<01:36, 61.67it/s]Training CobwebTree:  41%|      | 4085/10000 [01:07<01:38, 59.91it/s]Training CobwebTree:  41%|      | 4092/10000 [01:08<01:42, 57.91it/s]Training CobwebTree:  41%|      | 4098/10000 [01:08<01:46, 55.28it/s]Training CobwebTree:  41%|      | 4104/10000 [01:08<01:48, 54.56it/s]Training CobwebTree:  41%|      | 4110/10000 [01:08<01:52, 52.54it/s]Training CobwebTree:  41%|      | 4116/10000 [01:08<01:55, 51.01it/s]Training CobwebTree:  41%|      | 4122/10000 [01:08<01:59, 49.20it/s]Training CobwebTree:  41%|     | 4129/10000 [01:08<01:48, 53.99it/s]Training CobwebTree:  41%|     | 4135/10000 [01:08<01:45, 55.35it/s]Training CobwebTree:  41%|     | 4141/10000 [01:09<01:51, 52.65it/s]Training CobwebTree:  41%|     | 4148/10000 [01:09<01:47, 54.44it/s]Training CobwebTree:  42%|     | 4154/10000 [01:09<01:46, 55.05it/s]Training CobwebTree:  42%|     | 4160/10000 [01:09<01:47, 54.19it/s]Training CobwebTree:  42%|     | 4166/10000 [01:09<01:47, 54.07it/s]Training CobwebTree:  42%|     | 4172/10000 [01:09<01:51, 52.45it/s]Training CobwebTree:  42%|     | 4178/10000 [01:09<01:48, 53.76it/s]Training CobwebTree:  42%|     | 4184/10000 [01:09<01:49, 53.03it/s]Training CobwebTree:  42%|     | 4190/10000 [01:09<01:50, 52.71it/s]Training CobwebTree:  42%|     | 4196/10000 [01:10<01:53, 51.31it/s]Training CobwebTree:  42%|     | 4202/10000 [01:10<01:50, 52.26it/s]Training CobwebTree:  42%|     | 4208/10000 [01:10<01:50, 52.52it/s]Training CobwebTree:  42%|     | 4214/10000 [01:10<01:50, 52.53it/s]Training CobwebTree:  42%|     | 4220/10000 [01:10<01:48, 53.19it/s]Training CobwebTree:  42%|     | 4226/10000 [01:10<01:47, 53.79it/s]Training CobwebTree:  42%|     | 4232/10000 [01:10<01:51, 51.71it/s]Training CobwebTree:  42%|     | 4238/10000 [01:10<01:55, 49.83it/s]Training CobwebTree:  42%|     | 4244/10000 [01:10<01:52, 51.24it/s]Training CobwebTree:  42%|     | 4250/10000 [01:11<01:51, 51.49it/s]Training CobwebTree:  43%|     | 4256/10000 [01:11<01:51, 51.56it/s]Training CobwebTree:  43%|     | 4262/10000 [01:11<01:50, 52.07it/s]Training CobwebTree:  43%|     | 4268/10000 [01:11<01:47, 53.56it/s]Training CobwebTree:  43%|     | 4274/10000 [01:11<01:45, 54.36it/s]Training CobwebTree:  43%|     | 4280/10000 [01:11<01:44, 54.50it/s]Training CobwebTree:  43%|     | 4286/10000 [01:11<01:46, 53.78it/s]Training CobwebTree:  43%|     | 4293/10000 [01:11<01:41, 56.17it/s]Training CobwebTree:  43%|     | 4299/10000 [01:11<01:41, 56.27it/s]Training CobwebTree:  43%|     | 4306/10000 [01:12<01:38, 57.69it/s]Training CobwebTree:  43%|     | 4312/10000 [01:12<01:44, 54.52it/s]Training CobwebTree:  43%|     | 4318/10000 [01:12<01:47, 52.85it/s]Training CobwebTree:  43%|     | 4324/10000 [01:12<01:47, 53.03it/s]Training CobwebTree:  43%|     | 4330/10000 [01:12<01:50, 51.54it/s]Training CobwebTree:  43%|     | 4336/10000 [01:12<01:48, 52.03it/s]Training CobwebTree:  43%|     | 4342/10000 [01:12<01:47, 52.82it/s]Training CobwebTree:  43%|     | 4349/10000 [01:12<01:41, 55.93it/s]Training CobwebTree:  44%|     | 4355/10000 [01:13<01:41, 55.88it/s]Training CobwebTree:  44%|     | 4361/10000 [01:13<01:45, 53.69it/s]Training CobwebTree:  44%|     | 4367/10000 [01:13<01:43, 54.42it/s]Training CobwebTree:  44%|     | 4373/10000 [01:13<01:42, 54.90it/s]Training CobwebTree:  44%|     | 4380/10000 [01:13<01:40, 55.78it/s]Training CobwebTree:  44%|     | 4387/10000 [01:13<01:37, 57.49it/s]Training CobwebTree:  44%|     | 4393/10000 [01:13<01:38, 57.15it/s]Training CobwebTree:  44%|     | 4399/10000 [01:13<01:42, 54.83it/s]Training CobwebTree:  44%|     | 4405/10000 [01:13<01:43, 54.28it/s]Training CobwebTree:  44%|     | 4411/10000 [01:14<01:40, 55.54it/s]Training CobwebTree:  44%|     | 4417/10000 [01:14<01:41, 54.81it/s]Training CobwebTree:  44%|     | 4423/10000 [01:14<01:43, 53.76it/s]Training CobwebTree:  44%|     | 4429/10000 [01:14<01:43, 53.90it/s]Training CobwebTree:  44%|     | 4435/10000 [01:14<01:42, 54.18it/s]Training CobwebTree:  44%|     | 4441/10000 [01:14<01:44, 53.23it/s]Training CobwebTree:  44%|     | 4447/10000 [01:14<01:44, 53.17it/s]Training CobwebTree:  45%|     | 4453/10000 [01:14<01:47, 51.56it/s]Training CobwebTree:  45%|     | 4459/10000 [01:14<01:49, 50.53it/s]Training CobwebTree:  45%|     | 4465/10000 [01:15<01:44, 52.95it/s]Training CobwebTree:  45%|     | 4471/10000 [01:15<01:47, 51.47it/s]Training CobwebTree:  45%|     | 4477/10000 [01:15<01:45, 52.48it/s]Training CobwebTree:  45%|     | 4483/10000 [01:15<01:46, 51.75it/s]Training CobwebTree:  45%|     | 4489/10000 [01:15<01:48, 51.00it/s]Training CobwebTree:  45%|     | 4495/10000 [01:15<01:44, 52.49it/s]Training CobwebTree:  45%|     | 4501/10000 [01:15<01:44, 52.47it/s]Training CobwebTree:  45%|     | 4507/10000 [01:15<01:45, 52.01it/s]Training CobwebTree:  45%|     | 4513/10000 [01:15<01:45, 52.07it/s]Training CobwebTree:  45%|     | 4519/10000 [01:16<01:42, 53.55it/s]Training CobwebTree:  45%|     | 4525/10000 [01:16<01:45, 51.86it/s]Training CobwebTree:  45%|     | 4531/10000 [01:16<01:49, 49.98it/s]Training CobwebTree:  45%|     | 4537/10000 [01:16<01:45, 51.81it/s]Training CobwebTree:  45%|     | 4543/10000 [01:16<01:43, 52.94it/s]Training CobwebTree:  45%|     | 4549/10000 [01:16<01:44, 52.39it/s]Training CobwebTree:  46%|     | 4555/10000 [01:16<01:46, 51.12it/s]Training CobwebTree:  46%|     | 4561/10000 [01:16<01:42, 53.05it/s]Training CobwebTree:  46%|     | 4567/10000 [01:17<01:39, 54.33it/s]Training CobwebTree:  46%|     | 4573/10000 [01:17<01:37, 55.90it/s]Training CobwebTree:  46%|     | 4580/10000 [01:17<01:33, 57.92it/s]Training CobwebTree:  46%|     | 4586/10000 [01:17<01:34, 57.51it/s]Training CobwebTree:  46%|     | 4592/10000 [01:17<01:36, 55.78it/s]Training CobwebTree:  46%|     | 4598/10000 [01:17<01:38, 54.77it/s]Training CobwebTree:  46%|     | 4604/10000 [01:17<01:41, 53.17it/s]Training CobwebTree:  46%|     | 4610/10000 [01:17<01:45, 51.11it/s]Training CobwebTree:  46%|     | 4616/10000 [01:17<01:46, 50.48it/s]Training CobwebTree:  46%|     | 4622/10000 [01:18<01:47, 49.92it/s]Training CobwebTree:  46%|     | 4628/10000 [01:18<01:50, 48.67it/s]Training CobwebTree:  46%|     | 4633/10000 [01:18<01:51, 48.08it/s]Training CobwebTree:  46%|     | 4639/10000 [01:18<01:45, 50.64it/s]Training CobwebTree:  46%|     | 4645/10000 [01:18<01:45, 50.64it/s]Training CobwebTree:  47%|     | 4651/10000 [01:18<01:44, 51.18it/s]Training CobwebTree:  47%|     | 4657/10000 [01:18<01:50, 48.28it/s]Training CobwebTree:  47%|     | 4662/10000 [01:18<01:54, 46.47it/s]Training CobwebTree:  47%|     | 4668/10000 [01:19<01:52, 47.48it/s]Training CobwebTree:  47%|     | 4673/10000 [01:19<01:54, 46.59it/s]Training CobwebTree:  47%|     | 4679/10000 [01:19<01:48, 48.89it/s]Training CobwebTree:  47%|     | 4685/10000 [01:19<01:43, 51.34it/s]Training CobwebTree:  47%|     | 4691/10000 [01:19<01:41, 52.33it/s]Training CobwebTree:  47%|     | 4697/10000 [01:19<01:45, 50.19it/s]Training CobwebTree:  47%|     | 4703/10000 [01:19<01:43, 51.15it/s]Training CobwebTree:  47%|     | 4710/10000 [01:19<01:37, 54.26it/s]Training CobwebTree:  47%|     | 4716/10000 [01:19<01:39, 52.85it/s]Training CobwebTree:  47%|     | 4722/10000 [01:20<01:40, 52.34it/s]Training CobwebTree:  47%|     | 4729/10000 [01:20<01:34, 55.66it/s]Training CobwebTree:  47%|     | 4735/10000 [01:20<01:38, 53.72it/s]Training CobwebTree:  47%|     | 4741/10000 [01:20<01:43, 50.57it/s]Training CobwebTree:  47%|     | 4747/10000 [01:20<01:40, 52.38it/s]Training CobwebTree:  48%|     | 4753/10000 [01:20<01:37, 53.72it/s]Training CobwebTree:  48%|     | 4759/10000 [01:20<01:36, 54.39it/s]Training CobwebTree:  48%|     | 4765/10000 [01:20<01:35, 54.64it/s]Training CobwebTree:  48%|     | 4771/10000 [01:20<01:36, 54.01it/s]Training CobwebTree:  48%|     | 4777/10000 [01:21<01:34, 55.11it/s]Training CobwebTree:  48%|     | 4784/10000 [01:21<01:29, 58.14it/s]Training CobwebTree:  48%|     | 4790/10000 [01:21<01:34, 55.28it/s]Training CobwebTree:  48%|     | 4796/10000 [01:21<01:39, 52.46it/s]Training CobwebTree:  48%|     | 4802/10000 [01:21<01:40, 51.51it/s]Training CobwebTree:  48%|     | 4808/10000 [01:21<01:38, 52.63it/s]Training CobwebTree:  48%|     | 4814/10000 [01:21<01:40, 51.58it/s]Training CobwebTree:  48%|     | 4821/10000 [01:21<01:36, 53.52it/s]Training CobwebTree:  48%|     | 4827/10000 [01:21<01:35, 54.23it/s]Training CobwebTree:  48%|     | 4833/10000 [01:22<01:36, 53.77it/s]Training CobwebTree:  48%|     | 4839/10000 [01:22<01:36, 53.31it/s]Training CobwebTree:  48%|     | 4845/10000 [01:22<01:33, 54.88it/s]Training CobwebTree:  49%|     | 4851/10000 [01:22<01:38, 52.21it/s]Training CobwebTree:  49%|     | 4857/10000 [01:22<01:40, 51.03it/s]Training CobwebTree:  49%|     | 4863/10000 [01:22<01:39, 51.49it/s]Training CobwebTree:  49%|     | 4870/10000 [01:22<01:36, 53.19it/s]Training CobwebTree:  49%|     | 4876/10000 [01:22<01:36, 53.21it/s]Training CobwebTree:  49%|     | 4882/10000 [01:23<01:37, 52.61it/s]Training CobwebTree:  49%|     | 4888/10000 [01:23<01:39, 51.20it/s]Training CobwebTree:  49%|     | 4894/10000 [01:23<01:42, 49.68it/s]Training CobwebTree:  49%|     | 4901/10000 [01:23<01:36, 52.83it/s]Training CobwebTree:  49%|     | 4908/10000 [01:23<01:29, 57.11it/s]Training CobwebTree:  49%|     | 4914/10000 [01:23<01:30, 56.05it/s]Training CobwebTree:  49%|     | 4920/10000 [01:23<01:29, 56.94it/s]Training CobwebTree:  49%|     | 4926/10000 [01:23<01:32, 54.87it/s]Training CobwebTree:  49%|     | 4932/10000 [01:23<01:37, 51.96it/s]Training CobwebTree:  49%|     | 4938/10000 [01:24<01:41, 50.02it/s]Training CobwebTree:  49%|     | 4944/10000 [01:24<01:39, 50.97it/s]Training CobwebTree:  50%|     | 4950/10000 [01:24<01:37, 51.94it/s]Training CobwebTree:  50%|     | 4956/10000 [01:24<01:35, 52.74it/s]Training CobwebTree:  50%|     | 4962/10000 [01:24<01:36, 52.13it/s]Training CobwebTree:  50%|     | 4968/10000 [01:24<01:37, 51.66it/s]Training CobwebTree:  50%|     | 4974/10000 [01:24<01:36, 51.88it/s]Training CobwebTree:  50%|     | 4980/10000 [01:24<01:34, 53.18it/s]Training CobwebTree:  50%|     | 4986/10000 [01:24<01:34, 53.34it/s]Training CobwebTree:  50%|     | 4992/10000 [01:25<01:36, 52.01it/s]Training CobwebTree:  50%|     | 4999/10000 [01:25<01:31, 54.51it/s]Training CobwebTree:  50%|     | 5005/10000 [01:25<01:36, 51.78it/s]Training CobwebTree:  50%|     | 5011/10000 [01:25<01:32, 53.68it/s]Training CobwebTree:  50%|     | 5017/10000 [01:25<01:37, 51.07it/s]Training CobwebTree:  50%|     | 5023/10000 [01:25<01:39, 50.00it/s]Training CobwebTree:  50%|     | 5029/10000 [01:25<01:36, 51.26it/s]Training CobwebTree:  50%|     | 5035/10000 [01:25<01:35, 51.79it/s]Training CobwebTree:  50%|     | 5041/10000 [01:26<01:36, 51.63it/s]Training CobwebTree:  50%|     | 5047/10000 [01:26<01:39, 50.01it/s]Training CobwebTree:  51%|     | 5053/10000 [01:26<01:36, 51.16it/s]Training CobwebTree:  51%|     | 5059/10000 [01:26<01:36, 51.10it/s]Training CobwebTree:  51%|     | 5065/10000 [01:26<01:36, 51.06it/s]Training CobwebTree:  51%|     | 5071/10000 [01:26<01:36, 50.85it/s]Training CobwebTree:  51%|     | 5077/10000 [01:26<01:42, 48.04it/s]Training CobwebTree:  51%|     | 5083/10000 [01:26<01:36, 50.98it/s]Training CobwebTree:  51%|     | 5089/10000 [01:27<01:40, 49.00it/s]Training CobwebTree:  51%|     | 5095/10000 [01:27<01:34, 51.71it/s]Training CobwebTree:  51%|     | 5101/10000 [01:27<01:36, 50.54it/s]Training CobwebTree:  51%|     | 5107/10000 [01:27<01:32, 53.03it/s]Training CobwebTree:  51%|     | 5113/10000 [01:27<01:37, 50.16it/s]Training CobwebTree:  51%|     | 5119/10000 [01:27<01:38, 49.80it/s]Training CobwebTree:  51%|    | 5125/10000 [01:27<01:33, 52.37it/s]Training CobwebTree:  51%|    | 5131/10000 [01:27<01:36, 50.40it/s]Training CobwebTree:  51%|    | 5137/10000 [01:27<01:38, 49.57it/s]Training CobwebTree:  51%|    | 5143/10000 [01:28<01:36, 50.57it/s]Training CobwebTree:  51%|    | 5149/10000 [01:28<01:34, 51.50it/s]Training CobwebTree:  52%|    | 5155/10000 [01:28<01:33, 51.61it/s]Training CobwebTree:  52%|    | 5161/10000 [01:28<01:33, 51.57it/s]Training CobwebTree:  52%|    | 5167/10000 [01:28<01:38, 48.98it/s]Training CobwebTree:  52%|    | 5174/10000 [01:28<01:30, 53.57it/s]Training CobwebTree:  52%|    | 5180/10000 [01:28<01:34, 51.28it/s]Training CobwebTree:  52%|    | 5186/10000 [01:28<01:34, 51.18it/s]Training CobwebTree:  52%|    | 5192/10000 [01:29<01:33, 51.41it/s]Training CobwebTree:  52%|    | 5198/10000 [01:29<01:32, 52.19it/s]Training CobwebTree:  52%|    | 5204/10000 [01:29<01:29, 53.47it/s]Training CobwebTree:  52%|    | 5210/10000 [01:29<01:31, 52.61it/s]Training CobwebTree:  52%|    | 5216/10000 [01:29<01:30, 52.78it/s]Training CobwebTree:  52%|    | 5222/10000 [01:29<01:29, 53.63it/s]Training CobwebTree:  52%|    | 5228/10000 [01:29<01:30, 52.71it/s]Training CobwebTree:  52%|    | 5234/10000 [01:29<01:38, 48.40it/s]Training CobwebTree:  52%|    | 5240/10000 [01:29<01:36, 49.16it/s]Training CobwebTree:  52%|    | 5246/10000 [01:30<01:32, 51.40it/s]Training CobwebTree:  53%|    | 5252/10000 [01:30<01:31, 52.14it/s]Training CobwebTree:  53%|    | 5258/10000 [01:30<01:31, 51.90it/s]Training CobwebTree:  53%|    | 5264/10000 [01:30<01:32, 51.44it/s]Training CobwebTree:  53%|    | 5270/10000 [01:30<01:32, 51.01it/s]Training CobwebTree:  53%|    | 5276/10000 [01:30<01:31, 51.87it/s]Training CobwebTree:  53%|    | 5282/10000 [01:30<01:32, 51.24it/s]Training CobwebTree:  53%|    | 5288/10000 [01:30<01:35, 49.50it/s]Training CobwebTree:  53%|    | 5293/10000 [01:31<01:35, 49.14it/s]Training CobwebTree:  53%|    | 5298/10000 [01:31<01:37, 48.12it/s]Training CobwebTree:  53%|    | 5303/10000 [01:31<01:41, 46.11it/s]Training CobwebTree:  53%|    | 5308/10000 [01:31<01:40, 46.49it/s]Training CobwebTree:  53%|    | 5314/10000 [01:31<01:38, 47.37it/s]Training CobwebTree:  53%|    | 5319/10000 [01:31<01:38, 47.65it/s]Training CobwebTree:  53%|    | 5325/10000 [01:31<01:32, 50.37it/s]Training CobwebTree:  53%|    | 5331/10000 [01:31<01:36, 48.25it/s]Training CobwebTree:  53%|    | 5337/10000 [01:31<01:34, 49.28it/s]Training CobwebTree:  53%|    | 5342/10000 [01:32<01:34, 49.34it/s]Training CobwebTree:  53%|    | 5348/10000 [01:32<01:34, 49.47it/s]Training CobwebTree:  54%|    | 5354/10000 [01:32<01:30, 51.54it/s]Training CobwebTree:  54%|    | 5361/10000 [01:32<01:25, 53.97it/s]Training CobwebTree:  54%|    | 5368/10000 [01:32<01:23, 55.69it/s]Training CobwebTree:  54%|    | 5374/10000 [01:32<01:22, 56.09it/s]Training CobwebTree:  54%|    | 5380/10000 [01:32<01:22, 55.87it/s]Training CobwebTree:  54%|    | 5387/10000 [01:32<01:18, 58.91it/s]Training CobwebTree:  54%|    | 5393/10000 [01:32<01:18, 58.56it/s]Training CobwebTree:  54%|    | 5399/10000 [01:33<01:19, 57.53it/s]Training CobwebTree:  54%|    | 5405/10000 [01:33<01:23, 54.78it/s]Training CobwebTree:  54%|    | 5411/10000 [01:33<01:23, 54.75it/s]Training CobwebTree:  54%|    | 5417/10000 [01:33<01:24, 54.20it/s]Training CobwebTree:  54%|    | 5423/10000 [01:33<01:26, 52.76it/s]Training CobwebTree:  54%|    | 5429/10000 [01:33<01:28, 51.46it/s]Training CobwebTree:  54%|    | 5435/10000 [01:33<01:29, 50.87it/s]Training CobwebTree:  54%|    | 5441/10000 [01:33<01:26, 52.70it/s]Training CobwebTree:  54%|    | 5447/10000 [01:33<01:30, 50.56it/s]Training CobwebTree:  55%|    | 5453/10000 [01:34<01:28, 51.59it/s]Training CobwebTree:  55%|    | 5459/10000 [01:34<01:30, 50.28it/s]Training CobwebTree:  55%|    | 5466/10000 [01:34<01:23, 54.37it/s]Training CobwebTree:  55%|    | 5472/10000 [01:34<01:22, 54.79it/s]Training CobwebTree:  55%|    | 5478/10000 [01:34<01:25, 52.92it/s]Training CobwebTree:  55%|    | 5484/10000 [01:34<01:30, 49.84it/s]Training CobwebTree:  55%|    | 5490/10000 [01:34<01:33, 48.39it/s]Training CobwebTree:  55%|    | 5496/10000 [01:34<01:28, 51.11it/s]Training CobwebTree:  55%|    | 5502/10000 [01:35<01:26, 52.03it/s]Training CobwebTree:  55%|    | 5508/10000 [01:35<01:25, 52.39it/s]Training CobwebTree:  55%|    | 5514/10000 [01:35<01:29, 50.01it/s]Training CobwebTree:  55%|    | 5520/10000 [01:35<01:30, 49.27it/s]Training CobwebTree:  55%|    | 5525/10000 [01:35<01:31, 49.14it/s]Training CobwebTree:  55%|    | 5531/10000 [01:35<01:26, 51.61it/s]Training CobwebTree:  55%|    | 5537/10000 [01:35<01:23, 53.28it/s]Training CobwebTree:  55%|    | 5543/10000 [01:35<01:24, 52.57it/s]Training CobwebTree:  55%|    | 5549/10000 [01:35<01:22, 53.92it/s]Training CobwebTree:  56%|    | 5556/10000 [01:36<01:17, 57.10it/s]Training CobwebTree:  56%|    | 5563/10000 [01:36<01:15, 59.01it/s]Training CobwebTree:  56%|    | 5569/10000 [01:36<01:15, 58.51it/s]Training CobwebTree:  56%|    | 5575/10000 [01:36<01:19, 55.37it/s]Training CobwebTree:  56%|    | 5581/10000 [01:36<01:23, 52.79it/s]Training CobwebTree:  56%|    | 5587/10000 [01:36<01:25, 51.70it/s]Training CobwebTree:  56%|    | 5593/10000 [01:36<01:26, 50.71it/s]Training CobwebTree:  56%|    | 5599/10000 [01:36<01:25, 51.50it/s]Training CobwebTree:  56%|    | 5605/10000 [01:36<01:25, 51.51it/s]Training CobwebTree:  56%|    | 5611/10000 [01:37<01:28, 49.38it/s]Training CobwebTree:  56%|    | 5618/10000 [01:37<01:23, 52.79it/s]Training CobwebTree:  56%|    | 5624/10000 [01:37<01:27, 50.23it/s]Training CobwebTree:  56%|    | 5630/10000 [01:37<01:26, 50.33it/s]Training CobwebTree:  56%|    | 5636/10000 [01:37<01:29, 48.80it/s]Training CobwebTree:  56%|    | 5642/10000 [01:37<01:27, 49.60it/s]Training CobwebTree:  56%|    | 5647/10000 [01:37<01:28, 49.35it/s]Training CobwebTree:  57%|    | 5652/10000 [01:37<01:28, 49.27it/s]Training CobwebTree:  57%|    | 5657/10000 [01:38<01:28, 49.09it/s]Training CobwebTree:  57%|    | 5663/10000 [01:38<01:24, 51.48it/s]Training CobwebTree:  57%|    | 5669/10000 [01:38<01:20, 53.50it/s]Training CobwebTree:  57%|    | 5675/10000 [01:38<01:23, 51.63it/s]Training CobwebTree:  57%|    | 5681/10000 [01:38<01:25, 50.35it/s]Training CobwebTree:  57%|    | 5687/10000 [01:38<01:22, 52.14it/s]Training CobwebTree:  57%|    | 5693/10000 [01:38<01:25, 50.39it/s]Training CobwebTree:  57%|    | 5699/10000 [01:38<01:21, 52.54it/s]Training CobwebTree:  57%|    | 5705/10000 [01:38<01:19, 53.95it/s]Training CobwebTree:  57%|    | 5711/10000 [01:39<01:24, 51.03it/s]Training CobwebTree:  57%|    | 5717/10000 [01:39<01:26, 49.34it/s]Training CobwebTree:  57%|    | 5722/10000 [01:39<01:27, 49.00it/s]Training CobwebTree:  57%|    | 5727/10000 [01:39<01:30, 47.04it/s]Training CobwebTree:  57%|    | 5733/10000 [01:39<01:28, 48.04it/s]Training CobwebTree:  57%|    | 5739/10000 [01:39<01:25, 49.94it/s]Training CobwebTree:  57%|    | 5746/10000 [01:39<01:18, 54.53it/s]Training CobwebTree:  58%|    | 5752/10000 [01:39<01:17, 54.76it/s]Training CobwebTree:  58%|    | 5758/10000 [01:39<01:18, 53.82it/s]Training CobwebTree:  58%|    | 5764/10000 [01:40<01:20, 52.47it/s]Training CobwebTree:  58%|    | 5770/10000 [01:40<01:21, 52.18it/s]Training CobwebTree:  58%|    | 5776/10000 [01:40<01:25, 49.58it/s]Training CobwebTree:  58%|    | 5781/10000 [01:40<01:25, 49.59it/s]Training CobwebTree:  58%|    | 5787/10000 [01:40<01:24, 49.80it/s]Training CobwebTree:  58%|    | 5793/10000 [01:40<01:22, 51.11it/s]Training CobwebTree:  58%|    | 5799/10000 [01:40<01:20, 51.90it/s]Training CobwebTree:  58%|    | 5805/10000 [01:40<01:18, 53.66it/s]Training CobwebTree:  58%|    | 5811/10000 [01:41<01:20, 51.79it/s]Training CobwebTree:  58%|    | 5817/10000 [01:41<01:17, 53.72it/s]Training CobwebTree:  58%|    | 5823/10000 [01:41<01:15, 55.19it/s]Training CobwebTree:  58%|    | 5829/10000 [01:41<01:15, 55.54it/s]Training CobwebTree:  58%|    | 5835/10000 [01:41<01:17, 53.58it/s]Training CobwebTree:  58%|    | 5841/10000 [01:41<01:20, 51.62it/s]Training CobwebTree:  58%|    | 5847/10000 [01:41<01:23, 49.66it/s]Training CobwebTree:  59%|    | 5853/10000 [01:41<01:21, 50.71it/s]Training CobwebTree:  59%|    | 5859/10000 [01:41<01:22, 50.47it/s]Training CobwebTree:  59%|    | 5865/10000 [01:42<01:19, 52.22it/s]Training CobwebTree:  59%|    | 5871/10000 [01:42<01:17, 53.22it/s]Training CobwebTree:  59%|    | 5877/10000 [01:42<01:15, 54.32it/s]Training CobwebTree:  59%|    | 5883/10000 [01:42<01:18, 52.76it/s]Training CobwebTree:  59%|    | 5889/10000 [01:42<01:18, 52.55it/s]Training CobwebTree:  59%|    | 5895/10000 [01:42<01:20, 51.27it/s]Training CobwebTree:  59%|    | 5901/10000 [01:42<01:19, 51.59it/s]Training CobwebTree:  59%|    | 5907/10000 [01:42<01:20, 51.14it/s]Training CobwebTree:  59%|    | 5913/10000 [01:42<01:22, 49.52it/s]Training CobwebTree:  59%|    | 5919/10000 [01:43<01:18, 51.82it/s]Training CobwebTree:  59%|    | 5925/10000 [01:43<01:20, 50.79it/s]Training CobwebTree:  59%|    | 5931/10000 [01:43<01:23, 49.02it/s]Training CobwebTree:  59%|    | 5936/10000 [01:43<01:25, 47.54it/s]Training CobwebTree:  59%|    | 5942/10000 [01:43<01:20, 50.62it/s]Training CobwebTree:  59%|    | 5948/10000 [01:43<01:17, 51.99it/s]Training CobwebTree:  60%|    | 5954/10000 [01:43<01:20, 50.43it/s]Training CobwebTree:  60%|    | 5960/10000 [01:43<01:20, 49.97it/s]Training CobwebTree:  60%|    | 5966/10000 [01:44<01:19, 51.02it/s]Training CobwebTree:  60%|    | 5972/10000 [01:44<01:20, 50.27it/s]Training CobwebTree:  60%|    | 5978/10000 [01:44<01:21, 49.07it/s]Training CobwebTree:  60%|    | 5983/10000 [01:44<01:23, 48.33it/s]Training CobwebTree:  60%|    | 5989/10000 [01:44<01:18, 51.06it/s]Training CobwebTree:  60%|    | 5995/10000 [01:44<01:17, 51.67it/s]Training CobwebTree:  60%|    | 6001/10000 [01:44<01:15, 53.22it/s]Training CobwebTree:  60%|    | 6007/10000 [01:44<01:13, 54.68it/s]Training CobwebTree:  60%|    | 6013/10000 [01:44<01:16, 51.97it/s]Training CobwebTree:  60%|    | 6019/10000 [01:45<01:21, 48.75it/s]Training CobwebTree:  60%|    | 6025/10000 [01:45<01:17, 51.10it/s]Training CobwebTree:  60%|    | 6031/10000 [01:45<01:18, 50.53it/s]Training CobwebTree:  60%|    | 6037/10000 [01:45<01:20, 49.36it/s]Training CobwebTree:  60%|    | 6043/10000 [01:45<01:16, 51.60it/s]Training CobwebTree:  60%|    | 6049/10000 [01:45<01:17, 51.21it/s]Training CobwebTree:  61%|    | 6055/10000 [01:45<01:17, 51.21it/s]Training CobwebTree:  61%|    | 6061/10000 [01:45<01:18, 50.21it/s]Training CobwebTree:  61%|    | 6067/10000 [01:46<01:20, 49.01it/s]Training CobwebTree:  61%|    | 6072/10000 [01:46<01:20, 48.96it/s]Training CobwebTree:  61%|    | 6077/10000 [01:46<01:25, 45.90it/s]Training CobwebTree:  61%|    | 6082/10000 [01:46<01:23, 46.66it/s]Training CobwebTree:  61%|    | 6088/10000 [01:46<01:22, 47.64it/s]Training CobwebTree:  61%|    | 6094/10000 [01:46<01:17, 50.23it/s]Training CobwebTree:  61%|    | 6100/10000 [01:46<01:17, 50.46it/s]Training CobwebTree:  61%|    | 6106/10000 [01:46<01:16, 51.07it/s]Training CobwebTree:  61%|    | 6112/10000 [01:46<01:15, 51.61it/s]Training CobwebTree:  61%|    | 6118/10000 [01:47<01:15, 51.67it/s]Training CobwebTree:  61%|    | 6124/10000 [01:47<01:12, 53.15it/s]Training CobwebTree:  61%|   | 6130/10000 [01:47<01:13, 52.68it/s]Training CobwebTree:  61%|   | 6136/10000 [01:47<01:13, 52.87it/s]Training CobwebTree:  61%|   | 6142/10000 [01:47<01:13, 52.56it/s]Training CobwebTree:  61%|   | 6148/10000 [01:47<01:14, 51.57it/s]Training CobwebTree:  62%|   | 6154/10000 [01:47<01:14, 51.91it/s]Training CobwebTree:  62%|   | 6160/10000 [01:47<01:12, 52.67it/s]Training CobwebTree:  62%|   | 6166/10000 [01:47<01:11, 53.81it/s]Training CobwebTree:  62%|   | 6172/10000 [01:48<01:10, 54.08it/s]Training CobwebTree:  62%|   | 6178/10000 [01:48<01:09, 55.19it/s]Training CobwebTree:  62%|   | 6184/10000 [01:48<01:08, 55.71it/s]Training CobwebTree:  62%|   | 6190/10000 [01:48<01:08, 55.40it/s]Training CobwebTree:  62%|   | 6197/10000 [01:48<01:05, 57.79it/s]Training CobwebTree:  62%|   | 6203/10000 [01:48<01:07, 55.90it/s]Training CobwebTree:  62%|   | 6209/10000 [01:48<01:07, 55.90it/s]Training CobwebTree:  62%|   | 6215/10000 [01:48<01:12, 52.47it/s]Training CobwebTree:  62%|   | 6221/10000 [01:48<01:11, 53.00it/s]Training CobwebTree:  62%|   | 6227/10000 [01:49<01:13, 51.50it/s]Training CobwebTree:  62%|   | 6233/10000 [01:49<01:15, 49.67it/s]Training CobwebTree:  62%|   | 6239/10000 [01:49<01:15, 49.88it/s]Training CobwebTree:  62%|   | 6245/10000 [01:49<01:17, 48.49it/s]Training CobwebTree:  63%|   | 6251/10000 [01:49<01:16, 49.15it/s]Training CobwebTree:  63%|   | 6258/10000 [01:49<01:12, 51.57it/s]Training CobwebTree:  63%|   | 6264/10000 [01:49<01:12, 51.57it/s]Training CobwebTree:  63%|   | 6270/10000 [01:49<01:11, 52.28it/s]Training CobwebTree:  63%|   | 6276/10000 [01:50<01:11, 51.82it/s]Training CobwebTree:  63%|   | 6282/10000 [01:50<01:16, 48.77it/s]Training CobwebTree:  63%|   | 6287/10000 [01:50<01:18, 47.04it/s]Training CobwebTree:  63%|   | 6293/10000 [01:50<01:17, 47.97it/s]Training CobwebTree:  63%|   | 6298/10000 [01:50<01:16, 48.46it/s]Training CobwebTree:  63%|   | 6303/10000 [01:50<01:17, 47.83it/s]Training CobwebTree:  63%|   | 6309/10000 [01:50<01:13, 50.44it/s]Training CobwebTree:  63%|   | 6315/10000 [01:50<01:13, 50.22it/s]Training CobwebTree:  63%|   | 6321/10000 [01:50<01:14, 49.56it/s]Training CobwebTree:  63%|   | 6327/10000 [01:51<01:11, 51.57it/s]Training CobwebTree:  63%|   | 6333/10000 [01:51<01:10, 51.80it/s]Training CobwebTree:  63%|   | 6339/10000 [01:51<01:08, 53.28it/s]Training CobwebTree:  63%|   | 6345/10000 [01:51<01:07, 53.91it/s]Training CobwebTree:  64%|   | 6351/10000 [01:51<01:08, 53.05it/s]Training CobwebTree:  64%|   | 6357/10000 [01:51<01:09, 52.71it/s]Training CobwebTree:  64%|   | 6363/10000 [01:51<01:07, 53.68it/s]Training CobwebTree:  64%|   | 6369/10000 [01:51<01:07, 53.45it/s]Training CobwebTree:  64%|   | 6375/10000 [01:51<01:10, 51.41it/s]Training CobwebTree:  64%|   | 6381/10000 [01:52<01:13, 49.54it/s]Training CobwebTree:  64%|   | 6387/10000 [01:52<01:11, 50.52it/s]Training CobwebTree:  64%|   | 6393/10000 [01:52<01:09, 52.18it/s]Training CobwebTree:  64%|   | 6399/10000 [01:52<01:10, 51.08it/s]Training CobwebTree:  64%|   | 6405/10000 [01:52<01:12, 49.30it/s]Training CobwebTree:  64%|   | 6410/10000 [01:52<01:12, 49.25it/s]Training CobwebTree:  64%|   | 6416/10000 [01:52<01:10, 51.11it/s]Training CobwebTree:  64%|   | 6422/10000 [01:52<01:07, 53.01it/s]Training CobwebTree:  64%|   | 6428/10000 [01:53<01:05, 54.58it/s]Training CobwebTree:  64%|   | 6434/10000 [01:53<01:05, 54.26it/s]Training CobwebTree:  64%|   | 6440/10000 [01:53<01:07, 53.11it/s]Training CobwebTree:  64%|   | 6446/10000 [01:53<01:07, 52.66it/s]Training CobwebTree:  65%|   | 6452/10000 [01:53<01:09, 50.88it/s]Training CobwebTree:  65%|   | 6458/10000 [01:53<01:09, 50.84it/s]Training CobwebTree:  65%|   | 6464/10000 [01:53<01:07, 52.54it/s]Training CobwebTree:  65%|   | 6470/10000 [01:53<01:09, 51.08it/s]Training CobwebTree:  65%|   | 6476/10000 [01:53<01:09, 50.83it/s]Training CobwebTree:  65%|   | 6482/10000 [01:54<01:10, 49.99it/s]Training CobwebTree:  65%|   | 6489/10000 [01:54<01:07, 52.36it/s]Training CobwebTree:  65%|   | 6495/10000 [01:54<01:07, 51.80it/s]Training CobwebTree:  65%|   | 6501/10000 [01:54<01:06, 52.96it/s]Training CobwebTree:  65%|   | 6507/10000 [01:54<01:07, 51.89it/s]Training CobwebTree:  65%|   | 6513/10000 [01:54<01:08, 51.01it/s]Training CobwebTree:  65%|   | 6519/10000 [01:54<01:06, 52.53it/s]Training CobwebTree:  65%|   | 6525/10000 [01:54<01:07, 51.11it/s]Training CobwebTree:  65%|   | 6531/10000 [01:55<01:08, 50.72it/s]Training CobwebTree:  65%|   | 6537/10000 [01:55<01:06, 52.20it/s]Training CobwebTree:  65%|   | 6543/10000 [01:55<01:10, 49.13it/s]Training CobwebTree:  65%|   | 6549/10000 [01:55<01:10, 48.89it/s]Training CobwebTree:  66%|   | 6554/10000 [01:55<01:10, 48.84it/s]Training CobwebTree:  66%|   | 6560/10000 [01:55<01:09, 49.45it/s]Training CobwebTree:  66%|   | 6566/10000 [01:55<01:08, 50.39it/s]Training CobwebTree:  66%|   | 6572/10000 [01:55<01:06, 51.40it/s]Training CobwebTree:  66%|   | 6578/10000 [01:55<01:05, 52.16it/s]Training CobwebTree:  66%|   | 6584/10000 [01:56<01:05, 52.20it/s]Training CobwebTree:  66%|   | 6591/10000 [01:56<01:01, 55.62it/s]Training CobwebTree:  66%|   | 6597/10000 [01:56<01:00, 56.34it/s]Training CobwebTree:  66%|   | 6603/10000 [01:56<01:01, 55.09it/s]Training CobwebTree:  66%|   | 6609/10000 [01:56<01:01, 55.25it/s]Training CobwebTree:  66%|   | 6616/10000 [01:56<00:59, 57.12it/s]Training CobwebTree:  66%|   | 6622/10000 [01:56<00:59, 56.59it/s]Training CobwebTree:  66%|   | 6628/10000 [01:56<01:03, 53.49it/s]Training CobwebTree:  66%|   | 6634/10000 [01:56<01:02, 53.67it/s]Training CobwebTree:  66%|   | 6640/10000 [01:57<01:04, 52.25it/s]Training CobwebTree:  66%|   | 6646/10000 [01:57<01:05, 51.01it/s]Training CobwebTree:  67%|   | 6652/10000 [01:57<01:04, 51.73it/s]Training CobwebTree:  67%|   | 6658/10000 [01:57<01:04, 52.03it/s]Training CobwebTree:  67%|   | 6664/10000 [01:57<01:06, 50.19it/s]Training CobwebTree:  67%|   | 6670/10000 [01:57<01:04, 51.43it/s]Training CobwebTree:  67%|   | 6676/10000 [01:57<01:03, 52.20it/s]Training CobwebTree:  67%|   | 6682/10000 [01:57<01:04, 51.82it/s]Training CobwebTree:  67%|   | 6688/10000 [01:58<01:01, 54.01it/s]Training CobwebTree:  67%|   | 6695/10000 [01:58<00:59, 55.20it/s]Training CobwebTree:  67%|   | 6701/10000 [01:58<00:59, 55.71it/s]Training CobwebTree:  67%|   | 6707/10000 [01:58<01:00, 54.74it/s]Training CobwebTree:  67%|   | 6714/10000 [01:58<00:57, 57.38it/s]Training CobwebTree:  67%|   | 6720/10000 [01:58<01:00, 54.27it/s]Training CobwebTree:  67%|   | 6726/10000 [01:58<01:02, 52.04it/s]Training CobwebTree:  67%|   | 6732/10000 [01:58<01:02, 52.31it/s]Training CobwebTree:  67%|   | 6738/10000 [01:58<01:01, 53.16it/s]Training CobwebTree:  67%|   | 6744/10000 [01:59<01:02, 51.75it/s]Training CobwebTree:  68%|   | 6750/10000 [01:59<01:03, 50.93it/s]Training CobwebTree:  68%|   | 6756/10000 [01:59<01:05, 49.51it/s]Training CobwebTree:  68%|   | 6762/10000 [01:59<01:02, 51.48it/s]Training CobwebTree:  68%|   | 6768/10000 [01:59<01:04, 49.82it/s]Training CobwebTree:  68%|   | 6774/10000 [01:59<01:06, 48.29it/s]Training CobwebTree:  68%|   | 6780/10000 [01:59<01:05, 49.17it/s]Training CobwebTree:  68%|   | 6785/10000 [01:59<01:06, 48.28it/s]Training CobwebTree:  68%|   | 6791/10000 [02:00<01:03, 50.37it/s]Training CobwebTree:  68%|   | 6797/10000 [02:00<01:06, 48.20it/s]Training CobwebTree:  68%|   | 6803/10000 [02:00<01:04, 49.86it/s]Training CobwebTree:  68%|   | 6809/10000 [02:00<01:03, 50.20it/s]Training CobwebTree:  68%|   | 6815/10000 [02:00<01:02, 50.95it/s]Training CobwebTree:  68%|   | 6821/10000 [02:00<01:03, 49.83it/s]Training CobwebTree:  68%|   | 6827/10000 [02:00<01:05, 48.75it/s]Training CobwebTree:  68%|   | 6833/10000 [02:00<01:03, 49.59it/s]Training CobwebTree:  68%|   | 6839/10000 [02:00<01:02, 50.81it/s]Training CobwebTree:  68%|   | 6845/10000 [02:01<01:01, 51.33it/s]Training CobwebTree:  69%|   | 6851/10000 [02:01<01:02, 50.17it/s]Training CobwebTree:  69%|   | 6857/10000 [02:01<01:02, 50.37it/s]Training CobwebTree:  69%|   | 6863/10000 [02:01<01:00, 52.09it/s]Training CobwebTree:  69%|   | 6869/10000 [02:01<01:01, 51.20it/s]Training CobwebTree:  69%|   | 6875/10000 [02:01<00:59, 52.86it/s]Training CobwebTree:  69%|   | 6881/10000 [02:01<00:58, 53.34it/s]Training CobwebTree:  69%|   | 6887/10000 [02:01<00:59, 52.18it/s]Training CobwebTree:  69%|   | 6893/10000 [02:02<01:01, 50.32it/s]Training CobwebTree:  69%|   | 6899/10000 [02:02<01:03, 48.96it/s]Training CobwebTree:  69%|   | 6905/10000 [02:02<01:01, 50.24it/s]Training CobwebTree:  69%|   | 6911/10000 [02:02<01:01, 50.53it/s]Training CobwebTree:  69%|   | 6917/10000 [02:02<01:00, 50.95it/s]Training CobwebTree:  69%|   | 6923/10000 [02:02<01:00, 51.01it/s]Training CobwebTree:  69%|   | 6929/10000 [02:02<01:00, 51.04it/s]Training CobwebTree:  69%|   | 6935/10000 [02:02<01:00, 50.35it/s]Training CobwebTree:  69%|   | 6941/10000 [02:02<01:01, 49.66it/s]Training CobwebTree:  69%|   | 6947/10000 [02:03<00:59, 51.49it/s]Training CobwebTree:  70%|   | 6953/10000 [02:03<00:57, 53.14it/s]Training CobwebTree:  70%|   | 6959/10000 [02:03<00:57, 53.11it/s]Training CobwebTree:  70%|   | 6965/10000 [02:03<00:58, 52.02it/s]Training CobwebTree:  70%|   | 6972/10000 [02:03<00:54, 55.19it/s]Training CobwebTree:  70%|   | 6978/10000 [02:03<00:57, 52.37it/s]Training CobwebTree:  70%|   | 6984/10000 [02:03<00:59, 50.82it/s]Training CobwebTree:  70%|   | 6990/10000 [02:03<01:02, 48.51it/s]Training CobwebTree:  70%|   | 6997/10000 [02:04<00:58, 51.72it/s]Training CobwebTree:  70%|   | 7003/10000 [02:04<00:58, 51.09it/s]Training CobwebTree:  70%|   | 7009/10000 [02:04<01:00, 49.36it/s]Training CobwebTree:  70%|   | 7014/10000 [02:04<01:02, 48.04it/s]Training CobwebTree:  70%|   | 7020/10000 [02:04<01:00, 49.11it/s]Training CobwebTree:  70%|   | 7025/10000 [02:04<01:01, 48.70it/s]Training CobwebTree:  70%|   | 7031/10000 [02:04<01:00, 49.14it/s]Training CobwebTree:  70%|   | 7036/10000 [02:04<01:03, 46.83it/s]Training CobwebTree:  70%|   | 7041/10000 [02:04<01:02, 47.50it/s]Training CobwebTree:  70%|   | 7046/10000 [02:05<01:02, 47.21it/s]Training CobwebTree:  71%|   | 7052/10000 [02:05<01:00, 49.11it/s]Training CobwebTree:  71%|   | 7057/10000 [02:05<01:02, 47.45it/s]Training CobwebTree:  71%|   | 7062/10000 [02:05<01:04, 45.46it/s]Training CobwebTree:  71%|   | 7068/10000 [02:05<01:01, 47.75it/s]Training CobwebTree:  71%|   | 7073/10000 [02:05<01:00, 48.07it/s]Training CobwebTree:  71%|   | 7078/10000 [02:05<01:03, 46.19it/s]Training CobwebTree:  71%|   | 7084/10000 [02:05<01:00, 48.58it/s]Training CobwebTree:  71%|   | 7089/10000 [02:05<01:01, 47.60it/s]Training CobwebTree:  71%|   | 7095/10000 [02:06<01:00, 48.30it/s]Training CobwebTree:  71%|   | 7100/10000 [02:06<01:02, 46.62it/s]Training CobwebTree:  71%|   | 7106/10000 [02:06<00:59, 48.76it/s]Training CobwebTree:  71%|   | 7112/10000 [02:06<00:58, 49.74it/s]Training CobwebTree:  71%|   | 7117/10000 [02:06<01:00, 47.40it/s]Training CobwebTree:  71%|   | 7123/10000 [02:06<00:57, 50.01it/s]Training CobwebTree:  71%|  | 7129/10000 [02:06<00:57, 49.60it/s]Training CobwebTree:  71%|  | 7134/10000 [02:06<00:59, 48.38it/s]Training CobwebTree:  71%|  | 7139/10000 [02:07<01:01, 46.74it/s]Training CobwebTree:  71%|  | 7145/10000 [02:07<00:59, 47.83it/s]Training CobwebTree:  72%|  | 7151/10000 [02:07<00:57, 49.24it/s]Training CobwebTree:  72%|  | 7157/10000 [02:07<00:55, 50.93it/s]Training CobwebTree:  72%|  | 7163/10000 [02:07<00:56, 50.36it/s]Training CobwebTree:  72%|  | 7169/10000 [02:07<00:57, 49.22it/s]Training CobwebTree:  72%|  | 7174/10000 [02:07<00:57, 49.20it/s]Training CobwebTree:  72%|  | 7180/10000 [02:07<00:55, 51.15it/s]Training CobwebTree:  72%|  | 7186/10000 [02:07<00:53, 52.39it/s]Training CobwebTree:  72%|  | 7192/10000 [02:08<00:52, 53.42it/s]Training CobwebTree:  72%|  | 7198/10000 [02:08<00:54, 51.81it/s]Training CobwebTree:  72%|  | 7204/10000 [02:08<00:54, 51.21it/s]Training CobwebTree:  72%|  | 7210/10000 [02:08<00:54, 51.26it/s]Training CobwebTree:  72%|  | 7216/10000 [02:08<00:54, 51.14it/s]Training CobwebTree:  72%|  | 7222/10000 [02:08<01:43, 26.82it/s]Training CobwebTree:  72%|  | 7227/10000 [02:09<01:31, 30.42it/s]Training CobwebTree:  72%|  | 7232/10000 [02:09<01:21, 33.95it/s]Training CobwebTree:  72%|  | 7238/10000 [02:09<01:13, 37.57it/s]Training CobwebTree:  72%|  | 7245/10000 [02:09<01:03, 43.58it/s]Training CobwebTree:  73%|  | 7251/10000 [02:09<00:58, 46.80it/s]Training CobwebTree:  73%|  | 7257/10000 [02:09<00:56, 48.65it/s]Training CobwebTree:  73%|  | 7263/10000 [02:09<00:54, 50.06it/s]Training CobwebTree:  73%|  | 7269/10000 [02:09<00:53, 51.31it/s]Training CobwebTree:  73%|  | 7275/10000 [02:09<00:54, 50.19it/s]Training CobwebTree:  73%|  | 7281/10000 [02:10<00:54, 50.19it/s]Training CobwebTree:  73%|  | 7287/10000 [02:10<00:53, 50.80it/s]Training CobwebTree:  73%|  | 7293/10000 [02:10<00:53, 50.98it/s]Training CobwebTree:  73%|  | 7299/10000 [02:10<00:56, 47.89it/s]Training CobwebTree:  73%|  | 7305/10000 [02:10<00:54, 49.03it/s]Training CobwebTree:  73%|  | 7310/10000 [02:10<00:54, 49.04it/s]Training CobwebTree:  73%|  | 7316/10000 [02:10<00:52, 51.15it/s]Training CobwebTree:  73%|  | 7322/10000 [02:10<00:53, 49.86it/s]Training CobwebTree:  73%|  | 7328/10000 [02:11<00:53, 49.65it/s]Training CobwebTree:  73%|  | 7333/10000 [02:11<00:54, 49.38it/s]Training CobwebTree:  73%|  | 7340/10000 [02:11<00:50, 52.72it/s]Training CobwebTree:  73%|  | 7346/10000 [02:11<00:50, 52.32it/s]Training CobwebTree:  74%|  | 7352/10000 [02:11<00:50, 52.21it/s]Training CobwebTree:  74%|  | 7358/10000 [02:11<00:52, 50.09it/s]Training CobwebTree:  74%|  | 7364/10000 [02:11<00:50, 52.43it/s]Training CobwebTree:  74%|  | 7370/10000 [02:11<00:49, 53.63it/s]Training CobwebTree:  74%|  | 7376/10000 [02:11<00:48, 54.21it/s]Training CobwebTree:  74%|  | 7382/10000 [02:12<00:48, 54.01it/s]Training CobwebTree:  74%|  | 7388/10000 [02:12<00:50, 51.86it/s]Training CobwebTree:  74%|  | 7394/10000 [02:12<00:51, 50.41it/s]Training CobwebTree:  74%|  | 7400/10000 [02:12<00:49, 52.21it/s]Training CobwebTree:  74%|  | 7406/10000 [02:12<00:51, 50.33it/s]Training CobwebTree:  74%|  | 7412/10000 [02:12<00:50, 51.09it/s]Training CobwebTree:  74%|  | 7418/10000 [02:12<00:51, 49.79it/s]Training CobwebTree:  74%|  | 7424/10000 [02:12<00:51, 50.02it/s]Training CobwebTree:  74%|  | 7430/10000 [02:13<00:49, 51.85it/s]Training CobwebTree:  74%|  | 7436/10000 [02:13<00:49, 52.18it/s]Training CobwebTree:  74%|  | 7442/10000 [02:13<00:50, 50.63it/s]Training CobwebTree:  74%|  | 7448/10000 [02:13<00:52, 49.01it/s]Training CobwebTree:  75%|  | 7453/10000 [02:13<00:52, 48.66it/s]Training CobwebTree:  75%|  | 7459/10000 [02:13<00:51, 49.67it/s]Training CobwebTree:  75%|  | 7464/10000 [02:13<00:52, 47.94it/s]Training CobwebTree:  75%|  | 7469/10000 [02:13<00:53, 47.28it/s]Training CobwebTree:  75%|  | 7474/10000 [02:13<00:53, 46.99it/s]Training CobwebTree:  75%|  | 7480/10000 [02:14<00:51, 48.57it/s]Training CobwebTree:  75%|  | 7485/10000 [02:14<00:53, 47.26it/s]Training CobwebTree:  75%|  | 7490/10000 [02:14<00:54, 46.40it/s]Training CobwebTree:  75%|  | 7496/10000 [02:14<00:51, 48.92it/s]Training CobwebTree:  75%|  | 7501/10000 [02:14<00:52, 47.33it/s]Training CobwebTree:  75%|  | 7506/10000 [02:14<00:53, 46.75it/s]Training CobwebTree:  75%|  | 7511/10000 [02:14<00:53, 46.91it/s]Training CobwebTree:  75%|  | 7516/10000 [02:14<00:54, 45.93it/s]Training CobwebTree:  75%|  | 7522/10000 [02:14<00:52, 47.03it/s]Training CobwebTree:  75%|  | 7527/10000 [02:15<00:52, 46.96it/s]Training CobwebTree:  75%|  | 7533/10000 [02:15<00:50, 49.22it/s]Training CobwebTree:  75%|  | 7538/10000 [02:15<00:50, 48.74it/s]Training CobwebTree:  75%|  | 7544/10000 [02:15<00:50, 49.08it/s]Training CobwebTree:  75%|  | 7549/10000 [02:15<00:51, 47.88it/s]Training CobwebTree:  76%|  | 7554/10000 [02:15<00:51, 47.42it/s]Training CobwebTree:  76%|  | 7560/10000 [02:15<00:49, 49.44it/s]Training CobwebTree:  76%|  | 7566/10000 [02:15<00:47, 51.40it/s]Training CobwebTree:  76%|  | 7572/10000 [02:15<00:48, 49.91it/s]Training CobwebTree:  76%|  | 7578/10000 [02:16<00:51, 47.18it/s]Training CobwebTree:  76%|  | 7584/10000 [02:16<00:49, 48.48it/s]Training CobwebTree:  76%|  | 7589/10000 [02:16<00:49, 48.87it/s]Training CobwebTree:  76%|  | 7595/10000 [02:16<00:49, 48.90it/s]Training CobwebTree:  76%|  | 7601/10000 [02:16<00:47, 50.24it/s]Training CobwebTree:  76%|  | 7607/10000 [02:16<00:48, 49.43it/s]Training CobwebTree:  76%|  | 7613/10000 [02:16<00:47, 50.33it/s]Training CobwebTree:  76%|  | 7619/10000 [02:16<00:48, 49.09it/s]Training CobwebTree:  76%|  | 7625/10000 [02:17<00:47, 49.57it/s]Training CobwebTree:  76%|  | 7630/10000 [02:17<00:48, 49.31it/s]Training CobwebTree:  76%|  | 7635/10000 [02:17<00:50, 46.75it/s]Training CobwebTree:  76%|  | 7640/10000 [02:17<00:51, 46.22it/s]Training CobwebTree:  76%|  | 7646/10000 [02:17<00:49, 48.00it/s]Training CobwebTree:  77%|  | 7652/10000 [02:17<00:46, 50.41it/s]Training CobwebTree:  77%|  | 7658/10000 [02:17<00:48, 47.92it/s]Training CobwebTree:  77%|  | 7664/10000 [02:17<00:46, 49.94it/s]Training CobwebTree:  77%|  | 7670/10000 [02:17<00:47, 49.24it/s]Training CobwebTree:  77%|  | 7675/10000 [02:18<00:47, 49.41it/s]Training CobwebTree:  77%|  | 7680/10000 [02:18<00:48, 48.11it/s]Training CobwebTree:  77%|  | 7685/10000 [02:18<00:49, 47.04it/s]Training CobwebTree:  77%|  | 7691/10000 [02:18<00:46, 49.84it/s]Training CobwebTree:  77%|  | 7697/10000 [02:18<00:47, 48.89it/s]Training CobwebTree:  77%|  | 7703/10000 [02:18<00:45, 50.33it/s]Training CobwebTree:  77%|  | 7709/10000 [02:18<00:46, 49.36it/s]Training CobwebTree:  77%|  | 7715/10000 [02:18<00:45, 50.20it/s]Training CobwebTree:  77%|  | 7721/10000 [02:19<00:45, 50.10it/s]Training CobwebTree:  77%|  | 7727/10000 [02:19<00:45, 49.87it/s]Training CobwebTree:  77%|  | 7733/10000 [02:19<00:45, 49.85it/s]Training CobwebTree:  77%|  | 7738/10000 [02:19<00:45, 49.71it/s]Training CobwebTree:  77%|  | 7744/10000 [02:19<00:45, 49.47it/s]Training CobwebTree:  77%|  | 7749/10000 [02:19<00:46, 48.25it/s]Training CobwebTree:  78%|  | 7754/10000 [02:19<00:46, 48.15it/s]Training CobwebTree:  78%|  | 7759/10000 [02:19<00:46, 48.15it/s]Training CobwebTree:  78%|  | 7765/10000 [02:19<00:44, 50.60it/s]Training CobwebTree:  78%|  | 7771/10000 [02:20<00:43, 50.91it/s]Training CobwebTree:  78%|  | 7777/10000 [02:20<00:42, 51.98it/s]Training CobwebTree:  78%|  | 7783/10000 [02:20<00:43, 50.41it/s]Training CobwebTree:  78%|  | 7789/10000 [02:20<00:43, 50.35it/s]Training CobwebTree:  78%|  | 7795/10000 [02:20<00:44, 49.18it/s]Training CobwebTree:  78%|  | 7800/10000 [02:20<00:45, 47.98it/s]Training CobwebTree:  78%|  | 7805/10000 [02:20<00:47, 46.25it/s]Training CobwebTree:  78%|  | 7810/10000 [02:20<00:47, 45.69it/s]Training CobwebTree:  78%|  | 7815/10000 [02:20<00:46, 46.74it/s]Training CobwebTree:  78%|  | 7820/10000 [02:21<00:46, 47.03it/s]Training CobwebTree:  78%|  | 7825/10000 [02:21<00:45, 47.37it/s]Training CobwebTree:  78%|  | 7831/10000 [02:21<00:44, 49.05it/s]Training CobwebTree:  78%|  | 7836/10000 [02:21<00:43, 49.21it/s]Training CobwebTree:  78%|  | 7842/10000 [02:21<00:42, 50.45it/s]Training CobwebTree:  78%|  | 7848/10000 [02:21<00:44, 48.70it/s]Training CobwebTree:  79%|  | 7853/10000 [02:21<00:43, 49.01it/s]Training CobwebTree:  79%|  | 7858/10000 [02:21<00:44, 47.76it/s]Training CobwebTree:  79%|  | 7863/10000 [02:21<00:46, 46.14it/s]Training CobwebTree:  79%|  | 7869/10000 [02:22<00:45, 46.57it/s]Training CobwebTree:  79%|  | 7874/10000 [02:22<00:46, 45.91it/s]Training CobwebTree:  79%|  | 7880/10000 [02:22<00:45, 46.57it/s]Training CobwebTree:  79%|  | 7886/10000 [02:22<00:43, 48.68it/s]Training CobwebTree:  79%|  | 7891/10000 [02:22<00:43, 48.93it/s]Training CobwebTree:  79%|  | 7896/10000 [02:22<00:43, 48.47it/s]Training CobwebTree:  79%|  | 7901/10000 [02:22<00:43, 47.94it/s]Training CobwebTree:  79%|  | 7907/10000 [02:22<00:41, 50.07it/s]Training CobwebTree:  79%|  | 7913/10000 [02:22<00:44, 46.48it/s]Training CobwebTree:  79%|  | 7919/10000 [02:23<00:42, 49.42it/s]Training CobwebTree:  79%|  | 7925/10000 [02:23<00:43, 47.96it/s]Training CobwebTree:  79%|  | 7931/10000 [02:23<00:41, 50.17it/s]Training CobwebTree:  79%|  | 7937/10000 [02:23<00:42, 48.89it/s]Training CobwebTree:  79%|  | 7942/10000 [02:23<00:42, 48.73it/s]Training CobwebTree:  79%|  | 7948/10000 [02:23<00:42, 48.14it/s]Training CobwebTree:  80%|  | 7953/10000 [02:23<00:42, 48.35it/s]Training CobwebTree:  80%|  | 7958/10000 [02:23<00:43, 46.99it/s]Training CobwebTree:  80%|  | 7963/10000 [02:24<00:43, 47.35it/s]Training CobwebTree:  80%|  | 7968/10000 [02:24<00:42, 47.45it/s]Training CobwebTree:  80%|  | 7974/10000 [02:24<00:41, 49.14it/s]Training CobwebTree:  80%|  | 7979/10000 [02:24<00:42, 47.51it/s]Training CobwebTree:  80%|  | 7985/10000 [02:24<00:40, 49.20it/s]Training CobwebTree:  80%|  | 7991/10000 [02:24<00:41, 48.75it/s]Training CobwebTree:  80%|  | 7996/10000 [02:24<00:41, 47.85it/s]Training CobwebTree:  80%|  | 8001/10000 [02:24<00:42, 47.36it/s]Training CobwebTree:  80%|  | 8006/10000 [02:24<00:42, 47.29it/s]Training CobwebTree:  80%|  | 8011/10000 [02:25<00:43, 45.69it/s]Training CobwebTree:  80%|  | 8017/10000 [02:25<00:41, 47.93it/s]Training CobwebTree:  80%|  | 8023/10000 [02:25<00:40, 49.10it/s]Training CobwebTree:  80%|  | 8028/10000 [02:25<00:41, 48.01it/s]Training CobwebTree:  80%|  | 8033/10000 [02:25<00:42, 46.75it/s]Training CobwebTree:  80%|  | 8039/10000 [02:25<00:40, 48.73it/s]Training CobwebTree:  80%|  | 8045/10000 [02:25<00:39, 49.08it/s]Training CobwebTree:  80%|  | 8050/10000 [02:25<00:40, 47.99it/s]Training CobwebTree:  81%|  | 8056/10000 [02:25<00:39, 48.68it/s]Training CobwebTree:  81%|  | 8062/10000 [02:26<00:38, 50.93it/s]Training CobwebTree:  81%|  | 8068/10000 [02:26<00:40, 47.96it/s]Training CobwebTree:  81%|  | 8074/10000 [02:26<00:38, 50.16it/s]Training CobwebTree:  81%|  | 8080/10000 [02:26<00:39, 48.64it/s]Training CobwebTree:  81%|  | 8085/10000 [02:26<00:40, 47.45it/s]Training CobwebTree:  81%|  | 8091/10000 [02:26<00:39, 48.66it/s]Training CobwebTree:  81%|  | 8097/10000 [02:26<00:38, 49.82it/s]Training CobwebTree:  81%|  | 8103/10000 [02:26<00:39, 47.49it/s]Training CobwebTree:  81%|  | 8109/10000 [02:27<00:38, 48.69it/s]Training CobwebTree:  81%|  | 8114/10000 [02:27<00:38, 48.82it/s]Training CobwebTree:  81%|  | 8119/10000 [02:27<00:38, 48.60it/s]Training CobwebTree:  81%|  | 8124/10000 [02:27<00:40, 46.90it/s]Training CobwebTree:  81%| | 8130/10000 [02:27<00:39, 46.98it/s]Training CobwebTree:  81%| | 8135/10000 [02:27<00:39, 47.49it/s]Training CobwebTree:  81%| | 8141/10000 [02:27<00:37, 49.04it/s]Training CobwebTree:  81%| | 8147/10000 [02:27<00:36, 50.12it/s]Training CobwebTree:  82%| | 8153/10000 [02:27<00:37, 49.21it/s]Training CobwebTree:  82%| | 8158/10000 [02:28<00:37, 49.18it/s]Training CobwebTree:  82%| | 8163/10000 [02:28<00:38, 47.31it/s]Training CobwebTree:  82%| | 8169/10000 [02:28<00:37, 49.40it/s]Training CobwebTree:  82%| | 8174/10000 [02:28<00:36, 49.55it/s]Training CobwebTree:  82%| | 8179/10000 [02:28<00:36, 49.52it/s]Training CobwebTree:  82%| | 8184/10000 [02:28<00:36, 49.12it/s]Training CobwebTree:  82%| | 8189/10000 [02:28<00:37, 47.81it/s]Training CobwebTree:  82%| | 8194/10000 [02:28<00:39, 45.42it/s]Training CobwebTree:  82%| | 8199/10000 [02:28<00:38, 46.66it/s]Training CobwebTree:  82%| | 8205/10000 [02:29<00:38, 46.74it/s]Training CobwebTree:  82%| | 8210/10000 [02:29<00:39, 45.86it/s]Training CobwebTree:  82%| | 8217/10000 [02:29<00:35, 50.21it/s]Training CobwebTree:  82%| | 8223/10000 [02:29<00:35, 49.57it/s]Training CobwebTree:  82%| | 8228/10000 [02:29<00:35, 49.32it/s]Training CobwebTree:  82%| | 8234/10000 [02:29<00:36, 48.75it/s]Training CobwebTree:  82%| | 8239/10000 [02:29<00:36, 48.63it/s]Training CobwebTree:  82%| | 8245/10000 [02:29<00:35, 49.41it/s]Training CobwebTree:  83%| | 8251/10000 [02:29<00:34, 50.99it/s]Training CobwebTree:  83%| | 8257/10000 [02:30<00:33, 51.45it/s]Training CobwebTree:  83%| | 8263/10000 [02:30<00:34, 50.35it/s]Training CobwebTree:  83%| | 8269/10000 [02:30<00:33, 51.39it/s]Training CobwebTree:  83%| | 8275/10000 [02:30<00:34, 50.25it/s]Training CobwebTree:  83%| | 8281/10000 [02:30<00:33, 51.07it/s]Training CobwebTree:  83%| | 8287/10000 [02:30<00:35, 48.64it/s]Training CobwebTree:  83%| | 8292/10000 [02:30<00:34, 48.83it/s]Training CobwebTree:  83%| | 8298/10000 [02:30<00:34, 48.90it/s]Training CobwebTree:  83%| | 8303/10000 [02:31<00:35, 48.15it/s]Training CobwebTree:  83%| | 8308/10000 [02:31<00:35, 47.15it/s]Training CobwebTree:  83%| | 8314/10000 [02:31<00:33, 49.94it/s]Training CobwebTree:  83%| | 8320/10000 [02:31<00:34, 49.18it/s]Training CobwebTree:  83%| | 8326/10000 [02:31<00:33, 50.53it/s]Training CobwebTree:  83%| | 8332/10000 [02:31<00:32, 50.95it/s]Training CobwebTree:  83%| | 8338/10000 [02:31<00:32, 50.83it/s]Training CobwebTree:  83%| | 8344/10000 [02:31<00:31, 51.76it/s]Training CobwebTree:  84%| | 8350/10000 [02:31<00:31, 52.02it/s]Training CobwebTree:  84%| | 8356/10000 [02:32<00:32, 50.13it/s]Training CobwebTree:  84%| | 8362/10000 [02:32<00:33, 49.14it/s]Training CobwebTree:  84%| | 8368/10000 [02:32<00:32, 49.55it/s]Training CobwebTree:  84%| | 8374/10000 [02:32<00:31, 50.90it/s]Training CobwebTree:  84%| | 8380/10000 [02:32<00:31, 50.95it/s]Training CobwebTree:  84%| | 8386/10000 [02:32<00:31, 50.53it/s]Training CobwebTree:  84%| | 8392/10000 [02:32<00:32, 49.50it/s]Training CobwebTree:  84%| | 8397/10000 [02:32<00:33, 47.94it/s]Training CobwebTree:  84%| | 8403/10000 [02:32<00:31, 50.30it/s]Training CobwebTree:  84%| | 8409/10000 [02:33<00:32, 48.81it/s]Training CobwebTree:  84%| | 8414/10000 [02:33<00:32, 48.86it/s]Training CobwebTree:  84%| | 8419/10000 [02:33<00:34, 45.78it/s]Training CobwebTree:  84%| | 8424/10000 [02:33<00:33, 46.53it/s]Training CobwebTree:  84%| | 8429/10000 [02:33<00:33, 46.83it/s]Training CobwebTree:  84%| | 8435/10000 [02:33<00:31, 49.23it/s]Training CobwebTree:  84%| | 8440/10000 [02:33<00:31, 48.97it/s]Training CobwebTree:  84%| | 8446/10000 [02:33<00:30, 50.23it/s]Training CobwebTree:  85%| | 8452/10000 [02:34<00:31, 49.24it/s]Training CobwebTree:  85%| | 8458/10000 [02:34<00:31, 49.59it/s]Training CobwebTree:  85%| | 8463/10000 [02:34<00:31, 49.40it/s]Training CobwebTree:  85%| | 8468/10000 [02:34<00:31, 48.34it/s]Training CobwebTree:  85%| | 8474/10000 [02:34<00:31, 49.08it/s]Training CobwebTree:  85%| | 8479/10000 [02:34<00:31, 48.66it/s]Training CobwebTree:  85%| | 8485/10000 [02:34<00:30, 48.95it/s]Training CobwebTree:  85%| | 8490/10000 [02:34<00:31, 48.46it/s]Training CobwebTree:  85%| | 8495/10000 [02:34<00:32, 45.77it/s]Training CobwebTree:  85%| | 8500/10000 [02:35<00:32, 45.78it/s]Training CobwebTree:  85%| | 8506/10000 [02:35<00:31, 47.37it/s]Training CobwebTree:  85%| | 8511/10000 [02:35<00:31, 47.61it/s]Training CobwebTree:  85%| | 8517/10000 [02:35<00:29, 50.77it/s]Training CobwebTree:  85%| | 8523/10000 [02:35<00:29, 49.85it/s]Training CobwebTree:  85%| | 8529/10000 [02:35<00:30, 48.61it/s]Training CobwebTree:  85%| | 8535/10000 [02:35<00:29, 49.47it/s]Training CobwebTree:  85%| | 8541/10000 [02:35<00:27, 52.24it/s]Training CobwebTree:  85%| | 8547/10000 [02:35<00:28, 51.20it/s]Training CobwebTree:  86%| | 8553/10000 [02:36<00:28, 51.30it/s]Training CobwebTree:  86%| | 8559/10000 [02:36<00:27, 51.98it/s]Training CobwebTree:  86%| | 8565/10000 [02:36<00:29, 49.45it/s]Training CobwebTree:  86%| | 8570/10000 [02:36<00:29, 49.07it/s]Training CobwebTree:  86%| | 8575/10000 [02:36<00:29, 49.01it/s]Training CobwebTree:  86%| | 8580/10000 [02:36<00:29, 47.41it/s]Training CobwebTree:  86%| | 8586/10000 [02:36<00:29, 48.45it/s]Training CobwebTree:  86%| | 8591/10000 [02:36<00:30, 46.78it/s]Training CobwebTree:  86%| | 8596/10000 [02:36<00:30, 46.66it/s]Training CobwebTree:  86%| | 8602/10000 [02:37<00:28, 48.58it/s]Training CobwebTree:  86%| | 8607/10000 [02:37<00:29, 46.90it/s]Training CobwebTree:  86%| | 8612/10000 [02:37<00:29, 47.30it/s]Training CobwebTree:  86%| | 8617/10000 [02:37<00:30, 45.31it/s]Training CobwebTree:  86%| | 8622/10000 [02:37<00:30, 45.69it/s]Training CobwebTree:  86%| | 8627/10000 [02:37<00:30, 44.93it/s]Training CobwebTree:  86%| | 8633/10000 [02:37<00:29, 46.65it/s]Training CobwebTree:  86%| | 8638/10000 [02:37<00:29, 46.96it/s]Training CobwebTree:  86%| | 8643/10000 [02:37<00:28, 47.19it/s]Training CobwebTree:  86%| | 8649/10000 [02:38<00:28, 47.44it/s]Training CobwebTree:  87%| | 8655/10000 [02:38<00:27, 49.31it/s]Training CobwebTree:  87%| | 8660/10000 [02:38<00:28, 47.82it/s]Training CobwebTree:  87%| | 8666/10000 [02:38<00:27, 48.97it/s]Training CobwebTree:  87%| | 8671/10000 [02:38<00:27, 48.53it/s]Training CobwebTree:  87%| | 8677/10000 [02:38<00:26, 50.00it/s]Training CobwebTree:  87%| | 8683/10000 [02:38<00:26, 50.65it/s]Training CobwebTree:  87%| | 8689/10000 [02:38<00:26, 50.25it/s]Training CobwebTree:  87%| | 8695/10000 [02:39<00:26, 49.60it/s]Training CobwebTree:  87%| | 8701/10000 [02:39<00:26, 49.73it/s]Training CobwebTree:  87%| | 8706/10000 [02:39<00:26, 48.90it/s]Training CobwebTree:  87%| | 8711/10000 [02:39<00:27, 47.22it/s]Training CobwebTree:  87%| | 8716/10000 [02:39<00:27, 47.15it/s]Training CobwebTree:  87%| | 8721/10000 [02:39<00:28, 45.23it/s]Training CobwebTree:  87%| | 8726/10000 [02:39<00:27, 46.42it/s]Training CobwebTree:  87%| | 8731/10000 [02:39<00:28, 44.32it/s]Training CobwebTree:  87%| | 8736/10000 [02:39<00:27, 45.65it/s]Training CobwebTree:  87%| | 8741/10000 [02:40<00:27, 46.29it/s]Training CobwebTree:  87%| | 8746/10000 [02:40<00:26, 46.62it/s]Training CobwebTree:  88%| | 8751/10000 [02:40<00:27, 46.04it/s]Training CobwebTree:  88%| | 8756/10000 [02:40<00:28, 44.43it/s]Training CobwebTree:  88%| | 8761/10000 [02:40<00:28, 42.89it/s]Training CobwebTree:  88%| | 8766/10000 [02:40<00:27, 44.56it/s]Training CobwebTree:  88%| | 8771/10000 [02:40<00:27, 44.02it/s]Training CobwebTree:  88%| | 8776/10000 [02:40<00:28, 43.60it/s]Training CobwebTree:  88%| | 8781/10000 [02:40<00:27, 44.68it/s]Training CobwebTree:  88%| | 8786/10000 [02:41<00:26, 45.28it/s]Training CobwebTree:  88%| | 8791/10000 [02:41<00:26, 45.37it/s]Training CobwebTree:  88%| | 8796/10000 [02:41<00:25, 46.32it/s]Training CobwebTree:  88%| | 8801/10000 [02:41<00:25, 46.55it/s]Training CobwebTree:  88%| | 8806/10000 [02:41<00:25, 46.76it/s]Training CobwebTree:  88%| | 8811/10000 [02:41<00:26, 45.66it/s]Training CobwebTree:  88%| | 8817/10000 [02:41<00:25, 46.93it/s]Training CobwebTree:  88%| | 8823/10000 [02:41<00:24, 48.67it/s]Training CobwebTree:  88%| | 8828/10000 [02:41<00:24, 48.45it/s]Training CobwebTree:  88%| | 8834/10000 [02:42<00:23, 48.84it/s]Training CobwebTree:  88%| | 8839/10000 [02:42<00:24, 47.50it/s]Training CobwebTree:  88%| | 8845/10000 [02:42<00:23, 48.37it/s]Training CobwebTree:  88%| | 8850/10000 [02:42<00:23, 48.62it/s]Training CobwebTree:  89%| | 8855/10000 [02:42<00:24, 47.33it/s]Training CobwebTree:  89%| | 8860/10000 [02:42<00:24, 46.49it/s]Training CobwebTree:  89%| | 8865/10000 [02:42<00:24, 47.27it/s]Training CobwebTree:  89%| | 8871/10000 [02:42<00:22, 49.60it/s]Training CobwebTree:  89%| | 8877/10000 [02:42<00:21, 51.83it/s]Training CobwebTree:  89%| | 8883/10000 [02:43<00:21, 51.78it/s]Training CobwebTree:  89%| | 8889/10000 [02:43<00:21, 51.08it/s]Training CobwebTree:  89%| | 8895/10000 [02:43<00:21, 52.08it/s]Training CobwebTree:  89%| | 8901/10000 [02:43<00:22, 49.03it/s]Training CobwebTree:  89%| | 8907/10000 [02:43<00:22, 49.51it/s]Training CobwebTree:  89%| | 8912/10000 [02:43<00:22, 48.42it/s]Training CobwebTree:  89%| | 8917/10000 [02:43<00:22, 47.81it/s]Training CobwebTree:  89%| | 8923/10000 [02:43<00:22, 47.71it/s]Training CobwebTree:  89%| | 8928/10000 [02:43<00:22, 47.33it/s]Training CobwebTree:  89%| | 8933/10000 [02:44<00:22, 47.97it/s]Training CobwebTree:  89%| | 8938/10000 [02:44<00:22, 46.53it/s]Training CobwebTree:  89%| | 8944/10000 [02:44<00:22, 47.85it/s]Training CobwebTree:  89%| | 8949/10000 [02:44<00:22, 46.54it/s]Training CobwebTree:  90%| | 8954/10000 [02:44<00:22, 46.95it/s]Training CobwebTree:  90%| | 8959/10000 [02:44<00:23, 45.12it/s]Training CobwebTree:  90%| | 8965/10000 [02:44<00:22, 46.66it/s]Training CobwebTree:  90%| | 8970/10000 [02:44<00:22, 45.79it/s]Training CobwebTree:  90%| | 8975/10000 [02:44<00:22, 46.14it/s]Training CobwebTree:  90%| | 8980/10000 [02:45<00:21, 46.82it/s]Training CobwebTree:  90%| | 8986/10000 [02:45<00:21, 47.00it/s]Training CobwebTree:  90%| | 8992/10000 [02:45<00:20, 48.18it/s]Training CobwebTree:  90%| | 8997/10000 [02:45<00:21, 47.59it/s]Training CobwebTree:  90%| | 9002/10000 [02:45<00:22, 44.72it/s]Training CobwebTree:  90%| | 9008/10000 [02:45<00:21, 45.99it/s]Training CobwebTree:  90%| | 9013/10000 [02:45<00:21, 45.11it/s]Training CobwebTree:  90%| | 9018/10000 [02:45<00:21, 45.55it/s]Training CobwebTree:  90%| | 9024/10000 [02:46<00:20, 46.60it/s]Training CobwebTree:  90%| | 9029/10000 [02:46<00:20, 46.45it/s]Training CobwebTree:  90%| | 9034/10000 [02:46<00:21, 45.96it/s]Training CobwebTree:  90%| | 9039/10000 [02:46<00:21, 44.93it/s]Training CobwebTree:  90%| | 9044/10000 [02:46<00:21, 44.63it/s]Training CobwebTree:  90%| | 9049/10000 [02:46<00:21, 44.59it/s]Training CobwebTree:  91%| | 9054/10000 [02:46<00:21, 44.48it/s]Training CobwebTree:  91%| | 9059/10000 [02:46<00:20, 45.84it/s]Training CobwebTree:  91%| | 9064/10000 [02:46<00:20, 45.88it/s]Training CobwebTree:  91%| | 9069/10000 [02:47<00:20, 45.13it/s]Training CobwebTree:  91%| | 9074/10000 [02:47<00:20, 45.27it/s]Training CobwebTree:  91%| | 9080/10000 [02:47<00:20, 45.65it/s]Training CobwebTree:  91%| | 9085/10000 [02:47<00:20, 45.71it/s]Training CobwebTree:  91%| | 9090/10000 [02:47<00:20, 44.38it/s]Training CobwebTree:  91%| | 9095/10000 [02:47<00:20, 44.22it/s]Training CobwebTree:  91%| | 9100/10000 [02:47<00:20, 44.12it/s]Training CobwebTree:  91%| | 9105/10000 [02:47<00:19, 45.60it/s]Training CobwebTree:  91%| | 9111/10000 [02:47<00:18, 47.49it/s]Training CobwebTree:  91%| | 9116/10000 [02:48<00:18, 47.53it/s]Training CobwebTree:  91%| | 9122/10000 [02:48<00:17, 48.97it/s]Training CobwebTree:  91%|| 9127/10000 [02:48<00:18, 46.54it/s]Training CobwebTree:  91%|| 9132/10000 [02:48<00:18, 47.46it/s]Training CobwebTree:  91%|| 9138/10000 [02:48<00:17, 49.23it/s]Training CobwebTree:  91%|| 9143/10000 [02:48<00:18, 45.51it/s]Training CobwebTree:  91%|| 9148/10000 [02:48<00:18, 44.87it/s]Training CobwebTree:  92%|| 9153/10000 [02:48<00:18, 46.02it/s]Training CobwebTree:  92%|| 9158/10000 [02:48<00:17, 46.83it/s]Training CobwebTree:  92%|| 9163/10000 [02:49<00:18, 45.58it/s]Training CobwebTree:  92%|| 9168/10000 [02:49<00:17, 46.30it/s]Training CobwebTree:  92%|| 9174/10000 [02:49<00:16, 50.00it/s]Training CobwebTree:  92%|| 9180/10000 [02:49<00:17, 47.60it/s]Training CobwebTree:  92%|| 9186/10000 [02:49<00:17, 47.34it/s]Training CobwebTree:  92%|| 9191/10000 [02:49<00:17, 45.27it/s]Training CobwebTree:  92%|| 9196/10000 [02:49<00:18, 44.64it/s]Training CobwebTree:  92%|| 9201/10000 [02:49<00:18, 42.76it/s]Training CobwebTree:  92%|| 9206/10000 [02:50<00:17, 44.28it/s]Training CobwebTree:  92%|| 9212/10000 [02:50<00:16, 46.52it/s]Training CobwebTree:  92%|| 9218/10000 [02:50<00:16, 48.53it/s]Training CobwebTree:  92%|| 9223/10000 [02:50<00:16, 48.16it/s]Training CobwebTree:  92%|| 9229/10000 [02:50<00:15, 49.95it/s]Training CobwebTree:  92%|| 9235/10000 [02:50<00:14, 51.17it/s]Training CobwebTree:  92%|| 9241/10000 [02:50<00:15, 50.25it/s]Training CobwebTree:  92%|| 9247/10000 [02:50<00:15, 48.88it/s]Training CobwebTree:  93%|| 9253/10000 [02:50<00:14, 50.00it/s]Training CobwebTree:  93%|| 9259/10000 [02:51<00:14, 52.26it/s]Training CobwebTree:  93%|| 9265/10000 [02:51<00:14, 51.89it/s]Training CobwebTree:  93%|| 9271/10000 [02:51<00:13, 52.40it/s]Training CobwebTree:  93%|| 9277/10000 [02:51<00:14, 50.38it/s]Training CobwebTree:  93%|| 9283/10000 [02:51<00:14, 48.90it/s]Training CobwebTree:  93%|| 9289/10000 [02:51<00:14, 49.33it/s]Training CobwebTree:  93%|| 9294/10000 [02:51<00:14, 49.45it/s]Training CobwebTree:  93%|| 9299/10000 [02:51<00:14, 47.42it/s]Training CobwebTree:  93%|| 9305/10000 [02:51<00:13, 49.96it/s]Training CobwebTree:  93%|| 9311/10000 [02:52<00:13, 49.56it/s]Training CobwebTree:  93%|| 9317/10000 [02:52<00:13, 51.65it/s]Training CobwebTree:  93%|| 9323/10000 [02:52<00:13, 50.95it/s]Training CobwebTree:  93%|| 9329/10000 [02:52<00:13, 50.44it/s]Training CobwebTree:  93%|| 9335/10000 [02:52<00:13, 49.42it/s]Training CobwebTree:  93%|| 9341/10000 [02:52<00:13, 49.55it/s]Training CobwebTree:  93%|| 9346/10000 [02:52<00:13, 49.47it/s]Training CobwebTree:  94%|| 9352/10000 [02:52<00:12, 51.44it/s]Training CobwebTree:  94%|| 9358/10000 [02:53<00:12, 51.12it/s]Training CobwebTree:  94%|| 9364/10000 [02:53<00:12, 52.95it/s]Training CobwebTree:  94%|| 9370/10000 [02:53<00:12, 51.63it/s]Training CobwebTree:  94%|| 9376/10000 [02:53<00:12, 49.96it/s]Training CobwebTree:  94%|| 9382/10000 [02:53<00:12, 48.54it/s]Training CobwebTree:  94%|| 9387/10000 [02:53<00:12, 47.85it/s]Training CobwebTree:  94%|| 9392/10000 [02:53<00:12, 47.82it/s]Training CobwebTree:  94%|| 9398/10000 [02:53<00:12, 47.92it/s]Training CobwebTree:  94%|| 9403/10000 [02:53<00:12, 46.34it/s]Training CobwebTree:  94%|| 9408/10000 [02:54<00:12, 46.13it/s]Training CobwebTree:  94%|| 9413/10000 [02:54<00:12, 45.50it/s]Training CobwebTree:  94%|| 9418/10000 [02:54<00:12, 46.43it/s]Training CobwebTree:  94%|| 9423/10000 [02:54<00:12, 46.57it/s]Training CobwebTree:  94%|| 9428/10000 [02:54<00:12, 47.09it/s]Training CobwebTree:  94%|| 9433/10000 [02:54<00:11, 47.61it/s]Training CobwebTree:  94%|| 9438/10000 [02:54<00:11, 47.72it/s]Training CobwebTree:  94%|| 9443/10000 [02:54<00:12, 44.69it/s]Training CobwebTree:  94%|| 9448/10000 [02:54<00:12, 44.40it/s]Training CobwebTree:  95%|| 9453/10000 [02:55<00:12, 45.28it/s]Training CobwebTree:  95%|| 9458/10000 [02:55<00:11, 45.71it/s]Training CobwebTree:  95%|| 9463/10000 [02:55<00:11, 46.87it/s]Training CobwebTree:  95%|| 9470/10000 [02:55<00:10, 51.01it/s]Training CobwebTree:  95%|| 9476/10000 [02:55<00:10, 48.72it/s]Training CobwebTree:  95%|| 9481/10000 [02:55<00:10, 48.11it/s]Training CobwebTree:  95%|| 9486/10000 [02:55<00:10, 48.13it/s]Training CobwebTree:  95%|| 9491/10000 [02:55<00:10, 46.29it/s]Training CobwebTree:  95%|| 9497/10000 [02:55<00:10, 49.60it/s]Training CobwebTree:  95%|| 9502/10000 [02:56<00:10, 48.45it/s]Training CobwebTree:  95%|| 9507/10000 [02:56<00:10, 47.74it/s]Training CobwebTree:  95%|| 9512/10000 [02:56<00:10, 47.16it/s]Training CobwebTree:  95%|| 9517/10000 [02:56<00:10, 47.66it/s]Training CobwebTree:  95%|| 9522/10000 [02:56<00:10, 46.55it/s]Training CobwebTree:  95%|| 9528/10000 [02:56<00:09, 48.04it/s]Training CobwebTree:  95%|| 9534/10000 [02:56<00:09, 48.54it/s]Training CobwebTree:  95%|| 9539/10000 [02:56<00:09, 48.13it/s]Training CobwebTree:  95%|| 9544/10000 [02:56<00:09, 47.41it/s]Training CobwebTree:  95%|| 9549/10000 [02:57<00:09, 47.01it/s]Training CobwebTree:  96%|| 9554/10000 [02:57<00:09, 47.21it/s]Training CobwebTree:  96%|| 9559/10000 [02:57<00:09, 46.92it/s]Training CobwebTree:  96%|| 9564/10000 [02:57<00:09, 46.17it/s]Training CobwebTree:  96%|| 9569/10000 [02:57<00:09, 45.80it/s]Training CobwebTree:  96%|| 9575/10000 [02:57<00:08, 47.49it/s]Training CobwebTree:  96%|| 9580/10000 [02:57<00:08, 48.18it/s]Training CobwebTree:  96%|| 9585/10000 [02:57<00:08, 48.38it/s]Training CobwebTree:  96%|| 9590/10000 [02:57<00:08, 48.10it/s]Training CobwebTree:  96%|| 9595/10000 [02:58<00:08, 46.45it/s]Training CobwebTree:  96%|| 9601/10000 [02:58<00:08, 47.89it/s]Training CobwebTree:  96%|| 9606/10000 [02:58<00:08, 48.17it/s]Training CobwebTree:  96%|| 9611/10000 [02:58<00:08, 46.72it/s]Training CobwebTree:  96%|| 9616/10000 [02:58<00:08, 45.86it/s]Training CobwebTree:  96%|| 9622/10000 [02:58<00:07, 47.52it/s]Training CobwebTree:  96%|| 9628/10000 [02:58<00:07, 49.43it/s]Training CobwebTree:  96%|| 9633/10000 [02:58<00:07, 48.63it/s]Training CobwebTree:  96%|| 9638/10000 [02:58<00:07, 47.81it/s]Training CobwebTree:  96%|| 9643/10000 [02:59<00:07, 47.42it/s]Training CobwebTree:  96%|| 9648/10000 [02:59<00:07, 44.45it/s]Training CobwebTree:  97%|| 9653/10000 [02:59<00:07, 44.81it/s]Training CobwebTree:  97%|| 9658/10000 [02:59<00:07, 45.51it/s]Training CobwebTree:  97%|| 9663/10000 [02:59<00:07, 46.28it/s]Training CobwebTree:  97%|| 9669/10000 [02:59<00:06, 47.94it/s]Training CobwebTree:  97%|| 9674/10000 [02:59<00:06, 47.57it/s]Training CobwebTree:  97%|| 9679/10000 [02:59<00:06, 46.26it/s]Training CobwebTree:  97%|| 9685/10000 [02:59<00:06, 47.43it/s]Training CobwebTree:  97%|| 9691/10000 [03:00<00:06, 47.80it/s]Training CobwebTree:  97%|| 9696/10000 [03:00<00:06, 47.58it/s]Training CobwebTree:  97%|| 9701/10000 [03:00<00:06, 47.56it/s]Training CobwebTree:  97%|| 9706/10000 [03:00<00:06, 47.85it/s]Training CobwebTree:  97%|| 9711/10000 [03:00<00:06, 47.80it/s]Training CobwebTree:  97%|| 9716/10000 [03:00<00:05, 48.14it/s]Training CobwebTree:  97%|| 9721/10000 [03:00<00:06, 45.48it/s]Training CobwebTree:  97%|| 9727/10000 [03:00<00:05, 48.36it/s]Training CobwebTree:  97%|| 9732/10000 [03:00<00:05, 47.61it/s]Training CobwebTree:  97%|| 9738/10000 [03:01<00:05, 48.53it/s]Training CobwebTree:  97%|| 9743/10000 [03:01<00:05, 47.18it/s]Training CobwebTree:  97%|| 9748/10000 [03:01<00:05, 47.79it/s]Training CobwebTree:  98%|| 9753/10000 [03:01<00:05, 47.49it/s]Training CobwebTree:  98%|| 9758/10000 [03:01<00:05, 46.79it/s]Training CobwebTree:  98%|| 9763/10000 [03:01<00:05, 46.07it/s]Training CobwebTree:  98%|| 9768/10000 [03:01<00:05, 45.78it/s]Training CobwebTree:  98%|| 9774/10000 [03:01<00:04, 48.99it/s]Training CobwebTree:  98%|| 9780/10000 [03:01<00:04, 51.33it/s]Training CobwebTree:  98%|| 9786/10000 [03:02<00:04, 52.11it/s]Training CobwebTree:  98%|| 9792/10000 [03:02<00:04, 49.51it/s]Training CobwebTree:  98%|| 9798/10000 [03:02<00:03, 50.79it/s]Training CobwebTree:  98%|| 9804/10000 [03:02<00:03, 50.62it/s]Training CobwebTree:  98%|| 9810/10000 [03:02<00:03, 49.01it/s]Training CobwebTree:  98%|| 9815/10000 [03:02<00:03, 48.53it/s]Training CobwebTree:  98%|| 9820/10000 [03:02<00:03, 46.54it/s]Training CobwebTree:  98%|| 9825/10000 [03:02<00:03, 47.41it/s]Training CobwebTree:  98%|| 9830/10000 [03:02<00:03, 47.71it/s]Training CobwebTree:  98%|| 9835/10000 [03:03<00:03, 47.35it/s]Training CobwebTree:  98%|| 9841/10000 [03:03<00:03, 49.01it/s]Training CobwebTree:  98%|| 9846/10000 [03:03<00:03, 48.86it/s]Training CobwebTree:  99%|| 9851/10000 [03:03<00:03, 46.93it/s]Training CobwebTree:  99%|| 9856/10000 [03:03<00:03, 47.35it/s]Training CobwebTree:  99%|| 9861/10000 [03:03<00:02, 47.60it/s]Training CobwebTree:  99%|| 9866/10000 [03:03<00:02, 47.22it/s]Training CobwebTree:  99%|| 9871/10000 [03:03<00:02, 44.89it/s]Training CobwebTree:  99%|| 9876/10000 [03:03<00:02, 44.62it/s]Training CobwebTree:  99%|| 9881/10000 [03:04<00:02, 43.78it/s]Training CobwebTree:  99%|| 9886/10000 [03:04<00:02, 42.91it/s]Training CobwebTree:  99%|| 9892/10000 [03:04<00:02, 44.58it/s]Training CobwebTree:  99%|| 9898/10000 [03:04<00:02, 46.84it/s]Training CobwebTree:  99%|| 9903/10000 [03:04<00:02, 46.21it/s]Training CobwebTree:  99%|| 9909/10000 [03:04<00:01, 46.57it/s]Training CobwebTree:  99%|| 9914/10000 [03:04<00:01, 44.84it/s]Training CobwebTree:  99%|| 9919/10000 [03:04<00:01, 43.65it/s]Training CobwebTree:  99%|| 9924/10000 [03:05<00:01, 43.98it/s]Training CobwebTree:  99%|| 9929/10000 [03:05<00:01, 45.21it/s]Training CobwebTree:  99%|| 9934/10000 [03:05<00:01, 45.92it/s]Training CobwebTree:  99%|| 9939/10000 [03:05<00:01, 44.95it/s]Training CobwebTree:  99%|| 9944/10000 [03:05<00:01, 44.31it/s]Training CobwebTree:  99%|| 9949/10000 [03:05<00:01, 43.27it/s]Training CobwebTree: 100%|| 9954/10000 [03:05<00:01, 43.63it/s]Training CobwebTree: 100%|| 9959/10000 [03:05<00:00, 44.71it/s]Training CobwebTree: 100%|| 9964/10000 [03:05<00:00, 43.19it/s]Training CobwebTree: 100%|| 9970/10000 [03:06<00:00, 45.94it/s]Training CobwebTree: 100%|| 9975/10000 [03:06<00:00, 45.33it/s]Training CobwebTree: 100%|| 9980/10000 [03:06<00:00, 45.26it/s]Training CobwebTree: 100%|| 9986/10000 [03:06<00:00, 46.52it/s]Training CobwebTree: 100%|| 9991/10000 [03:06<00:00, 46.92it/s]Training CobwebTree: 100%|| 9997/10000 [03:06<00:00, 48.07it/s]Training CobwebTree: 100%|| 10000/10000 [03:06<00:00, 53.57it/s]
2025-12-21 13:49:39,807 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:49:44,139 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,139 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,139 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,139 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,140 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,140 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,145 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,145 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,145 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,145 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,148 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,148 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,148 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,148 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,151 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,151 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,151 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,151 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,153 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,154 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,154 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,154 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,156 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,156 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,156 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,156 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,156 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,157 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,160 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,164 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:44,170 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,170 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,170 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,170 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:44,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:47,348 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:49:47,442 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 10020 virtual documents
2025-12-21 13:49:47,718 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 13:49:52,420 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (832 virtual)
2025-12-21 13:49:52,422 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (1924 virtual)
2025-12-21 13:49:52,424 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3626 virtual)
2025-12-21 13:49:52,425 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4519 virtual)
2025-12-21 13:49:52,426 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5830 virtual)
2025-12-21 13:49:52,427 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6488 virtual)
2025-12-21 13:49:52,428 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7700 virtual)
2025-12-21 13:49:52,429 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8749 virtual)
2025-12-21 13:49:52,429 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9867 virtual)
2025-12-21 13:49:52,430 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10999 virtual)
2025-12-21 13:49:52,431 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (12031 virtual)
2025-12-21 13:49:52,432 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (13003 virtual)
2025-12-21 13:49:52,433 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (13993 virtual)
2025-12-21 13:49:52,434 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15086 virtual)
2025-12-21 13:49:52,434 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16065 virtual)
2025-12-21 13:49:52,435 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17261 virtual)
2025-12-21 13:49:52,436 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18370 virtual)
2025-12-21 13:49:52,437 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19567 virtual)
2025-12-21 13:49:52,439 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20799 virtual)
2025-12-21 13:49:52,440 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21756 virtual)
2025-12-21 13:49:52,440 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22829 virtual)
2025-12-21 13:49:52,441 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23857 virtual)
2025-12-21 13:49:52,443 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (25040 virtual)
2025-12-21 13:49:52,443 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (26200 virtual)
2025-12-21 13:49:52,444 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (27276 virtual)
2025-12-21 13:49:52,445 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (28353 virtual)
2025-12-21 13:49:52,446 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (29465 virtual)
2025-12-21 13:49:52,447 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (30467 virtual)
2025-12-21 13:49:52,447 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (31528 virtual)
2025-12-21 13:49:52,448 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (32685 virtual)
2025-12-21 13:49:52,449 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (33850 virtual)
2025-12-21 13:49:52,450 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (35016 virtual)
2025-12-21 13:49:52,450 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (35995 virtual)
2025-12-21 13:49:52,451 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (37067 virtual)
2025-12-21 13:49:52,452 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (38078 virtual)
2025-12-21 13:49:52,453 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (39078 virtual)
2025-12-21 13:49:52,453 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (40093 virtual)
2025-12-21 13:49:52,454 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (41124 virtual)
2025-12-21 13:49:52,455 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (42139 virtual)
2025-12-21 13:49:52,456 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (43129 virtual)
2025-12-21 13:49:52,457 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (44122 virtual)
2025-12-21 13:49:52,457 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (45271 virtual)
2025-12-21 13:49:52,458 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (46375 virtual)
2025-12-21 13:49:52,459 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (47426 virtual)
2025-12-21 13:49:52,460 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (48587 virtual)
2025-12-21 13:49:52,460 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (49767 virtual)
2025-12-21 13:49:52,461 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (50793 virtual)
2025-12-21 13:49:52,461 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (51818 virtual)
2025-12-21 13:49:52,462 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (52966 virtual)
2025-12-21 13:49:52,462 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (54042 virtual)
2025-12-21 13:49:52,463 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (55171 virtual)
2025-12-21 13:49:52,463 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (56315 virtual)
2025-12-21 13:49:52,464 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (57371 virtual)
2025-12-21 13:49:52,465 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (58477 virtual)
2025-12-21 13:49:52,465 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (59510 virtual)
2025-12-21 13:49:52,466 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (60655 virtual)
2025-12-21 13:49:52,467 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (61674 virtual)
2025-12-21 13:49:52,467 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (62796 virtual)
2025-12-21 13:49:52,468 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (63895 virtual)
2025-12-21 13:49:52,469 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (65036 virtual)
2025-12-21 13:49:52,470 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (66236 virtual)
2025-12-21 13:49:52,470 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (67382 virtual)
2025-12-21 13:49:52,471 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (68639 virtual)
2025-12-21 13:49:52,472 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (69541 virtual)
2025-12-21 13:49:52,473 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (70651 virtual)
2025-12-21 13:49:52,474 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (71786 virtual)
2025-12-21 13:49:52,474 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (72824 virtual)
2025-12-21 13:49:52,475 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (74010 virtual)
2025-12-21 13:49:52,476 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (75076 virtual)
2025-12-21 13:49:52,477 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (76083 virtual)
2025-12-21 13:49:52,477 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (77189 virtual)
2025-12-21 13:49:52,478 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (78281 virtual)
2025-12-21 13:49:52,479 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (79296 virtual)
2025-12-21 13:49:52,480 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (80224 virtual)
2025-12-21 13:49:52,480 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (81222 virtual)
2025-12-21 13:49:52,481 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (82312 virtual)
2025-12-21 13:49:52,482 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (83492 virtual)
2025-12-21 13:49:52,483 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (84525 virtual)
2025-12-21 13:49:52,483 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (85614 virtual)
2025-12-21 13:49:52,484 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (86885 virtual)
2025-12-21 13:49:52,485 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (87974 virtual)
2025-12-21 13:49:52,485 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (89092 virtual)
2025-12-21 13:49:52,486 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (90195 virtual)
2025-12-21 13:49:52,486 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (91395 virtual)
2025-12-21 13:49:52,487 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (92480 virtual)
2025-12-21 13:49:52,487 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (93565 virtual)
2025-12-21 13:49:52,488 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (94695 virtual)
2025-12-21 13:49:52,489 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (95624 virtual)
2025-12-21 13:49:52,489 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (96569 virtual)
2025-12-21 13:49:52,490 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (97591 virtual)
2025-12-21 13:49:52,491 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (98664 virtual)
2025-12-21 13:49:52,491 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (99706 virtual)
2025-12-21 13:49:52,492 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (100778 virtual)
2025-12-21 13:49:52,493 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (101802 virtual)
2025-12-21 13:49:52,494 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (102774 virtual)
2025-12-21 13:49:52,495 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (103743 virtual)
2025-12-21 13:49:52,495 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (104776 virtual)
2025-12-21 13:49:52,496 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (105716 virtual)
2025-12-21 13:49:52,497 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (106808 virtual)
2025-12-21 13:49:52,498 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (107747 virtual)
2025-12-21 13:49:52,498 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (108833 virtual)
2025-12-21 13:49:52,499 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (109961 virtual)
2025-12-21 13:49:52,500 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (111006 virtual)
2025-12-21 13:49:52,500 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (112030 virtual)
2025-12-21 13:49:52,501 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (112979 virtual)
2025-12-21 13:49:52,502 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (114023 virtual)
2025-12-21 13:49:52,503 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (115059 virtual)
2025-12-21 13:49:52,503 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (116157 virtual)
2025-12-21 13:49:52,504 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (117171 virtual)
2025-12-21 13:49:52,505 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (118037 virtual)
2025-12-21 13:49:52,505 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (118996 virtual)
2025-12-21 13:49:52,506 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (119978 virtual)
2025-12-21 13:49:52,507 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (120915 virtual)
2025-12-21 13:49:52,507 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (121907 virtual)
2025-12-21 13:49:52,508 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (123029 virtual)
2025-12-21 13:49:52,508 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (124130 virtual)
2025-12-21 13:49:52,509 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (125205 virtual)
2025-12-21 13:49:52,509 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (126354 virtual)
2025-12-21 13:49:52,510 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (127438 virtual)
2025-12-21 13:49:52,510 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (128401 virtual)
2025-12-21 13:49:52,510 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (129417 virtual)
2025-12-21 13:49:52,511 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (130490 virtual)
2025-12-21 13:49:52,511 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (131612 virtual)
2025-12-21 13:49:52,512 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (132706 virtual)
2025-12-21 13:49:52,513 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (133808 virtual)
2025-12-21 13:49:52,513 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (134837 virtual)
2025-12-21 13:49:52,514 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (135783 virtual)
2025-12-21 13:49:52,515 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (136786 virtual)
2025-12-21 13:49:52,516 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (137882 virtual)
2025-12-21 13:49:52,516 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (138825 virtual)
2025-12-21 13:49:52,517 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (139828 virtual)
2025-12-21 13:49:52,518 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (140832 virtual)
2025-12-21 13:49:52,518 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (141891 virtual)
2025-12-21 13:49:52,519 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (142800 virtual)
2025-12-21 13:49:52,520 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (143710 virtual)
2025-12-21 13:49:52,520 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (144779 virtual)
2025-12-21 13:49:52,521 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (145967 virtual)
2025-12-21 13:49:52,522 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (147037 virtual)
2025-12-21 13:49:52,523 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (148021 virtual)
2025-12-21 13:49:52,523 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (149173 virtual)
2025-12-21 13:49:52,524 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (150217 virtual)
2025-12-21 13:49:52,525 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (151355 virtual)
2025-12-21 13:49:52,525 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (152357 virtual)
2025-12-21 13:49:52,639 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (153374 virtual)
2025-12-21 13:49:52,641 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (154460 virtual)
2025-12-21 13:49:52,641 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (155398 virtual)
2025-12-21 13:49:52,642 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (156510 virtual)
2025-12-21 13:49:52,643 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (157620 virtual)
2025-12-21 13:49:52,643 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (158691 virtual)
2025-12-21 13:49:52,655 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (159855 virtual)
2025-12-21 13:49:52,656 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (160835 virtual)
2025-12-21 13:49:52,666 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (161769 virtual)
2025-12-21 13:49:52,666 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (162793 virtual)
2025-12-21 13:49:52,671 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (163835 virtual)
2025-12-21 13:49:52,793 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (165041 virtual)
2025-12-21 13:49:52,794 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (166004 virtual)
2025-12-21 13:49:52,794 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (166241 virtual)
2025-12-21 13:49:53,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,207 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,207 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,208 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,208 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,208 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,209 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,209 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,209 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,209 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,209 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,210 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,210 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,210 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,240 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,252 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,252 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,252 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,260 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,260 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,260 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,282 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,288 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,288 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,292 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,323 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,360 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,364 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,373 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,374 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,424 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,462 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:53,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:53,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:55,208 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 13:49:55,209 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 13:49:57,004 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 13:49:57,103 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 166253 virtual documents
2025-12-21 13:49:57,340 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.7610455019961212, 'coherence_npmi': 0.24921055458462257, 'topic_diversity': 0.8404109589041096, 'inter_topic_similarity': 0.13739442825317383}
2025-12-21 13:49:57,341 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6431382800820502, 'coherence_npmi': 0.15464086931558804, 'topic_diversity': 0.878, 'inter_topic_similarity': 0.21195729076862335}
2025-12-21 13:49:57,341 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6837764685998421, 'coherence_npmi': 0.1751036938571652, 'topic_diversity': 0.85, 'inter_topic_similarity': 0.24703751504421234}
