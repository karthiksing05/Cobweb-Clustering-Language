2025-12-23 01:07:41,429 INFO __main__: Starting benchmark for dataset=agnews
2025-12-23 01:07:56,604 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-23 01:07:56,722 INFO gensim.corpora.dictionary: built Dictionary<21572 unique tokens: ['disappointed', 'fears', 'federal', 'firm', 'mogul']...> from 7600 documents (total 189278 corpus positions)
2025-12-23 01:07:56,726 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<21572 unique tokens: ['disappointed', 'fears', 'federal', 'firm', 'mogul']...> from 7600 documents (total 189278 corpus positions)", 'datetime': '2025-12-23T01:07:56.722564', 'gensim': '4.4.0', 'python': '3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-152-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-23 01:07:58,408 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-23 01:07:58,408 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-23 01:08:06,134 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 7600 docs
2025-12-23 01:08:58,659 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 01:09:03,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,358 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,365 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,367 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,369 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,369 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,373 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,373 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,378 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,381 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,391 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,391 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,391 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,391 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,392 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,394 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,413 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,413 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,413 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,418 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,418 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:03,424 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,438 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,438 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,438 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,469 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,469 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,469 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,473 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,473 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,473 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,473 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,493 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,493 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,501 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,529 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,537 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:03,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:06,767 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 01:09:06,982 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 7600 virtual documents
2025-12-23 01:09:07,960 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 01:09:12,260 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (1222 virtual)
2025-12-23 01:09:12,261 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (2388 virtual)
2025-12-23 01:09:12,266 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3473 virtual)
2025-12-23 01:09:12,267 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4548 virtual)
2025-12-23 01:09:12,268 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5650 virtual)
2025-12-23 01:09:12,269 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6680 virtual)
2025-12-23 01:09:12,270 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7623 virtual)
2025-12-23 01:09:12,270 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8704 virtual)
2025-12-23 01:09:12,271 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9794 virtual)
2025-12-23 01:09:12,272 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10897 virtual)
2025-12-23 01:09:12,273 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (11911 virtual)
2025-12-23 01:09:12,274 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (12921 virtual)
2025-12-23 01:09:12,274 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (14032 virtual)
2025-12-23 01:09:12,275 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15005 virtual)
2025-12-23 01:09:12,276 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16044 virtual)
2025-12-23 01:09:12,276 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17093 virtual)
2025-12-23 01:09:12,277 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18108 virtual)
2025-12-23 01:09:12,278 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19117 virtual)
2025-12-23 01:09:12,282 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20085 virtual)
2025-12-23 01:09:12,283 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21140 virtual)
2025-12-23 01:09:12,284 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22139 virtual)
2025-12-23 01:09:12,284 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23227 virtual)
2025-12-23 01:09:12,285 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (24166 virtual)
2025-12-23 01:09:12,286 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (25137 virtual)
2025-12-23 01:09:12,286 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (26187 virtual)
2025-12-23 01:09:12,287 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (27113 virtual)
2025-12-23 01:09:12,288 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (28084 virtual)
2025-12-23 01:09:12,288 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (29113 virtual)
2025-12-23 01:09:12,289 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (30069 virtual)
2025-12-23 01:09:12,290 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (31017 virtual)
2025-12-23 01:09:12,290 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (31951 virtual)
2025-12-23 01:09:12,291 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (32954 virtual)
2025-12-23 01:09:12,292 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (34063 virtual)
2025-12-23 01:09:12,293 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (35021 virtual)
2025-12-23 01:09:12,293 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (36080 virtual)
2025-12-23 01:09:12,294 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (37103 virtual)
2025-12-23 01:09:12,295 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (38185 virtual)
2025-12-23 01:09:12,295 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (39172 virtual)
2025-12-23 01:09:12,296 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (40185 virtual)
2025-12-23 01:09:12,297 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (41113 virtual)
2025-12-23 01:09:12,298 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (42261 virtual)
2025-12-23 01:09:12,299 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (43352 virtual)
2025-12-23 01:09:12,300 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (44331 virtual)
2025-12-23 01:09:12,300 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (45325 virtual)
2025-12-23 01:09:12,301 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (46441 virtual)
2025-12-23 01:09:12,302 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (47389 virtual)
2025-12-23 01:09:12,302 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (48333 virtual)
2025-12-23 01:09:12,303 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (49375 virtual)
2025-12-23 01:09:12,304 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (50465 virtual)
2025-12-23 01:09:12,305 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (51456 virtual)
2025-12-23 01:09:12,305 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (52441 virtual)
2025-12-23 01:09:12,306 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (53484 virtual)
2025-12-23 01:09:12,307 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (54533 virtual)
2025-12-23 01:09:12,308 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (55592 virtual)
2025-12-23 01:09:12,308 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (56681 virtual)
2025-12-23 01:09:12,309 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (57717 virtual)
2025-12-23 01:09:12,310 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (58705 virtual)
2025-12-23 01:09:12,311 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (59654 virtual)
2025-12-23 01:09:12,311 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (60700 virtual)
2025-12-23 01:09:12,312 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (61672 virtual)
2025-12-23 01:09:12,312 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (62694 virtual)
2025-12-23 01:09:12,313 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (63757 virtual)
2025-12-23 01:09:12,314 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (64840 virtual)
2025-12-23 01:09:12,314 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (65744 virtual)
2025-12-23 01:09:12,315 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (66827 virtual)
2025-12-23 01:09:12,316 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (67785 virtual)
2025-12-23 01:09:12,317 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (68804 virtual)
2025-12-23 01:09:12,317 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (69764 virtual)
2025-12-23 01:09:12,318 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (70807 virtual)
2025-12-23 01:09:12,319 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (71846 virtual)
2025-12-23 01:09:12,320 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (72847 virtual)
2025-12-23 01:09:12,320 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (73925 virtual)
2025-12-23 01:09:12,321 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (75010 virtual)
2025-12-23 01:09:12,322 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (75968 virtual)
2025-12-23 01:09:12,323 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (76955 virtual)
2025-12-23 01:09:12,323 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (77973 virtual)
2025-12-23 01:09:12,324 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (78900 virtual)
2025-12-23 01:09:12,325 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (79915 virtual)
2025-12-23 01:09:12,326 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (80921 virtual)
2025-12-23 01:09:12,326 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (81894 virtual)
2025-12-23 01:09:12,327 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (82979 virtual)
2025-12-23 01:09:12,328 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (83937 virtual)
2025-12-23 01:09:12,328 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (84989 virtual)
2025-12-23 01:09:12,329 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (85991 virtual)
2025-12-23 01:09:12,330 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (87013 virtual)
2025-12-23 01:09:12,331 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (88050 virtual)
2025-12-23 01:09:12,332 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (88989 virtual)
2025-12-23 01:09:12,333 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (89975 virtual)
2025-12-23 01:09:12,333 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (90892 virtual)
2025-12-23 01:09:12,334 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (91944 virtual)
2025-12-23 01:09:12,335 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (93008 virtual)
2025-12-23 01:09:12,336 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (93938 virtual)
2025-12-23 01:09:12,337 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (94952 virtual)
2025-12-23 01:09:12,338 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (95989 virtual)
2025-12-23 01:09:12,338 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (97083 virtual)
2025-12-23 01:09:12,339 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (97968 virtual)
2025-12-23 01:09:12,340 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (98973 virtual)
2025-12-23 01:09:12,341 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (99956 virtual)
2025-12-23 01:09:12,341 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (101124 virtual)
2025-12-23 01:09:12,342 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (102046 virtual)
2025-12-23 01:09:12,343 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (103005 virtual)
2025-12-23 01:09:12,344 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (104049 virtual)
2025-12-23 01:09:12,344 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (105026 virtual)
2025-12-23 01:09:12,345 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (106094 virtual)
2025-12-23 01:09:12,346 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (107077 virtual)
2025-12-23 01:09:12,347 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (108131 virtual)
2025-12-23 01:09:12,348 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (109133 virtual)
2025-12-23 01:09:12,349 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (110094 virtual)
2025-12-23 01:09:12,349 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (111108 virtual)
2025-12-23 01:09:12,350 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (112228 virtual)
2025-12-23 01:09:12,351 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (113209 virtual)
2025-12-23 01:09:12,352 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (114200 virtual)
2025-12-23 01:09:12,353 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (115208 virtual)
2025-12-23 01:09:12,353 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (116189 virtual)
2025-12-23 01:09:12,354 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (117185 virtual)
2025-12-23 01:09:12,355 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (118161 virtual)
2025-12-23 01:09:12,356 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (119147 virtual)
2025-12-23 01:09:12,356 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (120162 virtual)
2025-12-23 01:09:12,357 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (120878 virtual)
2025-12-23 01:09:12,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,755 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,758 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,760 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,760 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,762 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,762 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,763 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,765 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,765 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,766 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,767 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,768 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,797 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,805 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,809 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,813 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,813 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,817 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,817 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,861 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,865 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,865 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,873 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,910 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,937 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,941 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,945 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,945 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,949 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,953 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,953 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,957 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,961 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,965 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:12,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,001 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,014 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,014 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,035 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,035 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,036 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,036 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,037 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,037 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,039 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,040 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,041 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,042 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,042 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,045 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,047 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,048 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,049 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,050 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,053 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,058 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,062 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,062 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,065 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,065 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,066 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,066 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,066 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,077 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,077 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,077 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,085 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,085 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,085 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,086 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:12,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,093 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,093 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,095 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,096 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,097 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,097 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,097 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,098 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,098 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,098 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,108 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,123 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,125 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,126 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,126 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,136 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,149 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,161 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,161 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,161 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,173 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,173 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,177 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,177 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,181 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,229 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,237 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,249 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,249 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,265 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,266 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,274 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,275 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,277 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,287 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,288 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,290 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,290 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,321 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,325 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,337 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,337 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,338 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,345 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,345 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,357 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,381 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,549 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,135 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:13,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:13,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:16,526 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 01:09:16,726 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 120892 virtual documents
2025-12-23 01:09:17,136 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 7600 docs
2025-12-23 01:09:48,549 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 01:09:53,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,168 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,169 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,170 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,172 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,172 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,172 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,182 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,190 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,191 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,193 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,199 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,200 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,200 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,202 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,202 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,206 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,209 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,209 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,209 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,213 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,213 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,221 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,221 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,221 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,222 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,222 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,229 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,229 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,229 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,233 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,237 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,237 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,237 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,241 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:09:53,242 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,245 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,245 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,245 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,249 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,249 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,253 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,253 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,253 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,258 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,258 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,261 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,261 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,261 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,261 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,265 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,270 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,270 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,270 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,270 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,280 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,286 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,286 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,297 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,297 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,301 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,302 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,309 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,309 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,313 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,313 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,314 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,321 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,321 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,325 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,325 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,330 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,330 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,333 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,333 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,337 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,337 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,341 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,348 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,349 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,353 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,354 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,354 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,354 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,370 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,370 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,370 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,378 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,378 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,394 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,394 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:53,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:09:56,777 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 01:09:56,898 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 7600 virtual documents
2025-12-23 01:09:57,334 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 01:10:01,704 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (1222 virtual)
2025-12-23 01:10:01,706 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (2388 virtual)
2025-12-23 01:10:01,707 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3473 virtual)
2025-12-23 01:10:01,708 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4548 virtual)
2025-12-23 01:10:01,708 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5650 virtual)
2025-12-23 01:10:01,709 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6680 virtual)
2025-12-23 01:10:01,711 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7623 virtual)
2025-12-23 01:10:01,712 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8704 virtual)
2025-12-23 01:10:01,713 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9794 virtual)
2025-12-23 01:10:01,714 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10897 virtual)
2025-12-23 01:10:01,715 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (11911 virtual)
2025-12-23 01:10:01,715 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (12921 virtual)
2025-12-23 01:10:01,716 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (14032 virtual)
2025-12-23 01:10:01,717 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15005 virtual)
2025-12-23 01:10:01,717 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16044 virtual)
2025-12-23 01:10:01,718 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17093 virtual)
2025-12-23 01:10:01,719 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18108 virtual)
2025-12-23 01:10:01,720 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19117 virtual)
2025-12-23 01:10:01,720 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20085 virtual)
2025-12-23 01:10:01,721 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21140 virtual)
2025-12-23 01:10:01,722 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22139 virtual)
2025-12-23 01:10:01,722 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23227 virtual)
2025-12-23 01:10:01,723 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (24166 virtual)
2025-12-23 01:10:01,724 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (25137 virtual)
2025-12-23 01:10:01,724 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (26187 virtual)
2025-12-23 01:10:01,725 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (27113 virtual)
2025-12-23 01:10:01,726 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (28084 virtual)
2025-12-23 01:10:01,727 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (29113 virtual)
2025-12-23 01:10:01,727 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (30069 virtual)
2025-12-23 01:10:01,728 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (31017 virtual)
2025-12-23 01:10:01,729 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (31951 virtual)
2025-12-23 01:10:01,729 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (32954 virtual)
2025-12-23 01:10:01,730 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (34063 virtual)
2025-12-23 01:10:01,731 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (35021 virtual)
2025-12-23 01:10:01,731 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (36080 virtual)
2025-12-23 01:10:01,731 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (37103 virtual)
2025-12-23 01:10:01,732 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (38185 virtual)
2025-12-23 01:10:01,732 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (39172 virtual)
2025-12-23 01:10:01,733 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (40185 virtual)
2025-12-23 01:10:01,733 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (41113 virtual)
2025-12-23 01:10:01,733 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (42261 virtual)
2025-12-23 01:10:01,734 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (43352 virtual)
2025-12-23 01:10:01,734 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (44331 virtual)
2025-12-23 01:10:01,734 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (45325 virtual)
2025-12-23 01:10:01,735 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (46441 virtual)
2025-12-23 01:10:01,735 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (47389 virtual)
2025-12-23 01:10:01,736 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (48333 virtual)
2025-12-23 01:10:01,736 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (49375 virtual)
2025-12-23 01:10:01,736 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (50465 virtual)
2025-12-23 01:10:01,737 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (51456 virtual)
2025-12-23 01:10:01,737 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (52441 virtual)
2025-12-23 01:10:01,738 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (53484 virtual)
2025-12-23 01:10:01,739 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (54533 virtual)
2025-12-23 01:10:01,739 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (55592 virtual)
2025-12-23 01:10:01,740 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (56681 virtual)
2025-12-23 01:10:01,741 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (57717 virtual)
2025-12-23 01:10:01,741 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (58705 virtual)
2025-12-23 01:10:01,742 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (59654 virtual)
2025-12-23 01:10:01,743 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (60700 virtual)
2025-12-23 01:10:01,743 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (61672 virtual)
2025-12-23 01:10:01,744 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (62694 virtual)
2025-12-23 01:10:01,745 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (63757 virtual)
2025-12-23 01:10:01,745 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (64840 virtual)
2025-12-23 01:10:01,746 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (65744 virtual)
2025-12-23 01:10:01,747 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (66827 virtual)
2025-12-23 01:10:01,747 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (67785 virtual)
2025-12-23 01:10:01,748 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (68804 virtual)
2025-12-23 01:10:01,749 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (69764 virtual)
2025-12-23 01:10:01,749 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (70807 virtual)
2025-12-23 01:10:01,750 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (71846 virtual)
2025-12-23 01:10:01,751 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (72847 virtual)
2025-12-23 01:10:01,752 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (73925 virtual)
2025-12-23 01:10:01,752 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (75010 virtual)
2025-12-23 01:10:01,753 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (75968 virtual)
2025-12-23 01:10:01,754 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (76955 virtual)
2025-12-23 01:10:01,754 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (77973 virtual)
2025-12-23 01:10:01,755 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (78900 virtual)
2025-12-23 01:10:01,756 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (79915 virtual)
2025-12-23 01:10:01,756 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (80921 virtual)
2025-12-23 01:10:01,757 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (81894 virtual)
2025-12-23 01:10:01,758 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (82979 virtual)
2025-12-23 01:10:01,758 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (83937 virtual)
2025-12-23 01:10:01,759 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (84989 virtual)
2025-12-23 01:10:01,759 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (85991 virtual)
2025-12-23 01:10:01,760 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (87013 virtual)
2025-12-23 01:10:01,760 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (88050 virtual)
2025-12-23 01:10:01,761 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (88989 virtual)
2025-12-23 01:10:01,761 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (89975 virtual)
2025-12-23 01:10:01,762 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (90892 virtual)
2025-12-23 01:10:01,763 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (91944 virtual)
2025-12-23 01:10:01,763 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (93008 virtual)
2025-12-23 01:10:01,764 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (93938 virtual)
2025-12-23 01:10:01,765 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (94952 virtual)
2025-12-23 01:10:01,765 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (95989 virtual)
2025-12-23 01:10:01,766 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (97083 virtual)
2025-12-23 01:10:01,767 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (97968 virtual)
2025-12-23 01:10:01,767 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (98973 virtual)
2025-12-23 01:10:01,768 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (99956 virtual)
2025-12-23 01:10:01,769 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (101124 virtual)
2025-12-23 01:10:01,769 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (102046 virtual)
2025-12-23 01:10:01,770 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (103005 virtual)
2025-12-23 01:10:01,771 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (104049 virtual)
2025-12-23 01:10:01,771 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (105026 virtual)
2025-12-23 01:10:01,772 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (106094 virtual)
2025-12-23 01:10:01,773 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (107077 virtual)
2025-12-23 01:10:01,773 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (108131 virtual)
2025-12-23 01:10:01,774 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (109133 virtual)
2025-12-23 01:10:01,775 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (110094 virtual)
2025-12-23 01:10:01,775 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (111108 virtual)
2025-12-23 01:10:01,776 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (112228 virtual)
2025-12-23 01:10:01,777 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (113209 virtual)
2025-12-23 01:10:01,778 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (114200 virtual)
2025-12-23 01:10:01,778 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (115208 virtual)
2025-12-23 01:10:01,779 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (116189 virtual)
2025-12-23 01:10:01,780 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (117185 virtual)
2025-12-23 01:10:01,780 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (118161 virtual)
2025-12-23 01:10:01,781 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (119147 virtual)
2025-12-23 01:10:01,782 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (120162 virtual)
2025-12-23 01:10:01,782 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (120878 virtual)
2025-12-23 01:10:02,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,352 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,355 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,356 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,359 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,358 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,359 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,359 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,361 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,364 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,365 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,367 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,367 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,373 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,390 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,404 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,406 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,412 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,412 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,413 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,414 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,416 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,434 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,454 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,460 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,466 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,469 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,473 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,492 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,492 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,493 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,493 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,498 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,513 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,517 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,517 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,524 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,525 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,529 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,529 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,529 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,529 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,533 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,537 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,537 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,541 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,553 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,573 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,578 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,597 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,692 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:02,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,741 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,808 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:02,856 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:03,034 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:03,077 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:03,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:04,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:10:04,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:10:06,307 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 01:10:06,430 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 120892 virtual documents
2025-12-23 01:10:06,719 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 7600 docs
Training CobwebTree:   0%|          | 0/7600 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 20/7600 [00:00<00:38, 196.50it/s]Training CobwebTree:   1%|          | 40/7600 [00:00<00:45, 166.73it/s]Training CobwebTree:   1%|          | 57/7600 [00:00<00:51, 146.05it/s]Training CobwebTree:   1%|          | 72/7600 [00:00<00:55, 134.52it/s]Training CobwebTree:   1%|          | 86/7600 [00:00<01:00, 124.98it/s]Training CobwebTree:   1%|         | 99/7600 [00:00<01:09, 108.01it/s]Training CobwebTree:   1%|         | 111/7600 [00:00<01:10, 106.45it/s]Training CobwebTree:   2%|         | 122/7600 [00:01<01:11, 104.43it/s]Training CobwebTree:   2%|         | 133/7600 [00:01<01:15, 98.62it/s] Training CobwebTree:   2%|         | 143/7600 [00:01<01:17, 96.12it/s]Training CobwebTree:   2%|         | 153/7600 [00:01<01:18, 95.08it/s]Training CobwebTree:   2%|         | 163/7600 [00:01<01:19, 93.64it/s]Training CobwebTree:   2%|         | 173/7600 [00:01<01:24, 88.13it/s]Training CobwebTree:   2%|         | 182/7600 [00:01<01:27, 85.26it/s]Training CobwebTree:   3%|         | 192/7600 [00:01<01:23, 88.93it/s]Training CobwebTree:   3%|         | 201/7600 [00:01<01:27, 84.44it/s]Training CobwebTree:   3%|         | 210/7600 [00:02<01:26, 85.36it/s]Training CobwebTree:   3%|         | 219/7600 [00:02<01:25, 86.19it/s]Training CobwebTree:   3%|         | 228/7600 [00:02<01:25, 86.23it/s]Training CobwebTree:   3%|         | 238/7600 [00:02<01:24, 87.13it/s]Training CobwebTree:   3%|         | 247/7600 [00:02<01:25, 85.77it/s]Training CobwebTree:   3%|         | 256/7600 [00:02<01:31, 80.65it/s]Training CobwebTree:   3%|         | 265/7600 [00:02<01:33, 78.49it/s]Training CobwebTree:   4%|         | 273/7600 [00:02<01:35, 76.46it/s]Training CobwebTree:   4%|         | 283/7600 [00:02<01:28, 82.33it/s]Training CobwebTree:   4%|         | 292/7600 [00:03<01:30, 80.98it/s]Training CobwebTree:   4%|         | 301/7600 [00:03<01:30, 81.09it/s]Training CobwebTree:   4%|         | 310/7600 [00:03<01:27, 82.88it/s]Training CobwebTree:   4%|         | 319/7600 [00:03<01:30, 80.20it/s]Training CobwebTree:   4%|         | 328/7600 [00:03<01:31, 79.56it/s]Training CobwebTree:   4%|         | 336/7600 [00:03<01:32, 78.34it/s]Training CobwebTree:   5%|         | 344/7600 [00:03<01:34, 76.86it/s]Training CobwebTree:   5%|         | 352/7600 [00:03<01:34, 76.68it/s]Training CobwebTree:   5%|         | 360/7600 [00:03<01:34, 76.80it/s]Training CobwebTree:   5%|         | 368/7600 [00:04<01:35, 75.75it/s]Training CobwebTree:   5%|         | 376/7600 [00:04<01:37, 74.30it/s]Training CobwebTree:   5%|         | 384/7600 [00:04<01:37, 74.25it/s]Training CobwebTree:   5%|         | 393/7600 [00:04<01:33, 77.39it/s]Training CobwebTree:   5%|         | 401/7600 [00:04<01:35, 75.74it/s]Training CobwebTree:   5%|         | 409/7600 [00:04<01:33, 76.57it/s]Training CobwebTree:   5%|         | 417/7600 [00:04<01:32, 77.54it/s]Training CobwebTree:   6%|         | 425/7600 [00:04<01:34, 75.86it/s]Training CobwebTree:   6%|         | 433/7600 [00:04<01:38, 72.85it/s]Training CobwebTree:   6%|         | 442/7600 [00:05<01:35, 75.15it/s]Training CobwebTree:   6%|         | 451/7600 [00:05<01:33, 76.54it/s]Training CobwebTree:   6%|         | 459/7600 [00:05<01:33, 76.64it/s]Training CobwebTree:   6%|         | 467/7600 [00:05<01:37, 73.11it/s]Training CobwebTree:   6%|         | 475/7600 [00:05<01:35, 74.98it/s]Training CobwebTree:   6%|         | 483/7600 [00:05<01:34, 75.68it/s]Training CobwebTree:   6%|         | 491/7600 [00:05<01:34, 75.18it/s]Training CobwebTree:   7%|         | 499/7600 [00:05<01:35, 74.02it/s]Training CobwebTree:   7%|         | 507/7600 [00:05<01:34, 75.29it/s]Training CobwebTree:   7%|         | 515/7600 [00:06<01:38, 71.99it/s]Training CobwebTree:   7%|         | 523/7600 [00:06<01:40, 70.74it/s]Training CobwebTree:   7%|         | 531/7600 [00:06<01:40, 70.64it/s]Training CobwebTree:   7%|         | 539/7600 [00:06<01:40, 70.19it/s]Training CobwebTree:   7%|         | 547/7600 [00:06<01:40, 70.07it/s]Training CobwebTree:   7%|         | 555/7600 [00:06<01:40, 70.27it/s]Training CobwebTree:   7%|         | 563/7600 [00:06<01:36, 72.87it/s]Training CobwebTree:   8%|         | 571/7600 [00:06<01:38, 71.34it/s]Training CobwebTree:   8%|         | 579/7600 [00:06<01:38, 71.24it/s]Training CobwebTree:   8%|         | 587/7600 [00:07<01:38, 71.04it/s]Training CobwebTree:   8%|         | 595/7600 [00:07<01:39, 70.60it/s]Training CobwebTree:   8%|         | 603/7600 [00:07<01:37, 71.75it/s]Training CobwebTree:   8%|         | 611/7600 [00:07<01:35, 73.49it/s]Training CobwebTree:   8%|         | 619/7600 [00:07<01:36, 72.41it/s]Training CobwebTree:   8%|         | 627/7600 [00:07<01:37, 71.22it/s]Training CobwebTree:   8%|         | 635/7600 [00:07<01:41, 68.60it/s]Training CobwebTree:   8%|         | 642/7600 [00:07<01:45, 65.76it/s]Training CobwebTree:   9%|         | 650/7600 [00:07<01:40, 69.46it/s]Training CobwebTree:   9%|         | 658/7600 [00:08<01:40, 68.78it/s]Training CobwebTree:   9%|         | 666/7600 [00:08<01:42, 67.98it/s]Training CobwebTree:   9%|         | 673/7600 [00:08<01:41, 68.52it/s]Training CobwebTree:   9%|         | 680/7600 [00:08<01:44, 66.12it/s]Training CobwebTree:   9%|         | 687/7600 [00:08<01:45, 65.58it/s]Training CobwebTree:   9%|         | 694/7600 [00:08<01:43, 66.41it/s]Training CobwebTree:   9%|         | 701/7600 [00:08<01:44, 65.89it/s]Training CobwebTree:   9%|         | 708/7600 [00:08<01:45, 65.36it/s]Training CobwebTree:   9%|         | 715/7600 [00:08<01:43, 66.61it/s]Training CobwebTree:  10%|         | 722/7600 [00:09<01:43, 66.53it/s]Training CobwebTree:  10%|         | 729/7600 [00:09<01:42, 67.29it/s]Training CobwebTree:  10%|         | 736/7600 [00:09<01:41, 67.77it/s]Training CobwebTree:  10%|         | 743/7600 [00:09<01:44, 65.87it/s]Training CobwebTree:  10%|         | 750/7600 [00:09<01:44, 65.36it/s]Training CobwebTree:  10%|         | 757/7600 [00:09<01:43, 66.04it/s]Training CobwebTree:  10%|         | 765/7600 [00:09<01:39, 68.38it/s]Training CobwebTree:  10%|         | 773/7600 [00:09<01:37, 69.96it/s]Training CobwebTree:  10%|         | 781/7600 [00:09<01:37, 70.17it/s]Training CobwebTree:  10%|         | 789/7600 [00:09<01:40, 67.75it/s]Training CobwebTree:  10%|         | 796/7600 [00:10<01:40, 67.70it/s]Training CobwebTree:  11%|         | 803/7600 [00:10<01:40, 67.96it/s]Training CobwebTree:  11%|         | 810/7600 [00:10<01:40, 67.36it/s]Training CobwebTree:  11%|         | 817/7600 [00:10<01:40, 67.16it/s]Training CobwebTree:  11%|         | 824/7600 [00:10<01:43, 65.71it/s]Training CobwebTree:  11%|         | 831/7600 [00:10<01:42, 66.23it/s]Training CobwebTree:  11%|         | 838/7600 [00:10<01:41, 66.61it/s]Training CobwebTree:  11%|         | 846/7600 [00:10<01:36, 69.69it/s]Training CobwebTree:  11%|         | 853/7600 [00:10<01:38, 68.36it/s]Training CobwebTree:  11%|        | 860/7600 [00:11<01:39, 67.85it/s]Training CobwebTree:  11%|        | 868/7600 [00:11<01:37, 68.98it/s]Training CobwebTree:  12%|        | 875/7600 [00:11<01:43, 64.84it/s]Training CobwebTree:  12%|        | 882/7600 [00:11<01:44, 64.54it/s]Training CobwebTree:  12%|        | 890/7600 [00:11<01:40, 66.81it/s]Training CobwebTree:  12%|        | 897/7600 [00:11<01:39, 67.46it/s]Training CobwebTree:  12%|        | 904/7600 [00:11<01:42, 65.37it/s]Training CobwebTree:  12%|        | 912/7600 [00:11<01:38, 68.22it/s]Training CobwebTree:  12%|        | 919/7600 [00:11<01:40, 66.45it/s]Training CobwebTree:  12%|        | 927/7600 [00:12<01:40, 66.28it/s]Training CobwebTree:  12%|        | 935/7600 [00:12<01:39, 66.78it/s]Training CobwebTree:  12%|        | 943/7600 [00:12<01:36, 68.69it/s]Training CobwebTree:  12%|        | 950/7600 [00:12<01:36, 68.74it/s]Training CobwebTree:  13%|        | 957/7600 [00:12<01:39, 66.95it/s]Training CobwebTree:  13%|        | 964/7600 [00:12<01:37, 67.74it/s]Training CobwebTree:  13%|        | 972/7600 [00:12<01:34, 69.87it/s]Training CobwebTree:  13%|        | 979/7600 [00:12<01:37, 67.86it/s]Training CobwebTree:  13%|        | 986/7600 [00:12<01:42, 64.83it/s]Training CobwebTree:  13%|        | 993/7600 [00:13<01:40, 65.48it/s]Training CobwebTree:  13%|        | 1001/7600 [00:13<01:37, 67.91it/s]Training CobwebTree:  13%|        | 1008/7600 [00:13<01:36, 68.11it/s]Training CobwebTree:  13%|        | 1015/7600 [00:13<01:37, 67.80it/s]Training CobwebTree:  13%|        | 1023/7600 [00:13<01:35, 68.97it/s]Training CobwebTree:  14%|        | 1031/7600 [00:13<01:34, 69.36it/s]Training CobwebTree:  14%|        | 1038/7600 [00:13<01:37, 67.54it/s]Training CobwebTree:  14%|        | 1045/7600 [00:13<01:36, 67.88it/s]Training CobwebTree:  14%|        | 1052/7600 [00:13<01:45, 62.33it/s]Training CobwebTree:  14%|        | 1060/7600 [00:14<01:41, 64.63it/s]Training CobwebTree:  14%|        | 1068/7600 [00:14<01:37, 67.16it/s]Training CobwebTree:  14%|        | 1076/7600 [00:14<01:35, 68.52it/s]Training CobwebTree:  14%|        | 1083/7600 [00:14<01:36, 67.39it/s]Training CobwebTree:  14%|        | 1090/7600 [00:14<01:38, 66.11it/s]Training CobwebTree:  14%|        | 1097/7600 [00:14<01:39, 65.29it/s]Training CobwebTree:  15%|        | 1105/7600 [00:14<01:39, 65.41it/s]Training CobwebTree:  15%|        | 1112/7600 [00:14<01:39, 65.02it/s]Training CobwebTree:  15%|        | 1121/7600 [00:14<01:33, 69.16it/s]Training CobwebTree:  15%|        | 1128/7600 [00:15<01:34, 68.80it/s]Training CobwebTree:  15%|        | 1135/7600 [00:15<01:36, 67.18it/s]Training CobwebTree:  15%|        | 1143/7600 [00:15<01:33, 69.36it/s]Training CobwebTree:  15%|        | 1150/7600 [00:15<01:34, 68.56it/s]Training CobwebTree:  15%|        | 1157/7600 [00:15<01:34, 67.90it/s]Training CobwebTree:  15%|        | 1164/7600 [00:15<01:34, 68.21it/s]Training CobwebTree:  15%|        | 1171/7600 [00:15<01:37, 65.66it/s]Training CobwebTree:  16%|        | 1178/7600 [00:15<01:37, 65.78it/s]Training CobwebTree:  16%|        | 1185/7600 [00:15<01:37, 65.81it/s]Training CobwebTree:  16%|        | 1192/7600 [00:16<01:42, 62.23it/s]Training CobwebTree:  16%|        | 1199/7600 [00:16<01:39, 64.18it/s]Training CobwebTree:  16%|        | 1206/7600 [00:16<01:38, 64.82it/s]Training CobwebTree:  16%|        | 1213/7600 [00:16<01:39, 64.01it/s]Training CobwebTree:  16%|        | 1220/7600 [00:16<01:39, 64.25it/s]Training CobwebTree:  16%|        | 1227/7600 [00:16<01:42, 61.94it/s]Training CobwebTree:  16%|        | 1235/7600 [00:16<01:36, 65.69it/s]Training CobwebTree:  16%|        | 1242/7600 [00:16<01:37, 64.90it/s]Training CobwebTree:  16%|        | 1249/7600 [00:16<01:38, 64.69it/s]Training CobwebTree:  17%|        | 1256/7600 [00:17<01:36, 65.40it/s]Training CobwebTree:  17%|        | 1263/7600 [00:17<01:35, 66.61it/s]Training CobwebTree:  17%|        | 1270/7600 [00:17<01:36, 65.47it/s]Training CobwebTree:  17%|        | 1277/7600 [00:17<01:38, 64.00it/s]Training CobwebTree:  17%|        | 1284/7600 [00:17<01:39, 63.37it/s]Training CobwebTree:  17%|        | 1292/7600 [00:17<01:34, 66.69it/s]Training CobwebTree:  17%|        | 1299/7600 [00:17<01:36, 65.54it/s]Training CobwebTree:  17%|        | 1306/7600 [00:17<01:36, 65.53it/s]Training CobwebTree:  17%|        | 1313/7600 [00:17<01:39, 63.39it/s]Training CobwebTree:  17%|        | 1320/7600 [00:18<01:40, 62.57it/s]Training CobwebTree:  17%|        | 1327/7600 [00:18<01:41, 61.75it/s]Training CobwebTree:  18%|        | 1334/7600 [00:18<01:42, 61.29it/s]Training CobwebTree:  18%|        | 1341/7600 [00:18<01:42, 60.81it/s]Training CobwebTree:  18%|        | 1348/7600 [00:18<01:41, 61.63it/s]Training CobwebTree:  18%|        | 1355/7600 [00:18<01:39, 62.52it/s]Training CobwebTree:  18%|        | 1362/7600 [00:18<01:39, 62.82it/s]Training CobwebTree:  18%|        | 1369/7600 [00:18<01:41, 61.17it/s]Training CobwebTree:  18%|        | 1376/7600 [00:18<01:38, 63.25it/s]Training CobwebTree:  18%|        | 1383/7600 [00:19<01:36, 64.52it/s]Training CobwebTree:  18%|        | 1390/7600 [00:19<01:36, 64.48it/s]Training CobwebTree:  18%|        | 1397/7600 [00:19<01:38, 62.74it/s]Training CobwebTree:  18%|        | 1404/7600 [00:19<01:39, 62.23it/s]Training CobwebTree:  19%|        | 1411/7600 [00:19<01:36, 64.13it/s]Training CobwebTree:  19%|        | 1418/7600 [00:19<01:34, 65.51it/s]Training CobwebTree:  19%|        | 1425/7600 [00:19<01:33, 65.81it/s]Training CobwebTree:  19%|        | 1433/7600 [00:19<01:31, 67.49it/s]Training CobwebTree:  19%|        | 1440/7600 [00:19<01:36, 63.85it/s]Training CobwebTree:  19%|        | 1447/7600 [00:20<01:34, 65.44it/s]Training CobwebTree:  19%|        | 1454/7600 [00:20<01:35, 64.46it/s]Training CobwebTree:  19%|        | 1461/7600 [00:20<01:35, 64.00it/s]Training CobwebTree:  19%|        | 1468/7600 [00:20<01:39, 61.64it/s]Training CobwebTree:  19%|        | 1475/7600 [00:20<01:39, 61.83it/s]Training CobwebTree:  20%|        | 1482/7600 [00:20<01:39, 61.63it/s]Training CobwebTree:  20%|        | 1489/7600 [00:20<01:37, 62.92it/s]Training CobwebTree:  20%|        | 1496/7600 [00:20<01:36, 63.19it/s]Training CobwebTree:  20%|        | 1503/7600 [00:20<01:33, 64.95it/s]Training CobwebTree:  20%|        | 1510/7600 [00:21<01:35, 63.48it/s]Training CobwebTree:  20%|        | 1517/7600 [00:21<01:34, 64.15it/s]Training CobwebTree:  20%|        | 1524/7600 [00:21<01:34, 64.21it/s]Training CobwebTree:  20%|        | 1531/7600 [00:21<01:37, 61.94it/s]Training CobwebTree:  20%|        | 1538/7600 [00:21<01:36, 62.54it/s]Training CobwebTree:  20%|        | 1545/7600 [00:21<01:37, 62.19it/s]Training CobwebTree:  20%|        | 1552/7600 [00:21<01:39, 60.82it/s]Training CobwebTree:  21%|        | 1559/7600 [00:21<01:37, 61.93it/s]Training CobwebTree:  21%|        | 1566/7600 [00:21<01:35, 63.41it/s]Training CobwebTree:  21%|        | 1573/7600 [00:22<01:35, 63.36it/s]Training CobwebTree:  21%|        | 1580/7600 [00:22<01:34, 63.50it/s]Training CobwebTree:  21%|        | 1587/7600 [00:22<01:35, 62.71it/s]Training CobwebTree:  21%|        | 1594/7600 [00:22<01:36, 62.42it/s]Training CobwebTree:  21%|        | 1601/7600 [00:22<01:40, 59.96it/s]Training CobwebTree:  21%|        | 1608/7600 [00:22<01:37, 61.40it/s]Training CobwebTree:  21%|       | 1615/7600 [00:22<01:39, 60.41it/s]Training CobwebTree:  21%|       | 1622/7600 [00:22<01:40, 59.27it/s]Training CobwebTree:  21%|       | 1630/7600 [00:22<01:33, 63.72it/s]Training CobwebTree:  22%|       | 1637/7600 [00:23<01:32, 64.20it/s]Training CobwebTree:  22%|       | 1644/7600 [00:23<01:32, 64.55it/s]Training CobwebTree:  22%|       | 1651/7600 [00:23<01:31, 64.68it/s]Training CobwebTree:  22%|       | 1658/7600 [00:23<01:33, 63.24it/s]Training CobwebTree:  22%|       | 1665/7600 [00:23<01:33, 63.59it/s]Training CobwebTree:  22%|       | 1672/7600 [00:23<01:35, 62.20it/s]Training CobwebTree:  22%|       | 1679/7600 [00:23<01:33, 63.36it/s]Training CobwebTree:  22%|       | 1686/7600 [00:23<01:35, 62.03it/s]Training CobwebTree:  22%|       | 1694/7600 [00:23<01:31, 64.79it/s]Training CobwebTree:  22%|       | 1701/7600 [00:24<01:34, 62.39it/s]Training CobwebTree:  22%|       | 1708/7600 [00:24<01:37, 60.59it/s]Training CobwebTree:  23%|       | 1715/7600 [00:24<01:39, 59.31it/s]Training CobwebTree:  23%|       | 1721/7600 [00:24<01:39, 59.03it/s]Training CobwebTree:  23%|       | 1727/7600 [00:24<01:42, 57.19it/s]Training CobwebTree:  23%|       | 1733/7600 [00:24<01:42, 57.19it/s]Training CobwebTree:  23%|       | 1740/7600 [00:24<01:38, 59.58it/s]Training CobwebTree:  23%|       | 1746/7600 [00:24<01:39, 58.84it/s]Training CobwebTree:  23%|       | 1753/7600 [00:24<01:34, 61.62it/s]Training CobwebTree:  23%|       | 1760/7600 [00:25<01:33, 62.22it/s]Training CobwebTree:  23%|       | 1767/7600 [00:25<01:34, 61.87it/s]Training CobwebTree:  23%|       | 1774/7600 [00:25<01:36, 60.27it/s]Training CobwebTree:  23%|       | 1781/7600 [00:25<01:39, 58.54it/s]Training CobwebTree:  24%|       | 1788/7600 [00:25<01:36, 60.35it/s]Training CobwebTree:  24%|       | 1795/7600 [00:25<01:32, 62.48it/s]Training CobwebTree:  24%|       | 1802/7600 [00:25<01:35, 60.79it/s]Training CobwebTree:  24%|       | 1809/7600 [00:25<01:36, 60.22it/s]Training CobwebTree:  24%|       | 1816/7600 [00:25<01:36, 59.70it/s]Training CobwebTree:  24%|       | 1822/7600 [00:26<01:40, 57.45it/s]Training CobwebTree:  24%|       | 1828/7600 [00:26<01:39, 58.09it/s]Training CobwebTree:  24%|       | 1835/7600 [00:26<01:34, 61.07it/s]Training CobwebTree:  24%|       | 1842/7600 [00:26<01:34, 61.21it/s]Training CobwebTree:  24%|       | 1849/7600 [00:26<01:34, 60.69it/s]Training CobwebTree:  24%|       | 1856/7600 [00:26<01:33, 61.20it/s]Training CobwebTree:  25%|       | 1863/7600 [00:26<01:31, 62.59it/s]Training CobwebTree:  25%|       | 1870/7600 [00:26<01:30, 63.21it/s]Training CobwebTree:  25%|       | 1877/7600 [00:26<01:31, 62.42it/s]Training CobwebTree:  25%|       | 1884/7600 [00:27<01:30, 63.11it/s]Training CobwebTree:  25%|       | 1891/7600 [00:27<01:32, 61.95it/s]Training CobwebTree:  25%|       | 1898/7600 [00:27<01:36, 59.25it/s]Training CobwebTree:  25%|       | 1904/7600 [00:27<01:38, 57.83it/s]Training CobwebTree:  25%|       | 1910/7600 [00:27<01:39, 57.45it/s]Training CobwebTree:  25%|       | 1917/7600 [00:27<01:34, 60.02it/s]Training CobwebTree:  25%|       | 1924/7600 [00:27<01:35, 59.37it/s]Training CobwebTree:  25%|       | 1931/7600 [00:27<01:34, 60.13it/s]Training CobwebTree:  26%|       | 1938/7600 [00:28<01:34, 60.01it/s]Training CobwebTree:  26%|       | 1945/7600 [00:28<01:36, 58.83it/s]Training CobwebTree:  26%|       | 1951/7600 [00:28<01:38, 57.37it/s]Training CobwebTree:  26%|       | 1958/7600 [00:28<01:33, 60.22it/s]Training CobwebTree:  26%|       | 1965/7600 [00:28<01:34, 59.57it/s]Training CobwebTree:  26%|       | 1971/7600 [00:28<01:35, 59.10it/s]Training CobwebTree:  26%|       | 1977/7600 [00:28<01:35, 58.73it/s]Training CobwebTree:  26%|       | 1983/7600 [00:28<01:35, 58.94it/s]Training CobwebTree:  26%|       | 1989/7600 [00:28<01:36, 58.25it/s]Training CobwebTree:  26%|       | 1996/7600 [00:29<01:35, 58.76it/s]Training CobwebTree:  26%|       | 2003/7600 [00:29<01:32, 60.70it/s]Training CobwebTree:  26%|       | 2010/7600 [00:29<01:30, 61.98it/s]Training CobwebTree:  27%|       | 2017/7600 [00:29<01:32, 60.24it/s]Training CobwebTree:  27%|       | 2024/7600 [00:29<01:34, 59.22it/s]Training CobwebTree:  27%|       | 2030/7600 [00:29<01:34, 58.66it/s]Training CobwebTree:  27%|       | 2036/7600 [00:29<01:35, 58.12it/s]Training CobwebTree:  27%|       | 2043/7600 [00:29<01:33, 59.53it/s]Training CobwebTree:  27%|       | 2050/7600 [00:29<01:31, 60.47it/s]Training CobwebTree:  27%|       | 2057/7600 [00:30<01:36, 57.44it/s]Training CobwebTree:  27%|       | 2063/7600 [00:30<01:35, 57.86it/s]Training CobwebTree:  27%|       | 2070/7600 [00:30<01:33, 58.85it/s]Training CobwebTree:  27%|       | 2076/7600 [00:30<01:35, 57.97it/s]Training CobwebTree:  27%|       | 2083/7600 [00:30<01:32, 59.78it/s]Training CobwebTree:  27%|       | 2089/7600 [00:30<01:32, 59.69it/s]Training CobwebTree:  28%|       | 2095/7600 [00:30<01:32, 59.49it/s]Training CobwebTree:  28%|       | 2101/7600 [00:30<01:32, 59.22it/s]Training CobwebTree:  28%|       | 2108/7600 [00:30<01:33, 58.92it/s]Training CobwebTree:  28%|       | 2114/7600 [00:30<01:33, 58.79it/s]Training CobwebTree:  28%|       | 2121/7600 [00:31<01:32, 59.48it/s]Training CobwebTree:  28%|       | 2128/7600 [00:31<01:28, 61.73it/s]Training CobwebTree:  28%|       | 2135/7600 [00:31<01:30, 60.70it/s]Training CobwebTree:  28%|       | 2142/7600 [00:31<01:29, 61.18it/s]Training CobwebTree:  28%|       | 2149/7600 [00:31<01:27, 62.12it/s]Training CobwebTree:  28%|       | 2156/7600 [00:31<01:28, 61.43it/s]Training CobwebTree:  28%|       | 2163/7600 [00:31<01:31, 59.62it/s]Training CobwebTree:  29%|       | 2170/7600 [00:31<01:30, 59.94it/s]Training CobwebTree:  29%|       | 2177/7600 [00:32<01:29, 60.65it/s]Training CobwebTree:  29%|       | 2184/7600 [00:32<01:29, 60.84it/s]Training CobwebTree:  29%|       | 2191/7600 [00:32<01:29, 60.56it/s]Training CobwebTree:  29%|       | 2198/7600 [00:32<01:28, 61.17it/s]Training CobwebTree:  29%|       | 2205/7600 [00:32<01:28, 60.91it/s]Training CobwebTree:  29%|       | 2212/7600 [00:32<01:27, 61.31it/s]Training CobwebTree:  29%|       | 2219/7600 [00:32<01:29, 60.37it/s]Training CobwebTree:  29%|       | 2226/7600 [00:32<01:30, 59.60it/s]Training CobwebTree:  29%|       | 2233/7600 [00:32<01:29, 59.97it/s]Training CobwebTree:  29%|       | 2240/7600 [00:33<01:29, 59.82it/s]Training CobwebTree:  30%|       | 2247/7600 [00:33<01:29, 59.92it/s]Training CobwebTree:  30%|       | 2253/7600 [00:33<01:30, 59.12it/s]Training CobwebTree:  30%|       | 2259/7600 [00:33<01:32, 57.94it/s]Training CobwebTree:  30%|       | 2265/7600 [00:33<01:32, 57.61it/s]Training CobwebTree:  30%|       | 2272/7600 [00:33<01:30, 58.65it/s]Training CobwebTree:  30%|       | 2278/7600 [00:33<01:31, 58.12it/s]Training CobwebTree:  30%|       | 2285/7600 [00:33<01:29, 59.25it/s]Training CobwebTree:  30%|       | 2291/7600 [00:33<01:31, 58.25it/s]Training CobwebTree:  30%|       | 2297/7600 [00:34<01:32, 57.62it/s]Training CobwebTree:  30%|       | 2304/7600 [00:34<01:27, 60.81it/s]Training CobwebTree:  30%|       | 2311/7600 [00:34<01:25, 61.79it/s]Training CobwebTree:  30%|       | 2318/7600 [00:34<01:23, 63.05it/s]Training CobwebTree:  31%|       | 2325/7600 [00:34<01:27, 60.52it/s]Training CobwebTree:  31%|       | 2332/7600 [00:34<01:27, 60.54it/s]Training CobwebTree:  31%|       | 2339/7600 [00:34<01:24, 62.51it/s]Training CobwebTree:  31%|       | 2346/7600 [00:34<01:27, 59.89it/s]Training CobwebTree:  31%|       | 2353/7600 [00:34<01:27, 59.69it/s]Training CobwebTree:  31%|       | 2360/7600 [00:35<01:30, 58.20it/s]Training CobwebTree:  31%|       | 2366/7600 [00:35<01:29, 58.42it/s]Training CobwebTree:  31%|       | 2372/7600 [00:35<01:30, 58.07it/s]Training CobwebTree:  31%|      | 2378/7600 [00:35<01:29, 58.48it/s]Training CobwebTree:  31%|      | 2384/7600 [00:35<01:29, 58.12it/s]Training CobwebTree:  31%|      | 2390/7600 [00:35<01:29, 58.12it/s]Training CobwebTree:  32%|      | 2397/7600 [00:35<01:28, 58.64it/s]Training CobwebTree:  32%|      | 2403/7600 [00:35<01:30, 57.41it/s]Training CobwebTree:  32%|      | 2410/7600 [00:35<01:27, 59.23it/s]Training CobwebTree:  32%|      | 2417/7600 [00:36<01:25, 60.32it/s]Training CobwebTree:  32%|      | 2424/7600 [00:36<01:27, 59.00it/s]Training CobwebTree:  32%|      | 2431/7600 [00:36<01:27, 59.39it/s]Training CobwebTree:  32%|      | 2437/7600 [00:36<01:28, 58.28it/s]Training CobwebTree:  32%|      | 2443/7600 [00:36<01:30, 56.72it/s]Training CobwebTree:  32%|      | 2449/7600 [00:36<01:30, 56.76it/s]Training CobwebTree:  32%|      | 2456/7600 [00:36<01:28, 57.82it/s]Training CobwebTree:  32%|      | 2462/7600 [00:36<01:29, 57.68it/s]Training CobwebTree:  32%|      | 2469/7600 [00:36<01:25, 60.25it/s]Training CobwebTree:  33%|      | 2476/7600 [00:37<01:25, 59.80it/s]Training CobwebTree:  33%|      | 2482/7600 [00:37<01:27, 58.17it/s]Training CobwebTree:  33%|      | 2489/7600 [00:37<01:26, 59.32it/s]Training CobwebTree:  33%|      | 2495/7600 [00:37<01:27, 58.27it/s]Training CobwebTree:  33%|      | 2501/7600 [00:37<01:28, 57.88it/s]Training CobwebTree:  33%|      | 2507/7600 [00:37<01:30, 56.43it/s]Training CobwebTree:  33%|      | 2513/7600 [00:37<01:32, 54.91it/s]Training CobwebTree:  33%|      | 2519/7600 [00:37<01:32, 54.76it/s]Training CobwebTree:  33%|      | 2525/7600 [00:37<01:31, 55.42it/s]Training CobwebTree:  33%|      | 2532/7600 [00:38<01:29, 56.63it/s]Training CobwebTree:  33%|      | 2539/7600 [00:38<01:27, 57.98it/s]Training CobwebTree:  33%|      | 2545/7600 [00:38<01:27, 57.94it/s]Training CobwebTree:  34%|      | 2552/7600 [00:38<01:26, 58.39it/s]Training CobwebTree:  34%|      | 2558/7600 [00:38<01:29, 56.08it/s]Training CobwebTree:  34%|      | 2564/7600 [00:38<01:31, 54.86it/s]Training CobwebTree:  34%|      | 2570/7600 [00:38<01:31, 55.17it/s]Training CobwebTree:  34%|      | 2577/7600 [00:38<01:25, 58.62it/s]Training CobwebTree:  34%|      | 2584/7600 [00:38<01:23, 60.05it/s]Training CobwebTree:  34%|      | 2591/7600 [00:39<01:21, 61.22it/s]Training CobwebTree:  34%|      | 2598/7600 [00:39<01:24, 59.04it/s]Training CobwebTree:  34%|      | 2604/7600 [00:39<01:25, 58.77it/s]Training CobwebTree:  34%|      | 2610/7600 [00:39<01:24, 58.82it/s]Training CobwebTree:  34%|      | 2616/7600 [00:39<01:24, 58.74it/s]Training CobwebTree:  34%|      | 2622/7600 [00:39<01:24, 58.87it/s]Training CobwebTree:  35%|      | 2628/7600 [00:39<01:25, 57.87it/s]Training CobwebTree:  35%|      | 2635/7600 [00:39<01:24, 58.48it/s]Training CobwebTree:  35%|      | 2641/7600 [00:39<01:24, 58.56it/s]Training CobwebTree:  35%|      | 2647/7600 [00:40<01:24, 58.94it/s]Training CobwebTree:  35%|      | 2653/7600 [00:40<01:26, 56.94it/s]Training CobwebTree:  35%|      | 2659/7600 [00:40<01:25, 57.48it/s]Training CobwebTree:  35%|      | 2665/7600 [00:40<01:28, 55.97it/s]Training CobwebTree:  35%|      | 2672/7600 [00:40<01:25, 57.96it/s]Training CobwebTree:  35%|      | 2678/7600 [00:40<01:26, 56.59it/s]Training CobwebTree:  35%|      | 2684/7600 [00:40<01:26, 56.79it/s]Training CobwebTree:  35%|      | 2690/7600 [00:40<01:27, 55.99it/s]Training CobwebTree:  35%|      | 2696/7600 [00:40<01:27, 55.96it/s]Training CobwebTree:  36%|      | 2703/7600 [00:41<01:25, 57.14it/s]Training CobwebTree:  36%|      | 2709/7600 [00:41<01:26, 56.40it/s]Training CobwebTree:  36%|      | 2715/7600 [00:41<01:27, 55.96it/s]Training CobwebTree:  36%|      | 2721/7600 [00:41<01:25, 57.00it/s]Training CobwebTree:  36%|      | 2728/7600 [00:41<01:21, 60.02it/s]Training CobwebTree:  36%|      | 2735/7600 [00:41<01:21, 59.73it/s]Training CobwebTree:  36%|      | 2741/7600 [00:41<01:21, 59.60it/s]Training CobwebTree:  36%|      | 2747/7600 [00:41<01:21, 59.33it/s]Training CobwebTree:  36%|      | 2754/7600 [00:41<01:21, 59.60it/s]Training CobwebTree:  36%|      | 2761/7600 [00:42<01:21, 59.40it/s]Training CobwebTree:  36%|      | 2767/7600 [00:42<01:22, 58.90it/s]Training CobwebTree:  36%|      | 2773/7600 [00:42<01:26, 55.90it/s]Training CobwebTree:  37%|      | 2779/7600 [00:42<01:26, 55.65it/s]Training CobwebTree:  37%|      | 2785/7600 [00:42<01:30, 53.13it/s]Training CobwebTree:  37%|      | 2791/7600 [00:42<01:30, 53.20it/s]Training CobwebTree:  37%|      | 2797/7600 [00:42<01:29, 53.92it/s]Training CobwebTree:  37%|      | 2803/7600 [00:42<01:28, 54.36it/s]Training CobwebTree:  37%|      | 2809/7600 [00:42<01:28, 54.09it/s]Training CobwebTree:  37%|      | 2815/7600 [00:43<01:26, 55.12it/s]Training CobwebTree:  37%|      | 2821/7600 [00:43<01:26, 54.98it/s]Training CobwebTree:  37%|      | 2827/7600 [00:43<01:26, 55.50it/s]Training CobwebTree:  37%|      | 2833/7600 [00:43<01:25, 56.00it/s]Training CobwebTree:  37%|      | 2839/7600 [00:43<01:23, 56.92it/s]Training CobwebTree:  37%|      | 2845/7600 [00:43<01:23, 57.15it/s]Training CobwebTree:  38%|      | 2851/7600 [00:43<01:22, 57.32it/s]Training CobwebTree:  38%|      | 2857/7600 [00:43<01:21, 58.05it/s]Training CobwebTree:  38%|      | 2864/7600 [00:43<01:18, 60.08it/s]Training CobwebTree:  38%|      | 2871/7600 [00:43<01:18, 60.00it/s]Training CobwebTree:  38%|      | 2878/7600 [00:44<01:19, 59.70it/s]Training CobwebTree:  38%|      | 2884/7600 [00:44<01:19, 59.52it/s]Training CobwebTree:  38%|      | 2890/7600 [00:44<01:19, 59.58it/s]Training CobwebTree:  38%|      | 2896/7600 [00:44<01:19, 58.91it/s]Training CobwebTree:  38%|      | 2902/7600 [00:44<01:20, 58.22it/s]Training CobwebTree:  38%|      | 2908/7600 [00:44<01:23, 56.14it/s]Training CobwebTree:  38%|      | 2914/7600 [00:44<01:22, 56.48it/s]Training CobwebTree:  38%|      | 2920/7600 [00:44<01:22, 56.87it/s]Training CobwebTree:  39%|      | 2927/7600 [00:44<01:20, 57.78it/s]Training CobwebTree:  39%|      | 2933/7600 [00:45<01:24, 55.50it/s]Training CobwebTree:  39%|      | 2939/7600 [00:45<01:22, 56.37it/s]Training CobwebTree:  39%|      | 2946/7600 [00:45<01:20, 57.77it/s]Training CobwebTree:  39%|      | 2952/7600 [00:45<01:20, 57.88it/s]Training CobwebTree:  39%|      | 2958/7600 [00:45<01:22, 56.08it/s]Training CobwebTree:  39%|      | 2964/7600 [00:45<01:23, 55.33it/s]Training CobwebTree:  39%|      | 2970/7600 [00:45<01:23, 55.55it/s]Training CobwebTree:  39%|      | 2976/7600 [00:45<01:24, 54.84it/s]Training CobwebTree:  39%|      | 2982/7600 [00:45<01:26, 53.23it/s]Training CobwebTree:  39%|      | 2988/7600 [00:46<01:24, 54.82it/s]Training CobwebTree:  39%|      | 2994/7600 [00:46<01:22, 55.94it/s]Training CobwebTree:  39%|      | 3000/7600 [00:46<01:23, 55.20it/s]Training CobwebTree:  40%|      | 3006/7600 [00:46<01:22, 55.70it/s]Training CobwebTree:  40%|      | 3012/7600 [00:46<01:21, 56.04it/s]Training CobwebTree:  40%|      | 3018/7600 [00:46<01:21, 55.89it/s]Training CobwebTree:  40%|      | 3025/7600 [00:46<01:19, 57.55it/s]Training CobwebTree:  40%|      | 3032/7600 [00:46<01:17, 59.27it/s]Training CobwebTree:  40%|      | 3038/7600 [00:46<01:19, 57.21it/s]Training CobwebTree:  40%|      | 3044/7600 [00:47<01:20, 56.86it/s]Training CobwebTree:  40%|      | 3050/7600 [00:47<01:20, 56.81it/s]Training CobwebTree:  40%|      | 3056/7600 [00:47<01:21, 55.43it/s]Training CobwebTree:  40%|      | 3063/7600 [00:47<01:18, 57.68it/s]Training CobwebTree:  40%|      | 3069/7600 [00:47<01:18, 57.83it/s]Training CobwebTree:  40%|      | 3075/7600 [00:47<01:17, 58.17it/s]Training CobwebTree:  41%|      | 3082/7600 [00:47<01:16, 59.21it/s]Training CobwebTree:  41%|      | 3088/7600 [00:47<01:16, 58.63it/s]Training CobwebTree:  41%|      | 3094/7600 [00:47<01:19, 56.59it/s]Training CobwebTree:  41%|      | 3100/7600 [00:47<01:19, 56.90it/s]Training CobwebTree:  41%|      | 3106/7600 [00:48<01:19, 56.43it/s]Training CobwebTree:  41%|      | 3112/7600 [00:48<01:19, 56.30it/s]Training CobwebTree:  41%|      | 3118/7600 [00:48<01:20, 55.40it/s]Training CobwebTree:  41%|      | 3124/7600 [00:48<01:24, 53.15it/s]Training CobwebTree:  41%|      | 3131/7600 [00:48<01:20, 55.72it/s]Training CobwebTree:  41%|     | 3137/7600 [00:48<01:21, 54.53it/s]Training CobwebTree:  41%|     | 3143/7600 [00:48<01:20, 55.10it/s]Training CobwebTree:  41%|     | 3149/7600 [00:48<01:21, 54.33it/s]Training CobwebTree:  42%|     | 3155/7600 [00:49<01:20, 54.91it/s]Training CobwebTree:  42%|     | 3161/7600 [00:49<01:22, 53.89it/s]Training CobwebTree:  42%|     | 3167/7600 [00:49<01:20, 55.04it/s]Training CobwebTree:  42%|     | 3173/7600 [00:49<01:20, 55.21it/s]Training CobwebTree:  42%|     | 3179/7600 [00:49<01:20, 55.07it/s]Training CobwebTree:  42%|     | 3185/7600 [00:49<01:19, 55.70it/s]Training CobwebTree:  42%|     | 3191/7600 [00:49<01:20, 55.04it/s]Training CobwebTree:  42%|     | 3197/7600 [00:49<01:19, 55.16it/s]Training CobwebTree:  42%|     | 3203/7600 [00:49<01:21, 53.84it/s]Training CobwebTree:  42%|     | 3209/7600 [00:49<01:19, 54.91it/s]Training CobwebTree:  42%|     | 3215/7600 [00:50<01:21, 53.78it/s]Training CobwebTree:  42%|     | 3221/7600 [00:50<01:18, 55.47it/s]Training CobwebTree:  42%|     | 3227/7600 [00:50<01:17, 56.36it/s]Training CobwebTree:  43%|     | 3234/7600 [00:50<01:15, 57.61it/s]Training CobwebTree:  43%|     | 3241/7600 [00:50<01:15, 57.84it/s]Training CobwebTree:  43%|     | 3247/7600 [00:50<01:16, 57.27it/s]Training CobwebTree:  43%|     | 3253/7600 [00:50<01:18, 55.69it/s]Training CobwebTree:  43%|     | 3259/7600 [00:50<01:19, 54.84it/s]Training CobwebTree:  43%|     | 3265/7600 [00:50<01:18, 55.16it/s]Training CobwebTree:  43%|     | 3271/7600 [00:51<01:20, 54.08it/s]Training CobwebTree:  43%|     | 3277/7600 [00:51<01:18, 55.15it/s]Training CobwebTree:  43%|     | 3283/7600 [00:51<01:20, 53.75it/s]Training CobwebTree:  43%|     | 3289/7600 [00:51<01:17, 55.42it/s]Training CobwebTree:  43%|     | 3295/7600 [00:51<01:17, 55.50it/s]Training CobwebTree:  43%|     | 3301/7600 [00:51<01:18, 54.45it/s]Training CobwebTree:  44%|     | 3308/7600 [00:51<01:14, 57.66it/s]Training CobwebTree:  44%|     | 3314/7600 [00:51<01:15, 57.09it/s]Training CobwebTree:  44%|     | 3320/7600 [00:51<01:16, 55.75it/s]Training CobwebTree:  44%|     | 3326/7600 [00:52<01:16, 56.14it/s]Training CobwebTree:  44%|     | 3332/7600 [00:52<01:18, 54.25it/s]Training CobwebTree:  44%|     | 3339/7600 [00:52<01:14, 57.43it/s]Training CobwebTree:  44%|     | 3345/7600 [00:52<01:16, 55.48it/s]Training CobwebTree:  44%|     | 3351/7600 [00:52<01:15, 56.45it/s]Training CobwebTree:  44%|     | 3357/7600 [00:52<01:15, 55.91it/s]Training CobwebTree:  44%|     | 3364/7600 [00:52<01:11, 58.89it/s]Training CobwebTree:  44%|     | 3370/7600 [00:52<01:12, 58.63it/s]Training CobwebTree:  44%|     | 3377/7600 [00:52<01:10, 59.62it/s]Training CobwebTree:  45%|     | 3383/7600 [00:53<01:11, 58.70it/s]Training CobwebTree:  45%|     | 3389/7600 [00:53<01:13, 57.26it/s]Training CobwebTree:  45%|     | 3395/7600 [00:53<01:15, 55.90it/s]Training CobwebTree:  45%|     | 3402/7600 [00:53<01:12, 57.96it/s]Training CobwebTree:  45%|     | 3408/7600 [00:53<01:12, 57.81it/s]Training CobwebTree:  45%|     | 3414/7600 [00:53<01:12, 57.37it/s]Training CobwebTree:  45%|     | 3420/7600 [00:53<01:16, 54.92it/s]Training CobwebTree:  45%|     | 3426/7600 [00:53<01:16, 54.29it/s]Training CobwebTree:  45%|     | 3433/7600 [00:53<01:14, 56.01it/s]Training CobwebTree:  45%|     | 3440/7600 [00:54<01:13, 56.69it/s]Training CobwebTree:  45%|     | 3446/7600 [00:54<01:15, 55.26it/s]Training CobwebTree:  45%|     | 3452/7600 [00:54<01:15, 54.79it/s]Training CobwebTree:  46%|     | 3458/7600 [00:54<01:16, 54.14it/s]Training CobwebTree:  46%|     | 3464/7600 [00:54<01:16, 53.99it/s]Training CobwebTree:  46%|     | 3470/7600 [00:54<01:15, 54.82it/s]Training CobwebTree:  46%|     | 3477/7600 [00:54<01:13, 56.32it/s]Training CobwebTree:  46%|     | 3483/7600 [00:54<01:14, 55.54it/s]Training CobwebTree:  46%|     | 3489/7600 [00:55<01:19, 52.04it/s]Training CobwebTree:  46%|     | 3495/7600 [00:55<01:18, 52.54it/s]Training CobwebTree:  46%|     | 3501/7600 [00:55<01:17, 52.71it/s]Training CobwebTree:  46%|     | 3507/7600 [00:55<01:18, 52.37it/s]Training CobwebTree:  46%|     | 3513/7600 [00:55<01:19, 51.50it/s]Training CobwebTree:  46%|     | 3519/7600 [00:55<01:21, 50.13it/s]Training CobwebTree:  46%|     | 3525/7600 [00:55<01:18, 51.96it/s]Training CobwebTree:  46%|     | 3531/7600 [00:55<01:16, 53.44it/s]Training CobwebTree:  47%|     | 3537/7600 [00:55<01:16, 53.07it/s]Training CobwebTree:  47%|     | 3543/7600 [00:56<01:14, 54.52it/s]Training CobwebTree:  47%|     | 3549/7600 [00:56<01:13, 54.88it/s]Training CobwebTree:  47%|     | 3555/7600 [00:56<01:13, 55.00it/s]Training CobwebTree:  47%|     | 3561/7600 [00:56<01:13, 55.03it/s]Training CobwebTree:  47%|     | 3567/7600 [00:56<01:12, 55.60it/s]Training CobwebTree:  47%|     | 3573/7600 [00:56<01:11, 56.35it/s]Training CobwebTree:  47%|     | 3579/7600 [00:56<01:12, 55.75it/s]Training CobwebTree:  47%|     | 3585/7600 [00:56<01:12, 55.66it/s]Training CobwebTree:  47%|     | 3592/7600 [00:56<01:10, 56.68it/s]Training CobwebTree:  47%|     | 3598/7600 [00:57<01:14, 53.84it/s]Training CobwebTree:  47%|     | 3604/7600 [00:57<01:13, 54.09it/s]Training CobwebTree:  48%|     | 3610/7600 [00:57<01:18, 50.80it/s]Training CobwebTree:  48%|     | 3616/7600 [00:57<01:18, 50.84it/s]Training CobwebTree:  48%|     | 3622/7600 [00:57<01:15, 52.93it/s]Training CobwebTree:  48%|     | 3628/7600 [00:57<01:14, 53.09it/s]Training CobwebTree:  48%|     | 3634/7600 [00:57<01:14, 53.17it/s]Training CobwebTree:  48%|     | 3640/7600 [00:57<01:15, 52.39it/s]Training CobwebTree:  48%|     | 3646/7600 [00:57<01:12, 54.33it/s]Training CobwebTree:  48%|     | 3652/7600 [00:58<01:11, 54.85it/s]Training CobwebTree:  48%|     | 3658/7600 [00:58<01:13, 53.34it/s]Training CobwebTree:  48%|     | 3664/7600 [00:58<01:13, 53.31it/s]Training CobwebTree:  48%|     | 3670/7600 [00:58<01:14, 52.71it/s]Training CobwebTree:  48%|     | 3676/7600 [00:58<01:14, 52.79it/s]Training CobwebTree:  48%|     | 3682/7600 [00:58<01:12, 53.75it/s]Training CobwebTree:  49%|     | 3688/7600 [00:58<01:12, 54.27it/s]Training CobwebTree:  49%|     | 3694/7600 [00:58<01:12, 54.19it/s]Training CobwebTree:  49%|     | 3700/7600 [00:58<01:11, 54.57it/s]Training CobwebTree:  49%|     | 3706/7600 [00:59<01:11, 54.20it/s]Training CobwebTree:  49%|     | 3712/7600 [00:59<01:09, 55.58it/s]Training CobwebTree:  49%|     | 3718/7600 [00:59<01:11, 54.38it/s]Training CobwebTree:  49%|     | 3724/7600 [00:59<01:10, 55.20it/s]Training CobwebTree:  49%|     | 3730/7600 [00:59<01:10, 54.55it/s]Training CobwebTree:  49%|     | 3736/7600 [00:59<01:09, 55.75it/s]Training CobwebTree:  49%|     | 3742/7600 [00:59<01:10, 54.79it/s]Training CobwebTree:  49%|     | 3748/7600 [00:59<01:11, 53.92it/s]Training CobwebTree:  49%|     | 3754/7600 [00:59<01:10, 54.54it/s]Training CobwebTree:  49%|     | 3760/7600 [01:00<01:11, 53.60it/s]Training CobwebTree:  50%|     | 3766/7600 [01:00<01:12, 53.19it/s]Training CobwebTree:  50%|     | 3772/7600 [01:00<01:10, 54.08it/s]Training CobwebTree:  50%|     | 3778/7600 [01:00<01:08, 55.73it/s]Training CobwebTree:  50%|     | 3784/7600 [01:00<01:10, 54.39it/s]Training CobwebTree:  50%|     | 3790/7600 [01:00<01:11, 53.15it/s]Training CobwebTree:  50%|     | 3796/7600 [01:00<01:10, 53.67it/s]Training CobwebTree:  50%|     | 3802/7600 [01:00<01:11, 53.14it/s]Training CobwebTree:  50%|     | 3808/7600 [01:00<01:10, 54.13it/s]Training CobwebTree:  50%|     | 3814/7600 [01:01<01:10, 53.48it/s]Training CobwebTree:  50%|     | 3820/7600 [01:01<01:08, 54.91it/s]Training CobwebTree:  50%|     | 3826/7600 [01:01<01:09, 54.68it/s]Training CobwebTree:  50%|     | 3832/7600 [01:01<01:09, 53.89it/s]Training CobwebTree:  50%|     | 3838/7600 [01:01<01:13, 51.49it/s]Training CobwebTree:  51%|     | 3844/7600 [01:01<01:11, 52.67it/s]Training CobwebTree:  51%|     | 3850/7600 [01:01<01:12, 52.05it/s]Training CobwebTree:  51%|     | 3856/7600 [01:01<01:12, 51.41it/s]Training CobwebTree:  51%|     | 3862/7600 [01:01<01:10, 52.74it/s]Training CobwebTree:  51%|     | 3868/7600 [01:02<01:09, 53.63it/s]Training CobwebTree:  51%|     | 3874/7600 [01:02<01:10, 52.64it/s]Training CobwebTree:  51%|     | 3880/7600 [01:02<01:11, 52.05it/s]Training CobwebTree:  51%|     | 3886/7600 [01:02<01:09, 53.82it/s]Training CobwebTree:  51%|     | 3892/7600 [01:02<01:08, 54.01it/s]Training CobwebTree:  51%|    | 3898/7600 [01:02<01:09, 53.50it/s]Training CobwebTree:  51%|    | 3904/7600 [01:02<01:11, 52.01it/s]Training CobwebTree:  51%|    | 3910/7600 [01:02<01:09, 53.33it/s]Training CobwebTree:  52%|    | 3916/7600 [01:02<01:11, 51.53it/s]Training CobwebTree:  52%|    | 3922/7600 [01:03<01:12, 50.71it/s]Training CobwebTree:  52%|    | 3928/7600 [01:03<01:11, 51.45it/s]Training CobwebTree:  52%|    | 3934/7600 [01:03<01:12, 50.38it/s]Training CobwebTree:  52%|    | 3940/7600 [01:03<01:09, 52.72it/s]Training CobwebTree:  52%|    | 3946/7600 [01:03<01:07, 53.96it/s]Training CobwebTree:  52%|    | 3952/7600 [01:03<01:07, 54.08it/s]Training CobwebTree:  52%|    | 3958/7600 [01:03<01:10, 51.93it/s]Training CobwebTree:  52%|    | 3964/7600 [01:03<01:11, 50.65it/s]Training CobwebTree:  52%|    | 3970/7600 [01:04<01:10, 51.25it/s]Training CobwebTree:  52%|    | 3976/7600 [01:04<01:08, 53.21it/s]Training CobwebTree:  52%|    | 3982/7600 [01:04<01:07, 53.32it/s]Training CobwebTree:  52%|    | 3988/7600 [01:04<01:07, 53.38it/s]Training CobwebTree:  53%|    | 3994/7600 [01:04<01:06, 54.46it/s]Training CobwebTree:  53%|    | 4000/7600 [01:04<01:05, 54.92it/s]Training CobwebTree:  53%|    | 4006/7600 [01:04<01:06, 54.22it/s]Training CobwebTree:  53%|    | 4012/7600 [01:04<01:04, 55.27it/s]Training CobwebTree:  53%|    | 4018/7600 [01:04<01:03, 56.41it/s]Training CobwebTree:  53%|    | 4024/7600 [01:05<01:07, 53.35it/s]Training CobwebTree:  53%|    | 4030/7600 [01:05<01:06, 53.71it/s]Training CobwebTree:  53%|    | 4036/7600 [01:05<01:07, 52.78it/s]Training CobwebTree:  53%|    | 4042/7600 [01:05<01:07, 52.68it/s]Training CobwebTree:  53%|    | 4048/7600 [01:05<01:07, 52.25it/s]Training CobwebTree:  53%|    | 4054/7600 [01:05<01:05, 54.11it/s]Training CobwebTree:  53%|    | 4060/7600 [01:05<01:04, 54.63it/s]Training CobwebTree:  54%|    | 4067/7600 [01:05<01:02, 56.45it/s]Training CobwebTree:  54%|    | 4073/7600 [01:05<01:04, 54.31it/s]Training CobwebTree:  54%|    | 4079/7600 [01:06<01:04, 54.89it/s]Training CobwebTree:  54%|    | 4085/7600 [01:06<01:04, 54.17it/s]Training CobwebTree:  54%|    | 4091/7600 [01:06<01:07, 52.21it/s]Training CobwebTree:  54%|    | 4097/7600 [01:06<01:05, 53.20it/s]Training CobwebTree:  54%|    | 4103/7600 [01:06<01:04, 53.82it/s]Training CobwebTree:  54%|    | 4109/7600 [01:06<01:06, 52.52it/s]Training CobwebTree:  54%|    | 4115/7600 [01:06<01:05, 53.31it/s]Training CobwebTree:  54%|    | 4121/7600 [01:06<01:07, 51.84it/s]Training CobwebTree:  54%|    | 4127/7600 [01:06<01:06, 52.32it/s]Training CobwebTree:  54%|    | 4133/7600 [01:07<01:07, 51.51it/s]Training CobwebTree:  54%|    | 4139/7600 [01:07<01:06, 52.16it/s]Training CobwebTree:  55%|    | 4145/7600 [01:07<01:05, 52.89it/s]Training CobwebTree:  55%|    | 4151/7600 [01:07<01:08, 50.52it/s]Training CobwebTree:  55%|    | 4157/7600 [01:07<01:07, 50.79it/s]Training CobwebTree:  55%|    | 4163/7600 [01:07<01:08, 50.30it/s]Training CobwebTree:  55%|    | 4169/7600 [01:07<01:10, 48.97it/s]Training CobwebTree:  55%|    | 4175/7600 [01:07<01:08, 50.08it/s]Training CobwebTree:  55%|    | 4181/7600 [01:08<01:07, 50.89it/s]Training CobwebTree:  55%|    | 4187/7600 [01:08<01:08, 49.71it/s]Training CobwebTree:  55%|    | 4192/7600 [01:08<01:10, 48.24it/s]Training CobwebTree:  55%|    | 4197/7600 [01:08<01:10, 48.24it/s]Training CobwebTree:  55%|    | 4203/7600 [01:08<01:07, 50.12it/s]Training CobwebTree:  55%|    | 4209/7600 [01:08<01:12, 46.85it/s]Training CobwebTree:  55%|    | 4214/7600 [01:08<01:14, 45.57it/s]Training CobwebTree:  56%|    | 4220/7600 [01:08<01:13, 45.98it/s]Training CobwebTree:  56%|    | 4226/7600 [01:08<01:10, 48.04it/s]Training CobwebTree:  56%|    | 4232/7600 [01:09<01:06, 50.29it/s]Training CobwebTree:  56%|    | 4238/7600 [01:09<01:05, 51.33it/s]Training CobwebTree:  56%|    | 4244/7600 [01:09<01:02, 53.46it/s]Training CobwebTree:  56%|    | 4250/7600 [01:09<01:01, 54.33it/s]Training CobwebTree:  56%|    | 4256/7600 [01:09<01:01, 54.01it/s]Training CobwebTree:  56%|    | 4262/7600 [01:09<01:01, 54.19it/s]Training CobwebTree:  56%|    | 4268/7600 [01:09<01:01, 54.21it/s]Training CobwebTree:  56%|    | 4274/7600 [01:09<01:00, 54.57it/s]Training CobwebTree:  56%|    | 4280/7600 [01:09<01:00, 54.66it/s]Training CobwebTree:  56%|    | 4286/7600 [01:10<01:01, 53.90it/s]Training CobwebTree:  56%|    | 4292/7600 [01:10<01:02, 53.26it/s]Training CobwebTree:  57%|    | 4298/7600 [01:10<01:01, 53.26it/s]Training CobwebTree:  57%|    | 4304/7600 [01:10<01:02, 52.52it/s]Training CobwebTree:  57%|    | 4310/7600 [01:10<01:03, 51.97it/s]Training CobwebTree:  57%|    | 4316/7600 [01:10<01:02, 52.25it/s]Training CobwebTree:  57%|    | 4322/7600 [01:10<01:00, 54.15it/s]Training CobwebTree:  57%|    | 4328/7600 [01:10<01:00, 54.27it/s]Training CobwebTree:  57%|    | 4334/7600 [01:10<01:01, 53.40it/s]Training CobwebTree:  57%|    | 4340/7600 [01:11<01:01, 52.61it/s]Training CobwebTree:  57%|    | 4346/7600 [01:11<01:01, 53.09it/s]Training CobwebTree:  57%|    | 4352/7600 [01:11<01:01, 52.49it/s]Training CobwebTree:  57%|    | 4358/7600 [01:11<01:00, 53.93it/s]Training CobwebTree:  57%|    | 4364/7600 [01:11<01:00, 53.83it/s]Training CobwebTree:  57%|    | 4370/7600 [01:11<00:58, 54.77it/s]Training CobwebTree:  58%|    | 4376/7600 [01:11<00:59, 54.15it/s]Training CobwebTree:  58%|    | 4382/7600 [01:11<01:01, 52.52it/s]Training CobwebTree:  58%|    | 4388/7600 [01:11<01:00, 52.84it/s]Training CobwebTree:  58%|    | 4394/7600 [01:12<01:03, 50.61it/s]Training CobwebTree:  58%|    | 4400/7600 [01:12<01:02, 51.46it/s]Training CobwebTree:  58%|    | 4406/7600 [01:12<01:01, 52.16it/s]Training CobwebTree:  58%|    | 4412/7600 [01:12<01:02, 51.25it/s]Training CobwebTree:  58%|    | 4418/7600 [01:12<01:03, 50.20it/s]Training CobwebTree:  58%|    | 4424/7600 [01:12<01:04, 49.31it/s]Training CobwebTree:  58%|    | 4430/7600 [01:12<01:03, 49.68it/s]Training CobwebTree:  58%|    | 4436/7600 [01:12<01:02, 50.65it/s]Training CobwebTree:  58%|    | 4442/7600 [01:13<01:01, 51.12it/s]Training CobwebTree:  59%|    | 4448/7600 [01:13<01:01, 51.42it/s]Training CobwebTree:  59%|    | 4454/7600 [01:13<01:01, 51.12it/s]Training CobwebTree:  59%|    | 4460/7600 [01:13<01:02, 50.14it/s]Training CobwebTree:  59%|    | 4466/7600 [01:13<01:02, 49.93it/s]Training CobwebTree:  59%|    | 4472/7600 [01:13<01:00, 51.36it/s]Training CobwebTree:  59%|    | 4478/7600 [01:13<01:00, 51.91it/s]Training CobwebTree:  59%|    | 4484/7600 [01:13<01:02, 49.94it/s]Training CobwebTree:  59%|    | 4490/7600 [01:14<01:03, 49.30it/s]Training CobwebTree:  59%|    | 4496/7600 [01:14<01:01, 50.83it/s]Training CobwebTree:  59%|    | 4502/7600 [01:14<01:00, 50.86it/s]Training CobwebTree:  59%|    | 4508/7600 [01:14<00:58, 52.41it/s]Training CobwebTree:  59%|    | 4514/7600 [01:14<00:57, 53.58it/s]Training CobwebTree:  59%|    | 4520/7600 [01:14<00:57, 53.23it/s]Training CobwebTree:  60%|    | 4526/7600 [01:14<00:57, 53.25it/s]Training CobwebTree:  60%|    | 4532/7600 [01:14<00:57, 53.76it/s]Training CobwebTree:  60%|    | 4538/7600 [01:14<00:55, 54.84it/s]Training CobwebTree:  60%|    | 4544/7600 [01:14<00:55, 54.71it/s]Training CobwebTree:  60%|    | 4550/7600 [01:15<00:56, 53.81it/s]Training CobwebTree:  60%|    | 4556/7600 [01:15<00:55, 54.64it/s]Training CobwebTree:  60%|    | 4562/7600 [01:15<00:57, 52.69it/s]Training CobwebTree:  60%|    | 4569/7600 [01:15<00:55, 54.66it/s]Training CobwebTree:  60%|    | 4575/7600 [01:15<00:56, 53.67it/s]Training CobwebTree:  60%|    | 4581/7600 [01:15<00:57, 52.96it/s]Training CobwebTree:  60%|    | 4587/7600 [01:15<00:55, 54.69it/s]Training CobwebTree:  60%|    | 4593/7600 [01:15<00:57, 52.70it/s]Training CobwebTree:  61%|    | 4599/7600 [01:16<00:57, 52.21it/s]Training CobwebTree:  61%|    | 4605/7600 [01:16<00:56, 52.88it/s]Training CobwebTree:  61%|    | 4611/7600 [01:16<00:58, 51.27it/s]Training CobwebTree:  61%|    | 4617/7600 [01:16<00:58, 51.42it/s]Training CobwebTree:  61%|    | 4623/7600 [01:16<00:57, 51.63it/s]Training CobwebTree:  61%|    | 4629/7600 [01:16<00:57, 51.89it/s]Training CobwebTree:  61%|    | 4635/7600 [01:16<00:55, 53.43it/s]Training CobwebTree:  61%|    | 4641/7600 [01:16<00:57, 51.26it/s]Training CobwebTree:  61%|    | 4647/7600 [01:16<00:57, 51.47it/s]Training CobwebTree:  61%|    | 4653/7600 [01:17<00:58, 50.74it/s]Training CobwebTree:  61%|   | 4659/7600 [01:17<00:57, 51.28it/s]Training CobwebTree:  61%|   | 4665/7600 [01:17<00:55, 52.53it/s]Training CobwebTree:  61%|   | 4671/7600 [01:17<00:56, 51.97it/s]Training CobwebTree:  62%|   | 4677/7600 [01:17<00:54, 53.30it/s]Training CobwebTree:  62%|   | 4683/7600 [01:17<00:56, 51.54it/s]Training CobwebTree:  62%|   | 4689/7600 [01:17<00:55, 52.62it/s]Training CobwebTree:  62%|   | 4695/7600 [01:17<00:54, 53.44it/s]Training CobwebTree:  62%|   | 4701/7600 [01:17<00:56, 51.54it/s]Training CobwebTree:  62%|   | 4707/7600 [01:18<00:55, 51.85it/s]Training CobwebTree:  62%|   | 4713/7600 [01:18<00:55, 52.38it/s]Training CobwebTree:  62%|   | 4719/7600 [01:18<00:54, 52.91it/s]Training CobwebTree:  62%|   | 4725/7600 [01:18<00:54, 53.10it/s]Training CobwebTree:  62%|   | 4731/7600 [01:18<00:53, 53.99it/s]Training CobwebTree:  62%|   | 4737/7600 [01:18<00:52, 55.04it/s]Training CobwebTree:  62%|   | 4743/7600 [01:18<00:51, 55.04it/s]Training CobwebTree:  62%|   | 4749/7600 [01:18<00:52, 54.07it/s]Training CobwebTree:  63%|   | 4755/7600 [01:18<00:53, 53.27it/s]Training CobwebTree:  63%|   | 4761/7600 [01:19<00:52, 53.71it/s]Training CobwebTree:  63%|   | 4767/7600 [01:19<00:53, 53.28it/s]Training CobwebTree:  63%|   | 4773/7600 [01:19<00:53, 52.52it/s]Training CobwebTree:  63%|   | 4779/7600 [01:19<00:52, 53.70it/s]Training CobwebTree:  63%|   | 4785/7600 [01:19<00:55, 50.85it/s]Training CobwebTree:  63%|   | 4791/7600 [01:19<00:52, 53.29it/s]Training CobwebTree:  63%|   | 4797/7600 [01:19<00:52, 52.97it/s]Training CobwebTree:  63%|   | 4803/7600 [01:19<00:55, 50.77it/s]Training CobwebTree:  63%|   | 4809/7600 [01:20<00:54, 51.59it/s]Training CobwebTree:  63%|   | 4815/7600 [01:20<00:54, 51.03it/s]Training CobwebTree:  63%|   | 4821/7600 [01:20<00:54, 51.39it/s]Training CobwebTree:  64%|   | 4827/7600 [01:20<00:54, 51.32it/s]Training CobwebTree:  64%|   | 4833/7600 [01:20<00:56, 49.38it/s]Training CobwebTree:  64%|   | 4838/7600 [01:20<00:56, 49.21it/s]Training CobwebTree:  64%|   | 4843/7600 [01:20<00:57, 48.11it/s]Training CobwebTree:  64%|   | 4849/7600 [01:20<00:55, 49.38it/s]Training CobwebTree:  64%|   | 4854/7600 [01:20<00:56, 48.18it/s]Training CobwebTree:  64%|   | 4859/7600 [01:21<00:59, 46.39it/s]Training CobwebTree:  64%|   | 4864/7600 [01:21<00:59, 46.31it/s]Training CobwebTree:  64%|   | 4869/7600 [01:21<00:57, 47.17it/s]Training CobwebTree:  64%|   | 4874/7600 [01:21<00:58, 46.51it/s]Training CobwebTree:  64%|   | 4879/7600 [01:21<01:00, 45.26it/s]Training CobwebTree:  64%|   | 4884/7600 [01:21<01:01, 44.43it/s]Training CobwebTree:  64%|   | 4890/7600 [01:21<00:57, 46.93it/s]Training CobwebTree:  64%|   | 4895/7600 [01:21<00:57, 46.96it/s]Training CobwebTree:  64%|   | 4900/7600 [01:21<00:56, 47.41it/s]Training CobwebTree:  65%|   | 4905/7600 [01:22<00:56, 47.90it/s]Training CobwebTree:  65%|   | 4911/7600 [01:22<00:54, 49.46it/s]Training CobwebTree:  65%|   | 4917/7600 [01:22<00:52, 51.10it/s]Training CobwebTree:  65%|   | 4923/7600 [01:22<00:52, 51.45it/s]Training CobwebTree:  65%|   | 4929/7600 [01:22<00:52, 51.32it/s]Training CobwebTree:  65%|   | 4935/7600 [01:22<00:51, 52.07it/s]Training CobwebTree:  65%|   | 4941/7600 [01:22<00:53, 50.07it/s]Training CobwebTree:  65%|   | 4947/7600 [01:22<00:53, 49.54it/s]Training CobwebTree:  65%|   | 4952/7600 [01:22<00:55, 48.03it/s]Training CobwebTree:  65%|   | 4958/7600 [01:23<00:55, 47.48it/s]Training CobwebTree:  65%|   | 4963/7600 [01:23<00:55, 47.17it/s]Training CobwebTree:  65%|   | 4969/7600 [01:23<00:53, 49.12it/s]Training CobwebTree:  65%|   | 4976/7600 [01:23<00:48, 53.84it/s]Training CobwebTree:  66%|   | 4982/7600 [01:23<00:48, 54.19it/s]Training CobwebTree:  66%|   | 4988/7600 [01:23<00:49, 53.23it/s]Training CobwebTree:  66%|   | 4994/7600 [01:23<00:48, 53.31it/s]Training CobwebTree:  66%|   | 5000/7600 [01:23<00:49, 52.34it/s]Training CobwebTree:  66%|   | 5006/7600 [01:24<00:49, 52.06it/s]Training CobwebTree:  66%|   | 5012/7600 [01:24<00:50, 50.96it/s]Training CobwebTree:  66%|   | 5018/7600 [01:24<00:51, 49.71it/s]Training CobwebTree:  66%|   | 5024/7600 [01:24<00:49, 51.55it/s]Training CobwebTree:  66%|   | 5030/7600 [01:24<00:49, 51.74it/s]Training CobwebTree:  66%|   | 5036/7600 [01:24<00:50, 50.90it/s]Training CobwebTree:  66%|   | 5042/7600 [01:24<00:49, 51.46it/s]Training CobwebTree:  66%|   | 5048/7600 [01:24<00:49, 52.05it/s]Training CobwebTree:  66%|   | 5054/7600 [01:24<00:47, 53.05it/s]Training CobwebTree:  67%|   | 5060/7600 [01:25<00:48, 52.65it/s]Training CobwebTree:  67%|   | 5066/7600 [01:25<00:47, 53.78it/s]Training CobwebTree:  67%|   | 5072/7600 [01:25<00:51, 49.50it/s]Training CobwebTree:  67%|   | 5078/7600 [01:25<00:50, 50.02it/s]Training CobwebTree:  67%|   | 5084/7600 [01:25<00:50, 49.97it/s]Training CobwebTree:  67%|   | 5090/7600 [01:25<00:49, 50.40it/s]Training CobwebTree:  67%|   | 5096/7600 [01:25<00:49, 50.22it/s]Training CobwebTree:  67%|   | 5103/7600 [01:25<00:47, 53.09it/s]Training CobwebTree:  67%|   | 5109/7600 [01:26<00:45, 54.37it/s]Training CobwebTree:  67%|   | 5115/7600 [01:26<00:46, 53.15it/s]Training CobwebTree:  67%|   | 5121/7600 [01:26<00:47, 52.17it/s]Training CobwebTree:  67%|   | 5127/7600 [01:26<00:47, 51.66it/s]Training CobwebTree:  68%|   | 5133/7600 [01:26<00:46, 52.62it/s]Training CobwebTree:  68%|   | 5139/7600 [01:26<00:45, 53.71it/s]Training CobwebTree:  68%|   | 5145/7600 [01:26<00:46, 52.49it/s]Training CobwebTree:  68%|   | 5151/7600 [01:26<00:47, 51.17it/s]Training CobwebTree:  68%|   | 5157/7600 [01:26<00:46, 52.51it/s]Training CobwebTree:  68%|   | 5163/7600 [01:27<00:45, 53.18it/s]Training CobwebTree:  68%|   | 5169/7600 [01:27<00:44, 54.64it/s]Training CobwebTree:  68%|   | 5175/7600 [01:27<00:43, 55.20it/s]Training CobwebTree:  68%|   | 5181/7600 [01:27<00:43, 55.36it/s]Training CobwebTree:  68%|   | 5187/7600 [01:27<00:44, 53.72it/s]Training CobwebTree:  68%|   | 5193/7600 [01:27<00:45, 52.45it/s]Training CobwebTree:  68%|   | 5199/7600 [01:27<00:46, 52.04it/s]Training CobwebTree:  68%|   | 5205/7600 [01:27<00:47, 50.44it/s]Training CobwebTree:  69%|   | 5211/7600 [01:27<00:48, 49.24it/s]Training CobwebTree:  69%|   | 5217/7600 [01:28<00:48, 49.39it/s]Training CobwebTree:  69%|   | 5222/7600 [01:28<00:48, 49.51it/s]Training CobwebTree:  69%|   | 5228/7600 [01:28<00:46, 51.28it/s]Training CobwebTree:  69%|   | 5234/7600 [01:28<00:46, 50.60it/s]Training CobwebTree:  69%|   | 5240/7600 [01:28<00:48, 49.13it/s]Training CobwebTree:  69%|   | 5246/7600 [01:28<00:45, 51.46it/s]Training CobwebTree:  69%|   | 5252/7600 [01:28<00:46, 50.18it/s]Training CobwebTree:  69%|   | 5258/7600 [01:28<00:49, 47.61it/s]Training CobwebTree:  69%|   | 5264/7600 [01:29<00:46, 50.36it/s]Training CobwebTree:  69%|   | 5270/7600 [01:29<00:46, 49.74it/s]Training CobwebTree:  69%|   | 5276/7600 [01:29<00:46, 50.14it/s]Training CobwebTree:  70%|   | 5282/7600 [01:29<00:45, 51.00it/s]Training CobwebTree:  70%|   | 5288/7600 [01:29<00:46, 49.70it/s]Training CobwebTree:  70%|   | 5294/7600 [01:29<00:45, 51.20it/s]Training CobwebTree:  70%|   | 5300/7600 [01:29<00:46, 49.86it/s]Training CobwebTree:  70%|   | 5306/7600 [01:29<00:45, 50.85it/s]Training CobwebTree:  70%|   | 5312/7600 [01:29<00:43, 52.61it/s]Training CobwebTree:  70%|   | 5318/7600 [01:30<00:42, 53.20it/s]Training CobwebTree:  70%|   | 5324/7600 [01:30<00:43, 51.98it/s]Training CobwebTree:  70%|   | 5330/7600 [01:30<00:43, 52.22it/s]Training CobwebTree:  70%|   | 5336/7600 [01:30<00:43, 51.94it/s]Training CobwebTree:  70%|   | 5342/7600 [01:30<00:42, 52.55it/s]Training CobwebTree:  70%|   | 5348/7600 [01:30<00:43, 51.86it/s]Training CobwebTree:  70%|   | 5354/7600 [01:30<00:42, 52.47it/s]Training CobwebTree:  71%|   | 5360/7600 [01:30<00:44, 49.98it/s]Training CobwebTree:  71%|   | 5366/7600 [01:31<00:45, 49.24it/s]Training CobwebTree:  71%|   | 5372/7600 [01:31<00:44, 50.12it/s]Training CobwebTree:  71%|   | 5378/7600 [01:31<00:45, 49.36it/s]Training CobwebTree:  71%|   | 5384/7600 [01:31<00:44, 49.50it/s]Training CobwebTree:  71%|   | 5390/7600 [01:31<00:42, 51.69it/s]Training CobwebTree:  71%|   | 5396/7600 [01:31<00:42, 51.86it/s]Training CobwebTree:  71%|   | 5402/7600 [01:31<00:41, 53.53it/s]Training CobwebTree:  71%|   | 5408/7600 [01:31<00:40, 54.51it/s]Training CobwebTree:  71%|   | 5414/7600 [01:31<00:40, 54.55it/s]Training CobwebTree:  71%|  | 5420/7600 [01:32<00:41, 52.71it/s]Training CobwebTree:  71%|  | 5426/7600 [01:32<00:40, 53.11it/s]Training CobwebTree:  71%|  | 5432/7600 [01:32<00:40, 53.13it/s]Training CobwebTree:  72%|  | 5438/7600 [01:32<00:40, 53.01it/s]Training CobwebTree:  72%|  | 5444/7600 [01:32<00:42, 50.78it/s]Training CobwebTree:  72%|  | 5450/7600 [01:32<00:42, 51.11it/s]Training CobwebTree:  72%|  | 5456/7600 [01:32<00:42, 49.96it/s]Training CobwebTree:  72%|  | 5462/7600 [01:32<00:40, 52.38it/s]Training CobwebTree:  72%|  | 5468/7600 [01:32<00:40, 52.93it/s]Training CobwebTree:  72%|  | 5474/7600 [01:33<00:39, 53.30it/s]Training CobwebTree:  72%|  | 5481/7600 [01:33<00:38, 55.47it/s]Training CobwebTree:  72%|  | 5487/7600 [01:33<00:39, 53.75it/s]Training CobwebTree:  72%|  | 5494/7600 [01:33<00:38, 55.31it/s]Training CobwebTree:  72%|  | 5500/7600 [01:33<00:38, 54.10it/s]Training CobwebTree:  72%|  | 5506/7600 [01:33<00:37, 55.22it/s]Training CobwebTree:  73%|  | 5512/7600 [01:33<00:40, 52.00it/s]Training CobwebTree:  73%|  | 5518/7600 [01:33<00:41, 49.81it/s]Training CobwebTree:  73%|  | 5524/7600 [01:34<00:42, 48.78it/s]Training CobwebTree:  73%|  | 5530/7600 [01:34<00:41, 49.53it/s]Training CobwebTree:  73%|  | 5535/7600 [01:34<00:42, 48.66it/s]Training CobwebTree:  73%|  | 5541/7600 [01:34<00:40, 50.38it/s]Training CobwebTree:  73%|  | 5547/7600 [01:34<00:40, 50.71it/s]Training CobwebTree:  73%|  | 5553/7600 [01:34<00:39, 51.70it/s]Training CobwebTree:  73%|  | 5559/7600 [01:34<00:37, 53.80it/s]Training CobwebTree:  73%|  | 5565/7600 [01:34<00:39, 51.49it/s]Training CobwebTree:  73%|  | 5571/7600 [01:34<00:39, 51.76it/s]Training CobwebTree:  73%|  | 5577/7600 [01:35<00:38, 52.54it/s]Training CobwebTree:  73%|  | 5583/7600 [01:35<00:39, 51.61it/s]Training CobwebTree:  74%|  | 5589/7600 [01:35<00:39, 51.26it/s]Training CobwebTree:  74%|  | 5595/7600 [01:35<00:37, 53.34it/s]Training CobwebTree:  74%|  | 5601/7600 [01:35<00:38, 52.44it/s]Training CobwebTree:  74%|  | 5607/7600 [01:35<00:37, 53.44it/s]Training CobwebTree:  74%|  | 5613/7600 [01:35<00:38, 51.20it/s]Training CobwebTree:  74%|  | 5619/7600 [01:35<00:37, 52.67it/s]Training CobwebTree:  74%|  | 5625/7600 [01:35<00:37, 52.18it/s]Training CobwebTree:  74%|  | 5631/7600 [01:36<00:37, 52.13it/s]Training CobwebTree:  74%|  | 5637/7600 [01:36<00:38, 51.13it/s]Training CobwebTree:  74%|  | 5643/7600 [01:36<00:39, 49.43it/s]Training CobwebTree:  74%|  | 5649/7600 [01:36<00:38, 50.37it/s]Training CobwebTree:  74%|  | 5655/7600 [01:36<00:37, 51.36it/s]Training CobwebTree:  74%|  | 5661/7600 [01:36<00:36, 53.35it/s]Training CobwebTree:  75%|  | 5667/7600 [01:36<00:36, 52.78it/s]Training CobwebTree:  75%|  | 5673/7600 [01:36<00:37, 51.50it/s]Training CobwebTree:  75%|  | 5679/7600 [01:37<00:37, 51.12it/s]Training CobwebTree:  75%|  | 5685/7600 [01:37<00:37, 51.62it/s]Training CobwebTree:  75%|  | 5691/7600 [01:37<00:37, 50.90it/s]Training CobwebTree:  75%|  | 5697/7600 [01:37<00:37, 50.86it/s]Training CobwebTree:  75%|  | 5704/7600 [01:37<00:35, 53.24it/s]Training CobwebTree:  75%|  | 5710/7600 [01:37<00:36, 51.66it/s]Training CobwebTree:  75%|  | 5716/7600 [01:37<00:36, 51.64it/s]Training CobwebTree:  75%|  | 5722/7600 [01:37<00:35, 52.20it/s]Training CobwebTree:  75%|  | 5728/7600 [01:37<00:36, 50.77it/s]Training CobwebTree:  75%|  | 5734/7600 [01:38<00:35, 52.55it/s]Training CobwebTree:  76%|  | 5740/7600 [01:38<00:35, 52.76it/s]Training CobwebTree:  76%|  | 5746/7600 [01:38<00:35, 52.22it/s]Training CobwebTree:  76%|  | 5752/7600 [01:38<00:35, 52.15it/s]Training CobwebTree:  76%|  | 5758/7600 [01:38<00:35, 52.37it/s]Training CobwebTree:  76%|  | 5764/7600 [01:38<00:34, 53.84it/s]Training CobwebTree:  76%|  | 5770/7600 [01:38<00:33, 54.63it/s]Training CobwebTree:  76%|  | 5776/7600 [01:38<00:34, 52.16it/s]Training CobwebTree:  76%|  | 5782/7600 [01:39<00:35, 51.30it/s]Training CobwebTree:  76%|  | 5788/7600 [01:39<00:34, 53.25it/s]Training CobwebTree:  76%|  | 5794/7600 [01:39<00:34, 52.69it/s]Training CobwebTree:  76%|  | 5800/7600 [01:39<00:34, 51.89it/s]Training CobwebTree:  76%|  | 5806/7600 [01:39<00:35, 51.15it/s]Training CobwebTree:  76%|  | 5812/7600 [01:39<00:35, 50.72it/s]Training CobwebTree:  77%|  | 5818/7600 [01:39<00:34, 51.34it/s]Training CobwebTree:  77%|  | 5824/7600 [01:39<00:35, 50.48it/s]Training CobwebTree:  77%|  | 5830/7600 [01:39<00:34, 51.57it/s]Training CobwebTree:  77%|  | 5836/7600 [01:40<00:34, 50.71it/s]Training CobwebTree:  77%|  | 5842/7600 [01:40<00:36, 48.83it/s]Training CobwebTree:  77%|  | 5847/7600 [01:40<00:35, 49.04it/s]Training CobwebTree:  77%|  | 5852/7600 [01:40<00:35, 49.27it/s]Training CobwebTree:  77%|  | 5857/7600 [01:40<00:37, 46.63it/s]Training CobwebTree:  77%|  | 5862/7600 [01:40<00:36, 47.52it/s]Training CobwebTree:  77%|  | 5868/7600 [01:40<00:36, 47.66it/s]Training CobwebTree:  77%|  | 5873/7600 [01:40<00:36, 47.83it/s]Training CobwebTree:  77%|  | 5878/7600 [01:40<00:36, 47.63it/s]Training CobwebTree:  77%|  | 5883/7600 [01:41<00:37, 46.22it/s]Training CobwebTree:  77%|  | 5889/7600 [01:41<00:35, 48.74it/s]Training CobwebTree:  78%|  | 5895/7600 [01:41<00:35, 48.23it/s]Training CobwebTree:  78%|  | 5900/7600 [01:41<00:35, 47.77it/s]Training CobwebTree:  78%|  | 5905/7600 [01:41<00:35, 47.66it/s]Training CobwebTree:  78%|  | 5910/7600 [01:41<00:35, 47.27it/s]Training CobwebTree:  78%|  | 5915/7600 [01:41<00:37, 44.77it/s]Training CobwebTree:  78%|  | 5921/7600 [01:41<00:36, 46.31it/s]Training CobwebTree:  78%|  | 5927/7600 [01:42<00:35, 47.25it/s]Training CobwebTree:  78%|  | 5933/7600 [01:42<00:34, 48.73it/s]Training CobwebTree:  78%|  | 5939/7600 [01:42<00:33, 49.58it/s]Training CobwebTree:  78%|  | 5944/7600 [01:42<00:33, 49.44it/s]Training CobwebTree:  78%|  | 5950/7600 [01:42<00:32, 51.11it/s]Training CobwebTree:  78%|  | 5956/7600 [01:42<00:32, 49.94it/s]Training CobwebTree:  78%|  | 5962/7600 [01:42<00:33, 48.95it/s]Training CobwebTree:  79%|  | 5967/7600 [01:42<00:34, 47.94it/s]Training CobwebTree:  79%|  | 5973/7600 [01:42<00:33, 48.39it/s]Training CobwebTree:  79%|  | 5978/7600 [01:43<00:34, 47.17it/s]Training CobwebTree:  79%|  | 5983/7600 [01:43<00:34, 47.16it/s]Training CobwebTree:  79%|  | 5988/7600 [01:43<00:34, 46.55it/s]Training CobwebTree:  79%|  | 5994/7600 [01:43<00:33, 48.01it/s]Training CobwebTree:  79%|  | 6000/7600 [01:43<00:32, 49.71it/s]Training CobwebTree:  79%|  | 6005/7600 [01:43<00:32, 48.72it/s]Training CobwebTree:  79%|  | 6010/7600 [01:43<00:32, 48.87it/s]Training CobwebTree:  79%|  | 6015/7600 [01:43<00:32, 48.78it/s]Training CobwebTree:  79%|  | 6021/7600 [01:43<00:32, 48.95it/s]Training CobwebTree:  79%|  | 6027/7600 [01:44<00:30, 51.37it/s]Training CobwebTree:  79%|  | 6033/7600 [01:44<00:31, 49.27it/s]Training CobwebTree:  79%|  | 6039/7600 [01:44<00:30, 50.81it/s]Training CobwebTree:  80%|  | 6045/7600 [01:44<00:30, 51.39it/s]Training CobwebTree:  80%|  | 6051/7600 [01:44<00:31, 49.63it/s]Training CobwebTree:  80%|  | 6057/7600 [01:44<00:30, 50.73it/s]Training CobwebTree:  80%|  | 6063/7600 [01:44<00:30, 49.89it/s]Training CobwebTree:  80%|  | 6069/7600 [01:44<00:31, 49.14it/s]Training CobwebTree:  80%|  | 6074/7600 [01:44<00:31, 48.85it/s]Training CobwebTree:  80%|  | 6080/7600 [01:45<00:30, 50.14it/s]Training CobwebTree:  80%|  | 6086/7600 [01:45<00:29, 51.48it/s]Training CobwebTree:  80%|  | 6092/7600 [01:45<00:29, 50.50it/s]Training CobwebTree:  80%|  | 6098/7600 [01:45<00:28, 52.69it/s]Training CobwebTree:  80%|  | 6104/7600 [01:45<00:29, 51.46it/s]Training CobwebTree:  80%|  | 6110/7600 [01:45<00:28, 51.71it/s]Training CobwebTree:  80%|  | 6116/7600 [01:45<00:28, 51.62it/s]Training CobwebTree:  81%|  | 6122/7600 [01:45<00:29, 49.66it/s]Training CobwebTree:  81%|  | 6128/7600 [01:46<00:28, 50.94it/s]Training CobwebTree:  81%|  | 6134/7600 [01:46<00:28, 51.40it/s]Training CobwebTree:  81%|  | 6140/7600 [01:46<00:29, 50.13it/s]Training CobwebTree:  81%|  | 6146/7600 [01:46<00:28, 50.88it/s]Training CobwebTree:  81%|  | 6152/7600 [01:46<00:29, 49.03it/s]Training CobwebTree:  81%|  | 6158/7600 [01:46<00:28, 50.16it/s]Training CobwebTree:  81%|  | 6164/7600 [01:46<00:28, 49.61it/s]Training CobwebTree:  81%|  | 6170/7600 [01:46<00:28, 50.73it/s]Training CobwebTree:  81%| | 6176/7600 [01:46<00:29, 48.84it/s]Training CobwebTree:  81%| | 6181/7600 [01:47<00:29, 47.82it/s]Training CobwebTree:  81%| | 6186/7600 [01:47<00:29, 47.73it/s]Training CobwebTree:  81%| | 6191/7600 [01:47<00:29, 47.52it/s]Training CobwebTree:  82%| | 6197/7600 [01:47<00:28, 49.64it/s]Training CobwebTree:  82%| | 6203/7600 [01:47<00:27, 50.77it/s]Training CobwebTree:  82%| | 6209/7600 [01:47<00:26, 52.84it/s]Training CobwebTree:  82%| | 6215/7600 [01:47<00:27, 50.85it/s]Training CobwebTree:  82%| | 6221/7600 [01:47<00:26, 51.91it/s]Training CobwebTree:  82%| | 6227/7600 [01:48<00:26, 50.94it/s]Training CobwebTree:  82%| | 6233/7600 [01:48<00:26, 51.93it/s]Training CobwebTree:  82%| | 6239/7600 [01:48<00:26, 51.57it/s]Training CobwebTree:  82%| | 6245/7600 [01:48<00:25, 52.18it/s]Training CobwebTree:  82%| | 6251/7600 [01:48<00:24, 54.15it/s]Training CobwebTree:  82%| | 6257/7600 [01:48<00:26, 50.63it/s]Training CobwebTree:  82%| | 6263/7600 [01:48<00:26, 51.02it/s]Training CobwebTree:  82%| | 6269/7600 [01:48<00:26, 50.28it/s]Training CobwebTree:  83%| | 6275/7600 [01:48<00:26, 49.97it/s]Training CobwebTree:  83%| | 6281/7600 [01:49<00:26, 49.71it/s]Training CobwebTree:  83%| | 6286/7600 [01:49<00:26, 49.32it/s]Training CobwebTree:  83%| | 6291/7600 [01:49<00:26, 49.07it/s]Training CobwebTree:  83%| | 6296/7600 [01:49<00:26, 48.38it/s]Training CobwebTree:  83%| | 6302/7600 [01:49<00:25, 50.09it/s]Training CobwebTree:  83%| | 6308/7600 [01:49<00:25, 50.64it/s]Training CobwebTree:  83%| | 6314/7600 [01:49<00:25, 49.69it/s]Training CobwebTree:  83%| | 6320/7600 [01:49<00:24, 51.98it/s]Training CobwebTree:  83%| | 6326/7600 [01:49<00:24, 51.78it/s]Training CobwebTree:  83%| | 6332/7600 [01:50<00:24, 52.28it/s]Training CobwebTree:  83%| | 6338/7600 [01:50<00:25, 49.49it/s]Training CobwebTree:  83%| | 6343/7600 [01:50<00:25, 48.97it/s]Training CobwebTree:  84%| | 6348/7600 [01:50<00:25, 49.10it/s]Training CobwebTree:  84%| | 6354/7600 [01:50<00:25, 49.59it/s]Training CobwebTree:  84%| | 6359/7600 [01:50<00:25, 49.06it/s]Training CobwebTree:  84%| | 6365/7600 [01:50<00:25, 48.90it/s]Training CobwebTree:  84%| | 6372/7600 [01:50<00:23, 52.63it/s]Training CobwebTree:  84%| | 6378/7600 [01:50<00:23, 51.51it/s]Training CobwebTree:  84%| | 6384/7600 [01:51<00:23, 51.60it/s]Training CobwebTree:  84%| | 6390/7600 [01:51<00:23, 50.95it/s]Training CobwebTree:  84%| | 6396/7600 [01:51<00:23, 50.66it/s]Training CobwebTree:  84%| | 6402/7600 [01:51<00:24, 49.77it/s]Training CobwebTree:  84%| | 6408/7600 [01:51<00:23, 51.12it/s]Training CobwebTree:  84%| | 6414/7600 [01:51<00:23, 51.17it/s]Training CobwebTree:  84%| | 6420/7600 [01:51<00:22, 52.71it/s]Training CobwebTree:  85%| | 6426/7600 [01:51<00:21, 54.39it/s]Training CobwebTree:  85%| | 6432/7600 [01:52<00:22, 52.62it/s]Training CobwebTree:  85%| | 6438/7600 [01:52<00:21, 52.87it/s]Training CobwebTree:  85%| | 6444/7600 [01:52<00:22, 51.78it/s]Training CobwebTree:  85%| | 6450/7600 [01:52<00:21, 52.93it/s]Training CobwebTree:  85%| | 6456/7600 [01:52<00:21, 52.70it/s]Training CobwebTree:  85%| | 6462/7600 [01:52<00:21, 53.48it/s]Training CobwebTree:  85%| | 6468/7600 [01:52<00:22, 49.68it/s]Training CobwebTree:  85%| | 6474/7600 [01:52<00:23, 47.53it/s]Training CobwebTree:  85%| | 6480/7600 [01:52<00:22, 48.95it/s]Training CobwebTree:  85%| | 6485/7600 [01:53<00:23, 48.46it/s]Training CobwebTree:  85%| | 6491/7600 [01:53<00:22, 50.14it/s]Training CobwebTree:  85%| | 6497/7600 [01:53<00:21, 50.77it/s]Training CobwebTree:  86%| | 6503/7600 [01:53<00:22, 48.79it/s]Training CobwebTree:  86%| | 6509/7600 [01:53<00:21, 50.87it/s]Training CobwebTree:  86%| | 6515/7600 [01:53<00:20, 52.43it/s]Training CobwebTree:  86%| | 6521/7600 [01:53<00:20, 52.70it/s]Training CobwebTree:  86%| | 6527/7600 [01:53<00:21, 50.83it/s]Training CobwebTree:  86%| | 6533/7600 [01:54<00:20, 52.50it/s]Training CobwebTree:  86%| | 6539/7600 [01:54<00:20, 51.87it/s]Training CobwebTree:  86%| | 6545/7600 [01:54<00:20, 51.87it/s]Training CobwebTree:  86%| | 6551/7600 [01:54<00:20, 50.90it/s]Training CobwebTree:  86%| | 6557/7600 [01:54<00:20, 51.75it/s]Training CobwebTree:  86%| | 6563/7600 [01:54<00:20, 50.88it/s]Training CobwebTree:  86%| | 6569/7600 [01:54<00:20, 50.78it/s]Training CobwebTree:  87%| | 6575/7600 [01:54<00:19, 51.88it/s]Training CobwebTree:  87%| | 6581/7600 [01:54<00:19, 51.71it/s]Training CobwebTree:  87%| | 6587/7600 [01:55<00:19, 51.64it/s]Training CobwebTree:  87%| | 6593/7600 [01:55<00:19, 52.24it/s]Training CobwebTree:  87%| | 6599/7600 [01:55<00:19, 51.57it/s]Training CobwebTree:  87%| | 6605/7600 [01:55<00:19, 50.64it/s]Training CobwebTree:  87%| | 6611/7600 [01:55<00:19, 50.57it/s]Training CobwebTree:  87%| | 6617/7600 [01:55<00:19, 50.52it/s]Training CobwebTree:  87%| | 6623/7600 [01:55<00:19, 50.56it/s]Training CobwebTree:  87%| | 6629/7600 [01:55<00:19, 49.89it/s]Training CobwebTree:  87%| | 6634/7600 [01:56<00:19, 49.47it/s]Training CobwebTree:  87%| | 6640/7600 [01:56<00:18, 51.17it/s]Training CobwebTree:  87%| | 6646/7600 [01:56<00:18, 50.58it/s]Training CobwebTree:  88%| | 6652/7600 [01:56<00:18, 50.46it/s]Training CobwebTree:  88%| | 6658/7600 [01:56<00:18, 50.99it/s]Training CobwebTree:  88%| | 6664/7600 [01:56<00:18, 51.04it/s]Training CobwebTree:  88%| | 6670/7600 [01:56<00:19, 48.07it/s]Training CobwebTree:  88%| | 6675/7600 [01:56<00:19, 47.46it/s]Training CobwebTree:  88%| | 6680/7600 [01:56<00:20, 45.65it/s]Training CobwebTree:  88%| | 6686/7600 [01:57<00:19, 48.04it/s]Training CobwebTree:  88%| | 6692/7600 [01:57<00:18, 48.94it/s]Training CobwebTree:  88%| | 6698/7600 [01:57<00:18, 48.92it/s]Training CobwebTree:  88%| | 6703/7600 [01:57<00:18, 48.94it/s]Training CobwebTree:  88%| | 6708/7600 [01:57<00:18, 49.05it/s]Training CobwebTree:  88%| | 6714/7600 [01:57<00:17, 49.26it/s]Training CobwebTree:  88%| | 6719/7600 [01:57<00:18, 47.09it/s]Training CobwebTree:  88%| | 6724/7600 [01:57<00:18, 47.72it/s]Training CobwebTree:  89%| | 6729/7600 [01:57<00:18, 47.78it/s]Training CobwebTree:  89%| | 6735/7600 [01:58<00:17, 50.43it/s]Training CobwebTree:  89%| | 6741/7600 [01:58<00:17, 49.78it/s]Training CobwebTree:  89%| | 6746/7600 [01:58<00:18, 47.19it/s]Training CobwebTree:  89%| | 6752/7600 [01:58<00:17, 48.26it/s]Training CobwebTree:  89%| | 6758/7600 [01:58<00:17, 49.01it/s]Training CobwebTree:  89%| | 6764/7600 [01:58<00:16, 50.88it/s]Training CobwebTree:  89%| | 6770/7600 [01:58<00:15, 52.38it/s]Training CobwebTree:  89%| | 6776/7600 [01:58<00:15, 52.19it/s]Training CobwebTree:  89%| | 6782/7600 [01:59<00:16, 50.59it/s]Training CobwebTree:  89%| | 6788/7600 [01:59<00:16, 49.94it/s]Training CobwebTree:  89%| | 6794/7600 [01:59<00:15, 51.76it/s]Training CobwebTree:  89%| | 6800/7600 [01:59<00:16, 49.18it/s]Training CobwebTree:  90%| | 6806/7600 [01:59<00:15, 49.97it/s]Training CobwebTree:  90%| | 6812/7600 [01:59<00:16, 49.09it/s]Training CobwebTree:  90%| | 6817/7600 [01:59<00:16, 47.81it/s]Training CobwebTree:  90%| | 6823/7600 [01:59<00:15, 49.09it/s]Training CobwebTree:  90%| | 6829/7600 [01:59<00:15, 49.11it/s]Training CobwebTree:  90%| | 6835/7600 [02:00<00:14, 51.71it/s]Training CobwebTree:  90%| | 6841/7600 [02:00<00:15, 50.18it/s]Training CobwebTree:  90%| | 6847/7600 [02:00<00:15, 49.88it/s]Training CobwebTree:  90%| | 6853/7600 [02:00<00:14, 50.29it/s]Training CobwebTree:  90%| | 6859/7600 [02:00<00:14, 50.15it/s]Training CobwebTree:  90%| | 6865/7600 [02:00<00:14, 50.04it/s]Training CobwebTree:  90%| | 6871/7600 [02:00<00:14, 49.95it/s]Training CobwebTree:  90%| | 6877/7600 [02:00<00:14, 49.36it/s]Training CobwebTree:  91%| | 6883/7600 [02:01<00:14, 50.77it/s]Training CobwebTree:  91%| | 6889/7600 [02:01<00:14, 50.46it/s]Training CobwebTree:  91%| | 6895/7600 [02:01<00:14, 49.97it/s]Training CobwebTree:  91%| | 6901/7600 [02:01<00:13, 50.27it/s]Training CobwebTree:  91%| | 6907/7600 [02:01<00:13, 51.04it/s]Training CobwebTree:  91%| | 6913/7600 [02:01<00:13, 51.61it/s]Training CobwebTree:  91%| | 6919/7600 [02:01<00:13, 51.25it/s]Training CobwebTree:  91%| | 6925/7600 [02:01<00:12, 52.61it/s]Training CobwebTree:  91%| | 6931/7600 [02:01<00:13, 50.76it/s]Training CobwebTree:  91%|| 6937/7600 [02:02<00:12, 52.05it/s]Training CobwebTree:  91%|| 6943/7600 [02:02<00:12, 52.28it/s]Training CobwebTree:  91%|| 6949/7600 [02:02<00:12, 51.07it/s]Training CobwebTree:  92%|| 6955/7600 [02:02<00:12, 53.30it/s]Training CobwebTree:  92%|| 6961/7600 [02:02<00:12, 50.86it/s]Training CobwebTree:  92%|| 6967/7600 [02:02<00:12, 50.85it/s]Training CobwebTree:  92%|| 6973/7600 [02:02<00:12, 49.88it/s]Training CobwebTree:  92%|| 6979/7600 [02:02<00:12, 49.55it/s]Training CobwebTree:  92%|| 6985/7600 [02:03<00:11, 51.30it/s]Training CobwebTree:  92%|| 6991/7600 [02:03<00:12, 49.76it/s]Training CobwebTree:  92%|| 6998/7600 [02:03<00:11, 52.66it/s]Training CobwebTree:  92%|| 7004/7600 [02:03<00:11, 52.95it/s]Training CobwebTree:  92%|| 7010/7600 [02:03<00:11, 51.62it/s]Training CobwebTree:  92%|| 7017/7600 [02:03<00:10, 53.47it/s]Training CobwebTree:  92%|| 7023/7600 [02:03<00:10, 53.57it/s]Training CobwebTree:  92%|| 7029/7600 [02:03<00:10, 53.10it/s]Training CobwebTree:  93%|| 7035/7600 [02:03<00:10, 52.01it/s]Training CobwebTree:  93%|| 7041/7600 [02:04<00:11, 50.78it/s]Training CobwebTree:  93%|| 7047/7600 [02:04<00:10, 51.04it/s]Training CobwebTree:  93%|| 7053/7600 [02:04<00:10, 50.95it/s]Training CobwebTree:  93%|| 7059/7600 [02:04<00:10, 49.80it/s]Training CobwebTree:  93%|| 7064/7600 [02:04<00:10, 49.81it/s]Training CobwebTree:  93%|| 7070/7600 [02:04<00:10, 49.43it/s]Training CobwebTree:  93%|| 7075/7600 [02:04<00:10, 48.37it/s]Training CobwebTree:  93%|| 7081/7600 [02:04<00:10, 48.77it/s]Training CobwebTree:  93%|| 7087/7600 [02:05<00:10, 48.72it/s]Training CobwebTree:  93%|| 7092/7600 [02:05<00:10, 48.79it/s]Training CobwebTree:  93%|| 7098/7600 [02:05<00:10, 48.89it/s]Training CobwebTree:  93%|| 7104/7600 [02:05<00:10, 49.13it/s]Training CobwebTree:  94%|| 7110/7600 [02:05<00:09, 49.96it/s]Training CobwebTree:  94%|| 7116/7600 [02:05<00:09, 50.84it/s]Training CobwebTree:  94%|| 7122/7600 [02:05<00:09, 52.32it/s]Training CobwebTree:  94%|| 7128/7600 [02:05<00:08, 52.58it/s]Training CobwebTree:  94%|| 7134/7600 [02:05<00:09, 48.86it/s]Training CobwebTree:  94%|| 7139/7600 [02:06<00:09, 49.06it/s]Training CobwebTree:  94%|| 7145/7600 [02:06<00:09, 49.95it/s]Training CobwebTree:  94%|| 7151/7600 [02:06<00:08, 51.30it/s]Training CobwebTree:  94%|| 7157/7600 [02:06<00:08, 50.50it/s]Training CobwebTree:  94%|| 7163/7600 [02:06<00:09, 47.69it/s]Training CobwebTree:  94%|| 7170/7600 [02:06<00:08, 51.56it/s]Training CobwebTree:  94%|| 7176/7600 [02:06<00:08, 50.36it/s]Training CobwebTree:  94%|| 7182/7600 [02:06<00:08, 50.64it/s]Training CobwebTree:  95%|| 7188/7600 [02:07<00:07, 51.75it/s]Training CobwebTree:  95%|| 7194/7600 [02:07<00:08, 50.22it/s]Training CobwebTree:  95%|| 7200/7600 [02:07<00:08, 49.11it/s]Training CobwebTree:  95%|| 7205/7600 [02:07<00:08, 48.71it/s]Training CobwebTree:  95%|| 7210/7600 [02:07<00:07, 48.95it/s]Training CobwebTree:  95%|| 7216/7600 [02:07<00:07, 49.62it/s]Training CobwebTree:  95%|| 7222/7600 [02:07<00:07, 51.58it/s]Training CobwebTree:  95%|| 7228/7600 [02:07<00:07, 50.74it/s]Training CobwebTree:  95%|| 7234/7600 [02:07<00:07, 50.95it/s]Training CobwebTree:  95%|| 7240/7600 [02:08<00:07, 49.76it/s]Training CobwebTree:  95%|| 7245/7600 [02:08<00:07, 49.20it/s]Training CobwebTree:  95%|| 7251/7600 [02:08<00:07, 49.01it/s]Training CobwebTree:  95%|| 7257/7600 [02:08<00:06, 49.49it/s]Training CobwebTree:  96%|| 7263/7600 [02:08<00:06, 49.83it/s]Training CobwebTree:  96%|| 7268/7600 [02:08<00:06, 47.78it/s]Training CobwebTree:  96%|| 7274/7600 [02:08<00:06, 48.99it/s]Training CobwebTree:  96%|| 7280/7600 [02:08<00:06, 49.23it/s]Training CobwebTree:  96%|| 7286/7600 [02:09<00:06, 50.30it/s]Training CobwebTree:  96%|| 7292/7600 [02:09<00:06, 49.16it/s]Training CobwebTree:  96%|| 7298/7600 [02:09<00:05, 50.34it/s]Training CobwebTree:  96%|| 7304/7600 [02:09<00:06, 49.06it/s]Training CobwebTree:  96%|| 7310/7600 [02:09<00:05, 50.67it/s]Training CobwebTree:  96%|| 7316/7600 [02:09<00:05, 49.51it/s]Training CobwebTree:  96%|| 7322/7600 [02:09<00:05, 50.38it/s]Training CobwebTree:  96%|| 7328/7600 [02:09<00:05, 51.58it/s]Training CobwebTree:  96%|| 7334/7600 [02:09<00:05, 51.99it/s]Training CobwebTree:  97%|| 7340/7600 [02:10<00:04, 53.08it/s]Training CobwebTree:  97%|| 7346/7600 [02:10<00:04, 51.91it/s]Training CobwebTree:  97%|| 7352/7600 [02:10<00:04, 51.72it/s]Training CobwebTree:  97%|| 7358/7600 [02:10<00:04, 49.62it/s]Training CobwebTree:  97%|| 7363/7600 [02:10<00:04, 49.44it/s]Training CobwebTree:  97%|| 7369/7600 [02:10<00:04, 50.95it/s]Training CobwebTree:  97%|| 7375/7600 [02:10<00:04, 49.72it/s]Training CobwebTree:  97%|| 7381/7600 [02:10<00:04, 50.24it/s]Training CobwebTree:  97%|| 7387/7600 [02:11<00:04, 51.09it/s]Training CobwebTree:  97%|| 7393/7600 [02:11<00:03, 52.24it/s]Training CobwebTree:  97%|| 7399/7600 [02:11<00:03, 52.20it/s]Training CobwebTree:  97%|| 7405/7600 [02:11<00:03, 50.58it/s]Training CobwebTree:  98%|| 7411/7600 [02:11<00:03, 51.19it/s]Training CobwebTree:  98%|| 7417/7600 [02:11<00:03, 51.73it/s]Training CobwebTree:  98%|| 7423/7600 [02:11<00:03, 50.84it/s]Training CobwebTree:  98%|| 7429/7600 [02:11<00:03, 50.77it/s]Training CobwebTree:  98%|| 7435/7600 [02:11<00:03, 52.11it/s]Training CobwebTree:  98%|| 7441/7600 [02:12<00:03, 49.39it/s]Training CobwebTree:  98%|| 7447/7600 [02:12<00:02, 51.66it/s]Training CobwebTree:  98%|| 7453/7600 [02:12<00:03, 48.94it/s]Training CobwebTree:  98%|| 7459/7600 [02:12<00:02, 51.13it/s]Training CobwebTree:  98%|| 7465/7600 [02:12<00:02, 49.41it/s]Training CobwebTree:  98%|| 7471/7600 [02:12<00:02, 46.31it/s]Training CobwebTree:  98%|| 7477/7600 [02:12<00:02, 47.59it/s]Training CobwebTree:  98%|| 7482/7600 [02:12<00:02, 47.73it/s]Training CobwebTree:  99%|| 7488/7600 [02:13<00:02, 48.86it/s]Training CobwebTree:  99%|| 7494/7600 [02:13<00:02, 50.35it/s]Training CobwebTree:  99%|| 7500/7600 [02:13<00:02, 49.94it/s]Training CobwebTree:  99%|| 7506/7600 [02:13<00:01, 50.49it/s]Training CobwebTree:  99%|| 7512/7600 [02:13<00:01, 50.75it/s]Training CobwebTree:  99%|| 7518/7600 [02:13<00:01, 49.96it/s]Training CobwebTree:  99%|| 7524/7600 [02:13<00:01, 50.64it/s]Training CobwebTree:  99%|| 7530/7600 [02:13<00:01, 51.06it/s]Training CobwebTree:  99%|| 7536/7600 [02:13<00:01, 53.05it/s]Training CobwebTree:  99%|| 7542/7600 [02:14<00:01, 51.27it/s]Training CobwebTree:  99%|| 7548/7600 [02:14<00:01, 51.63it/s]Training CobwebTree:  99%|| 7554/7600 [02:14<00:00, 51.20it/s]Training CobwebTree:  99%|| 7560/7600 [02:14<00:00, 49.57it/s]Training CobwebTree: 100%|| 7565/7600 [02:14<00:00, 48.89it/s]Training CobwebTree: 100%|| 7570/7600 [02:14<00:00, 48.78it/s]Training CobwebTree: 100%|| 7576/7600 [02:14<00:00, 50.80it/s]Training CobwebTree: 100%|| 7582/7600 [02:14<00:00, 51.18it/s]Training CobwebTree: 100%|| 7588/7600 [02:14<00:00, 51.87it/s]Training CobwebTree: 100%|| 7594/7600 [02:15<00:00, 52.69it/s]Training CobwebTree: 100%|| 7600/7600 [02:15<00:00, 52.82it/s]Training CobwebTree: 100%|| 7600/7600 [02:15<00:00, 56.21it/s]
2025-12-23 01:12:53,450 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 01:12:59,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:12:59,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:12:59,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:12:59,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:12:59,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:12:59,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,000 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,000 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,013 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,017 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,026 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,026 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,027 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,029 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,029 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,031 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,033 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,033 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,033 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,033 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,034 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,037 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,037 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,041 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,045 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,045 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,048 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,049 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,049 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,053 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,053 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,057 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,057 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,057 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,061 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,061 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,061 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,065 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,069 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,073 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,073 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,077 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,080 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,081 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,082 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,085 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,089 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,089 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,091 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,091 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,091 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,092 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,092 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,093 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,097 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,101 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,101 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,101 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,101 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,109 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,109 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,109 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,109 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,121 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,121 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,121 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,123 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,133 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,141 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,141 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,141 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,149 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,157 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,157 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,157 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,161 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,169 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,181 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,181 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,197 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,197 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,200 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,209 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,210 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,216 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,221 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,236 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,236 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,249 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,269 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,333 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,349 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,369 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,458 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,477 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,569 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,444 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,669 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:00,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:00,966 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,017 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:01,028 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,045 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:01,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,089 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:01,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:01,168 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:01,192 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:01,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:01,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:01,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:04,070 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 01:13:04,267 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 7600 virtual documents
2025-12-23 01:13:05,363 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 01:13:10,106 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (1222 virtual)
2025-12-23 01:13:10,108 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (2388 virtual)
2025-12-23 01:13:10,109 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (3473 virtual)
2025-12-23 01:13:10,110 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (4548 virtual)
2025-12-23 01:13:10,110 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (5650 virtual)
2025-12-23 01:13:10,111 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (6680 virtual)
2025-12-23 01:13:10,112 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (7623 virtual)
2025-12-23 01:13:10,113 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (8704 virtual)
2025-12-23 01:13:10,114 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (9794 virtual)
2025-12-23 01:13:10,114 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (10897 virtual)
2025-12-23 01:13:10,115 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (11911 virtual)
2025-12-23 01:13:10,116 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (12921 virtual)
2025-12-23 01:13:10,117 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (14032 virtual)
2025-12-23 01:13:10,118 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (15005 virtual)
2025-12-23 01:13:10,119 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (16044 virtual)
2025-12-23 01:13:10,120 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (17093 virtual)
2025-12-23 01:13:10,120 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (18108 virtual)
2025-12-23 01:13:10,121 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (19117 virtual)
2025-12-23 01:13:10,122 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (20085 virtual)
2025-12-23 01:13:10,123 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (21140 virtual)
2025-12-23 01:13:10,124 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (22139 virtual)
2025-12-23 01:13:10,124 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (23227 virtual)
2025-12-23 01:13:10,125 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (24166 virtual)
2025-12-23 01:13:10,126 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (25137 virtual)
2025-12-23 01:13:10,127 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (26187 virtual)
2025-12-23 01:13:10,127 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (27113 virtual)
2025-12-23 01:13:10,128 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (28084 virtual)
2025-12-23 01:13:10,129 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (29113 virtual)
2025-12-23 01:13:10,130 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (30069 virtual)
2025-12-23 01:13:10,131 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (31017 virtual)
2025-12-23 01:13:10,132 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (31951 virtual)
2025-12-23 01:13:10,132 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (32954 virtual)
2025-12-23 01:13:10,133 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (34063 virtual)
2025-12-23 01:13:10,134 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (35021 virtual)
2025-12-23 01:13:10,135 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (36080 virtual)
2025-12-23 01:13:10,136 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (37103 virtual)
2025-12-23 01:13:10,136 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (38185 virtual)
2025-12-23 01:13:10,137 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (39172 virtual)
2025-12-23 01:13:10,138 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (40185 virtual)
2025-12-23 01:13:10,140 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (41113 virtual)
2025-12-23 01:13:10,141 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (42261 virtual)
2025-12-23 01:13:10,142 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (43352 virtual)
2025-12-23 01:13:10,143 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (44331 virtual)
2025-12-23 01:13:10,144 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (45325 virtual)
2025-12-23 01:13:10,144 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (46441 virtual)
2025-12-23 01:13:10,145 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (47389 virtual)
2025-12-23 01:13:10,146 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (48333 virtual)
2025-12-23 01:13:10,147 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (49375 virtual)
2025-12-23 01:13:10,147 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (50465 virtual)
2025-12-23 01:13:10,148 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (51456 virtual)
2025-12-23 01:13:10,149 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (52441 virtual)
2025-12-23 01:13:10,150 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (53484 virtual)
2025-12-23 01:13:10,150 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (54533 virtual)
2025-12-23 01:13:10,154 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (55592 virtual)
2025-12-23 01:13:10,159 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (56681 virtual)
2025-12-23 01:13:10,170 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (57717 virtual)
2025-12-23 01:13:10,171 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (58705 virtual)
2025-12-23 01:13:10,172 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (59654 virtual)
2025-12-23 01:13:10,180 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (60700 virtual)
2025-12-23 01:13:10,181 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (61672 virtual)
2025-12-23 01:13:10,182 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (62694 virtual)
2025-12-23 01:13:10,183 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (63757 virtual)
2025-12-23 01:13:10,184 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (64840 virtual)
2025-12-23 01:13:10,196 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (65744 virtual)
2025-12-23 01:13:10,197 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (66827 virtual)
2025-12-23 01:13:10,198 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (67785 virtual)
2025-12-23 01:13:10,199 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (68804 virtual)
2025-12-23 01:13:10,200 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (69764 virtual)
2025-12-23 01:13:10,200 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (70807 virtual)
2025-12-23 01:13:10,201 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (71846 virtual)
2025-12-23 01:13:10,202 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (72847 virtual)
2025-12-23 01:13:10,202 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (73925 virtual)
2025-12-23 01:13:10,203 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (75010 virtual)
2025-12-23 01:13:10,204 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (75968 virtual)
2025-12-23 01:13:10,205 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (76955 virtual)
2025-12-23 01:13:10,206 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (77973 virtual)
2025-12-23 01:13:10,207 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (78900 virtual)
2025-12-23 01:13:10,207 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (79915 virtual)
2025-12-23 01:13:10,208 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (80921 virtual)
2025-12-23 01:13:10,209 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (81894 virtual)
2025-12-23 01:13:10,210 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (82979 virtual)
2025-12-23 01:13:10,211 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (83937 virtual)
2025-12-23 01:13:10,211 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (84989 virtual)
2025-12-23 01:13:10,212 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (85991 virtual)
2025-12-23 01:13:10,213 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (87013 virtual)
2025-12-23 01:13:10,214 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (88050 virtual)
2025-12-23 01:13:10,214 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (88989 virtual)
2025-12-23 01:13:10,215 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (89975 virtual)
2025-12-23 01:13:10,216 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (90892 virtual)
2025-12-23 01:13:10,217 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (91944 virtual)
2025-12-23 01:13:10,218 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (93008 virtual)
2025-12-23 01:13:10,218 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (93938 virtual)
2025-12-23 01:13:10,219 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (94952 virtual)
2025-12-23 01:13:10,220 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (95989 virtual)
2025-12-23 01:13:10,221 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (97083 virtual)
2025-12-23 01:13:10,221 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (97968 virtual)
2025-12-23 01:13:10,222 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (98973 virtual)
2025-12-23 01:13:10,223 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (99956 virtual)
2025-12-23 01:13:10,223 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (101124 virtual)
2025-12-23 01:13:10,224 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (102046 virtual)
2025-12-23 01:13:10,225 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (103005 virtual)
2025-12-23 01:13:10,225 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (104049 virtual)
2025-12-23 01:13:10,226 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (105026 virtual)
2025-12-23 01:13:10,227 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (106094 virtual)
2025-12-23 01:13:10,227 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (107077 virtual)
2025-12-23 01:13:10,228 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (108131 virtual)
2025-12-23 01:13:10,228 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (109133 virtual)
2025-12-23 01:13:10,237 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (110094 virtual)
2025-12-23 01:13:10,249 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (111108 virtual)
2025-12-23 01:13:10,251 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (112228 virtual)
2025-12-23 01:13:10,251 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (113209 virtual)
2025-12-23 01:13:10,252 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (114200 virtual)
2025-12-23 01:13:10,253 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (115208 virtual)
2025-12-23 01:13:10,253 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (116189 virtual)
2025-12-23 01:13:10,254 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (117185 virtual)
2025-12-23 01:13:10,255 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (118161 virtual)
2025-12-23 01:13:10,265 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (119147 virtual)
2025-12-23 01:13:10,265 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (120162 virtual)
2025-12-23 01:13:10,266 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (120878 virtual)
2025-12-23 01:13:10,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:10,963 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:10,964 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,075 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,075 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,085 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,095 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,098 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,098 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,115 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,116 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,118 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,118 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,118 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,118 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,120 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,120 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,124 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,126 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,130 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,137 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,137 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,140 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,146 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,146 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,146 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,148 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,158 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,161 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,169 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,169 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,188 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,204 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,208 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,233 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,245 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,282 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,290 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,333 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,352 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,368 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,392 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,392 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,078 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,521 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,530 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,537 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,576 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,576 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,578 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,582 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,608 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,683 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,688 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,690 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,698 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,740 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,756 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,766 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,774 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,812 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,836 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,842 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,845 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,850 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,860 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,861 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,861 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,870 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,904 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,905 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,905 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,910 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,921 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,925 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,932 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,948 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,964 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,973 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,986 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:11,783 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,010 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,024 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,024 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,029 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,042 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,044 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,045 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,045 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,050 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,069 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:11,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,081 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,081 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,095 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,097 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,097 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,106 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,112 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,119 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,133 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,137 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,158 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,158 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,178 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,208 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,213 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,221 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,228 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,233 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,241 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,333 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:12,372 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:12,385 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:13,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 01:13:13,353 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 01:13:15,362 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 01:13:15,937 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 120892 virtual documents
2025-12-23 01:13:16,404 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.6685362931829657, 'coherence_npmi': 0.1428146911146273, 'topic_diversity': 0.8531645569620253, 'inter_topic_similarity': 0.16992951929569244}
2025-12-23 01:13:16,405 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.5885271319968265, 'coherence_npmi': 0.07285931287912337, 'topic_diversity': 0.846, 'inter_topic_similarity': 0.25032442808151245}
2025-12-23 01:13:16,405 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.5812095512257472, 'coherence_npmi': 0.06739966748716736, 'topic_diversity': 0.8042553191489362, 'inter_topic_similarity': 0.22617118060588837}
2025-12-23 01:13:16,405 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model HDBSCAN for hierarchical conversion
2025-12-23 01:13:28,211 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model KMeans for hierarchical conversion
2025-12-23 01:13:32,641 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model BERTopicCobwebWrapper for hierarchical conversion
2025-12-23 01:13:38,452 INFO __main__: Hierarchical Model 0 (HDBSCAN) metrics: {'hier_coherence_npmi': 0.09246699262186192, 'hier_topic_uniqueness': 0.8185827748821047, 'hier_topic_diversity': 0.8185827748821047, 'hier_topic_specialization': 0.2866203064285153, 'hier_affinity_child': 0.7300034761428833, 'hier_affinity_non_child': 0.1811189204454422, 'hier_coherence_clnpmi': 0.0180330795751072, 'hier_PC_TD': 0.2659325396825397, 'hier_PnonC_TD': 0.8244298623252196, 'hier_sibling_TD': 0.9400686274509804, 'hier_sibling_clnpmi': 0.02362093334507602}
2025-12-23 01:13:38,452 INFO __main__: Hierarchical Model 1 (KMeans) metrics: {'hier_coherence_npmi': 0.08528452206060874, 'hier_topic_uniqueness': 0.7727621832358674, 'hier_topic_diversity': 0.7727621832358674, 'hier_topic_specialization': 0.2491555442776001, 'hier_affinity_child': 0.7911347150802612, 'hier_affinity_non_child': 0.3054691553115845, 'hier_coherence_clnpmi': 0.015701424143816933, 'hier_PC_TD': 0.2662280701754386, 'hier_PnonC_TD': 0.7924680371432222, 'hier_sibling_TD': 0.8681578947368421, 'hier_sibling_clnpmi': 0.03131443138378622}
2025-12-23 01:13:38,452 INFO __main__: Hierarchical Model 2 (BERTopicCobwebWrapper) metrics: {'hier_coherence_npmi': 0.09961574357489288, 'hier_topic_uniqueness': 0.7720794316007082, 'hier_topic_diversity': 0.7720794316007081, 'hier_topic_specialization': 0.27111340957114566, 'hier_affinity_child': 0.7670618891716003, 'hier_affinity_non_child': 0.24528223276138306, 'hier_coherence_clnpmi': 0.019047237517211855, 'hier_PC_TD': 0.26843167606803975, 'hier_PnonC_TD': 0.769853966994331, 'hier_sibling_TD': 0.9456611570247934, 'hier_sibling_clnpmi': 0.10009584235164648}
