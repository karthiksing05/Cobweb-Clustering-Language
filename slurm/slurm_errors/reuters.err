2025-12-23 00:25:39,847 INFO __main__: Starting benchmark for dataset=reuters
2025-12-23 00:25:47,465 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-23 00:25:47,973 INFO gensim.corpora.dictionary: adding document #10000 to Dictionary<29631 unique tokens: ['10', '15', '17', '1985', '30']...>
2025-12-23 00:25:48,015 INFO gensim.corpora.dictionary: built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)
2025-12-23 00:25:48,019 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)", 'datetime': '2025-12-23T00:25:48.015519', 'gensim': '4.4.0', 'python': '3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-187-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-23 00:25:51,006 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-23 00:25:51,006 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-23 00:25:57,079 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10788 docs
2025-12-23 00:28:04,445 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 00:28:07,219 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-23 00:28:07,231 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-23 00:28:07,238 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-23 00:28:07,257 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-23 00:28:07,470 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-23 00:28:07,635 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-23 00:28:07,738 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-23 00:28:09,090 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-23 00:28:09,277 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-23 00:28:09,608 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-23 00:28:09,611 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-23 00:28:09,819 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-23 00:28:10,406 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-23 00:28:10,809 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-23 00:28:11,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:11,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:11,783 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:11,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:11,868 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:11,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:11,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,033 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,041 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,123 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,153 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,479 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,482 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,773 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,800 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,853 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,856 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:12,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:12,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,034 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,868 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:13,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:13,962 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:14,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:14,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:14,087 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:14,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:14,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:14,261 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:14,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:14,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:14,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:14,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:14,536 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:16,226 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-23 00:28:16,445 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-23 00:28:17,590 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 00:28:20,573 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-23 00:28:20,575 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-23 00:28:20,577 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-23 00:28:20,579 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-23 00:28:20,581 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-23 00:28:20,583 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-23 00:28:20,585 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-23 00:28:20,587 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-23 00:28:20,589 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-23 00:28:20,592 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-23 00:28:20,594 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-23 00:28:20,596 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-23 00:28:20,598 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-23 00:28:20,600 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-23 00:28:20,601 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-23 00:28:20,603 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-23 00:28:20,605 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-23 00:28:20,607 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-23 00:28:20,609 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-23 00:28:20,611 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-23 00:28:20,613 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-23 00:28:20,615 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-23 00:28:20,618 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-23 00:28:20,620 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-23 00:28:20,622 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-23 00:28:20,624 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-23 00:28:20,626 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-23 00:28:20,644 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-23 00:28:20,661 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-23 00:28:20,663 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-23 00:28:20,665 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-23 00:28:20,668 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-23 00:28:20,670 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-23 00:28:20,671 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-23 00:28:20,684 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-23 00:28:20,687 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-23 00:28:20,688 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-23 00:28:20,690 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-23 00:28:20,692 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-23 00:28:20,694 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-23 00:28:20,695 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-23 00:28:20,699 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-23 00:28:20,701 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-23 00:28:20,705 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-23 00:28:20,709 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-23 00:28:20,713 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-23 00:28:20,721 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-23 00:28:20,729 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-23 00:28:20,731 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-23 00:28:20,734 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-23 00:28:20,742 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-23 00:28:20,745 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-23 00:28:20,753 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-23 00:28:20,755 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-23 00:28:20,758 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-23 00:28:20,877 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-23 00:28:20,973 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-23 00:28:21,021 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-23 00:28:21,023 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-23 00:28:21,111 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-23 00:28:21,125 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-23 00:28:21,130 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-23 00:28:21,246 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-23 00:28:21,248 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-23 00:28:21,251 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-23 00:28:21,349 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-23 00:28:21,352 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-23 00:28:21,354 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-23 00:28:21,422 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-23 00:28:21,425 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-23 00:28:21,427 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-23 00:28:21,429 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-23 00:28:21,457 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-23 00:28:21,488 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-23 00:28:21,525 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-23 00:28:21,560 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-23 00:28:21,597 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-23 00:28:21,628 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-23 00:28:21,665 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-23 00:28:21,708 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-23 00:28:21,732 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-23 00:28:21,776 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-23 00:28:21,800 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-23 00:28:21,832 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-23 00:28:21,864 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-23 00:28:21,900 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-23 00:28:21,933 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-23 00:28:21,969 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-23 00:28:22,001 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-23 00:28:22,044 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-23 00:28:22,077 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-23 00:28:22,121 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-23 00:28:22,161 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-23 00:28:22,192 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-23 00:28:22,228 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-23 00:28:22,261 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-23 00:28:22,296 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-23 00:28:22,329 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-23 00:28:22,365 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-23 00:28:22,401 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-23 00:28:22,444 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-23 00:28:22,468 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-23 00:28:22,573 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-23 00:28:22,576 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-23 00:28:22,596 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-23 00:28:22,628 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-23 00:28:22,657 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-23 00:28:22,793 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-23 00:28:22,796 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-23 00:28:22,869 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-23 00:28:22,903 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-23 00:28:22,905 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-23 00:28:22,907 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-23 00:28:22,995 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-23 00:28:22,998 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-23 00:28:23,066 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-23 00:28:23,068 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-23 00:28:23,071 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-23 00:28:23,154 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-23 00:28:23,157 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-23 00:28:23,235 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-23 00:28:23,237 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-23 00:28:23,311 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-23 00:28:23,313 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-23 00:28:23,315 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-23 00:28:23,398 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-23 00:28:23,402 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-23 00:28:23,465 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-23 00:28:23,467 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-23 00:28:23,474 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-23 00:28:23,578 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-23 00:28:23,581 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-23 00:28:23,584 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-23 00:28:23,586 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-23 00:28:23,609 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-23 00:28:23,636 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-23 00:28:23,736 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-23 00:28:23,739 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-23 00:28:23,838 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-23 00:28:23,841 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-23 00:28:23,843 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-23 00:28:23,845 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-23 00:28:23,873 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-23 00:28:23,965 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-23 00:28:23,967 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-23 00:28:23,968 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-23 00:28:23,992 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-23 00:28:24,088 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-23 00:28:24,090 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-23 00:28:24,171 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-23 00:28:24,173 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-23 00:28:24,174 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-23 00:28:24,184 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-23 00:28:24,280 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-23 00:28:24,283 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-23 00:28:24,297 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-23 00:28:24,321 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-23 00:28:24,362 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-23 00:28:24,473 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-23 00:28:24,475 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-23 00:28:24,477 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-23 00:28:24,504 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-23 00:28:24,626 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-23 00:28:24,629 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-23 00:28:24,631 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-23 00:28:24,735 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-23 00:28:24,738 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-23 00:28:24,795 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-23 00:28:24,800 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-23 00:28:24,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:24,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:24,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:24,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:24,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:24,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:24,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:24,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,957 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:24,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,083 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,117 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,183 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,491 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,654 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,776 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,924 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,949 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,949 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:25,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:25,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,049 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,691 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:28:26,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:26,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:28:28,807 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-23 00:28:28,975 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-23 00:28:29,544 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10788 docs
2025-12-23 00:30:11,273 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 00:30:14,339 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-23 00:30:14,350 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-23 00:30:14,357 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-23 00:30:14,376 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-23 00:30:14,727 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-23 00:30:14,899 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-23 00:30:14,994 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-23 00:30:16,187 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-23 00:30:16,288 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-23 00:30:16,569 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-23 00:30:16,617 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-23 00:30:16,771 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-23 00:30:17,320 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-23 00:30:17,609 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-23 00:30:17,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,773 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,773 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,774 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,783 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:17,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:17,966 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,048 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,054 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,083 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,109 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,134 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,206 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,432 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,443 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,462 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,484 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,523 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,678 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:18,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:18,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,117 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,467 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:19,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:19,508 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:21,338 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-23 00:30:21,488 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-23 00:30:21,967 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 00:30:25,123 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-23 00:30:25,126 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-23 00:30:25,128 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-23 00:30:25,130 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-23 00:30:25,132 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-23 00:30:25,134 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-23 00:30:25,137 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-23 00:30:25,138 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-23 00:30:25,141 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-23 00:30:25,143 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-23 00:30:25,145 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-23 00:30:25,148 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-23 00:30:25,150 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-23 00:30:25,152 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-23 00:30:25,153 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-23 00:30:25,155 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-23 00:30:25,157 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-23 00:30:25,159 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-23 00:30:25,161 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-23 00:30:25,163 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-23 00:30:25,164 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-23 00:30:25,166 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-23 00:30:25,169 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-23 00:30:25,171 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-23 00:30:25,174 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-23 00:30:25,176 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-23 00:30:25,178 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-23 00:30:25,179 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-23 00:30:25,265 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-23 00:30:25,267 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-23 00:30:25,268 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-23 00:30:25,288 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-23 00:30:25,352 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-23 00:30:25,354 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-23 00:30:25,356 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-23 00:30:25,358 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-23 00:30:25,360 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-23 00:30:25,392 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-23 00:30:25,395 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-23 00:30:25,444 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-23 00:30:25,481 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-23 00:30:25,517 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-23 00:30:25,548 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-23 00:30:25,550 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-23 00:30:25,613 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-23 00:30:25,650 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-23 00:30:25,653 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-23 00:30:25,655 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-23 00:30:25,727 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-23 00:30:25,741 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-23 00:30:25,803 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-23 00:30:25,805 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-23 00:30:25,821 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-23 00:30:25,889 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-23 00:30:25,942 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-23 00:30:25,983 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-23 00:30:25,997 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-23 00:30:26,002 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-23 00:30:26,102 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-23 00:30:26,105 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-23 00:30:26,107 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-23 00:30:26,185 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-23 00:30:26,227 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-23 00:30:26,240 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-23 00:30:26,246 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-23 00:30:26,335 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-23 00:30:26,337 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-23 00:30:26,339 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-23 00:30:26,417 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-23 00:30:26,420 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-23 00:30:26,422 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-23 00:30:26,490 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-23 00:30:26,493 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-23 00:30:26,495 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-23 00:30:26,580 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-23 00:30:26,593 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-23 00:30:26,598 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-23 00:30:26,667 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-23 00:30:26,672 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-23 00:30:26,692 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-23 00:30:26,739 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-23 00:30:26,752 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-23 00:30:26,758 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-23 00:30:26,821 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-23 00:30:26,854 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-23 00:30:26,890 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-23 00:30:26,892 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-23 00:30:26,895 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-23 00:30:26,958 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-23 00:30:26,972 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-23 00:30:27,034 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-23 00:30:27,037 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-23 00:30:27,039 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-23 00:30:27,113 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-23 00:30:27,115 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-23 00:30:27,118 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-23 00:30:27,174 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-23 00:30:27,189 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-23 00:30:27,270 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-23 00:30:27,273 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-23 00:30:27,275 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-23 00:30:27,343 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-23 00:30:27,345 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-23 00:30:27,413 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-23 00:30:27,416 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-23 00:30:27,418 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-23 00:30:27,461 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-23 00:30:27,463 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-23 00:30:27,535 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-23 00:30:27,537 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-23 00:30:27,579 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-23 00:30:27,581 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-23 00:30:27,596 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-23 00:30:27,673 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-23 00:30:27,676 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-23 00:30:27,678 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-23 00:30:27,739 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-23 00:30:27,745 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-23 00:30:27,811 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-23 00:30:27,825 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-23 00:30:27,894 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-23 00:30:27,896 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-23 00:30:27,899 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-23 00:30:27,902 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-23 00:30:27,975 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-23 00:30:27,977 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-23 00:30:28,018 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-23 00:30:28,037 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-23 00:30:28,039 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-23 00:30:28,113 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-23 00:30:28,150 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-23 00:30:28,165 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-23 00:30:28,226 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-23 00:30:28,229 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-23 00:30:28,231 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-23 00:30:28,299 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-23 00:30:28,312 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-23 00:30:28,328 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-23 00:30:28,410 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-23 00:30:28,413 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-23 00:30:28,415 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-23 00:30:28,473 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-23 00:30:28,476 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-23 00:30:28,478 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-23 00:30:28,515 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-23 00:30:28,517 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-23 00:30:28,519 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-23 00:30:28,557 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-23 00:30:28,559 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-23 00:30:28,643 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-23 00:30:28,645 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-23 00:30:28,660 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-23 00:30:28,723 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-23 00:30:28,726 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-23 00:30:28,728 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-23 00:30:28,787 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-23 00:30:28,789 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-23 00:30:28,837 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-23 00:30:28,879 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-23 00:30:28,882 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-23 00:30:28,884 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-23 00:30:28,890 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-23 00:30:28,951 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-23 00:30:28,953 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-23 00:30:28,955 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-23 00:30:29,019 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-23 00:30:29,022 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-23 00:30:29,024 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-23 00:30:29,066 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-23 00:30:29,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,079 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,331 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,418 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,541 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,660 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,786 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,794 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,837 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,969 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,969 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:29,988 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:29,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,096 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,213 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,216 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,317 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,373 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,470 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,521 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,522 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,576 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:30,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:30:30,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:30:32,514 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-23 00:30:32,611 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-23 00:30:32,991 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10788 docs
Training CobwebTree:   0%|          | 0/10788 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 19/10788 [00:00<01:00, 177.49it/s]Training CobwebTree:   0%|          | 37/10788 [00:00<01:31, 117.72it/s]Training CobwebTree:   0%|          | 50/10788 [00:00<01:46, 100.96it/s]Training CobwebTree:   1%|          | 61/10788 [00:00<01:49, 98.14it/s] Training CobwebTree:   1%|          | 72/10788 [00:00<01:58, 90.75it/s]Training CobwebTree:   1%|          | 82/10788 [00:00<02:10, 81.99it/s]Training CobwebTree:   1%|          | 91/10788 [00:00<02:14, 79.82it/s]Training CobwebTree:   1%|          | 100/10788 [00:01<02:17, 77.83it/s]Training CobwebTree:   1%|          | 108/10788 [00:01<02:29, 71.23it/s]Training CobwebTree:   1%|          | 116/10788 [00:01<02:27, 72.41it/s]Training CobwebTree:   1%|          | 124/10788 [00:01<02:40, 66.27it/s]Training CobwebTree:   1%|          | 131/10788 [00:01<02:38, 67.15it/s]Training CobwebTree:   1%|         | 138/10788 [00:01<02:48, 63.26it/s]Training CobwebTree:   1%|         | 145/10788 [00:01<03:00, 58.82it/s]Training CobwebTree:   1%|         | 151/10788 [00:01<03:07, 56.78it/s]Training CobwebTree:   1%|         | 157/10788 [00:02<03:04, 57.57it/s]Training CobwebTree:   2%|         | 163/10788 [00:02<03:12, 55.17it/s]Training CobwebTree:   2%|         | 169/10788 [00:02<03:17, 53.87it/s]Training CobwebTree:   2%|         | 175/10788 [00:02<03:15, 54.18it/s]Training CobwebTree:   2%|         | 182/10788 [00:02<03:10, 55.69it/s]Training CobwebTree:   2%|         | 189/10788 [00:02<03:02, 58.15it/s]Training CobwebTree:   2%|         | 195/10788 [00:02<03:11, 55.26it/s]Training CobwebTree:   2%|         | 201/10788 [00:02<03:10, 55.48it/s]Training CobwebTree:   2%|         | 207/10788 [00:03<03:12, 54.92it/s]Training CobwebTree:   2%|         | 213/10788 [00:03<03:08, 56.06it/s]Training CobwebTree:   2%|         | 219/10788 [00:03<03:13, 54.63it/s]Training CobwebTree:   2%|         | 225/10788 [00:03<03:08, 56.07it/s]Training CobwebTree:   2%|         | 231/10788 [00:03<03:16, 53.76it/s]Training CobwebTree:   2%|         | 237/10788 [00:03<03:17, 53.47it/s]Training CobwebTree:   2%|         | 243/10788 [00:03<03:15, 53.87it/s]Training CobwebTree:   2%|         | 249/10788 [00:03<03:22, 52.03it/s]Training CobwebTree:   2%|         | 255/10788 [00:03<03:31, 49.91it/s]Training CobwebTree:   2%|         | 261/10788 [00:04<03:28, 50.48it/s]Training CobwebTree:   2%|         | 267/10788 [00:04<03:25, 51.26it/s]Training CobwebTree:   3%|         | 273/10788 [00:04<03:27, 50.73it/s]Training CobwebTree:   3%|         | 279/10788 [00:04<03:25, 51.23it/s]Training CobwebTree:   3%|         | 285/10788 [00:04<03:21, 52.01it/s]Training CobwebTree:   3%|         | 291/10788 [00:04<03:14, 54.02it/s]Training CobwebTree:   3%|         | 297/10788 [00:04<03:17, 53.06it/s]Training CobwebTree:   3%|         | 303/10788 [00:04<03:23, 51.52it/s]Training CobwebTree:   3%|         | 309/10788 [00:04<03:19, 52.63it/s]Training CobwebTree:   3%|         | 315/10788 [00:05<03:16, 53.36it/s]Training CobwebTree:   3%|         | 321/10788 [00:05<03:15, 53.46it/s]Training CobwebTree:   3%|         | 327/10788 [00:05<03:15, 53.49it/s]Training CobwebTree:   3%|         | 333/10788 [00:05<03:26, 50.68it/s]Training CobwebTree:   3%|         | 339/10788 [00:05<03:29, 49.97it/s]Training CobwebTree:   3%|         | 345/10788 [00:05<03:35, 48.45it/s]Training CobwebTree:   3%|         | 350/10788 [00:05<03:39, 47.63it/s]Training CobwebTree:   3%|         | 355/10788 [00:05<03:38, 47.79it/s]Training CobwebTree:   3%|         | 360/10788 [00:05<03:39, 47.61it/s]Training CobwebTree:   3%|         | 366/10788 [00:06<03:26, 50.52it/s]Training CobwebTree:   3%|         | 372/10788 [00:06<03:19, 52.14it/s]Training CobwebTree:   4%|         | 378/10788 [00:06<03:20, 51.79it/s]Training CobwebTree:   4%|         | 384/10788 [00:06<03:30, 49.41it/s]Training CobwebTree:   4%|         | 390/10788 [00:06<03:24, 50.75it/s]Training CobwebTree:   4%|         | 396/10788 [00:06<03:24, 50.80it/s]Training CobwebTree:   4%|         | 402/10788 [00:06<03:33, 48.62it/s]Training CobwebTree:   4%|         | 407/10788 [00:06<03:32, 48.88it/s]Training CobwebTree:   4%|         | 412/10788 [00:07<03:33, 48.70it/s]Training CobwebTree:   4%|         | 418/10788 [00:07<03:29, 49.60it/s]Training CobwebTree:   4%|         | 424/10788 [00:07<03:30, 49.13it/s]Training CobwebTree:   4%|         | 430/10788 [00:07<03:23, 51.00it/s]Training CobwebTree:   4%|         | 436/10788 [00:07<03:30, 49.25it/s]Training CobwebTree:   4%|         | 441/10788 [00:07<03:35, 47.96it/s]Training CobwebTree:   4%|         | 447/10788 [00:07<03:28, 49.64it/s]Training CobwebTree:   4%|         | 453/10788 [00:07<03:22, 51.14it/s]Training CobwebTree:   4%|         | 459/10788 [00:07<03:22, 50.91it/s]Training CobwebTree:   4%|         | 465/10788 [00:08<03:33, 48.44it/s]Training CobwebTree:   4%|         | 470/10788 [00:08<03:33, 48.34it/s]Training CobwebTree:   4%|         | 475/10788 [00:08<03:40, 46.81it/s]Training CobwebTree:   4%|         | 480/10788 [00:08<03:48, 45.16it/s]Training CobwebTree:   5%|         | 486/10788 [00:08<03:33, 48.20it/s]Training CobwebTree:   5%|         | 491/10788 [00:08<03:36, 47.53it/s]Training CobwebTree:   5%|         | 496/10788 [00:08<03:41, 46.41it/s]Training CobwebTree:   5%|         | 501/10788 [00:08<03:40, 46.72it/s]Training CobwebTree:   5%|         | 506/10788 [00:08<03:42, 46.24it/s]Training CobwebTree:   5%|         | 511/10788 [00:09<03:38, 46.96it/s]Training CobwebTree:   5%|         | 517/10788 [00:09<03:37, 47.12it/s]Training CobwebTree:   5%|         | 523/10788 [00:09<03:38, 46.90it/s]Training CobwebTree:   5%|         | 528/10788 [00:09<03:36, 47.38it/s]Training CobwebTree:   5%|         | 533/10788 [00:09<03:38, 46.96it/s]Training CobwebTree:   5%|         | 538/10788 [00:09<03:42, 46.02it/s]Training CobwebTree:   5%|         | 543/10788 [00:09<03:52, 44.03it/s]Training CobwebTree:   5%|         | 549/10788 [00:09<03:43, 45.91it/s]Training CobwebTree:   5%|         | 554/10788 [00:10<03:42, 46.07it/s]Training CobwebTree:   5%|         | 559/10788 [00:10<03:50, 44.44it/s]Training CobwebTree:   5%|         | 564/10788 [00:10<04:00, 42.48it/s]Training CobwebTree:   5%|         | 569/10788 [00:10<04:08, 41.08it/s]Training CobwebTree:   5%|         | 574/10788 [00:10<04:19, 39.35it/s]Training CobwebTree:   5%|         | 579/10788 [00:10<04:08, 41.15it/s]Training CobwebTree:   5%|         | 584/10788 [00:10<04:05, 41.48it/s]Training CobwebTree:   5%|         | 589/10788 [00:10<03:54, 43.43it/s]Training CobwebTree:   6%|         | 594/10788 [00:10<03:58, 42.77it/s]Training CobwebTree:   6%|         | 599/10788 [00:11<04:02, 41.95it/s]Training CobwebTree:   6%|         | 604/10788 [00:11<03:59, 42.60it/s]Training CobwebTree:   6%|         | 609/10788 [00:11<03:57, 42.85it/s]Training CobwebTree:   6%|         | 614/10788 [00:11<03:54, 43.46it/s]Training CobwebTree:   6%|         | 620/10788 [00:11<03:44, 45.34it/s]Training CobwebTree:   6%|         | 627/10788 [00:11<03:19, 51.03it/s]Training CobwebTree:   6%|         | 633/10788 [00:11<03:27, 48.84it/s]Training CobwebTree:   6%|         | 638/10788 [00:11<03:28, 48.72it/s]Training CobwebTree:   6%|         | 643/10788 [00:12<03:27, 48.78it/s]Training CobwebTree:   6%|         | 648/10788 [00:12<03:33, 47.57it/s]Training CobwebTree:   6%|         | 653/10788 [00:12<03:31, 48.00it/s]Training CobwebTree:   6%|         | 658/10788 [00:12<03:31, 47.87it/s]Training CobwebTree:   6%|         | 664/10788 [00:12<03:23, 49.76it/s]Training CobwebTree:   6%|         | 669/10788 [00:12<03:25, 49.16it/s]Training CobwebTree:   6%|         | 674/10788 [00:12<03:39, 46.14it/s]Training CobwebTree:   6%|         | 679/10788 [00:12<03:39, 46.03it/s]Training CobwebTree:   6%|         | 685/10788 [00:12<03:30, 48.00it/s]Training CobwebTree:   6%|         | 690/10788 [00:13<03:31, 47.72it/s]Training CobwebTree:   6%|         | 695/10788 [00:13<03:36, 46.52it/s]Training CobwebTree:   6%|         | 700/10788 [00:13<03:43, 45.21it/s]Training CobwebTree:   7%|         | 705/10788 [00:13<03:51, 43.52it/s]Training CobwebTree:   7%|         | 710/10788 [00:13<03:48, 44.15it/s]Training CobwebTree:   7%|         | 715/10788 [00:13<03:45, 44.75it/s]Training CobwebTree:   7%|         | 720/10788 [00:13<03:40, 45.61it/s]Training CobwebTree:   7%|         | 726/10788 [00:13<03:23, 49.56it/s]Training CobwebTree:   7%|         | 731/10788 [00:13<03:23, 49.32it/s]Training CobwebTree:   7%|         | 737/10788 [00:14<03:20, 50.20it/s]Training CobwebTree:   7%|         | 743/10788 [00:14<03:32, 47.23it/s]Training CobwebTree:   7%|         | 749/10788 [00:14<03:26, 48.55it/s]Training CobwebTree:   7%|         | 754/10788 [00:14<03:34, 46.81it/s]Training CobwebTree:   7%|         | 759/10788 [00:14<03:49, 43.63it/s]Training CobwebTree:   7%|         | 764/10788 [00:14<03:47, 44.00it/s]Training CobwebTree:   7%|         | 769/10788 [00:14<03:40, 45.37it/s]Training CobwebTree:   7%|         | 774/10788 [00:14<03:39, 45.66it/s]Training CobwebTree:   7%|         | 780/10788 [00:14<03:37, 46.00it/s]Training CobwebTree:   7%|         | 785/10788 [00:15<03:39, 45.66it/s]Training CobwebTree:   7%|         | 790/10788 [00:15<03:36, 46.17it/s]Training CobwebTree:   7%|         | 796/10788 [00:15<03:28, 47.85it/s]Training CobwebTree:   7%|         | 801/10788 [00:15<03:28, 47.83it/s]Training CobwebTree:   7%|         | 807/10788 [00:15<03:22, 49.34it/s]Training CobwebTree:   8%|         | 812/10788 [00:15<03:35, 46.20it/s]Training CobwebTree:   8%|         | 817/10788 [00:15<03:43, 44.68it/s]Training CobwebTree:   8%|         | 822/10788 [00:15<03:48, 43.58it/s]Training CobwebTree:   8%|         | 827/10788 [00:15<03:42, 44.81it/s]Training CobwebTree:   8%|         | 832/10788 [00:16<03:36, 45.92it/s]Training CobwebTree:   8%|         | 838/10788 [00:16<03:25, 48.35it/s]Training CobwebTree:   8%|         | 844/10788 [00:16<03:19, 49.74it/s]Training CobwebTree:   8%|         | 849/10788 [00:16<03:31, 46.97it/s]Training CobwebTree:   8%|         | 854/10788 [00:16<03:36, 45.86it/s]Training CobwebTree:   8%|         | 859/10788 [00:16<03:40, 45.01it/s]Training CobwebTree:   8%|         | 864/10788 [00:16<03:48, 43.50it/s]Training CobwebTree:   8%|         | 869/10788 [00:16<03:41, 44.73it/s]Training CobwebTree:   8%|         | 874/10788 [00:17<03:44, 44.13it/s]Training CobwebTree:   8%|         | 879/10788 [00:17<03:48, 43.43it/s]Training CobwebTree:   8%|         | 884/10788 [00:17<03:43, 44.29it/s]Training CobwebTree:   8%|         | 890/10788 [00:17<03:27, 47.62it/s]Training CobwebTree:   8%|         | 895/10788 [00:17<03:28, 47.51it/s]Training CobwebTree:   8%|         | 900/10788 [00:17<03:30, 46.97it/s]Training CobwebTree:   8%|         | 905/10788 [00:17<03:29, 47.25it/s]Training CobwebTree:   8%|         | 910/10788 [00:17<03:36, 45.66it/s]Training CobwebTree:   8%|         | 915/10788 [00:17<03:38, 45.27it/s]Training CobwebTree:   9%|         | 920/10788 [00:18<03:48, 43.13it/s]Training CobwebTree:   9%|         | 925/10788 [00:18<03:48, 43.23it/s]Training CobwebTree:   9%|         | 930/10788 [00:18<03:44, 43.90it/s]Training CobwebTree:   9%|         | 935/10788 [00:18<03:51, 42.59it/s]Training CobwebTree:   9%|         | 941/10788 [00:18<03:34, 45.83it/s]Training CobwebTree:   9%|         | 946/10788 [00:18<03:29, 46.90it/s]Training CobwebTree:   9%|         | 951/10788 [00:18<03:41, 44.38it/s]Training CobwebTree:   9%|         | 956/10788 [00:18<03:39, 44.85it/s]Training CobwebTree:   9%|         | 961/10788 [00:18<03:40, 44.49it/s]Training CobwebTree:   9%|         | 966/10788 [00:19<03:46, 43.38it/s]Training CobwebTree:   9%|         | 971/10788 [00:19<03:46, 43.26it/s]Training CobwebTree:   9%|         | 976/10788 [00:19<03:45, 43.56it/s]Training CobwebTree:   9%|         | 981/10788 [00:19<03:39, 44.73it/s]Training CobwebTree:   9%|         | 986/10788 [00:19<03:39, 44.68it/s]Training CobwebTree:   9%|         | 991/10788 [00:19<03:48, 42.82it/s]Training CobwebTree:   9%|         | 996/10788 [00:19<04:00, 40.74it/s]Training CobwebTree:   9%|         | 1001/10788 [00:19<04:05, 39.83it/s]Training CobwebTree:   9%|         | 1006/10788 [00:20<04:08, 39.40it/s]Training CobwebTree:   9%|         | 1011/10788 [00:20<03:57, 41.16it/s]Training CobwebTree:   9%|         | 1016/10788 [00:20<03:55, 41.54it/s]Training CobwebTree:   9%|         | 1021/10788 [00:20<03:55, 41.56it/s]Training CobwebTree:  10%|         | 1026/10788 [00:20<03:54, 41.60it/s]Training CobwebTree:  10%|         | 1031/10788 [00:20<03:55, 41.37it/s]Training CobwebTree:  10%|         | 1036/10788 [00:20<03:51, 42.05it/s]Training CobwebTree:  10%|         | 1042/10788 [00:20<03:38, 44.61it/s]Training CobwebTree:  10%|         | 1047/10788 [00:20<03:43, 43.62it/s]Training CobwebTree:  10%|         | 1052/10788 [00:21<03:46, 43.06it/s]Training CobwebTree:  10%|         | 1057/10788 [00:21<03:41, 43.87it/s]Training CobwebTree:  10%|         | 1062/10788 [00:21<03:49, 42.34it/s]Training CobwebTree:  10%|         | 1067/10788 [00:21<03:58, 40.82it/s]Training CobwebTree:  10%|         | 1072/10788 [00:21<04:06, 39.45it/s]Training CobwebTree:  10%|         | 1077/10788 [00:21<03:59, 40.48it/s]Training CobwebTree:  10%|         | 1082/10788 [00:21<03:57, 40.88it/s]Training CobwebTree:  10%|         | 1087/10788 [00:21<03:54, 41.38it/s]Training CobwebTree:  10%|         | 1092/10788 [00:22<03:52, 41.64it/s]Training CobwebTree:  10%|         | 1097/10788 [00:22<03:52, 41.62it/s]Training CobwebTree:  10%|         | 1103/10788 [00:22<03:40, 43.85it/s]Training CobwebTree:  10%|         | 1109/10788 [00:22<03:30, 45.90it/s]Training CobwebTree:  10%|         | 1114/10788 [00:22<03:33, 45.37it/s]Training CobwebTree:  10%|         | 1119/10788 [00:22<03:39, 44.01it/s]Training CobwebTree:  10%|         | 1124/10788 [00:22<03:47, 42.48it/s]Training CobwebTree:  10%|         | 1129/10788 [00:22<03:47, 42.43it/s]Training CobwebTree:  11%|         | 1134/10788 [00:23<03:44, 43.05it/s]Training CobwebTree:  11%|         | 1139/10788 [00:23<03:56, 40.84it/s]Training CobwebTree:  11%|         | 1144/10788 [00:23<04:02, 39.82it/s]Training CobwebTree:  11%|         | 1149/10788 [00:23<03:56, 40.83it/s]Training CobwebTree:  11%|         | 1154/10788 [00:23<03:50, 41.82it/s]Training CobwebTree:  11%|         | 1159/10788 [00:23<03:50, 41.69it/s]Training CobwebTree:  11%|         | 1164/10788 [00:23<03:52, 41.40it/s]Training CobwebTree:  11%|         | 1169/10788 [00:23<03:45, 42.72it/s]Training CobwebTree:  11%|         | 1174/10788 [00:24<03:55, 40.91it/s]Training CobwebTree:  11%|         | 1179/10788 [00:24<03:48, 42.05it/s]Training CobwebTree:  11%|         | 1184/10788 [00:24<03:50, 41.75it/s]Training CobwebTree:  11%|         | 1189/10788 [00:24<03:53, 41.11it/s]Training CobwebTree:  11%|         | 1194/10788 [00:24<03:52, 41.20it/s]Training CobwebTree:  11%|         | 1199/10788 [00:24<03:56, 40.50it/s]Training CobwebTree:  11%|         | 1204/10788 [00:24<03:44, 42.76it/s]Training CobwebTree:  11%|         | 1209/10788 [00:24<03:41, 43.27it/s]Training CobwebTree:  11%|        | 1214/10788 [00:24<03:37, 43.92it/s]Training CobwebTree:  11%|        | 1219/10788 [00:25<03:33, 44.82it/s]Training CobwebTree:  11%|        | 1225/10788 [00:25<03:21, 47.35it/s]Training CobwebTree:  11%|        | 1230/10788 [00:25<03:24, 46.65it/s]Training CobwebTree:  11%|        | 1235/10788 [00:25<03:28, 45.83it/s]Training CobwebTree:  11%|        | 1240/10788 [00:25<03:27, 46.05it/s]Training CobwebTree:  12%|        | 1245/10788 [00:25<03:24, 46.74it/s]Training CobwebTree:  12%|        | 1250/10788 [00:25<03:33, 44.61it/s]Training CobwebTree:  12%|        | 1255/10788 [00:25<03:34, 44.45it/s]Training CobwebTree:  12%|        | 1260/10788 [00:25<03:38, 43.54it/s]Training CobwebTree:  12%|        | 1265/10788 [00:26<03:50, 41.36it/s]Training CobwebTree:  12%|        | 1270/10788 [00:26<03:41, 43.02it/s]Training CobwebTree:  12%|        | 1275/10788 [00:26<03:38, 43.57it/s]Training CobwebTree:  12%|        | 1280/10788 [00:26<03:45, 42.22it/s]Training CobwebTree:  12%|        | 1285/10788 [00:26<03:38, 43.51it/s]Training CobwebTree:  12%|        | 1290/10788 [00:26<03:54, 40.53it/s]Training CobwebTree:  12%|        | 1295/10788 [00:26<03:47, 41.77it/s]Training CobwebTree:  12%|        | 1300/10788 [00:26<03:51, 41.02it/s]Training CobwebTree:  12%|        | 1305/10788 [00:27<03:47, 41.60it/s]Training CobwebTree:  12%|        | 1310/10788 [00:27<03:43, 42.37it/s]Training CobwebTree:  12%|        | 1315/10788 [00:27<03:46, 41.88it/s]Training CobwebTree:  12%|        | 1320/10788 [00:27<04:03, 38.87it/s]Training CobwebTree:  12%|        | 1325/10788 [00:27<04:05, 38.47it/s]Training CobwebTree:  12%|        | 1330/10788 [00:27<03:58, 39.64it/s]Training CobwebTree:  12%|        | 1335/10788 [00:27<03:53, 40.56it/s]Training CobwebTree:  12%|        | 1340/10788 [00:27<03:52, 40.69it/s]Training CobwebTree:  12%|        | 1345/10788 [00:28<03:48, 41.33it/s]Training CobwebTree:  13%|        | 1350/10788 [00:28<03:55, 40.16it/s]Training CobwebTree:  13%|        | 1355/10788 [00:28<04:00, 39.25it/s]Training CobwebTree:  13%|        | 1360/10788 [00:28<03:48, 41.23it/s]Training CobwebTree:  13%|        | 1365/10788 [00:28<03:54, 40.16it/s]Training CobwebTree:  13%|        | 1370/10788 [00:28<03:53, 40.30it/s]Training CobwebTree:  13%|        | 1375/10788 [00:28<03:46, 41.52it/s]Training CobwebTree:  13%|        | 1380/10788 [00:28<03:44, 41.84it/s]Training CobwebTree:  13%|        | 1385/10788 [00:29<03:57, 39.62it/s]Training CobwebTree:  13%|        | 1389/10788 [00:29<04:03, 38.53it/s]Training CobwebTree:  13%|        | 1394/10788 [00:29<04:01, 38.84it/s]Training CobwebTree:  13%|        | 1398/10788 [00:29<04:00, 39.02it/s]Training CobwebTree:  13%|        | 1403/10788 [00:29<03:58, 39.34it/s]Training CobwebTree:  13%|        | 1407/10788 [00:29<03:59, 39.21it/s]Training CobwebTree:  13%|        | 1412/10788 [00:29<03:55, 39.87it/s]Training CobwebTree:  13%|        | 1417/10788 [00:29<03:41, 42.36it/s]Training CobwebTree:  13%|        | 1422/10788 [00:29<03:33, 43.85it/s]Training CobwebTree:  13%|        | 1428/10788 [00:30<03:22, 46.19it/s]Training CobwebTree:  13%|        | 1433/10788 [00:30<03:30, 44.51it/s]Training CobwebTree:  13%|        | 1438/10788 [00:30<03:35, 43.33it/s]Training CobwebTree:  13%|        | 1443/10788 [00:30<03:38, 42.84it/s]Training CobwebTree:  13%|        | 1448/10788 [00:30<03:31, 44.21it/s]Training CobwebTree:  13%|        | 1453/10788 [00:30<03:32, 44.00it/s]Training CobwebTree:  14%|        | 1458/10788 [00:30<03:37, 42.97it/s]Training CobwebTree:  14%|        | 1463/10788 [00:30<03:46, 41.09it/s]Training CobwebTree:  14%|        | 1468/10788 [00:31<03:51, 40.30it/s]Training CobwebTree:  14%|        | 1473/10788 [00:31<03:46, 41.08it/s]Training CobwebTree:  14%|        | 1478/10788 [00:31<03:48, 40.80it/s]Training CobwebTree:  14%|        | 1484/10788 [00:31<03:36, 42.90it/s]Training CobwebTree:  14%|        | 1489/10788 [00:31<03:45, 41.25it/s]Training CobwebTree:  14%|        | 1494/10788 [00:31<03:48, 40.73it/s]Training CobwebTree:  14%|        | 1499/10788 [00:31<03:54, 39.60it/s]Training CobwebTree:  14%|        | 1503/10788 [00:31<03:55, 39.43it/s]Training CobwebTree:  14%|        | 1508/10788 [00:32<03:52, 39.92it/s]Training CobwebTree:  14%|        | 1513/10788 [00:32<03:46, 40.96it/s]Training CobwebTree:  14%|        | 1518/10788 [00:32<03:52, 39.79it/s]Training CobwebTree:  14%|        | 1523/10788 [00:32<03:43, 41.45it/s]Training CobwebTree:  14%|        | 1528/10788 [00:32<03:47, 40.63it/s]Training CobwebTree:  14%|        | 1533/10788 [00:32<03:46, 40.87it/s]Training CobwebTree:  14%|        | 1538/10788 [00:32<03:45, 41.01it/s]Training CobwebTree:  14%|        | 1543/10788 [00:32<03:51, 39.98it/s]Training CobwebTree:  14%|        | 1548/10788 [00:33<03:52, 39.67it/s]Training CobwebTree:  14%|        | 1553/10788 [00:33<03:45, 40.95it/s]Training CobwebTree:  14%|        | 1558/10788 [00:33<03:41, 41.66it/s]Training CobwebTree:  14%|        | 1563/10788 [00:33<03:50, 40.01it/s]Training CobwebTree:  15%|        | 1568/10788 [00:33<03:41, 41.70it/s]Training CobwebTree:  15%|        | 1573/10788 [00:33<03:42, 41.51it/s]Training CobwebTree:  15%|        | 1578/10788 [00:33<03:37, 42.36it/s]Training CobwebTree:  15%|        | 1583/10788 [00:33<03:42, 41.44it/s]Training CobwebTree:  15%|        | 1588/10788 [00:33<03:38, 42.09it/s]Training CobwebTree:  15%|        | 1593/10788 [00:34<03:31, 43.40it/s]Training CobwebTree:  15%|        | 1598/10788 [00:34<03:39, 41.95it/s]Training CobwebTree:  15%|        | 1603/10788 [00:34<03:47, 40.37it/s]Training CobwebTree:  15%|        | 1608/10788 [00:34<03:53, 39.37it/s]Training CobwebTree:  15%|        | 1612/10788 [00:34<03:56, 38.79it/s]Training CobwebTree:  15%|        | 1617/10788 [00:34<03:42, 41.19it/s]Training CobwebTree:  15%|        | 1622/10788 [00:34<03:32, 43.17it/s]Training CobwebTree:  15%|        | 1627/10788 [00:34<03:35, 42.41it/s]Training CobwebTree:  15%|        | 1632/10788 [00:35<03:36, 42.27it/s]Training CobwebTree:  15%|        | 1637/10788 [00:35<03:30, 43.50it/s]Training CobwebTree:  15%|        | 1642/10788 [00:35<03:49, 39.84it/s]Training CobwebTree:  15%|        | 1647/10788 [00:35<03:45, 40.51it/s]Training CobwebTree:  15%|        | 1652/10788 [00:35<03:45, 40.49it/s]Training CobwebTree:  15%|        | 1657/10788 [00:35<03:43, 40.93it/s]Training CobwebTree:  15%|        | 1662/10788 [00:35<03:48, 39.99it/s]Training CobwebTree:  15%|        | 1667/10788 [00:35<03:47, 40.07it/s]Training CobwebTree:  15%|        | 1672/10788 [00:36<03:44, 40.66it/s]Training CobwebTree:  16%|        | 1677/10788 [00:36<03:43, 40.69it/s]Training CobwebTree:  16%|        | 1682/10788 [00:36<03:49, 39.60it/s]Training CobwebTree:  16%|        | 1686/10788 [00:36<03:49, 39.63it/s]Training CobwebTree:  16%|        | 1691/10788 [00:36<03:37, 41.81it/s]Training CobwebTree:  16%|        | 1696/10788 [00:36<03:33, 42.57it/s]Training CobwebTree:  16%|        | 1701/10788 [00:36<03:52, 39.00it/s]Training CobwebTree:  16%|        | 1705/10788 [00:36<03:52, 39.14it/s]Training CobwebTree:  16%|        | 1709/10788 [00:36<03:52, 39.12it/s]Training CobwebTree:  16%|        | 1714/10788 [00:37<03:41, 40.89it/s]Training CobwebTree:  16%|        | 1719/10788 [00:37<03:33, 42.43it/s]Training CobwebTree:  16%|        | 1724/10788 [00:37<03:43, 40.57it/s]Training CobwebTree:  16%|        | 1729/10788 [00:37<03:48, 39.62it/s]Training CobwebTree:  16%|        | 1733/10788 [00:37<03:55, 38.41it/s]Training CobwebTree:  16%|        | 1737/10788 [00:37<03:54, 38.52it/s]Training CobwebTree:  16%|        | 1741/10788 [00:37<04:00, 37.63it/s]Training CobwebTree:  16%|        | 1746/10788 [00:37<03:52, 38.85it/s]Training CobwebTree:  16%|        | 1751/10788 [00:37<03:44, 40.32it/s]Training CobwebTree:  16%|        | 1756/10788 [00:38<03:45, 40.02it/s]Training CobwebTree:  16%|        | 1761/10788 [00:38<03:45, 40.09it/s]Training CobwebTree:  16%|        | 1766/10788 [00:38<03:48, 39.46it/s]Training CobwebTree:  16%|        | 1770/10788 [00:38<03:56, 38.16it/s]Training CobwebTree:  16%|        | 1775/10788 [00:38<03:57, 37.90it/s]Training CobwebTree:  16%|        | 1780/10788 [00:38<03:49, 39.18it/s]Training CobwebTree:  17%|        | 1784/10788 [00:38<03:48, 39.33it/s]Training CobwebTree:  17%|        | 1789/10788 [00:38<03:40, 40.79it/s]Training CobwebTree:  17%|        | 1794/10788 [00:39<03:45, 39.84it/s]Training CobwebTree:  17%|        | 1798/10788 [00:39<03:45, 39.82it/s]Training CobwebTree:  17%|        | 1803/10788 [00:39<03:39, 40.91it/s]Training CobwebTree:  17%|        | 1808/10788 [00:39<03:48, 39.25it/s]Training CobwebTree:  17%|        | 1812/10788 [00:39<03:55, 38.09it/s]Training CobwebTree:  17%|        | 1818/10788 [00:39<03:37, 41.16it/s]Training CobwebTree:  17%|        | 1823/10788 [00:39<03:47, 39.44it/s]Training CobwebTree:  17%|        | 1827/10788 [00:39<03:52, 38.53it/s]Training CobwebTree:  17%|        | 1832/10788 [00:40<03:44, 39.98it/s]Training CobwebTree:  17%|        | 1837/10788 [00:40<03:47, 39.26it/s]Training CobwebTree:  17%|        | 1841/10788 [00:40<03:56, 37.86it/s]Training CobwebTree:  17%|        | 1846/10788 [00:40<03:44, 39.75it/s]Training CobwebTree:  17%|        | 1851/10788 [00:40<03:37, 41.12it/s]Training CobwebTree:  17%|        | 1856/10788 [00:40<03:34, 41.59it/s]Training CobwebTree:  17%|        | 1861/10788 [00:40<03:42, 40.14it/s]Training CobwebTree:  17%|        | 1866/10788 [00:40<03:50, 38.73it/s]Training CobwebTree:  17%|        | 1871/10788 [00:41<03:38, 40.81it/s]Training CobwebTree:  17%|        | 1876/10788 [00:41<03:49, 38.86it/s]Training CobwebTree:  17%|        | 1880/10788 [00:41<03:57, 37.43it/s]Training CobwebTree:  17%|        | 1884/10788 [00:41<03:54, 37.91it/s]Training CobwebTree:  18%|        | 1888/10788 [00:41<03:54, 37.95it/s]Training CobwebTree:  18%|        | 1892/10788 [00:41<03:57, 37.47it/s]Training CobwebTree:  18%|        | 1896/10788 [00:41<04:12, 35.15it/s]Training CobwebTree:  18%|        | 1900/10788 [00:41<04:15, 34.73it/s]Training CobwebTree:  18%|        | 1904/10788 [00:41<04:08, 35.80it/s]Training CobwebTree:  18%|        | 1909/10788 [00:42<03:57, 37.39it/s]Training CobwebTree:  18%|        | 1914/10788 [00:42<03:48, 38.85it/s]Training CobwebTree:  18%|        | 1918/10788 [00:42<03:48, 38.89it/s]Training CobwebTree:  18%|        | 1923/10788 [00:42<03:49, 38.70it/s]Training CobwebTree:  18%|        | 1928/10788 [00:42<03:45, 39.29it/s]Training CobwebTree:  18%|        | 1932/10788 [00:42<03:46, 39.05it/s]Training CobwebTree:  18%|        | 1937/10788 [00:42<03:44, 39.43it/s]Training CobwebTree:  18%|        | 1941/10788 [00:42<03:46, 39.03it/s]Training CobwebTree:  18%|        | 1945/10788 [00:42<03:51, 38.28it/s]Training CobwebTree:  18%|        | 1949/10788 [00:43<03:51, 38.15it/s]Training CobwebTree:  18%|        | 1953/10788 [00:43<03:54, 37.69it/s]Training CobwebTree:  18%|        | 1957/10788 [00:43<03:51, 38.07it/s]Training CobwebTree:  18%|        | 1961/10788 [00:43<03:55, 37.55it/s]Training CobwebTree:  18%|        | 1966/10788 [00:43<03:47, 38.76it/s]Training CobwebTree:  18%|        | 1971/10788 [00:43<03:47, 38.78it/s]Training CobwebTree:  18%|        | 1975/10788 [00:43<03:49, 38.46it/s]Training CobwebTree:  18%|        | 1979/10788 [00:43<03:50, 38.14it/s]Training CobwebTree:  18%|        | 1984/10788 [00:43<03:37, 40.54it/s]Training CobwebTree:  18%|        | 1989/10788 [00:44<03:33, 41.13it/s]Training CobwebTree:  18%|        | 1994/10788 [00:44<03:41, 39.65it/s]Training CobwebTree:  19%|        | 1999/10788 [00:44<03:38, 40.20it/s]Training CobwebTree:  19%|        | 2004/10788 [00:44<03:41, 39.68it/s]Training CobwebTree:  19%|        | 2008/10788 [00:44<03:47, 38.63it/s]Training CobwebTree:  19%|        | 2013/10788 [00:44<03:38, 40.08it/s]Training CobwebTree:  19%|        | 2018/10788 [00:44<03:47, 38.59it/s]Training CobwebTree:  19%|        | 2023/10788 [00:44<03:42, 39.44it/s]Training CobwebTree:  19%|        | 2028/10788 [00:45<03:36, 40.42it/s]Training CobwebTree:  19%|        | 2033/10788 [00:45<03:34, 40.85it/s]Training CobwebTree:  19%|        | 2038/10788 [00:45<03:40, 39.72it/s]Training CobwebTree:  19%|        | 2043/10788 [00:45<03:31, 41.30it/s]Training CobwebTree:  19%|        | 2048/10788 [00:45<03:36, 40.41it/s]Training CobwebTree:  19%|        | 2053/10788 [00:45<03:39, 39.72it/s]Training CobwebTree:  19%|        | 2057/10788 [00:45<03:40, 39.52it/s]Training CobwebTree:  19%|        | 2061/10788 [00:45<03:41, 39.46it/s]Training CobwebTree:  19%|        | 2065/10788 [00:46<03:45, 38.70it/s]Training CobwebTree:  19%|        | 2069/10788 [00:46<03:46, 38.51it/s]Training CobwebTree:  19%|        | 2073/10788 [00:46<03:51, 37.71it/s]Training CobwebTree:  19%|        | 2077/10788 [00:46<04:10, 34.80it/s]Training CobwebTree:  19%|        | 2082/10788 [00:46<03:51, 37.54it/s]Training CobwebTree:  19%|        | 2087/10788 [00:46<03:40, 39.40it/s]Training CobwebTree:  19%|        | 2091/10788 [00:46<03:41, 39.26it/s]Training CobwebTree:  19%|        | 2095/10788 [00:46<03:56, 36.72it/s]Training CobwebTree:  19%|        | 2100/10788 [00:46<03:41, 39.23it/s]Training CobwebTree:  20%|        | 2104/10788 [00:47<03:42, 39.03it/s]Training CobwebTree:  20%|        | 2108/10788 [00:47<03:41, 39.25it/s]Training CobwebTree:  20%|        | 2112/10788 [00:47<03:47, 38.18it/s]Training CobwebTree:  20%|        | 2116/10788 [00:47<03:47, 38.06it/s]Training CobwebTree:  20%|        | 2120/10788 [00:47<03:44, 38.56it/s]Training CobwebTree:  20%|        | 2124/10788 [00:47<03:43, 38.78it/s]Training CobwebTree:  20%|        | 2129/10788 [00:47<03:48, 37.93it/s]Training CobwebTree:  20%|        | 2133/10788 [00:47<03:46, 38.22it/s]Training CobwebTree:  20%|        | 2137/10788 [00:47<03:45, 38.41it/s]Training CobwebTree:  20%|        | 2141/10788 [00:48<03:44, 38.50it/s]Training CobwebTree:  20%|        | 2145/10788 [00:48<03:48, 37.78it/s]Training CobwebTree:  20%|        | 2150/10788 [00:48<03:42, 38.77it/s]Training CobwebTree:  20%|        | 2154/10788 [00:48<03:46, 38.18it/s]Training CobwebTree:  20%|        | 2158/10788 [00:48<03:57, 36.36it/s]Training CobwebTree:  20%|        | 2163/10788 [00:48<03:44, 38.45it/s]Training CobwebTree:  20%|        | 2167/10788 [00:48<03:51, 37.21it/s]Training CobwebTree:  20%|        | 2171/10788 [00:48<03:55, 36.59it/s]Training CobwebTree:  20%|        | 2175/10788 [00:48<03:53, 36.84it/s]Training CobwebTree:  20%|        | 2180/10788 [00:49<03:38, 39.38it/s]Training CobwebTree:  20%|        | 2184/10788 [00:49<03:38, 39.43it/s]Training CobwebTree:  20%|        | 2189/10788 [00:49<03:38, 39.40it/s]Training CobwebTree:  20%|        | 2194/10788 [00:49<03:30, 40.82it/s]Training CobwebTree:  20%|        | 2199/10788 [00:49<03:35, 39.79it/s]Training CobwebTree:  20%|        | 2203/10788 [00:49<03:40, 38.86it/s]Training CobwebTree:  20%|        | 2207/10788 [00:49<03:43, 38.41it/s]Training CobwebTree:  20%|        | 2211/10788 [00:49<03:41, 38.70it/s]Training CobwebTree:  21%|        | 2215/10788 [00:49<03:41, 38.73it/s]Training CobwebTree:  21%|        | 2220/10788 [00:50<03:36, 39.60it/s]Training CobwebTree:  21%|        | 2225/10788 [00:50<03:33, 40.06it/s]Training CobwebTree:  21%|        | 2230/10788 [00:50<03:29, 40.80it/s]Training CobwebTree:  21%|        | 2235/10788 [00:50<03:33, 40.00it/s]Training CobwebTree:  21%|        | 2240/10788 [00:50<03:32, 40.31it/s]Training CobwebTree:  21%|        | 2245/10788 [00:50<03:34, 39.80it/s]Training CobwebTree:  21%|        | 2249/10788 [00:50<03:41, 38.60it/s]Training CobwebTree:  21%|        | 2253/10788 [00:50<03:39, 38.94it/s]Training CobwebTree:  21%|        | 2257/10788 [00:50<03:40, 38.62it/s]Training CobwebTree:  21%|        | 2262/10788 [00:51<03:27, 41.10it/s]Training CobwebTree:  21%|        | 2267/10788 [00:51<03:20, 42.47it/s]Training CobwebTree:  21%|        | 2272/10788 [00:51<03:28, 40.75it/s]Training CobwebTree:  21%|        | 2277/10788 [00:51<03:46, 37.56it/s]Training CobwebTree:  21%|        | 2281/10788 [00:51<03:45, 37.79it/s]Training CobwebTree:  21%|        | 2285/10788 [00:51<03:46, 37.60it/s]Training CobwebTree:  21%|        | 2290/10788 [00:51<03:39, 38.79it/s]Training CobwebTree:  21%|       | 2294/10788 [00:51<03:44, 37.83it/s]Training CobwebTree:  21%|       | 2298/10788 [00:52<03:41, 38.29it/s]Training CobwebTree:  21%|       | 2302/10788 [00:52<03:42, 38.17it/s]Training CobwebTree:  21%|       | 2306/10788 [00:52<03:47, 37.31it/s]Training CobwebTree:  21%|       | 2310/10788 [00:52<03:47, 37.25it/s]Training CobwebTree:  21%|       | 2314/10788 [00:52<03:51, 36.61it/s]Training CobwebTree:  21%|       | 2319/10788 [00:52<03:45, 37.58it/s]Training CobwebTree:  22%|       | 2323/10788 [00:52<03:51, 36.62it/s]Training CobwebTree:  22%|       | 2328/10788 [00:52<03:32, 39.73it/s]Training CobwebTree:  22%|       | 2333/10788 [00:52<03:33, 39.55it/s]Training CobwebTree:  22%|       | 2338/10788 [00:53<03:25, 41.09it/s]Training CobwebTree:  22%|       | 2343/10788 [00:53<03:29, 40.25it/s]Training CobwebTree:  22%|       | 2348/10788 [00:53<03:46, 37.27it/s]Training CobwebTree:  22%|       | 2352/10788 [00:53<03:51, 36.41it/s]Training CobwebTree:  22%|       | 2356/10788 [00:53<03:58, 35.40it/s]Training CobwebTree:  22%|       | 2360/10788 [00:53<03:55, 35.73it/s]Training CobwebTree:  22%|       | 2364/10788 [00:53<03:53, 36.05it/s]Training CobwebTree:  22%|       | 2368/10788 [00:53<04:01, 34.88it/s]Training CobwebTree:  22%|       | 2372/10788 [00:54<03:59, 35.10it/s]Training CobwebTree:  22%|       | 2376/10788 [00:54<03:51, 36.40it/s]Training CobwebTree:  22%|       | 2380/10788 [00:54<03:51, 36.38it/s]Training CobwebTree:  22%|       | 2384/10788 [00:54<03:55, 35.66it/s]Training CobwebTree:  22%|       | 2388/10788 [00:54<03:50, 36.51it/s]Training CobwebTree:  22%|       | 2392/10788 [00:54<03:47, 36.90it/s]Training CobwebTree:  22%|       | 2396/10788 [00:54<03:43, 37.48it/s]Training CobwebTree:  22%|       | 2400/10788 [00:54<03:40, 37.98it/s]Training CobwebTree:  22%|       | 2404/10788 [00:54<03:57, 35.36it/s]Training CobwebTree:  22%|       | 2408/10788 [00:55<03:51, 36.13it/s]Training CobwebTree:  22%|       | 2412/10788 [00:55<04:03, 34.40it/s]Training CobwebTree:  22%|       | 2416/10788 [00:55<04:07, 33.85it/s]Training CobwebTree:  22%|       | 2420/10788 [00:55<04:06, 33.95it/s]Training CobwebTree:  22%|       | 2424/10788 [00:55<04:04, 34.17it/s]Training CobwebTree:  23%|       | 2428/10788 [00:55<04:12, 33.12it/s]Training CobwebTree:  23%|       | 2432/10788 [00:55<04:02, 34.39it/s]Training CobwebTree:  23%|       | 2436/10788 [00:55<03:59, 34.81it/s]Training CobwebTree:  23%|       | 2441/10788 [00:55<03:47, 36.64it/s]Training CobwebTree:  23%|       | 2446/10788 [00:56<03:37, 38.39it/s]Training CobwebTree:  23%|       | 2450/10788 [00:56<03:43, 37.30it/s]Training CobwebTree:  23%|       | 2454/10788 [00:56<03:45, 36.96it/s]Training CobwebTree:  23%|       | 2459/10788 [00:56<03:30, 39.58it/s]Training CobwebTree:  23%|       | 2463/10788 [00:56<03:53, 35.69it/s]Training CobwebTree:  23%|       | 2467/10788 [00:56<03:46, 36.79it/s]Training CobwebTree:  23%|       | 2471/10788 [00:56<03:45, 36.93it/s]Training CobwebTree:  23%|       | 2475/10788 [00:56<03:54, 35.49it/s]Training CobwebTree:  23%|       | 2479/10788 [00:57<04:02, 34.27it/s]Training CobwebTree:  23%|       | 2483/10788 [00:57<04:00, 34.55it/s]Training CobwebTree:  23%|       | 2487/10788 [00:57<03:51, 35.81it/s]Training CobwebTree:  23%|       | 2491/10788 [00:57<03:54, 35.34it/s]Training CobwebTree:  23%|       | 2495/10788 [00:57<03:58, 34.71it/s]Training CobwebTree:  23%|       | 2499/10788 [00:57<03:54, 35.37it/s]Training CobwebTree:  23%|       | 2503/10788 [00:57<03:50, 35.89it/s]Training CobwebTree:  23%|       | 2507/10788 [00:57<03:46, 36.53it/s]Training CobwebTree:  23%|       | 2512/10788 [00:57<03:36, 38.15it/s]Training CobwebTree:  23%|       | 2516/10788 [00:58<03:43, 36.98it/s]Training CobwebTree:  23%|       | 2520/10788 [00:58<03:42, 37.22it/s]Training CobwebTree:  23%|       | 2524/10788 [00:58<03:39, 37.70it/s]Training CobwebTree:  23%|       | 2528/10788 [00:58<03:39, 37.65it/s]Training CobwebTree:  23%|       | 2532/10788 [00:58<03:39, 37.65it/s]Training CobwebTree:  24%|       | 2536/10788 [00:58<03:56, 34.93it/s]Training CobwebTree:  24%|       | 2540/10788 [00:58<03:52, 35.54it/s]Training CobwebTree:  24%|       | 2544/10788 [00:58<03:55, 34.95it/s]Training CobwebTree:  24%|       | 2548/10788 [00:58<03:51, 35.63it/s]Training CobwebTree:  24%|       | 2552/10788 [00:59<03:50, 35.72it/s]Training CobwebTree:  24%|       | 2556/10788 [00:59<03:47, 36.13it/s]Training CobwebTree:  24%|       | 2560/10788 [00:59<03:45, 36.51it/s]Training CobwebTree:  24%|       | 2564/10788 [00:59<03:48, 36.05it/s]Training CobwebTree:  24%|       | 2568/10788 [00:59<03:50, 35.71it/s]Training CobwebTree:  24%|       | 2572/10788 [00:59<03:49, 35.81it/s]Training CobwebTree:  24%|       | 2576/10788 [00:59<03:49, 35.71it/s]Training CobwebTree:  24%|       | 2580/10788 [00:59<03:56, 34.66it/s]Training CobwebTree:  24%|       | 2585/10788 [00:59<03:46, 36.21it/s]Training CobwebTree:  24%|       | 2589/10788 [01:00<03:41, 36.94it/s]Training CobwebTree:  24%|       | 2593/10788 [01:00<03:39, 37.34it/s]Training CobwebTree:  24%|       | 2597/10788 [01:00<03:51, 35.38it/s]Training CobwebTree:  24%|       | 2601/10788 [01:00<03:49, 35.63it/s]Training CobwebTree:  24%|       | 2605/10788 [01:00<03:46, 36.10it/s]Training CobwebTree:  24%|       | 2610/10788 [01:00<03:36, 37.78it/s]Training CobwebTree:  24%|       | 2615/10788 [01:00<03:26, 39.58it/s]Training CobwebTree:  24%|       | 2620/10788 [01:00<03:24, 39.99it/s]Training CobwebTree:  24%|       | 2624/10788 [01:00<03:31, 38.53it/s]Training CobwebTree:  24%|       | 2628/10788 [01:01<03:45, 36.26it/s]Training CobwebTree:  24%|       | 2633/10788 [01:01<03:39, 37.14it/s]Training CobwebTree:  24%|       | 2638/10788 [01:01<03:40, 36.90it/s]Training CobwebTree:  24%|       | 2642/10788 [01:01<03:40, 37.01it/s]Training CobwebTree:  25%|       | 2647/10788 [01:01<03:39, 37.01it/s]Training CobwebTree:  25%|       | 2651/10788 [01:01<03:44, 36.30it/s]Training CobwebTree:  25%|       | 2655/10788 [01:01<03:49, 35.40it/s]Training CobwebTree:  25%|       | 2659/10788 [01:01<03:54, 34.68it/s]Training CobwebTree:  25%|       | 2663/10788 [01:02<03:49, 35.46it/s]Training CobwebTree:  25%|       | 2667/10788 [01:02<03:49, 35.32it/s]Training CobwebTree:  25%|       | 2671/10788 [01:02<03:52, 34.90it/s]Training CobwebTree:  25%|       | 2676/10788 [01:02<03:36, 37.50it/s]Training CobwebTree:  25%|       | 2680/10788 [01:02<03:36, 37.45it/s]Training CobwebTree:  25%|       | 2684/10788 [01:02<03:41, 36.59it/s]Training CobwebTree:  25%|       | 2688/10788 [01:02<03:37, 37.24it/s]Training CobwebTree:  25%|       | 2693/10788 [01:02<03:23, 39.88it/s]Training CobwebTree:  25%|       | 2698/10788 [01:02<03:21, 40.11it/s]Training CobwebTree:  25%|       | 2703/10788 [01:03<03:27, 38.90it/s]Training CobwebTree:  25%|       | 2707/10788 [01:03<03:33, 37.77it/s]Training CobwebTree:  25%|       | 2711/10788 [01:03<03:40, 36.66it/s]Training CobwebTree:  25%|       | 2715/10788 [01:03<03:49, 35.22it/s]Training CobwebTree:  25%|       | 2719/10788 [01:03<03:45, 35.83it/s]Training CobwebTree:  25%|       | 2723/10788 [01:03<03:39, 36.73it/s]Training CobwebTree:  25%|       | 2727/10788 [01:03<03:44, 35.92it/s]Training CobwebTree:  25%|       | 2732/10788 [01:03<03:33, 37.79it/s]Training CobwebTree:  25%|       | 2736/10788 [01:04<03:36, 37.24it/s]Training CobwebTree:  25%|       | 2740/10788 [01:04<03:32, 37.83it/s]Training CobwebTree:  25%|       | 2744/10788 [01:04<03:30, 38.21it/s]Training CobwebTree:  25%|       | 2748/10788 [01:04<03:29, 38.40it/s]Training CobwebTree:  26%|       | 2752/10788 [01:04<03:37, 36.99it/s]Training CobwebTree:  26%|       | 2756/10788 [01:04<03:43, 35.88it/s]Training CobwebTree:  26%|       | 2760/10788 [01:04<03:40, 36.48it/s]Training CobwebTree:  26%|       | 2764/10788 [01:04<03:35, 37.19it/s]Training CobwebTree:  26%|       | 2768/10788 [01:04<03:42, 36.07it/s]Training CobwebTree:  26%|       | 2772/10788 [01:05<03:40, 36.28it/s]Training CobwebTree:  26%|       | 2777/10788 [01:05<03:30, 38.07it/s]Training CobwebTree:  26%|       | 2781/10788 [01:05<03:38, 36.63it/s]Training CobwebTree:  26%|       | 2785/10788 [01:05<03:37, 36.78it/s]Training CobwebTree:  26%|       | 2789/10788 [01:05<03:42, 35.97it/s]Training CobwebTree:  26%|       | 2793/10788 [01:05<03:48, 35.06it/s]Training CobwebTree:  26%|       | 2798/10788 [01:05<03:43, 35.71it/s]Training CobwebTree:  26%|       | 2802/10788 [01:05<03:40, 36.26it/s]Training CobwebTree:  26%|       | 2806/10788 [01:05<03:41, 35.96it/s]Training CobwebTree:  26%|       | 2810/10788 [01:06<03:38, 36.55it/s]Training CobwebTree:  26%|       | 2815/10788 [01:06<03:33, 37.38it/s]Training CobwebTree:  26%|       | 2819/10788 [01:06<03:46, 35.14it/s]Training CobwebTree:  26%|       | 2823/10788 [01:06<03:55, 33.89it/s]Training CobwebTree:  26%|       | 2827/10788 [01:06<03:45, 35.27it/s]Training CobwebTree:  26%|       | 2831/10788 [01:06<03:49, 34.71it/s]Training CobwebTree:  26%|       | 2835/10788 [01:06<03:53, 34.10it/s]Training CobwebTree:  26%|       | 2839/10788 [01:06<03:48, 34.79it/s]Training CobwebTree:  26%|       | 2843/10788 [01:07<03:42, 35.65it/s]Training CobwebTree:  26%|       | 2847/10788 [01:07<03:53, 33.97it/s]Training CobwebTree:  26%|       | 2851/10788 [01:07<03:59, 33.10it/s]Training CobwebTree:  26%|       | 2855/10788 [01:07<03:53, 33.98it/s]Training CobwebTree:  27%|       | 2859/10788 [01:07<03:47, 34.81it/s]Training CobwebTree:  27%|       | 2864/10788 [01:07<03:34, 37.03it/s]Training CobwebTree:  27%|       | 2868/10788 [01:07<03:30, 37.71it/s]Training CobwebTree:  27%|       | 2872/10788 [01:07<03:29, 37.76it/s]Training CobwebTree:  27%|       | 2876/10788 [01:07<03:28, 37.92it/s]Training CobwebTree:  27%|       | 2880/10788 [01:08<03:30, 37.57it/s]Training CobwebTree:  27%|       | 2884/10788 [01:08<07:02, 18.71it/s]Training CobwebTree:  27%|       | 2888/10788 [01:08<06:00, 21.90it/s]Training CobwebTree:  27%|       | 2892/10788 [01:08<05:25, 24.28it/s]Training CobwebTree:  27%|       | 2896/10788 [01:08<04:52, 26.96it/s]Training CobwebTree:  27%|       | 2900/10788 [01:08<04:28, 29.33it/s]Training CobwebTree:  27%|       | 2905/10788 [01:09<03:59, 32.91it/s]Training CobwebTree:  27%|       | 2910/10788 [01:09<03:38, 35.99it/s]Training CobwebTree:  27%|       | 2914/10788 [01:09<03:40, 35.76it/s]Training CobwebTree:  27%|       | 2918/10788 [01:09<03:36, 36.40it/s]Training CobwebTree:  27%|       | 2922/10788 [01:09<03:32, 37.00it/s]Training CobwebTree:  27%|       | 2926/10788 [01:09<03:28, 37.76it/s]Training CobwebTree:  27%|       | 2930/10788 [01:09<03:27, 37.95it/s]Training CobwebTree:  27%|       | 2934/10788 [01:09<03:41, 35.52it/s]Training CobwebTree:  27%|       | 2938/10788 [01:09<03:33, 36.69it/s]Training CobwebTree:  27%|       | 2942/10788 [01:10<03:44, 34.90it/s]Training CobwebTree:  27%|       | 2946/10788 [01:10<03:57, 33.06it/s]Training CobwebTree:  27%|       | 2950/10788 [01:10<03:48, 34.32it/s]Training CobwebTree:  27%|       | 2954/10788 [01:10<03:57, 33.02it/s]Training CobwebTree:  27%|       | 2958/10788 [01:10<03:57, 32.99it/s]Training CobwebTree:  27%|       | 2963/10788 [01:10<03:39, 35.61it/s]Training CobwebTree:  28%|       | 2967/10788 [01:10<03:34, 36.52it/s]Training CobwebTree:  28%|       | 2971/10788 [01:10<03:33, 36.66it/s]Training CobwebTree:  28%|       | 2975/10788 [01:10<03:33, 36.53it/s]Training CobwebTree:  28%|       | 2979/10788 [01:11<03:37, 35.84it/s]Training CobwebTree:  28%|       | 2984/10788 [01:11<03:29, 37.31it/s]Training CobwebTree:  28%|       | 2989/10788 [01:11<03:23, 38.36it/s]Training CobwebTree:  28%|       | 2993/10788 [01:11<03:27, 37.55it/s]Training CobwebTree:  28%|       | 2997/10788 [01:11<03:25, 37.92it/s]Training CobwebTree:  28%|       | 3001/10788 [01:11<03:25, 37.92it/s]Training CobwebTree:  28%|       | 3006/10788 [01:11<03:14, 39.99it/s]Training CobwebTree:  28%|       | 3010/10788 [01:11<03:14, 39.89it/s]Training CobwebTree:  28%|       | 3014/10788 [01:12<03:26, 37.60it/s]Training CobwebTree:  28%|       | 3018/10788 [01:12<03:26, 37.57it/s]Training CobwebTree:  28%|       | 3023/10788 [01:12<03:22, 38.29it/s]Training CobwebTree:  28%|       | 3027/10788 [01:12<03:33, 36.43it/s]Training CobwebTree:  28%|       | 3031/10788 [01:12<03:34, 36.14it/s]Training CobwebTree:  28%|       | 3035/10788 [01:12<03:48, 33.87it/s]Training CobwebTree:  28%|       | 3040/10788 [01:12<03:33, 36.25it/s]Training CobwebTree:  28%|       | 3044/10788 [01:12<03:33, 36.32it/s]Training CobwebTree:  28%|       | 3049/10788 [01:12<03:19, 38.87it/s]Training CobwebTree:  28%|       | 3054/10788 [01:13<03:16, 39.34it/s]Training CobwebTree:  28%|       | 3059/10788 [01:13<03:08, 40.96it/s]Training CobwebTree:  28%|       | 3064/10788 [01:13<03:06, 41.33it/s]Training CobwebTree:  28%|       | 3069/10788 [01:13<03:04, 41.85it/s]Training CobwebTree:  28%|       | 3074/10788 [01:13<03:10, 40.43it/s]Training CobwebTree:  29%|       | 3079/10788 [01:13<03:14, 39.61it/s]Training CobwebTree:  29%|       | 3083/10788 [01:13<03:28, 36.87it/s]Training CobwebTree:  29%|       | 3088/10788 [01:13<03:22, 37.95it/s]Training CobwebTree:  29%|       | 3093/10788 [01:14<03:19, 38.55it/s]Training CobwebTree:  29%|       | 3097/10788 [01:14<03:19, 38.49it/s]Training CobwebTree:  29%|       | 3101/10788 [01:14<03:27, 36.97it/s]Training CobwebTree:  29%|       | 3105/10788 [01:14<03:29, 36.73it/s]Training CobwebTree:  29%|       | 3109/10788 [01:14<03:25, 37.37it/s]Training CobwebTree:  29%|       | 3113/10788 [01:14<03:23, 37.67it/s]Training CobwebTree:  29%|       | 3118/10788 [01:14<03:10, 40.20it/s]Training CobwebTree:  29%|       | 3123/10788 [01:14<03:10, 40.34it/s]Training CobwebTree:  29%|       | 3128/10788 [01:14<03:22, 37.86it/s]Training CobwebTree:  29%|       | 3132/10788 [01:15<03:22, 37.85it/s]Training CobwebTree:  29%|       | 3137/10788 [01:15<03:20, 38.14it/s]Training CobwebTree:  29%|       | 3142/10788 [01:15<03:17, 38.81it/s]Training CobwebTree:  29%|       | 3146/10788 [01:15<03:20, 38.10it/s]Training CobwebTree:  29%|       | 3151/10788 [01:15<03:12, 39.75it/s]Training CobwebTree:  29%|       | 3156/10788 [01:15<03:12, 39.63it/s]Training CobwebTree:  29%|       | 3161/10788 [01:15<03:12, 39.71it/s]Training CobwebTree:  29%|       | 3165/10788 [01:15<03:12, 39.67it/s]Training CobwebTree:  29%|       | 3169/10788 [01:16<03:11, 39.70it/s]Training CobwebTree:  29%|       | 3174/10788 [01:16<03:06, 40.77it/s]Training CobwebTree:  29%|       | 3179/10788 [01:16<03:08, 40.43it/s]Training CobwebTree:  30%|       | 3184/10788 [01:16<03:11, 39.62it/s]Training CobwebTree:  30%|       | 3188/10788 [01:16<03:15, 38.86it/s]Training CobwebTree:  30%|       | 3193/10788 [01:16<03:05, 40.87it/s]Training CobwebTree:  30%|       | 3198/10788 [01:16<03:10, 39.81it/s]Training CobwebTree:  30%|       | 3203/10788 [01:16<03:10, 39.73it/s]Training CobwebTree:  30%|       | 3208/10788 [01:17<03:12, 39.39it/s]Training CobwebTree:  30%|       | 3212/10788 [01:17<03:19, 38.05it/s]Training CobwebTree:  30%|       | 3217/10788 [01:17<03:10, 39.75it/s]Training CobwebTree:  30%|       | 3221/10788 [01:17<03:17, 38.34it/s]Training CobwebTree:  30%|       | 3225/10788 [01:17<03:18, 38.14it/s]Training CobwebTree:  30%|       | 3229/10788 [01:17<03:23, 37.09it/s]Training CobwebTree:  30%|       | 3233/10788 [01:17<03:24, 36.87it/s]Training CobwebTree:  30%|       | 3237/10788 [01:17<03:25, 36.79it/s]Training CobwebTree:  30%|       | 3242/10788 [01:17<03:20, 37.73it/s]Training CobwebTree:  30%|       | 3246/10788 [01:18<03:17, 38.28it/s]Training CobwebTree:  30%|       | 3251/10788 [01:18<03:08, 40.03it/s]Training CobwebTree:  30%|       | 3256/10788 [01:18<03:16, 38.34it/s]Training CobwebTree:  30%|       | 3261/10788 [01:18<03:11, 39.34it/s]Training CobwebTree:  30%|       | 3265/10788 [01:18<03:12, 39.05it/s]Training CobwebTree:  30%|       | 3270/10788 [01:18<03:03, 40.89it/s]Training CobwebTree:  30%|       | 3275/10788 [01:18<03:09, 39.67it/s]Training CobwebTree:  30%|       | 3280/10788 [01:18<03:00, 41.55it/s]Training CobwebTree:  30%|       | 3285/10788 [01:18<03:07, 39.98it/s]Training CobwebTree:  30%|       | 3290/10788 [01:19<03:06, 40.17it/s]Training CobwebTree:  31%|       | 3295/10788 [01:19<03:22, 37.07it/s]Training CobwebTree:  31%|       | 3299/10788 [01:19<03:22, 37.05it/s]Training CobwebTree:  31%|       | 3303/10788 [01:19<03:35, 34.73it/s]Training CobwebTree:  31%|       | 3307/10788 [01:19<03:30, 35.56it/s]Training CobwebTree:  31%|       | 3312/10788 [01:19<03:21, 37.03it/s]Training CobwebTree:  31%|       | 3316/10788 [01:19<03:18, 37.70it/s]Training CobwebTree:  31%|       | 3320/10788 [01:19<03:15, 38.16it/s]Training CobwebTree:  31%|       | 3324/10788 [01:20<03:19, 37.48it/s]Training CobwebTree:  31%|       | 3328/10788 [01:20<03:17, 37.85it/s]Training CobwebTree:  31%|       | 3332/10788 [01:20<03:15, 38.17it/s]Training CobwebTree:  31%|       | 3336/10788 [01:20<03:16, 37.89it/s]Training CobwebTree:  31%|       | 3341/10788 [01:20<03:06, 39.88it/s]Training CobwebTree:  31%|       | 3345/10788 [01:20<03:13, 38.39it/s]Training CobwebTree:  31%|       | 3349/10788 [01:20<03:17, 37.62it/s]Training CobwebTree:  31%|       | 3354/10788 [01:20<03:11, 38.81it/s]Training CobwebTree:  31%|       | 3359/10788 [01:20<03:09, 39.17it/s]Training CobwebTree:  31%|       | 3364/10788 [01:21<03:02, 40.73it/s]Training CobwebTree:  31%|       | 3369/10788 [01:21<03:06, 39.83it/s]Training CobwebTree:  31%|      | 3373/10788 [01:21<03:08, 39.42it/s]Training CobwebTree:  31%|      | 3378/10788 [01:21<03:07, 39.58it/s]Training CobwebTree:  31%|      | 3382/10788 [01:21<03:08, 39.20it/s]Training CobwebTree:  31%|      | 3386/10788 [01:21<03:11, 38.73it/s]Training CobwebTree:  31%|      | 3390/10788 [01:21<03:17, 37.52it/s]Training CobwebTree:  31%|      | 3395/10788 [01:21<03:13, 38.17it/s]Training CobwebTree:  32%|      | 3399/10788 [01:22<03:26, 35.84it/s]Training CobwebTree:  32%|      | 3403/10788 [01:22<03:32, 34.73it/s]Training CobwebTree:  32%|      | 3407/10788 [01:22<03:40, 33.54it/s]Training CobwebTree:  32%|      | 3411/10788 [01:22<03:42, 33.14it/s]Training CobwebTree:  32%|      | 3416/10788 [01:22<03:32, 34.77it/s]Training CobwebTree:  32%|      | 3420/10788 [01:22<03:29, 35.12it/s]Training CobwebTree:  32%|      | 3425/10788 [01:22<03:16, 37.40it/s]Training CobwebTree:  32%|      | 3429/10788 [01:22<03:23, 36.17it/s]Training CobwebTree:  32%|      | 3434/10788 [01:23<03:23, 36.13it/s]Training CobwebTree:  32%|      | 3438/10788 [01:23<03:28, 35.28it/s]Training CobwebTree:  32%|      | 3442/10788 [01:23<03:28, 35.23it/s]Training CobwebTree:  32%|      | 3446/10788 [01:23<03:24, 35.93it/s]Training CobwebTree:  32%|      | 3450/10788 [01:23<03:21, 36.34it/s]Training CobwebTree:  32%|      | 3454/10788 [01:23<03:26, 35.59it/s]Training CobwebTree:  32%|      | 3458/10788 [01:23<03:24, 35.90it/s]Training CobwebTree:  32%|      | 3462/10788 [01:23<03:25, 35.69it/s]Training CobwebTree:  32%|      | 3466/10788 [01:23<03:19, 36.77it/s]Training CobwebTree:  32%|      | 3470/10788 [01:24<03:17, 37.01it/s]Training CobwebTree:  32%|      | 3474/10788 [01:24<03:14, 37.56it/s]Training CobwebTree:  32%|      | 3479/10788 [01:24<03:05, 39.33it/s]Training CobwebTree:  32%|      | 3483/10788 [01:24<03:10, 38.41it/s]Training CobwebTree:  32%|      | 3487/10788 [01:24<03:09, 38.58it/s]Training CobwebTree:  32%|      | 3492/10788 [01:24<03:01, 40.12it/s]Training CobwebTree:  32%|      | 3497/10788 [01:24<03:02, 40.00it/s]Training CobwebTree:  32%|      | 3502/10788 [01:24<03:02, 39.98it/s]Training CobwebTree:  33%|      | 3507/10788 [01:24<02:58, 40.70it/s]Training CobwebTree:  33%|      | 3512/10788 [01:25<03:01, 40.00it/s]Training CobwebTree:  33%|      | 3517/10788 [01:25<03:04, 39.36it/s]Training CobwebTree:  33%|      | 3521/10788 [01:25<03:06, 39.00it/s]Training CobwebTree:  33%|      | 3525/10788 [01:25<03:08, 38.49it/s]Training CobwebTree:  33%|      | 3530/10788 [01:25<03:04, 39.24it/s]Training CobwebTree:  33%|      | 3535/10788 [01:25<03:05, 39.19it/s]Training CobwebTree:  33%|      | 3540/10788 [01:25<03:00, 40.06it/s]Training CobwebTree:  33%|      | 3545/10788 [01:25<02:56, 41.09it/s]Training CobwebTree:  33%|      | 3550/10788 [01:25<02:52, 42.05it/s]Training CobwebTree:  33%|      | 3555/10788 [01:26<03:00, 40.09it/s]Training CobwebTree:  33%|      | 3560/10788 [01:26<03:21, 35.92it/s]Training CobwebTree:  33%|      | 3565/10788 [01:26<03:14, 37.14it/s]Training CobwebTree:  33%|      | 3569/10788 [01:26<03:12, 37.57it/s]Training CobwebTree:  33%|      | 3573/10788 [01:26<03:11, 37.70it/s]Training CobwebTree:  33%|      | 3577/10788 [01:26<03:12, 37.55it/s]Training CobwebTree:  33%|      | 3581/10788 [01:26<03:23, 35.40it/s]Training CobwebTree:  33%|      | 3585/10788 [01:26<03:21, 35.72it/s]Training CobwebTree:  33%|      | 3590/10788 [01:27<03:13, 37.18it/s]Training CobwebTree:  33%|      | 3594/10788 [01:27<03:12, 37.36it/s]Training CobwebTree:  33%|      | 3598/10788 [01:27<03:14, 36.95it/s]Training CobwebTree:  33%|      | 3602/10788 [01:27<03:11, 37.56it/s]Training CobwebTree:  33%|      | 3606/10788 [01:27<03:24, 35.14it/s]Training CobwebTree:  33%|      | 3610/10788 [01:27<03:23, 35.34it/s]Training CobwebTree:  34%|      | 3614/10788 [01:27<03:21, 35.65it/s]Training CobwebTree:  34%|      | 3618/10788 [01:27<03:25, 34.92it/s]Training CobwebTree:  34%|      | 3622/10788 [01:28<03:29, 34.23it/s]Training CobwebTree:  34%|      | 3626/10788 [01:28<03:28, 34.38it/s]Training CobwebTree:  34%|      | 3630/10788 [01:28<03:28, 34.38it/s]Training CobwebTree:  34%|      | 3634/10788 [01:28<03:36, 32.99it/s]Training CobwebTree:  34%|      | 3638/10788 [01:28<03:34, 33.34it/s]Training CobwebTree:  34%|      | 3642/10788 [01:28<03:31, 33.73it/s]Training CobwebTree:  34%|      | 3646/10788 [01:28<03:24, 34.98it/s]Training CobwebTree:  34%|      | 3650/10788 [01:28<03:21, 35.49it/s]Training CobwebTree:  34%|      | 3654/10788 [01:28<03:26, 34.57it/s]Training CobwebTree:  34%|      | 3658/10788 [01:29<03:25, 34.62it/s]Training CobwebTree:  34%|      | 3662/10788 [01:29<03:21, 35.40it/s]Training CobwebTree:  34%|      | 3666/10788 [01:29<03:20, 35.46it/s]Training CobwebTree:  34%|      | 3670/10788 [01:29<03:22, 35.21it/s]Training CobwebTree:  34%|      | 3674/10788 [01:29<03:22, 35.21it/s]Training CobwebTree:  34%|      | 3678/10788 [01:29<03:23, 34.96it/s]Training CobwebTree:  34%|      | 3682/10788 [01:29<03:30, 33.80it/s]Training CobwebTree:  34%|      | 3686/10788 [01:29<03:39, 32.30it/s]Training CobwebTree:  34%|      | 3690/10788 [01:29<03:28, 34.08it/s]Training CobwebTree:  34%|      | 3694/10788 [01:30<03:28, 33.96it/s]Training CobwebTree:  34%|      | 3698/10788 [01:30<03:27, 34.12it/s]Training CobwebTree:  34%|      | 3702/10788 [01:30<03:19, 35.55it/s]Training CobwebTree:  34%|      | 3706/10788 [01:30<03:16, 36.01it/s]Training CobwebTree:  34%|      | 3711/10788 [01:30<03:03, 38.54it/s]Training CobwebTree:  34%|      | 3715/10788 [01:30<03:05, 38.14it/s]Training CobwebTree:  34%|      | 3719/10788 [01:30<03:10, 37.06it/s]Training CobwebTree:  35%|      | 3723/10788 [01:30<03:11, 36.86it/s]Training CobwebTree:  35%|      | 3727/10788 [01:31<03:23, 34.72it/s]Training CobwebTree:  35%|      | 3731/10788 [01:31<03:22, 34.81it/s]Training CobwebTree:  35%|      | 3735/10788 [01:31<03:15, 36.15it/s]Training CobwebTree:  35%|      | 3739/10788 [01:31<03:11, 36.77it/s]Training CobwebTree:  35%|      | 3743/10788 [01:31<03:11, 36.75it/s]Training CobwebTree:  35%|      | 3747/10788 [01:31<03:15, 36.04it/s]Training CobwebTree:  35%|      | 3752/10788 [01:31<03:07, 37.52it/s]Training CobwebTree:  35%|      | 3756/10788 [01:31<03:08, 37.38it/s]Training CobwebTree:  35%|      | 3760/10788 [01:31<03:10, 36.99it/s]Training CobwebTree:  35%|      | 3764/10788 [01:32<03:25, 34.22it/s]Training CobwebTree:  35%|      | 3768/10788 [01:32<03:21, 34.77it/s]Training CobwebTree:  35%|      | 3772/10788 [01:32<03:15, 35.86it/s]Training CobwebTree:  35%|      | 3777/10788 [01:32<03:05, 37.81it/s]Training CobwebTree:  35%|      | 3782/10788 [01:32<02:58, 39.23it/s]Training CobwebTree:  35%|      | 3787/10788 [01:32<02:51, 40.81it/s]Training CobwebTree:  35%|      | 3792/10788 [01:32<02:57, 39.44it/s]Training CobwebTree:  35%|      | 3797/10788 [01:32<03:00, 38.81it/s]Training CobwebTree:  35%|      | 3801/10788 [01:32<03:04, 37.88it/s]Training CobwebTree:  35%|      | 3805/10788 [01:33<03:04, 37.83it/s]Training CobwebTree:  35%|      | 3809/10788 [01:33<03:06, 37.48it/s]Training CobwebTree:  35%|      | 3813/10788 [01:33<03:10, 36.71it/s]Training CobwebTree:  35%|      | 3817/10788 [01:33<03:17, 35.35it/s]Training CobwebTree:  35%|      | 3822/10788 [01:33<03:04, 37.81it/s]Training CobwebTree:  35%|      | 3827/10788 [01:33<03:00, 38.56it/s]Training CobwebTree:  36%|      | 3831/10788 [01:33<03:09, 36.76it/s]Training CobwebTree:  36%|      | 3835/10788 [01:33<03:14, 35.82it/s]Training CobwebTree:  36%|      | 3839/10788 [01:34<03:12, 36.10it/s]Training CobwebTree:  36%|      | 3843/10788 [01:34<03:14, 35.66it/s]Training CobwebTree:  36%|      | 3847/10788 [01:34<03:15, 35.48it/s]Training CobwebTree:  36%|      | 3851/10788 [01:34<03:09, 36.65it/s]Training CobwebTree:  36%|      | 3856/10788 [01:34<03:03, 37.83it/s]Training CobwebTree:  36%|      | 3860/10788 [01:34<03:11, 36.11it/s]Training CobwebTree:  36%|      | 3864/10788 [01:34<03:26, 33.46it/s]Training CobwebTree:  36%|      | 3869/10788 [01:34<03:16, 35.12it/s]Training CobwebTree:  36%|      | 3873/10788 [01:34<03:14, 35.60it/s]Training CobwebTree:  36%|      | 3877/10788 [01:35<03:22, 34.11it/s]Training CobwebTree:  36%|      | 3881/10788 [01:35<03:28, 33.18it/s]Training CobwebTree:  36%|      | 3885/10788 [01:35<03:24, 33.83it/s]Training CobwebTree:  36%|      | 3889/10788 [01:35<03:21, 34.23it/s]Training CobwebTree:  36%|      | 3894/10788 [01:35<03:14, 35.41it/s]Training CobwebTree:  36%|      | 3898/10788 [01:35<03:08, 36.47it/s]Training CobwebTree:  36%|      | 3903/10788 [01:35<03:03, 37.52it/s]Training CobwebTree:  36%|      | 3907/10788 [01:35<03:00, 38.12it/s]Training CobwebTree:  36%|      | 3912/10788 [01:36<02:57, 38.82it/s]Training CobwebTree:  36%|      | 3917/10788 [01:36<02:55, 39.06it/s]Training CobwebTree:  36%|      | 3921/10788 [01:36<02:56, 39.01it/s]Training CobwebTree:  36%|      | 3926/10788 [01:36<02:54, 39.30it/s]Training CobwebTree:  36%|      | 3930/10788 [01:36<03:08, 36.37it/s]Training CobwebTree:  36%|      | 3934/10788 [01:36<03:14, 35.25it/s]Training CobwebTree:  37%|      | 3939/10788 [01:36<03:01, 37.69it/s]Training CobwebTree:  37%|      | 3944/10788 [01:36<02:53, 39.34it/s]Training CobwebTree:  37%|      | 3948/10788 [01:37<03:08, 36.27it/s]Training CobwebTree:  37%|      | 3953/10788 [01:37<03:04, 36.99it/s]Training CobwebTree:  37%|      | 3957/10788 [01:37<03:06, 36.68it/s]Training CobwebTree:  37%|      | 3962/10788 [01:37<03:01, 37.67it/s]Training CobwebTree:  37%|      | 3966/10788 [01:37<03:11, 35.55it/s]Training CobwebTree:  37%|      | 3970/10788 [01:37<03:10, 35.71it/s]Training CobwebTree:  37%|      | 3974/10788 [01:37<03:23, 33.45it/s]Training CobwebTree:  37%|      | 3978/10788 [01:37<03:14, 35.04it/s]Training CobwebTree:  37%|      | 3982/10788 [01:37<03:11, 35.54it/s]Training CobwebTree:  37%|      | 3986/10788 [01:38<03:07, 36.32it/s]Training CobwebTree:  37%|      | 3990/10788 [01:38<03:05, 36.65it/s]Training CobwebTree:  37%|      | 3994/10788 [01:38<03:07, 36.31it/s]Training CobwebTree:  37%|      | 3998/10788 [01:38<03:09, 35.78it/s]Training CobwebTree:  37%|      | 4002/10788 [01:38<03:05, 36.53it/s]Training CobwebTree:  37%|      | 4006/10788 [01:38<03:18, 34.09it/s]Training CobwebTree:  37%|      | 4010/10788 [01:38<03:11, 35.33it/s]Training CobwebTree:  37%|      | 4014/10788 [01:38<03:14, 34.84it/s]Training CobwebTree:  37%|      | 4018/10788 [01:39<03:17, 34.20it/s]Training CobwebTree:  37%|      | 4022/10788 [01:39<03:18, 34.08it/s]Training CobwebTree:  37%|      | 4026/10788 [01:39<03:14, 34.83it/s]Training CobwebTree:  37%|      | 4030/10788 [01:39<03:20, 33.78it/s]Training CobwebTree:  37%|      | 4034/10788 [01:39<03:26, 32.69it/s]Training CobwebTree:  37%|      | 4038/10788 [01:39<03:21, 33.46it/s]Training CobwebTree:  37%|      | 4042/10788 [01:39<03:12, 34.95it/s]Training CobwebTree:  38%|      | 4046/10788 [01:39<03:10, 35.42it/s]Training CobwebTree:  38%|      | 4050/10788 [01:39<03:13, 34.84it/s]Training CobwebTree:  38%|      | 4054/10788 [01:40<03:23, 33.12it/s]Training CobwebTree:  38%|      | 4059/10788 [01:40<03:06, 36.02it/s]Training CobwebTree:  38%|      | 4063/10788 [01:40<03:08, 35.76it/s]Training CobwebTree:  38%|      | 4067/10788 [01:40<03:08, 35.65it/s]Training CobwebTree:  38%|      | 4071/10788 [01:40<03:08, 35.57it/s]Training CobwebTree:  38%|      | 4075/10788 [01:40<03:06, 35.94it/s]Training CobwebTree:  38%|      | 4079/10788 [01:40<03:05, 36.18it/s]Training CobwebTree:  38%|      | 4083/10788 [01:40<03:12, 34.75it/s]Training CobwebTree:  38%|      | 4087/10788 [01:40<03:18, 33.68it/s]Training CobwebTree:  38%|      | 4091/10788 [01:41<03:13, 34.69it/s]Training CobwebTree:  38%|      | 4095/10788 [01:41<03:11, 34.89it/s]Training CobwebTree:  38%|      | 4100/10788 [01:41<03:04, 36.23it/s]Training CobwebTree:  38%|      | 4104/10788 [01:41<03:14, 34.29it/s]Training CobwebTree:  38%|      | 4108/10788 [01:41<03:11, 34.83it/s]Training CobwebTree:  38%|      | 4113/10788 [01:41<02:59, 37.23it/s]Training CobwebTree:  38%|      | 4117/10788 [01:41<03:02, 36.55it/s]Training CobwebTree:  38%|      | 4121/10788 [01:41<03:10, 35.08it/s]Training CobwebTree:  38%|      | 4125/10788 [01:42<03:08, 35.33it/s]Training CobwebTree:  38%|      | 4129/10788 [01:42<03:07, 35.49it/s]Training CobwebTree:  38%|      | 4134/10788 [01:42<03:01, 36.66it/s]Training CobwebTree:  38%|      | 4139/10788 [01:42<02:57, 37.46it/s]Training CobwebTree:  38%|      | 4143/10788 [01:42<02:58, 37.24it/s]Training CobwebTree:  38%|      | 4148/10788 [01:42<02:57, 37.51it/s]Training CobwebTree:  38%|      | 4152/10788 [01:42<03:00, 36.66it/s]Training CobwebTree:  39%|      | 4156/10788 [01:42<03:07, 35.34it/s]Training CobwebTree:  39%|      | 4161/10788 [01:43<02:57, 37.41it/s]Training CobwebTree:  39%|      | 4165/10788 [01:43<03:00, 36.67it/s]Training CobwebTree:  39%|      | 4169/10788 [01:43<03:03, 36.02it/s]Training CobwebTree:  39%|      | 4173/10788 [01:43<03:11, 34.63it/s]Training CobwebTree:  39%|      | 4177/10788 [01:43<03:10, 34.72it/s]Training CobwebTree:  39%|      | 4182/10788 [01:43<02:58, 36.97it/s]Training CobwebTree:  39%|      | 4186/10788 [01:43<03:05, 35.54it/s]Training CobwebTree:  39%|      | 4190/10788 [01:43<03:04, 35.73it/s]Training CobwebTree:  39%|      | 4194/10788 [01:43<03:03, 35.86it/s]Training CobwebTree:  39%|      | 4198/10788 [01:44<03:04, 35.74it/s]Training CobwebTree:  39%|      | 4202/10788 [01:44<02:58, 36.81it/s]Training CobwebTree:  39%|      | 4206/10788 [01:44<02:58, 36.95it/s]Training CobwebTree:  39%|      | 4210/10788 [01:44<03:05, 35.51it/s]Training CobwebTree:  39%|      | 4214/10788 [01:44<03:01, 36.25it/s]Training CobwebTree:  39%|      | 4218/10788 [01:44<03:08, 34.87it/s]Training CobwebTree:  39%|      | 4222/10788 [01:44<03:11, 34.38it/s]Training CobwebTree:  39%|      | 4227/10788 [01:44<03:02, 35.98it/s]Training CobwebTree:  39%|      | 4231/10788 [01:44<02:58, 36.79it/s]Training CobwebTree:  39%|      | 4235/10788 [01:45<03:01, 36.12it/s]Training CobwebTree:  39%|      | 4239/10788 [01:45<03:00, 36.33it/s]Training CobwebTree:  39%|      | 4243/10788 [01:45<02:58, 36.74it/s]Training CobwebTree:  39%|      | 4247/10788 [01:45<02:56, 37.07it/s]Training CobwebTree:  39%|      | 4251/10788 [01:45<03:01, 35.93it/s]Training CobwebTree:  39%|      | 4255/10788 [01:45<03:01, 36.04it/s]Training CobwebTree:  39%|      | 4259/10788 [01:45<03:05, 35.16it/s]Training CobwebTree:  40%|      | 4264/10788 [01:45<02:56, 36.89it/s]Training CobwebTree:  40%|      | 4268/10788 [01:45<02:55, 37.15it/s]Training CobwebTree:  40%|      | 4272/10788 [01:46<03:12, 33.80it/s]Training CobwebTree:  40%|      | 4277/10788 [01:46<02:54, 37.27it/s]Training CobwebTree:  40%|      | 4281/10788 [01:46<02:55, 37.03it/s]Training CobwebTree:  40%|      | 4286/10788 [01:46<02:55, 37.05it/s]Training CobwebTree:  40%|      | 4290/10788 [01:46<02:52, 37.75it/s]Training CobwebTree:  40%|      | 4294/10788 [01:46<03:00, 35.97it/s]Training CobwebTree:  40%|      | 4298/10788 [01:46<03:07, 34.55it/s]Training CobwebTree:  40%|      | 4302/10788 [01:46<03:02, 35.50it/s]Training CobwebTree:  40%|      | 4306/10788 [01:47<03:04, 35.11it/s]Training CobwebTree:  40%|      | 4310/10788 [01:47<03:02, 35.50it/s]Training CobwebTree:  40%|      | 4314/10788 [01:47<03:03, 35.24it/s]Training CobwebTree:  40%|      | 4318/10788 [01:47<03:13, 33.42it/s]Training CobwebTree:  40%|      | 4322/10788 [01:47<03:07, 34.56it/s]Training CobwebTree:  40%|      | 4326/10788 [01:47<03:02, 35.34it/s]Training CobwebTree:  40%|      | 4330/10788 [01:47<03:01, 35.59it/s]Training CobwebTree:  40%|      | 4334/10788 [01:47<03:04, 34.99it/s]Training CobwebTree:  40%|      | 4339/10788 [01:47<02:58, 36.11it/s]Training CobwebTree:  40%|      | 4343/10788 [01:48<02:54, 36.93it/s]Training CobwebTree:  40%|      | 4347/10788 [01:48<02:53, 37.05it/s]Training CobwebTree:  40%|      | 4351/10788 [01:48<03:02, 35.22it/s]Training CobwebTree:  40%|      | 4355/10788 [01:48<02:59, 35.83it/s]Training CobwebTree:  40%|      | 4359/10788 [01:48<02:55, 36.60it/s]Training CobwebTree:  40%|      | 4363/10788 [01:48<02:53, 36.99it/s]Training CobwebTree:  40%|      | 4367/10788 [01:48<02:58, 35.94it/s]Training CobwebTree:  41%|      | 4372/10788 [01:48<02:50, 37.55it/s]Training CobwebTree:  41%|      | 4376/10788 [01:49<02:55, 36.46it/s]Training CobwebTree:  41%|      | 4380/10788 [01:49<02:59, 35.72it/s]Training CobwebTree:  41%|      | 4384/10788 [01:49<03:00, 35.51it/s]Training CobwebTree:  41%|      | 4388/10788 [01:49<03:07, 34.04it/s]Training CobwebTree:  41%|      | 4392/10788 [01:49<03:06, 34.31it/s]Training CobwebTree:  41%|      | 4397/10788 [01:49<02:56, 36.19it/s]Training CobwebTree:  41%|      | 4401/10788 [01:49<03:02, 34.95it/s]Training CobwebTree:  41%|      | 4405/10788 [01:49<03:14, 32.87it/s]Training CobwebTree:  41%|      | 4409/10788 [01:49<03:09, 33.58it/s]Training CobwebTree:  41%|      | 4413/10788 [01:50<03:05, 34.38it/s]Training CobwebTree:  41%|      | 4417/10788 [01:50<03:05, 34.35it/s]Training CobwebTree:  41%|      | 4421/10788 [01:50<03:13, 32.83it/s]Training CobwebTree:  41%|      | 4425/10788 [01:50<03:16, 32.34it/s]Training CobwebTree:  41%|      | 4429/10788 [01:50<03:09, 33.55it/s]Training CobwebTree:  41%|      | 4433/10788 [01:50<03:13, 32.89it/s]Training CobwebTree:  41%|      | 4437/10788 [01:50<03:03, 34.64it/s]Training CobwebTree:  41%|      | 4441/10788 [01:50<02:58, 35.53it/s]Training CobwebTree:  41%|      | 4445/10788 [01:51<02:57, 35.82it/s]Training CobwebTree:  41%|      | 4449/10788 [01:51<02:56, 35.99it/s]Training CobwebTree:  41%|     | 4453/10788 [01:51<02:59, 35.30it/s]Training CobwebTree:  41%|     | 4457/10788 [01:51<03:03, 34.48it/s]Training CobwebTree:  41%|     | 4461/10788 [01:51<03:02, 34.57it/s]Training CobwebTree:  41%|     | 4465/10788 [01:51<02:59, 35.23it/s]Training CobwebTree:  41%|     | 4469/10788 [01:51<03:01, 34.76it/s]Training CobwebTree:  41%|     | 4473/10788 [01:51<02:57, 35.48it/s]Training CobwebTree:  41%|     | 4477/10788 [01:51<03:00, 34.89it/s]Training CobwebTree:  42%|     | 4481/10788 [01:52<02:56, 35.67it/s]Training CobwebTree:  42%|     | 4485/10788 [01:52<02:59, 35.08it/s]Training CobwebTree:  42%|     | 4490/10788 [01:52<02:54, 36.05it/s]Training CobwebTree:  42%|     | 4494/10788 [01:52<03:03, 34.33it/s]Training CobwebTree:  42%|     | 4498/10788 [01:52<02:55, 35.79it/s]Training CobwebTree:  42%|     | 4502/10788 [01:52<02:55, 35.89it/s]Training CobwebTree:  42%|     | 4507/10788 [01:52<02:50, 36.78it/s]Training CobwebTree:  42%|     | 4511/10788 [01:52<02:52, 36.31it/s]Training CobwebTree:  42%|     | 4515/10788 [01:52<02:51, 36.52it/s]Training CobwebTree:  42%|     | 4519/10788 [01:53<02:59, 34.86it/s]Training CobwebTree:  42%|     | 4523/10788 [01:53<02:56, 35.51it/s]Training CobwebTree:  42%|     | 4527/10788 [01:53<02:59, 34.85it/s]Training CobwebTree:  42%|     | 4531/10788 [01:53<02:53, 36.07it/s]Training CobwebTree:  42%|     | 4535/10788 [01:53<02:59, 34.85it/s]Training CobwebTree:  42%|     | 4539/10788 [01:53<03:01, 34.34it/s]Training CobwebTree:  42%|     | 4543/10788 [01:53<02:57, 35.26it/s]Training CobwebTree:  42%|     | 4547/10788 [01:53<02:59, 34.78it/s]Training CobwebTree:  42%|     | 4551/10788 [01:54<03:04, 33.88it/s]Training CobwebTree:  42%|     | 4555/10788 [01:54<02:56, 35.25it/s]Training CobwebTree:  42%|     | 4559/10788 [01:54<02:59, 34.78it/s]Training CobwebTree:  42%|     | 4563/10788 [01:54<03:01, 34.23it/s]Training CobwebTree:  42%|     | 4567/10788 [01:54<03:02, 34.13it/s]Training CobwebTree:  42%|     | 4571/10788 [01:54<02:59, 34.59it/s]Training CobwebTree:  42%|     | 4575/10788 [01:54<02:56, 35.17it/s]Training CobwebTree:  42%|     | 4579/10788 [01:54<02:52, 36.09it/s]Training CobwebTree:  42%|     | 4583/10788 [01:54<02:54, 35.53it/s]Training CobwebTree:  43%|     | 4588/10788 [01:55<02:42, 38.18it/s]Training CobwebTree:  43%|     | 4592/10788 [01:55<02:43, 37.85it/s]Training CobwebTree:  43%|     | 4596/10788 [01:55<02:43, 37.93it/s]Training CobwebTree:  43%|     | 4600/10788 [01:55<02:46, 37.06it/s]Training CobwebTree:  43%|     | 4604/10788 [01:55<02:51, 36.08it/s]Training CobwebTree:  43%|     | 4608/10788 [01:55<02:52, 35.90it/s]Training CobwebTree:  43%|     | 4612/10788 [01:55<02:54, 35.46it/s]Training CobwebTree:  43%|     | 4616/10788 [01:55<02:57, 34.86it/s]Training CobwebTree:  43%|     | 4621/10788 [01:55<02:43, 37.66it/s]Training CobwebTree:  43%|     | 4625/10788 [01:56<02:46, 37.08it/s]Training CobwebTree:  43%|     | 4629/10788 [01:56<02:43, 37.69it/s]Training CobwebTree:  43%|     | 4633/10788 [01:56<02:46, 36.96it/s]Training CobwebTree:  43%|     | 4637/10788 [01:56<02:43, 37.74it/s]Training CobwebTree:  43%|     | 4641/10788 [01:56<02:51, 35.89it/s]Training CobwebTree:  43%|     | 4645/10788 [01:56<02:49, 36.29it/s]Training CobwebTree:  43%|     | 4649/10788 [01:56<02:47, 36.70it/s]Training CobwebTree:  43%|     | 4653/10788 [01:56<02:43, 37.62it/s]Training CobwebTree:  43%|     | 4658/10788 [01:56<02:39, 38.41it/s]Training CobwebTree:  43%|     | 4662/10788 [01:57<02:45, 36.91it/s]Training CobwebTree:  43%|     | 4666/10788 [01:57<02:58, 34.22it/s]Training CobwebTree:  43%|     | 4670/10788 [01:57<02:53, 35.17it/s]Training CobwebTree:  43%|     | 4674/10788 [01:57<02:48, 36.19it/s]Training CobwebTree:  43%|     | 4678/10788 [01:57<02:52, 35.42it/s]Training CobwebTree:  43%|     | 4682/10788 [01:57<02:47, 36.37it/s]Training CobwebTree:  43%|     | 4687/10788 [01:57<02:41, 37.84it/s]Training CobwebTree:  43%|     | 4691/10788 [01:57<02:52, 35.36it/s]Training CobwebTree:  44%|     | 4695/10788 [01:58<02:57, 34.35it/s]Training CobwebTree:  44%|     | 4699/10788 [01:58<02:54, 34.80it/s]Training CobwebTree:  44%|     | 4703/10788 [01:58<02:52, 35.30it/s]Training CobwebTree:  44%|     | 4708/10788 [01:58<02:45, 36.77it/s]Training CobwebTree:  44%|     | 4712/10788 [01:58<02:42, 37.48it/s]Training CobwebTree:  44%|     | 4716/10788 [01:58<02:42, 37.40it/s]Training CobwebTree:  44%|     | 4720/10788 [01:58<02:51, 35.36it/s]Training CobwebTree:  44%|     | 4724/10788 [01:58<02:53, 35.05it/s]Training CobwebTree:  44%|     | 4729/10788 [01:58<02:46, 36.30it/s]Training CobwebTree:  44%|     | 4733/10788 [01:59<02:49, 35.73it/s]Training CobwebTree:  44%|     | 4737/10788 [01:59<02:49, 35.66it/s]Training CobwebTree:  44%|     | 4741/10788 [01:59<02:52, 34.99it/s]Training CobwebTree:  44%|     | 4745/10788 [01:59<02:54, 34.61it/s]Training CobwebTree:  44%|     | 4749/10788 [01:59<02:56, 34.28it/s]Training CobwebTree:  44%|     | 4753/10788 [01:59<02:51, 35.18it/s]Training CobwebTree:  44%|     | 4757/10788 [01:59<02:52, 35.02it/s]Training CobwebTree:  44%|     | 4761/10788 [01:59<02:46, 36.11it/s]Training CobwebTree:  44%|     | 4765/10788 [01:59<02:47, 36.04it/s]Training CobwebTree:  44%|     | 4769/10788 [02:00<02:45, 36.31it/s]Training CobwebTree:  44%|     | 4773/10788 [02:00<02:42, 36.94it/s]Training CobwebTree:  44%|     | 4777/10788 [02:00<02:48, 35.64it/s]Training CobwebTree:  44%|     | 4781/10788 [02:00<02:44, 36.47it/s]Training CobwebTree:  44%|     | 4785/10788 [02:00<02:51, 35.10it/s]Training CobwebTree:  44%|     | 4789/10788 [02:00<02:56, 33.94it/s]Training CobwebTree:  44%|     | 4793/10788 [02:00<02:55, 34.25it/s]Training CobwebTree:  44%|     | 4797/10788 [02:00<02:49, 35.33it/s]Training CobwebTree:  45%|     | 4801/10788 [02:00<02:47, 35.77it/s]Training CobwebTree:  45%|     | 4805/10788 [02:01<02:56, 33.93it/s]Training CobwebTree:  45%|     | 4809/10788 [02:01<02:51, 34.85it/s]Training CobwebTree:  45%|     | 4813/10788 [02:01<02:53, 34.51it/s]Training CobwebTree:  45%|     | 4817/10788 [02:01<02:50, 35.12it/s]Training CobwebTree:  45%|     | 4822/10788 [02:01<02:40, 37.12it/s]Training CobwebTree:  45%|     | 4826/10788 [02:01<02:46, 35.72it/s]Training CobwebTree:  45%|     | 4830/10788 [02:01<02:49, 35.09it/s]Training CobwebTree:  45%|     | 4834/10788 [02:01<02:49, 35.04it/s]Training CobwebTree:  45%|     | 4838/10788 [02:02<02:48, 35.21it/s]Training CobwebTree:  45%|     | 4842/10788 [02:02<02:52, 34.55it/s]Training CobwebTree:  45%|     | 4847/10788 [02:02<02:40, 37.11it/s]Training CobwebTree:  45%|     | 4851/10788 [02:02<02:39, 37.30it/s]Training CobwebTree:  45%|     | 4856/10788 [02:02<02:29, 39.55it/s]Training CobwebTree:  45%|     | 4861/10788 [02:02<02:26, 40.47it/s]Training CobwebTree:  45%|     | 4866/10788 [02:02<02:33, 38.56it/s]Training CobwebTree:  45%|     | 4870/10788 [02:02<02:32, 38.80it/s]Training CobwebTree:  45%|     | 4874/10788 [02:02<02:33, 38.44it/s]Training CobwebTree:  45%|     | 4878/10788 [02:03<02:33, 38.50it/s]Training CobwebTree:  45%|     | 4882/10788 [02:03<02:38, 37.36it/s]Training CobwebTree:  45%|     | 4886/10788 [02:03<02:37, 37.58it/s]Training CobwebTree:  45%|     | 4890/10788 [02:03<02:38, 37.14it/s]Training CobwebTree:  45%|     | 4894/10788 [02:03<02:36, 37.54it/s]Training CobwebTree:  45%|     | 4898/10788 [02:03<02:49, 34.77it/s]Training CobwebTree:  45%|     | 4902/10788 [02:03<02:54, 33.66it/s]Training CobwebTree:  45%|     | 4906/10788 [02:03<02:53, 33.82it/s]Training CobwebTree:  46%|     | 4910/10788 [02:03<02:46, 35.31it/s]Training CobwebTree:  46%|     | 4914/10788 [02:04<02:40, 36.50it/s]Training CobwebTree:  46%|     | 4918/10788 [02:04<02:49, 34.57it/s]Training CobwebTree:  46%|     | 4922/10788 [02:04<02:44, 35.64it/s]Training CobwebTree:  46%|     | 4926/10788 [02:04<02:44, 35.60it/s]Training CobwebTree:  46%|     | 4930/10788 [02:04<02:43, 35.77it/s]Training CobwebTree:  46%|     | 4934/10788 [02:04<02:38, 36.89it/s]Training CobwebTree:  46%|     | 4938/10788 [02:04<02:41, 36.13it/s]Training CobwebTree:  46%|     | 4942/10788 [02:04<02:44, 35.58it/s]Training CobwebTree:  46%|     | 4946/10788 [02:04<02:46, 35.12it/s]Training CobwebTree:  46%|     | 4950/10788 [02:05<02:40, 36.37it/s]Training CobwebTree:  46%|     | 4954/10788 [02:05<02:36, 37.20it/s]Training CobwebTree:  46%|     | 4958/10788 [02:05<02:42, 35.78it/s]Training CobwebTree:  46%|     | 4962/10788 [02:05<02:38, 36.77it/s]Training CobwebTree:  46%|     | 4966/10788 [02:05<02:36, 37.10it/s]Training CobwebTree:  46%|     | 4971/10788 [02:05<02:33, 37.82it/s]Training CobwebTree:  46%|     | 4975/10788 [02:05<02:42, 35.88it/s]Training CobwebTree:  46%|     | 4979/10788 [02:05<02:45, 35.05it/s]Training CobwebTree:  46%|     | 4983/10788 [02:06<02:43, 35.49it/s]Training CobwebTree:  46%|     | 4987/10788 [02:06<02:46, 34.79it/s]Training CobwebTree:  46%|     | 4991/10788 [02:06<02:52, 33.62it/s]Training CobwebTree:  46%|     | 4995/10788 [02:06<02:56, 32.82it/s]Training CobwebTree:  46%|     | 5000/10788 [02:06<02:48, 34.29it/s]Training CobwebTree:  46%|     | 5004/10788 [02:06<02:48, 34.23it/s]Training CobwebTree:  46%|     | 5008/10788 [02:06<02:44, 35.18it/s]Training CobwebTree:  46%|     | 5012/10788 [02:06<02:42, 35.49it/s]Training CobwebTree:  46%|     | 5016/10788 [02:06<02:45, 34.93it/s]Training CobwebTree:  47%|     | 5020/10788 [02:07<02:42, 35.45it/s]Training CobwebTree:  47%|     | 5024/10788 [02:07<02:38, 36.30it/s]Training CobwebTree:  47%|     | 5028/10788 [02:07<02:36, 36.90it/s]Training CobwebTree:  47%|     | 5032/10788 [02:07<02:35, 37.04it/s]Training CobwebTree:  47%|     | 5036/10788 [02:07<02:42, 35.36it/s]Training CobwebTree:  47%|     | 5040/10788 [02:07<02:43, 35.05it/s]Training CobwebTree:  47%|     | 5045/10788 [02:07<02:36, 36.60it/s]Training CobwebTree:  47%|     | 5050/10788 [02:07<02:32, 37.71it/s]Training CobwebTree:  47%|     | 5054/10788 [02:07<02:34, 37.20it/s]Training CobwebTree:  47%|     | 5058/10788 [02:08<02:37, 36.31it/s]Training CobwebTree:  47%|     | 5062/10788 [02:08<02:34, 37.08it/s]Training CobwebTree:  47%|     | 5066/10788 [02:08<02:35, 36.91it/s]Training CobwebTree:  47%|     | 5070/10788 [02:08<02:32, 37.57it/s]Training CobwebTree:  47%|     | 5074/10788 [02:08<02:33, 37.32it/s]Training CobwebTree:  47%|     | 5079/10788 [02:08<02:31, 37.76it/s]Training CobwebTree:  47%|     | 5083/10788 [02:08<02:31, 37.58it/s]Training CobwebTree:  47%|     | 5087/10788 [02:08<02:29, 38.17it/s]Training CobwebTree:  47%|     | 5091/10788 [02:08<02:31, 37.51it/s]Training CobwebTree:  47%|     | 5095/10788 [02:09<02:36, 36.35it/s]Training CobwebTree:  47%|     | 5099/10788 [02:09<02:37, 36.05it/s]Training CobwebTree:  47%|     | 5104/10788 [02:09<02:33, 37.04it/s]Training CobwebTree:  47%|     | 5108/10788 [02:09<02:33, 36.99it/s]Training CobwebTree:  47%|     | 5112/10788 [02:09<02:36, 36.22it/s]Training CobwebTree:  47%|     | 5116/10788 [02:09<02:36, 36.17it/s]Training CobwebTree:  47%|     | 5120/10788 [02:09<02:35, 36.34it/s]Training CobwebTree:  47%|     | 5124/10788 [02:09<02:34, 36.69it/s]Training CobwebTree:  48%|     | 5128/10788 [02:10<02:40, 35.18it/s]Training CobwebTree:  48%|     | 5133/10788 [02:10<02:33, 36.83it/s]Training CobwebTree:  48%|     | 5137/10788 [02:10<02:35, 36.45it/s]Training CobwebTree:  48%|     | 5141/10788 [02:10<02:35, 36.26it/s]Training CobwebTree:  48%|     | 5145/10788 [02:10<02:39, 35.35it/s]Training CobwebTree:  48%|     | 5149/10788 [02:10<02:47, 33.68it/s]Training CobwebTree:  48%|     | 5153/10788 [02:10<02:43, 34.55it/s]Training CobwebTree:  48%|     | 5157/10788 [02:10<02:40, 35.14it/s]Training CobwebTree:  48%|     | 5161/10788 [02:10<02:40, 34.99it/s]Training CobwebTree:  48%|     | 5166/10788 [02:11<02:37, 35.73it/s]Training CobwebTree:  48%|     | 5170/10788 [02:11<02:36, 35.91it/s]Training CobwebTree:  48%|     | 5174/10788 [02:11<02:34, 36.43it/s]Training CobwebTree:  48%|     | 5178/10788 [02:11<02:35, 36.08it/s]Training CobwebTree:  48%|     | 5182/10788 [02:11<02:39, 35.15it/s]Training CobwebTree:  48%|     | 5187/10788 [02:11<02:35, 36.01it/s]Training CobwebTree:  48%|     | 5192/10788 [02:11<02:31, 37.03it/s]Training CobwebTree:  48%|     | 5196/10788 [02:11<02:36, 35.63it/s]Training CobwebTree:  48%|     | 5200/10788 [02:12<02:34, 36.09it/s]Training CobwebTree:  48%|     | 5204/10788 [02:12<02:32, 36.72it/s]Training CobwebTree:  48%|     | 5208/10788 [02:12<02:37, 35.34it/s]Training CobwebTree:  48%|     | 5212/10788 [02:12<02:41, 34.54it/s]Training CobwebTree:  48%|     | 5217/10788 [02:12<02:32, 36.46it/s]Training CobwebTree:  48%|     | 5221/10788 [02:12<02:41, 34.53it/s]Training CobwebTree:  48%|     | 5225/10788 [02:12<02:39, 34.78it/s]Training CobwebTree:  48%|     | 5229/10788 [02:12<02:36, 35.62it/s]Training CobwebTree:  49%|     | 5233/10788 [02:12<02:37, 35.33it/s]Training CobwebTree:  49%|     | 5237/10788 [02:13<02:34, 35.99it/s]Training CobwebTree:  49%|     | 5241/10788 [02:13<02:40, 34.55it/s]Training CobwebTree:  49%|     | 5245/10788 [02:13<02:34, 35.93it/s]Training CobwebTree:  49%|     | 5249/10788 [02:13<02:35, 35.69it/s]Training CobwebTree:  49%|     | 5253/10788 [02:13<02:33, 35.99it/s]Training CobwebTree:  49%|     | 5257/10788 [02:13<02:32, 36.31it/s]Training CobwebTree:  49%|     | 5261/10788 [02:13<02:34, 35.75it/s]Training CobwebTree:  49%|     | 5265/10788 [02:13<02:38, 34.79it/s]Training CobwebTree:  49%|     | 5269/10788 [02:13<02:36, 35.23it/s]Training CobwebTree:  49%|     | 5273/10788 [02:14<02:40, 34.41it/s]Training CobwebTree:  49%|     | 5278/10788 [02:14<02:30, 36.72it/s]Training CobwebTree:  49%|     | 5282/10788 [02:14<02:27, 37.29it/s]Training CobwebTree:  49%|     | 5286/10788 [02:14<02:25, 37.77it/s]Training CobwebTree:  49%|     | 5292/10788 [02:14<02:12, 41.56it/s]Training CobwebTree:  49%|     | 5297/10788 [02:14<02:14, 40.67it/s]Training CobwebTree:  49%|     | 5302/10788 [02:14<02:23, 38.27it/s]Training CobwebTree:  49%|     | 5306/10788 [02:14<02:23, 38.27it/s]Training CobwebTree:  49%|     | 5310/10788 [02:15<02:26, 37.44it/s]Training CobwebTree:  49%|     | 5314/10788 [02:15<02:27, 37.14it/s]Training CobwebTree:  49%|     | 5319/10788 [02:15<02:23, 38.11it/s]Training CobwebTree:  49%|     | 5323/10788 [02:15<02:24, 37.79it/s]Training CobwebTree:  49%|     | 5328/10788 [02:15<02:17, 39.66it/s]Training CobwebTree:  49%|     | 5333/10788 [02:15<02:10, 41.92it/s]Training CobwebTree:  49%|     | 5338/10788 [02:15<02:14, 40.59it/s]Training CobwebTree:  50%|     | 5343/10788 [02:15<02:18, 39.25it/s]Training CobwebTree:  50%|     | 5347/10788 [02:15<02:24, 37.67it/s]Training CobwebTree:  50%|     | 5351/10788 [02:16<02:29, 36.40it/s]Training CobwebTree:  50%|     | 5355/10788 [02:16<02:34, 35.18it/s]Training CobwebTree:  50%|     | 5359/10788 [02:16<02:37, 34.49it/s]Training CobwebTree:  50%|     | 5363/10788 [02:16<02:38, 34.15it/s]Training CobwebTree:  50%|     | 5367/10788 [02:16<02:35, 34.77it/s]Training CobwebTree:  50%|     | 5371/10788 [02:16<02:31, 35.69it/s]Training CobwebTree:  50%|     | 5375/10788 [02:16<02:31, 35.77it/s]Training CobwebTree:  50%|     | 5379/10788 [02:16<02:28, 36.34it/s]Training CobwebTree:  50%|     | 5383/10788 [02:17<02:33, 35.10it/s]Training CobwebTree:  50%|     | 5387/10788 [02:17<02:32, 35.40it/s]Training CobwebTree:  50%|     | 5391/10788 [02:17<02:30, 35.97it/s]Training CobwebTree:  50%|     | 5395/10788 [02:17<02:31, 35.66it/s]Training CobwebTree:  50%|     | 5399/10788 [02:17<02:36, 34.47it/s]Training CobwebTree:  50%|     | 5403/10788 [02:17<02:33, 35.15it/s]Training CobwebTree:  50%|     | 5408/10788 [02:17<02:26, 36.65it/s]Training CobwebTree:  50%|     | 5412/10788 [02:17<02:29, 36.07it/s]Training CobwebTree:  50%|     | 5416/10788 [02:17<02:28, 36.29it/s]Training CobwebTree:  50%|     | 5420/10788 [02:18<02:26, 36.69it/s]Training CobwebTree:  50%|     | 5424/10788 [02:18<02:30, 35.53it/s]Training CobwebTree:  50%|     | 5428/10788 [02:18<02:31, 35.49it/s]Training CobwebTree:  50%|     | 5432/10788 [02:18<02:40, 33.35it/s]Training CobwebTree:  50%|     | 5436/10788 [02:18<02:42, 32.85it/s]Training CobwebTree:  50%|     | 5440/10788 [02:18<02:34, 34.62it/s]Training CobwebTree:  50%|     | 5444/10788 [02:18<02:32, 35.11it/s]Training CobwebTree:  51%|     | 5448/10788 [02:18<02:38, 33.71it/s]Training CobwebTree:  51%|     | 5452/10788 [02:19<02:42, 32.92it/s]Training CobwebTree:  51%|     | 5456/10788 [02:19<02:42, 32.82it/s]Training CobwebTree:  51%|     | 5460/10788 [02:19<02:44, 32.40it/s]Training CobwebTree:  51%|     | 5464/10788 [02:19<02:42, 32.84it/s]Training CobwebTree:  51%|     | 5468/10788 [02:19<02:42, 32.73it/s]Training CobwebTree:  51%|     | 5472/10788 [02:19<02:36, 33.99it/s]Training CobwebTree:  51%|     | 5476/10788 [02:19<02:37, 33.75it/s]Training CobwebTree:  51%|     | 5480/10788 [02:19<02:40, 32.99it/s]Training CobwebTree:  51%|     | 5484/10788 [02:19<02:43, 32.50it/s]Training CobwebTree:  51%|     | 5488/10788 [02:20<02:35, 34.18it/s]Training CobwebTree:  51%|     | 5492/10788 [02:20<02:37, 33.60it/s]Training CobwebTree:  51%|     | 5496/10788 [02:20<02:43, 32.44it/s]Training CobwebTree:  51%|     | 5500/10788 [02:20<02:39, 33.25it/s]Training CobwebTree:  51%|     | 5505/10788 [02:20<02:31, 34.88it/s]Training CobwebTree:  51%|     | 5509/10788 [02:20<02:38, 33.30it/s]Training CobwebTree:  51%|     | 5513/10788 [02:20<02:39, 33.06it/s]Training CobwebTree:  51%|     | 5517/10788 [02:20<02:33, 34.42it/s]Training CobwebTree:  51%|     | 5521/10788 [02:21<02:29, 35.34it/s]Training CobwebTree:  51%|     | 5525/10788 [02:21<02:23, 36.58it/s]Training CobwebTree:  51%|    | 5529/10788 [02:21<02:33, 34.23it/s]Training CobwebTree:  51%|    | 5533/10788 [02:21<02:30, 34.99it/s]Training CobwebTree:  51%|    | 5537/10788 [02:21<02:32, 34.37it/s]Training CobwebTree:  51%|    | 5541/10788 [02:21<02:37, 33.40it/s]Training CobwebTree:  51%|    | 5545/10788 [02:21<02:33, 34.15it/s]Training CobwebTree:  51%|    | 5549/10788 [02:21<02:31, 34.66it/s]Training CobwebTree:  51%|    | 5554/10788 [02:22<02:26, 35.68it/s]Training CobwebTree:  52%|    | 5558/10788 [02:22<02:22, 36.67it/s]Training CobwebTree:  52%|    | 5562/10788 [02:22<02:23, 36.50it/s]Training CobwebTree:  52%|    | 5566/10788 [02:22<02:26, 35.66it/s]Training CobwebTree:  52%|    | 5570/10788 [02:22<02:30, 34.78it/s]Training CobwebTree:  52%|    | 5574/10788 [02:22<02:29, 34.94it/s]Training CobwebTree:  52%|    | 5578/10788 [02:22<02:31, 34.35it/s]Training CobwebTree:  52%|    | 5582/10788 [02:22<02:28, 35.13it/s]Training CobwebTree:  52%|    | 5586/10788 [02:22<02:29, 34.78it/s]Training CobwebTree:  52%|    | 5590/10788 [02:23<02:25, 35.80it/s]Training CobwebTree:  52%|    | 5594/10788 [02:23<02:26, 35.36it/s]Training CobwebTree:  52%|    | 5599/10788 [02:23<02:21, 36.79it/s]Training CobwebTree:  52%|    | 5603/10788 [02:23<02:30, 34.38it/s]Training CobwebTree:  52%|    | 5607/10788 [02:23<02:39, 32.52it/s]Training CobwebTree:  52%|    | 5611/10788 [02:23<02:33, 33.72it/s]Training CobwebTree:  52%|    | 5615/10788 [02:23<02:34, 33.55it/s]Training CobwebTree:  52%|    | 5620/10788 [02:23<02:23, 36.08it/s]Training CobwebTree:  52%|    | 5624/10788 [02:23<02:20, 36.82it/s]Training CobwebTree:  52%|    | 5629/10788 [02:24<02:17, 37.49it/s]Training CobwebTree:  52%|    | 5633/10788 [02:24<02:18, 37.21it/s]Training CobwebTree:  52%|    | 5637/10788 [02:24<02:21, 36.28it/s]Training CobwebTree:  52%|    | 5641/10788 [02:24<02:24, 35.51it/s]Training CobwebTree:  52%|    | 5645/10788 [02:24<02:32, 33.67it/s]Training CobwebTree:  52%|    | 5650/10788 [02:24<02:22, 36.04it/s]Training CobwebTree:  52%|    | 5654/10788 [02:24<02:18, 37.05it/s]Training CobwebTree:  52%|    | 5658/10788 [02:24<02:19, 36.76it/s]Training CobwebTree:  52%|    | 5662/10788 [02:25<02:20, 36.54it/s]Training CobwebTree:  53%|    | 5666/10788 [02:25<02:21, 36.13it/s]Training CobwebTree:  53%|    | 5670/10788 [02:25<02:28, 34.40it/s]Training CobwebTree:  53%|    | 5674/10788 [02:25<02:26, 34.98it/s]Training CobwebTree:  53%|    | 5678/10788 [02:25<02:28, 34.40it/s]Training CobwebTree:  53%|    | 5682/10788 [02:25<02:26, 34.87it/s]Training CobwebTree:  53%|    | 5686/10788 [02:25<02:33, 33.14it/s]Training CobwebTree:  53%|    | 5690/10788 [02:25<02:29, 34.02it/s]Training CobwebTree:  53%|    | 5694/10788 [02:25<02:25, 35.07it/s]Training CobwebTree:  53%|    | 5698/10788 [02:26<02:23, 35.47it/s]Training CobwebTree:  53%|    | 5702/10788 [02:26<02:19, 36.53it/s]Training CobwebTree:  53%|    | 5706/10788 [02:26<02:21, 35.99it/s]Training CobwebTree:  53%|    | 5710/10788 [02:26<02:26, 34.58it/s]Training CobwebTree:  53%|    | 5714/10788 [02:26<02:23, 35.48it/s]Training CobwebTree:  53%|    | 5718/10788 [02:26<02:27, 34.28it/s]Training CobwebTree:  53%|    | 5722/10788 [02:26<02:26, 34.55it/s]Training CobwebTree:  53%|    | 5726/10788 [02:26<02:30, 33.73it/s]Training CobwebTree:  53%|    | 5730/10788 [02:27<02:33, 32.99it/s]Training CobwebTree:  53%|    | 5734/10788 [02:27<02:33, 32.98it/s]Training CobwebTree:  53%|    | 5738/10788 [02:27<02:27, 34.34it/s]Training CobwebTree:  53%|    | 5743/10788 [02:27<02:22, 35.50it/s]Training CobwebTree:  53%|    | 5747/10788 [02:27<02:19, 36.04it/s]Training CobwebTree:  53%|    | 5752/10788 [02:27<02:13, 37.73it/s]Training CobwebTree:  53%|    | 5756/10788 [02:27<02:20, 35.81it/s]Training CobwebTree:  53%|    | 5760/10788 [02:27<02:19, 36.05it/s]Training CobwebTree:  53%|    | 5764/10788 [02:27<02:19, 36.05it/s]Training CobwebTree:  53%|    | 5768/10788 [02:28<02:19, 35.95it/s]Training CobwebTree:  54%|    | 5772/10788 [02:28<02:24, 34.64it/s]Training CobwebTree:  54%|    | 5776/10788 [02:28<02:24, 34.73it/s]Training CobwebTree:  54%|    | 5780/10788 [02:28<02:19, 35.89it/s]Training CobwebTree:  54%|    | 5784/10788 [02:28<02:22, 35.14it/s]Training CobwebTree:  54%|    | 5788/10788 [02:28<02:22, 35.12it/s]Training CobwebTree:  54%|    | 5792/10788 [02:28<02:23, 34.87it/s]Training CobwebTree:  54%|    | 5796/10788 [02:28<02:25, 34.32it/s]Training CobwebTree:  54%|    | 5800/10788 [02:29<02:26, 33.93it/s]Training CobwebTree:  54%|    | 5804/10788 [02:29<02:28, 33.49it/s]Training CobwebTree:  54%|    | 5808/10788 [02:29<02:23, 34.74it/s]Training CobwebTree:  54%|    | 5812/10788 [02:29<02:18, 35.98it/s]Training CobwebTree:  54%|    | 5816/10788 [02:29<02:23, 34.56it/s]Training CobwebTree:  54%|    | 5820/10788 [02:29<02:28, 33.47it/s]Training CobwebTree:  54%|    | 5824/10788 [02:29<02:34, 32.23it/s]Training CobwebTree:  54%|    | 5829/10788 [02:29<02:22, 34.91it/s]Training CobwebTree:  54%|    | 5833/10788 [02:29<02:28, 33.37it/s]Training CobwebTree:  54%|    | 5837/10788 [02:30<02:30, 32.93it/s]Training CobwebTree:  54%|    | 5841/10788 [02:30<02:27, 33.59it/s]Training CobwebTree:  54%|    | 5845/10788 [02:30<02:28, 33.35it/s]Training CobwebTree:  54%|    | 5849/10788 [02:30<02:22, 34.61it/s]Training CobwebTree:  54%|    | 5853/10788 [02:30<02:23, 34.46it/s]Training CobwebTree:  54%|    | 5857/10788 [02:30<02:24, 34.19it/s]Training CobwebTree:  54%|    | 5861/10788 [02:30<02:33, 32.00it/s]Training CobwebTree:  54%|    | 5865/10788 [02:30<02:34, 31.87it/s]Training CobwebTree:  54%|    | 5869/10788 [02:31<02:31, 32.49it/s]Training CobwebTree:  54%|    | 5873/10788 [02:31<02:27, 33.33it/s]Training CobwebTree:  54%|    | 5877/10788 [02:31<02:23, 34.34it/s]Training CobwebTree:  55%|    | 5881/10788 [02:31<02:22, 34.32it/s]Training CobwebTree:  55%|    | 5885/10788 [02:31<02:22, 34.31it/s]Training CobwebTree:  55%|    | 5889/10788 [02:31<02:23, 34.10it/s]Training CobwebTree:  55%|    | 5893/10788 [02:31<02:23, 34.19it/s]Training CobwebTree:  55%|    | 5897/10788 [02:31<02:24, 33.88it/s]Training CobwebTree:  55%|    | 5901/10788 [02:31<02:21, 34.60it/s]Training CobwebTree:  55%|    | 5905/10788 [02:32<02:16, 35.71it/s]Training CobwebTree:  55%|    | 5909/10788 [02:32<02:21, 34.46it/s]Training CobwebTree:  55%|    | 5913/10788 [02:32<02:27, 33.15it/s]Training CobwebTree:  55%|    | 5917/10788 [02:32<02:26, 33.18it/s]Training CobwebTree:  55%|    | 5921/10788 [02:32<02:26, 33.24it/s]Training CobwebTree:  55%|    | 5925/10788 [02:32<02:28, 32.78it/s]Training CobwebTree:  55%|    | 5929/10788 [02:32<02:20, 34.64it/s]Training CobwebTree:  55%|    | 5933/10788 [02:32<02:17, 35.19it/s]Training CobwebTree:  55%|    | 5937/10788 [02:33<02:24, 33.59it/s]Training CobwebTree:  55%|    | 5941/10788 [02:33<02:20, 34.47it/s]Training CobwebTree:  55%|    | 5945/10788 [02:33<02:25, 33.34it/s]Training CobwebTree:  55%|    | 5949/10788 [02:33<02:21, 34.12it/s]Training CobwebTree:  55%|    | 5953/10788 [02:33<02:17, 35.13it/s]Training CobwebTree:  55%|    | 5958/10788 [02:33<02:11, 36.64it/s]Training CobwebTree:  55%|    | 5962/10788 [02:33<02:11, 36.79it/s]Training CobwebTree:  55%|    | 5966/10788 [02:33<02:15, 35.55it/s]Training CobwebTree:  55%|    | 5970/10788 [02:33<02:17, 34.96it/s]Training CobwebTree:  55%|    | 5974/10788 [02:34<02:18, 34.64it/s]Training CobwebTree:  55%|    | 5978/10788 [02:34<02:14, 35.82it/s]Training CobwebTree:  55%|    | 5982/10788 [02:34<02:22, 33.72it/s]Training CobwebTree:  55%|    | 5986/10788 [02:34<02:21, 33.93it/s]Training CobwebTree:  56%|    | 5990/10788 [02:34<02:18, 34.67it/s]Training CobwebTree:  56%|    | 5994/10788 [02:34<02:17, 34.94it/s]Training CobwebTree:  56%|    | 5998/10788 [02:34<02:16, 34.97it/s]Training CobwebTree:  56%|    | 6002/10788 [02:34<02:16, 35.13it/s]Training CobwebTree:  56%|    | 6006/10788 [02:35<02:23, 33.42it/s]Training CobwebTree:  56%|    | 6011/10788 [02:35<02:14, 35.50it/s]Training CobwebTree:  56%|    | 6015/10788 [02:35<02:16, 35.00it/s]Training CobwebTree:  56%|    | 6019/10788 [02:35<02:17, 34.65it/s]Training CobwebTree:  56%|    | 6023/10788 [02:35<02:18, 34.32it/s]Training CobwebTree:  56%|    | 6027/10788 [02:35<02:15, 35.03it/s]Training CobwebTree:  56%|    | 6031/10788 [02:35<02:12, 35.89it/s]Training CobwebTree:  56%|    | 6035/10788 [02:35<02:12, 35.92it/s]Training CobwebTree:  56%|    | 6039/10788 [02:35<02:12, 35.74it/s]Training CobwebTree:  56%|    | 6043/10788 [02:36<02:10, 36.37it/s]Training CobwebTree:  56%|    | 6048/10788 [02:36<02:08, 36.80it/s]Training CobwebTree:  56%|    | 6052/10788 [02:36<02:06, 37.32it/s]Training CobwebTree:  56%|    | 6056/10788 [02:36<02:05, 37.72it/s]Training CobwebTree:  56%|    | 6060/10788 [02:36<02:11, 35.91it/s]Training CobwebTree:  56%|    | 6064/10788 [02:36<02:12, 35.70it/s]Training CobwebTree:  56%|    | 6068/10788 [02:36<02:15, 34.89it/s]Training CobwebTree:  56%|    | 6073/10788 [02:36<02:08, 36.57it/s]Training CobwebTree:  56%|    | 6077/10788 [02:37<02:15, 34.83it/s]Training CobwebTree:  56%|    | 6081/10788 [02:37<02:15, 34.73it/s]Training CobwebTree:  56%|    | 6085/10788 [02:37<02:14, 34.97it/s]Training CobwebTree:  56%|    | 6089/10788 [02:37<02:19, 33.74it/s]Training CobwebTree:  56%|    | 6093/10788 [02:37<02:16, 34.47it/s]Training CobwebTree:  57%|    | 6097/10788 [02:37<02:21, 33.17it/s]Training CobwebTree:  57%|    | 6101/10788 [02:37<02:18, 33.72it/s]Training CobwebTree:  57%|    | 6105/10788 [02:37<02:18, 33.79it/s]Training CobwebTree:  57%|    | 6109/10788 [02:37<02:21, 32.99it/s]Training CobwebTree:  57%|    | 6113/10788 [02:38<02:16, 34.13it/s]Training CobwebTree:  57%|    | 6117/10788 [02:38<02:23, 32.51it/s]Training CobwebTree:  57%|    | 6121/10788 [02:38<02:24, 32.33it/s]Training CobwebTree:  57%|    | 6125/10788 [02:38<02:29, 31.10it/s]Training CobwebTree:  57%|    | 6129/10788 [02:38<02:26, 31.73it/s]Training CobwebTree:  57%|    | 6133/10788 [02:38<02:18, 33.57it/s]Training CobwebTree:  57%|    | 6137/10788 [02:38<02:18, 33.69it/s]Training CobwebTree:  57%|    | 6141/10788 [02:38<02:17, 33.72it/s]Training CobwebTree:  57%|    | 6145/10788 [02:39<02:16, 34.05it/s]Training CobwebTree:  57%|    | 6149/10788 [02:39<02:14, 34.48it/s]Training CobwebTree:  57%|    | 6153/10788 [02:39<02:19, 33.25it/s]Training CobwebTree:  57%|    | 6157/10788 [02:39<02:19, 33.28it/s]Training CobwebTree:  57%|    | 6161/10788 [02:39<02:21, 32.64it/s]Training CobwebTree:  57%|    | 6165/10788 [02:39<02:23, 32.16it/s]Training CobwebTree:  57%|    | 6169/10788 [02:39<02:18, 33.43it/s]Training CobwebTree:  57%|    | 6173/10788 [02:39<02:15, 34.14it/s]Training CobwebTree:  57%|    | 6177/10788 [02:40<02:15, 34.14it/s]Training CobwebTree:  57%|    | 6181/10788 [02:40<02:15, 34.00it/s]Training CobwebTree:  57%|    | 6185/10788 [02:40<02:16, 33.62it/s]Training CobwebTree:  57%|    | 6190/10788 [02:40<02:09, 35.37it/s]Training CobwebTree:  57%|    | 6194/10788 [02:40<02:08, 35.74it/s]Training CobwebTree:  57%|    | 6198/10788 [02:40<02:06, 36.42it/s]Training CobwebTree:  57%|    | 6202/10788 [02:40<02:08, 35.79it/s]Training CobwebTree:  58%|    | 6206/10788 [02:40<02:08, 35.67it/s]Training CobwebTree:  58%|    | 6210/10788 [02:40<02:11, 34.88it/s]Training CobwebTree:  58%|    | 6214/10788 [02:41<02:07, 35.86it/s]Training CobwebTree:  58%|    | 6218/10788 [02:41<02:07, 35.77it/s]Training CobwebTree:  58%|    | 6222/10788 [02:41<02:07, 35.87it/s]Training CobwebTree:  58%|    | 6226/10788 [02:41<02:10, 34.98it/s]Training CobwebTree:  58%|    | 6230/10788 [02:41<02:11, 34.70it/s]Training CobwebTree:  58%|    | 6234/10788 [02:41<02:14, 33.91it/s]Training CobwebTree:  58%|    | 6238/10788 [02:41<02:16, 33.33it/s]Training CobwebTree:  58%|    | 6242/10788 [02:41<02:19, 32.51it/s]Training CobwebTree:  58%|    | 6246/10788 [02:41<02:12, 34.22it/s]Training CobwebTree:  58%|    | 6250/10788 [02:42<02:12, 34.14it/s]Training CobwebTree:  58%|    | 6254/10788 [02:42<02:18, 32.79it/s]Training CobwebTree:  58%|    | 6258/10788 [02:42<02:18, 32.81it/s]Training CobwebTree:  58%|    | 6262/10788 [02:42<02:20, 32.25it/s]Training CobwebTree:  58%|    | 6266/10788 [02:42<02:17, 32.92it/s]Training CobwebTree:  58%|    | 6270/10788 [02:42<02:12, 34.21it/s]Training CobwebTree:  58%|    | 6274/10788 [02:42<02:06, 35.63it/s]Training CobwebTree:  58%|    | 6279/10788 [02:42<02:00, 37.37it/s]Training CobwebTree:  58%|    | 6283/10788 [02:43<02:03, 36.50it/s]Training CobwebTree:  58%|    | 6287/10788 [02:43<02:03, 36.35it/s]Training CobwebTree:  58%|    | 6291/10788 [02:43<02:06, 35.53it/s]Training CobwebTree:  58%|    | 6295/10788 [02:43<02:09, 34.58it/s]Training CobwebTree:  58%|    | 6299/10788 [02:43<02:07, 35.10it/s]Training CobwebTree:  58%|    | 6303/10788 [02:43<02:09, 34.52it/s]Training CobwebTree:  58%|    | 6307/10788 [02:43<02:12, 33.89it/s]Training CobwebTree:  59%|    | 6311/10788 [02:43<02:14, 33.32it/s]Training CobwebTree:  59%|    | 6315/10788 [02:43<02:10, 34.20it/s]Training CobwebTree:  59%|    | 6320/10788 [02:44<02:02, 36.37it/s]Training CobwebTree:  59%|    | 6324/10788 [02:44<02:00, 37.11it/s]Training CobwebTree:  59%|    | 6329/10788 [02:44<01:57, 37.87it/s]Training CobwebTree:  59%|    | 6334/10788 [02:44<01:53, 39.08it/s]Training CobwebTree:  59%|    | 6338/10788 [02:44<01:58, 37.57it/s]Training CobwebTree:  59%|    | 6342/10788 [02:44<02:01, 36.64it/s]Training CobwebTree:  59%|    | 6346/10788 [02:44<02:03, 36.07it/s]Training CobwebTree:  59%|    | 6350/10788 [02:44<02:07, 34.80it/s]Training CobwebTree:  59%|    | 6354/10788 [02:45<02:08, 34.56it/s]Training CobwebTree:  59%|    | 6358/10788 [02:45<02:06, 34.99it/s]Training CobwebTree:  59%|    | 6363/10788 [02:45<01:58, 37.26it/s]Training CobwebTree:  59%|    | 6367/10788 [02:45<02:00, 36.70it/s]Training CobwebTree:  59%|    | 6371/10788 [02:45<02:02, 35.96it/s]Training CobwebTree:  59%|    | 6375/10788 [02:45<02:08, 34.34it/s]Training CobwebTree:  59%|    | 6379/10788 [02:45<02:06, 34.83it/s]Training CobwebTree:  59%|    | 6383/10788 [02:45<02:06, 34.95it/s]Training CobwebTree:  59%|    | 6387/10788 [02:45<02:07, 34.47it/s]Training CobwebTree:  59%|    | 6392/10788 [02:46<02:04, 35.33it/s]Training CobwebTree:  59%|    | 6396/10788 [02:46<02:01, 36.10it/s]Training CobwebTree:  59%|    | 6400/10788 [02:46<02:02, 35.96it/s]Training CobwebTree:  59%|    | 6405/10788 [02:46<02:00, 36.52it/s]Training CobwebTree:  59%|    | 6409/10788 [02:46<02:02, 35.69it/s]Training CobwebTree:  59%|    | 6413/10788 [02:46<02:06, 34.58it/s]Training CobwebTree:  59%|    | 6417/10788 [02:46<02:04, 35.24it/s]Training CobwebTree:  60%|    | 6421/10788 [02:46<02:08, 33.99it/s]Training CobwebTree:  60%|    | 6425/10788 [02:47<02:06, 34.41it/s]Training CobwebTree:  60%|    | 6429/10788 [02:47<02:06, 34.34it/s]Training CobwebTree:  60%|    | 6433/10788 [02:47<02:09, 33.63it/s]Training CobwebTree:  60%|    | 6437/10788 [02:47<02:10, 33.36it/s]Training CobwebTree:  60%|    | 6441/10788 [02:47<02:07, 34.10it/s]Training CobwebTree:  60%|    | 6445/10788 [02:47<02:09, 33.49it/s]Training CobwebTree:  60%|    | 6449/10788 [02:47<02:08, 33.86it/s]Training CobwebTree:  60%|    | 6453/10788 [02:47<02:08, 33.72it/s]Training CobwebTree:  60%|    | 6457/10788 [02:48<02:05, 34.61it/s]Training CobwebTree:  60%|    | 6461/10788 [02:48<02:09, 33.50it/s]Training CobwebTree:  60%|    | 6465/10788 [02:48<02:11, 32.88it/s]Training CobwebTree:  60%|    | 6469/10788 [02:48<02:10, 33.18it/s]Training CobwebTree:  60%|    | 6473/10788 [02:48<02:08, 33.60it/s]Training CobwebTree:  60%|    | 6477/10788 [02:48<02:08, 33.45it/s]Training CobwebTree:  60%|    | 6481/10788 [02:48<02:05, 34.42it/s]Training CobwebTree:  60%|    | 6485/10788 [02:48<02:05, 34.24it/s]Training CobwebTree:  60%|    | 6489/10788 [02:48<02:06, 33.95it/s]Training CobwebTree:  60%|    | 6493/10788 [02:49<02:11, 32.74it/s]Training CobwebTree:  60%|    | 6497/10788 [02:49<02:08, 33.27it/s]Training CobwebTree:  60%|    | 6501/10788 [02:49<02:12, 32.38it/s]Training CobwebTree:  60%|    | 6505/10788 [02:49<02:09, 32.96it/s]Training CobwebTree:  60%|    | 6509/10788 [02:49<02:03, 34.56it/s]Training CobwebTree:  60%|    | 6513/10788 [02:49<02:02, 34.89it/s]Training CobwebTree:  60%|    | 6517/10788 [02:49<02:01, 35.01it/s]Training CobwebTree:  60%|    | 6521/10788 [02:49<02:01, 35.16it/s]Training CobwebTree:  60%|    | 6525/10788 [02:50<02:01, 35.13it/s]Training CobwebTree:  61%|    | 6529/10788 [02:50<02:03, 34.49it/s]Training CobwebTree:  61%|    | 6533/10788 [02:50<02:04, 34.05it/s]Training CobwebTree:  61%|    | 6537/10788 [02:50<02:00, 35.22it/s]Training CobwebTree:  61%|    | 6541/10788 [02:50<02:04, 34.14it/s]Training CobwebTree:  61%|    | 6545/10788 [02:50<02:05, 33.87it/s]Training CobwebTree:  61%|    | 6549/10788 [02:50<02:02, 34.74it/s]Training CobwebTree:  61%|    | 6553/10788 [02:50<02:07, 33.31it/s]Training CobwebTree:  61%|    | 6557/10788 [02:50<02:04, 33.88it/s]Training CobwebTree:  61%|    | 6561/10788 [02:51<02:11, 32.09it/s]Training CobwebTree:  61%|    | 6565/10788 [02:51<02:04, 33.90it/s]Training CobwebTree:  61%|    | 6569/10788 [02:51<02:06, 33.32it/s]Training CobwebTree:  61%|    | 6573/10788 [02:51<02:05, 33.53it/s]Training CobwebTree:  61%|    | 6577/10788 [02:51<02:08, 32.78it/s]Training CobwebTree:  61%|    | 6581/10788 [02:51<02:10, 32.26it/s]Training CobwebTree:  61%|    | 6585/10788 [02:51<02:04, 33.81it/s]Training CobwebTree:  61%|    | 6589/10788 [02:51<02:04, 33.62it/s]Training CobwebTree:  61%|    | 6593/10788 [02:52<02:04, 33.56it/s]Training CobwebTree:  61%|    | 6597/10788 [02:52<02:00, 34.76it/s]Training CobwebTree:  61%|    | 6601/10788 [02:52<02:02, 34.17it/s]Training CobwebTree:  61%|    | 6605/10788 [02:52<01:59, 34.93it/s]Training CobwebTree:  61%|   | 6610/10788 [02:52<01:54, 36.50it/s]Training CobwebTree:  61%|   | 6614/10788 [02:52<01:59, 34.81it/s]Training CobwebTree:  61%|   | 6618/10788 [02:52<01:56, 35.94it/s]Training CobwebTree:  61%|   | 6622/10788 [02:52<01:57, 35.45it/s]Training CobwebTree:  61%|   | 6626/10788 [02:52<01:54, 36.35it/s]Training CobwebTree:  61%|   | 6630/10788 [02:53<01:53, 36.58it/s]Training CobwebTree:  61%|   | 6634/10788 [02:53<02:02, 33.85it/s]Training CobwebTree:  62%|   | 6638/10788 [02:53<02:03, 33.47it/s]Training CobwebTree:  62%|   | 6642/10788 [02:53<02:01, 34.11it/s]Training CobwebTree:  62%|   | 6646/10788 [02:53<02:02, 33.75it/s]Training CobwebTree:  62%|   | 6650/10788 [02:53<02:09, 31.98it/s]Training CobwebTree:  62%|   | 6654/10788 [02:53<02:08, 32.24it/s]Training CobwebTree:  62%|   | 6658/10788 [02:53<02:05, 32.92it/s]Training CobwebTree:  62%|   | 6662/10788 [02:54<02:00, 34.22it/s]Training CobwebTree:  62%|   | 6666/10788 [02:54<01:58, 34.88it/s]Training CobwebTree:  62%|   | 6670/10788 [02:54<01:56, 35.49it/s]Training CobwebTree:  62%|   | 6674/10788 [02:54<01:57, 35.02it/s]Training CobwebTree:  62%|   | 6678/10788 [02:54<02:01, 33.82it/s]Training CobwebTree:  62%|   | 6682/10788 [02:54<01:55, 35.46it/s]Training CobwebTree:  62%|   | 6686/10788 [02:54<01:54, 35.78it/s]Training CobwebTree:  62%|   | 6690/10788 [02:54<01:51, 36.76it/s]Training CobwebTree:  62%|   | 6694/10788 [02:54<01:53, 36.03it/s]Training CobwebTree:  62%|   | 6698/10788 [02:55<01:57, 34.67it/s]Training CobwebTree:  62%|   | 6702/10788 [02:55<02:06, 32.34it/s]Training CobwebTree:  62%|   | 6706/10788 [02:55<02:01, 33.63it/s]Training CobwebTree:  62%|   | 6710/10788 [02:55<02:01, 33.54it/s]Training CobwebTree:  62%|   | 6714/10788 [02:55<02:01, 33.40it/s]Training CobwebTree:  62%|   | 6718/10788 [02:55<02:03, 32.98it/s]Training CobwebTree:  62%|   | 6722/10788 [02:55<02:02, 33.27it/s]Training CobwebTree:  62%|   | 6726/10788 [02:55<01:59, 33.96it/s]Training CobwebTree:  62%|   | 6730/10788 [02:56<01:58, 34.13it/s]Training CobwebTree:  62%|   | 6734/10788 [02:56<02:00, 33.77it/s]Training CobwebTree:  62%|   | 6738/10788 [02:56<02:01, 33.26it/s]Training CobwebTree:  62%|   | 6742/10788 [02:56<01:59, 33.84it/s]Training CobwebTree:  63%|   | 6746/10788 [02:56<01:58, 34.05it/s]Training CobwebTree:  63%|   | 6750/10788 [02:56<01:58, 34.13it/s]Training CobwebTree:  63%|   | 6754/10788 [02:56<02:02, 33.06it/s]Training CobwebTree:  63%|   | 6758/10788 [02:56<01:59, 33.80it/s]Training CobwebTree:  63%|   | 6762/10788 [02:56<01:57, 34.21it/s]Training CobwebTree:  63%|   | 6766/10788 [02:57<01:54, 35.05it/s]Training CobwebTree:  63%|   | 6771/10788 [02:57<01:48, 37.09it/s]Training CobwebTree:  63%|   | 6775/10788 [02:57<01:58, 33.77it/s]Training CobwebTree:  63%|   | 6779/10788 [02:57<01:56, 34.37it/s]Training CobwebTree:  63%|   | 6783/10788 [02:57<01:52, 35.66it/s]Training CobwebTree:  63%|   | 6787/10788 [02:57<01:57, 34.13it/s]Training CobwebTree:  63%|   | 6791/10788 [02:57<01:58, 33.87it/s]Training CobwebTree:  63%|   | 6795/10788 [02:57<01:54, 34.72it/s]Training CobwebTree:  63%|   | 6800/10788 [02:58<01:51, 35.65it/s]Training CobwebTree:  63%|   | 6804/10788 [02:58<01:51, 35.85it/s]Training CobwebTree:  63%|   | 6808/10788 [02:58<01:58, 33.53it/s]Training CobwebTree:  63%|   | 6812/10788 [02:58<01:57, 33.93it/s]Training CobwebTree:  63%|   | 6816/10788 [02:58<02:02, 32.52it/s]Training CobwebTree:  63%|   | 6821/10788 [02:58<01:54, 34.69it/s]Training CobwebTree:  63%|   | 6825/10788 [02:58<01:53, 34.80it/s]Training CobwebTree:  63%|   | 6829/10788 [02:58<01:56, 34.06it/s]Training CobwebTree:  63%|   | 6833/10788 [02:59<01:58, 33.46it/s]Training CobwebTree:  63%|   | 6837/10788 [02:59<02:04, 31.81it/s]Training CobwebTree:  63%|   | 6841/10788 [02:59<02:01, 32.45it/s]Training CobwebTree:  63%|   | 6845/10788 [02:59<02:02, 32.22it/s]Training CobwebTree:  63%|   | 6849/10788 [02:59<01:56, 33.94it/s]Training CobwebTree:  64%|   | 6853/10788 [02:59<01:50, 35.52it/s]Training CobwebTree:  64%|   | 6857/10788 [02:59<01:55, 34.02it/s]Training CobwebTree:  64%|   | 6861/10788 [02:59<02:01, 32.28it/s]Training CobwebTree:  64%|   | 6865/10788 [03:00<02:08, 30.53it/s]Training CobwebTree:  64%|   | 6869/10788 [03:00<01:59, 32.83it/s]Training CobwebTree:  64%|   | 6873/10788 [03:00<01:58, 33.08it/s]Training CobwebTree:  64%|   | 6877/10788 [03:00<01:59, 32.85it/s]Training CobwebTree:  64%|   | 6881/10788 [03:00<02:01, 32.06it/s]Training CobwebTree:  64%|   | 6885/10788 [03:00<01:59, 32.63it/s]Training CobwebTree:  64%|   | 6889/10788 [03:00<01:56, 33.38it/s]Training CobwebTree:  64%|   | 6893/10788 [03:00<01:55, 33.86it/s]Training CobwebTree:  64%|   | 6897/10788 [03:00<01:58, 32.85it/s]Training CobwebTree:  64%|   | 6901/10788 [03:01<01:54, 33.92it/s]Training CobwebTree:  64%|   | 6905/10788 [03:01<01:54, 33.86it/s]Training CobwebTree:  64%|   | 6909/10788 [03:01<01:53, 34.17it/s]Training CobwebTree:  64%|   | 6914/10788 [03:01<01:48, 35.76it/s]Training CobwebTree:  64%|   | 6918/10788 [03:01<01:52, 34.30it/s]Training CobwebTree:  64%|   | 6922/10788 [03:01<01:51, 34.83it/s]Training CobwebTree:  64%|   | 6926/10788 [03:01<01:50, 34.91it/s]Training CobwebTree:  64%|   | 6930/10788 [03:01<01:50, 34.86it/s]Training CobwebTree:  64%|   | 6935/10788 [03:02<01:48, 35.46it/s]Training CobwebTree:  64%|   | 6939/10788 [03:02<01:53, 33.84it/s]Training CobwebTree:  64%|   | 6943/10788 [03:02<01:58, 32.37it/s]Training CobwebTree:  64%|   | 6947/10788 [03:02<02:02, 31.45it/s]Training CobwebTree:  64%|   | 6951/10788 [03:02<02:04, 30.84it/s]Training CobwebTree:  64%|   | 6955/10788 [03:02<02:00, 31.74it/s]Training CobwebTree:  65%|   | 6959/10788 [03:02<01:58, 32.18it/s]Training CobwebTree:  65%|   | 6963/10788 [03:02<01:56, 32.91it/s]Training CobwebTree:  65%|   | 6967/10788 [03:03<01:57, 32.46it/s]Training CobwebTree:  65%|   | 6971/10788 [03:03<01:56, 32.88it/s]Training CobwebTree:  65%|   | 6975/10788 [03:03<01:52, 34.03it/s]Training CobwebTree:  65%|   | 6979/10788 [03:03<01:50, 34.44it/s]Training CobwebTree:  65%|   | 6983/10788 [03:03<01:49, 34.60it/s]Training CobwebTree:  65%|   | 6987/10788 [03:03<01:51, 34.07it/s]Training CobwebTree:  65%|   | 6992/10788 [03:03<01:44, 36.36it/s]Training CobwebTree:  65%|   | 6996/10788 [03:03<01:46, 35.71it/s]Training CobwebTree:  65%|   | 7000/10788 [03:04<01:52, 33.71it/s]Training CobwebTree:  65%|   | 7004/10788 [03:04<01:57, 32.28it/s]Training CobwebTree:  65%|   | 7008/10788 [03:04<01:59, 31.75it/s]Training CobwebTree:  65%|   | 7012/10788 [03:04<01:52, 33.62it/s]Training CobwebTree:  65%|   | 7016/10788 [03:04<01:51, 33.69it/s]Training CobwebTree:  65%|   | 7020/10788 [03:04<01:56, 32.31it/s]Training CobwebTree:  65%|   | 7024/10788 [03:04<01:51, 33.72it/s]Training CobwebTree:  65%|   | 7028/10788 [03:04<01:53, 33.01it/s]Training CobwebTree:  65%|   | 7032/10788 [03:05<01:50, 33.85it/s]Training CobwebTree:  65%|   | 7036/10788 [03:05<01:52, 33.41it/s]Training CobwebTree:  65%|   | 7040/10788 [03:05<01:49, 34.36it/s]Training CobwebTree:  65%|   | 7044/10788 [03:05<01:48, 34.58it/s]Training CobwebTree:  65%|   | 7048/10788 [03:05<01:45, 35.44it/s]Training CobwebTree:  65%|   | 7052/10788 [03:05<01:44, 35.59it/s]Training CobwebTree:  65%|   | 7056/10788 [03:05<01:44, 35.55it/s]Training CobwebTree:  65%|   | 7060/10788 [03:05<01:45, 35.43it/s]Training CobwebTree:  65%|   | 7064/10788 [03:05<01:46, 34.85it/s]Training CobwebTree:  66%|   | 7068/10788 [03:06<01:44, 35.72it/s]Training CobwebTree:  66%|   | 7072/10788 [03:06<01:42, 36.10it/s]Training CobwebTree:  66%|   | 7076/10788 [03:06<01:45, 35.34it/s]Training CobwebTree:  66%|   | 7080/10788 [03:06<01:44, 35.57it/s]Training CobwebTree:  66%|   | 7084/10788 [03:06<01:52, 32.88it/s]Training CobwebTree:  66%|   | 7088/10788 [03:06<01:56, 31.65it/s]Training CobwebTree:  66%|   | 7092/10788 [03:06<01:51, 33.11it/s]Training CobwebTree:  66%|   | 7096/10788 [03:06<01:53, 32.41it/s]Training CobwebTree:  66%|   | 7100/10788 [03:06<01:52, 32.89it/s]Training CobwebTree:  66%|   | 7104/10788 [03:07<01:53, 32.50it/s]Training CobwebTree:  66%|   | 7108/10788 [03:07<01:48, 33.90it/s]Training CobwebTree:  66%|   | 7112/10788 [03:07<01:50, 33.39it/s]Training CobwebTree:  66%|   | 7116/10788 [03:07<01:48, 33.89it/s]Training CobwebTree:  66%|   | 7120/10788 [03:07<01:47, 34.09it/s]Training CobwebTree:  66%|   | 7124/10788 [03:07<01:43, 35.48it/s]Training CobwebTree:  66%|   | 7128/10788 [03:07<01:42, 35.56it/s]Training CobwebTree:  66%|   | 7132/10788 [03:07<01:42, 35.73it/s]Training CobwebTree:  66%|   | 7137/10788 [03:08<01:39, 36.79it/s]Training CobwebTree:  66%|   | 7141/10788 [03:08<01:42, 35.69it/s]Training CobwebTree:  66%|   | 7145/10788 [03:08<01:44, 34.86it/s]Training CobwebTree:  66%|   | 7149/10788 [03:08<01:51, 32.69it/s]Training CobwebTree:  66%|   | 7153/10788 [03:08<01:50, 32.95it/s]Training CobwebTree:  66%|   | 7157/10788 [03:08<01:49, 33.30it/s]Training CobwebTree:  66%|   | 7161/10788 [03:08<01:46, 33.96it/s]Training CobwebTree:  66%|   | 7165/10788 [03:08<01:48, 33.24it/s]Training CobwebTree:  66%|   | 7169/10788 [03:09<01:53, 31.76it/s]Training CobwebTree:  66%|   | 7173/10788 [03:09<01:55, 31.22it/s]Training CobwebTree:  67%|   | 7177/10788 [03:09<01:53, 31.85it/s]Training CobwebTree:  67%|   | 7181/10788 [03:09<01:49, 32.85it/s]Training CobwebTree:  67%|   | 7185/10788 [03:09<01:50, 32.51it/s]Training CobwebTree:  67%|   | 7189/10788 [03:09<01:48, 33.11it/s]Training CobwebTree:  67%|   | 7193/10788 [03:09<01:55, 31.19it/s]Training CobwebTree:  67%|   | 7197/10788 [03:09<01:51, 32.28it/s]Training CobwebTree:  67%|   | 7201/10788 [03:10<01:47, 33.48it/s]Training CobwebTree:  67%|   | 7205/10788 [03:10<01:48, 33.12it/s]Training CobwebTree:  67%|   | 7209/10788 [03:10<01:52, 31.84it/s]Training CobwebTree:  67%|   | 7213/10788 [03:10<01:52, 31.88it/s]Training CobwebTree:  67%|   | 7217/10788 [03:10<01:49, 32.71it/s]Training CobwebTree:  67%|   | 7221/10788 [03:10<01:48, 32.91it/s]Training CobwebTree:  67%|   | 7226/10788 [03:10<01:39, 35.68it/s]Training CobwebTree:  67%|   | 7231/10788 [03:10<01:37, 36.67it/s]Training CobwebTree:  67%|   | 7235/10788 [03:10<01:35, 37.15it/s]Training CobwebTree:  67%|   | 7239/10788 [03:11<01:37, 36.38it/s]Training CobwebTree:  67%|   | 7243/10788 [03:11<01:40, 35.22it/s]Training CobwebTree:  67%|   | 7247/10788 [03:11<01:38, 35.86it/s]Training CobwebTree:  67%|   | 7251/10788 [03:11<01:47, 33.00it/s]Training CobwebTree:  67%|   | 7255/10788 [03:11<01:49, 32.28it/s]Training CobwebTree:  67%|   | 7259/10788 [03:11<01:50, 31.82it/s]Training CobwebTree:  67%|   | 7263/10788 [03:11<01:47, 32.89it/s]Training CobwebTree:  67%|   | 7267/10788 [03:11<01:46, 32.95it/s]Training CobwebTree:  67%|   | 7271/10788 [03:12<01:49, 32.26it/s]Training CobwebTree:  67%|   | 7275/10788 [03:12<01:43, 33.88it/s]Training CobwebTree:  67%|   | 7279/10788 [03:12<01:44, 33.46it/s]Training CobwebTree:  68%|   | 7283/10788 [03:12<01:41, 34.36it/s]Training CobwebTree:  68%|   | 7287/10788 [03:12<01:39, 35.18it/s]Training CobwebTree:  68%|   | 7291/10788 [03:12<01:40, 34.96it/s]Training CobwebTree:  68%|   | 7295/10788 [03:12<01:42, 33.95it/s]Training CobwebTree:  68%|   | 7299/10788 [03:12<01:40, 34.88it/s]Training CobwebTree:  68%|   | 7303/10788 [03:13<01:44, 33.48it/s]Training CobwebTree:  68%|   | 7307/10788 [03:13<01:43, 33.59it/s]Training CobwebTree:  68%|   | 7311/10788 [03:13<01:45, 33.10it/s]Training CobwebTree:  68%|   | 7315/10788 [03:13<01:46, 32.60it/s]Training CobwebTree:  68%|   | 7319/10788 [03:13<01:46, 32.60it/s]Training CobwebTree:  68%|   | 7323/10788 [03:13<01:49, 31.63it/s]Training CobwebTree:  68%|   | 7327/10788 [03:13<01:48, 32.04it/s]Training CobwebTree:  68%|   | 7331/10788 [03:13<01:43, 33.39it/s]Training CobwebTree:  68%|   | 7335/10788 [03:13<01:45, 32.71it/s]Training CobwebTree:  68%|   | 7339/10788 [03:14<01:47, 31.94it/s]Training CobwebTree:  68%|   | 7343/10788 [03:14<01:42, 33.62it/s]Training CobwebTree:  68%|   | 7347/10788 [03:14<01:42, 33.70it/s]Training CobwebTree:  68%|   | 7351/10788 [03:14<01:37, 35.13it/s]Training CobwebTree:  68%|   | 7355/10788 [03:14<01:38, 35.01it/s]Training CobwebTree:  68%|   | 7359/10788 [03:14<01:38, 34.94it/s]Training CobwebTree:  68%|   | 7364/10788 [03:14<01:31, 37.33it/s]Training CobwebTree:  68%|   | 7369/10788 [03:14<01:30, 37.82it/s]Training CobwebTree:  68%|   | 7373/10788 [03:15<01:29, 38.23it/s]Training CobwebTree:  68%|   | 7377/10788 [03:15<01:35, 35.68it/s]Training CobwebTree:  68%|   | 7381/10788 [03:15<01:38, 34.48it/s]Training CobwebTree:  68%|   | 7385/10788 [03:15<01:38, 34.56it/s]Training CobwebTree:  68%|   | 7389/10788 [03:15<01:36, 35.15it/s]Training CobwebTree:  69%|   | 7393/10788 [03:15<01:34, 35.75it/s]Training CobwebTree:  69%|   | 7397/10788 [03:15<01:35, 35.61it/s]Training CobwebTree:  69%|   | 7401/10788 [03:15<01:37, 34.57it/s]Training CobwebTree:  69%|   | 7405/10788 [03:15<01:39, 33.91it/s]Training CobwebTree:  69%|   | 7409/10788 [03:16<01:44, 32.37it/s]Training CobwebTree:  69%|   | 7413/10788 [03:16<01:48, 31.12it/s]Training CobwebTree:  69%|   | 7417/10788 [03:16<01:45, 31.94it/s]Training CobwebTree:  69%|   | 7421/10788 [03:16<01:43, 32.46it/s]Training CobwebTree:  69%|   | 7425/10788 [03:16<01:41, 33.17it/s]Training CobwebTree:  69%|   | 7429/10788 [03:16<01:39, 33.63it/s]Training CobwebTree:  69%|   | 7433/10788 [03:16<01:38, 33.98it/s]Training CobwebTree:  69%|   | 7437/10788 [03:16<01:38, 33.89it/s]Training CobwebTree:  69%|   | 7441/10788 [03:17<01:41, 33.03it/s]Training CobwebTree:  69%|   | 7445/10788 [03:17<01:37, 34.45it/s]Training CobwebTree:  69%|   | 7449/10788 [03:17<01:36, 34.62it/s]Training CobwebTree:  69%|   | 7453/10788 [03:17<01:38, 33.91it/s]Training CobwebTree:  69%|   | 7457/10788 [03:17<01:45, 31.52it/s]Training CobwebTree:  69%|   | 7461/10788 [03:17<01:42, 32.52it/s]Training CobwebTree:  69%|   | 7465/10788 [03:17<01:40, 32.92it/s]Training CobwebTree:  69%|   | 7469/10788 [03:17<01:44, 31.74it/s]Training CobwebTree:  69%|   | 7473/10788 [03:18<01:41, 32.52it/s]Training CobwebTree:  69%|   | 7477/10788 [03:18<01:41, 32.74it/s]Training CobwebTree:  69%|   | 7481/10788 [03:18<01:41, 32.52it/s]Training CobwebTree:  69%|   | 7485/10788 [03:18<01:42, 32.07it/s]Training CobwebTree:  69%|   | 7489/10788 [03:18<01:41, 32.42it/s]Training CobwebTree:  69%|   | 7493/10788 [03:18<01:41, 32.35it/s]Training CobwebTree:  69%|   | 7497/10788 [03:18<01:41, 32.39it/s]Training CobwebTree:  70%|   | 7501/10788 [03:18<01:44, 31.58it/s]Training CobwebTree:  70%|   | 7505/10788 [03:19<01:45, 31.01it/s]Training CobwebTree:  70%|   | 7509/10788 [03:19<01:41, 32.33it/s]Training CobwebTree:  70%|   | 7513/10788 [03:19<01:42, 32.06it/s]Training CobwebTree:  70%|   | 7517/10788 [03:19<01:39, 33.04it/s]Training CobwebTree:  70%|   | 7521/10788 [03:19<01:41, 32.16it/s]Training CobwebTree:  70%|   | 7525/10788 [03:19<01:40, 32.58it/s]Training CobwebTree:  70%|   | 7529/10788 [03:19<01:37, 33.27it/s]Training CobwebTree:  70%|   | 7533/10788 [03:19<01:41, 32.01it/s]Training CobwebTree:  70%|   | 7537/10788 [03:20<01:36, 33.72it/s]Training CobwebTree:  70%|   | 7541/10788 [03:20<01:35, 34.12it/s]Training CobwebTree:  70%|   | 7545/10788 [03:20<01:42, 31.54it/s]Training CobwebTree:  70%|   | 7549/10788 [03:20<01:44, 30.92it/s]Training CobwebTree:  70%|   | 7553/10788 [03:20<01:39, 32.36it/s]Training CobwebTree:  70%|   | 7557/10788 [03:20<01:43, 31.33it/s]Training CobwebTree:  70%|   | 7561/10788 [03:20<01:39, 32.58it/s]Training CobwebTree:  70%|   | 7565/10788 [03:20<01:34, 33.96it/s]Training CobwebTree:  70%|   | 7569/10788 [03:21<01:35, 33.80it/s]Training CobwebTree:  70%|   | 7573/10788 [03:21<01:31, 34.97it/s]Training CobwebTree:  70%|   | 7577/10788 [03:21<01:32, 34.62it/s]Training CobwebTree:  70%|   | 7581/10788 [03:21<01:32, 34.65it/s]Training CobwebTree:  70%|   | 7585/10788 [03:21<01:36, 33.06it/s]Training CobwebTree:  70%|   | 7589/10788 [03:21<01:34, 33.73it/s]Training CobwebTree:  70%|   | 7593/10788 [03:21<01:34, 33.98it/s]Training CobwebTree:  70%|   | 7598/10788 [03:21<01:29, 35.81it/s]Training CobwebTree:  70%|   | 7602/10788 [03:21<01:29, 35.72it/s]Training CobwebTree:  71%|   | 7606/10788 [03:22<01:30, 35.14it/s]Training CobwebTree:  71%|   | 7610/10788 [03:22<01:35, 33.17it/s]Training CobwebTree:  71%|   | 7614/10788 [03:22<01:38, 32.19it/s]Training CobwebTree:  71%|   | 7618/10788 [03:22<01:40, 31.53it/s]Training CobwebTree:  71%|   | 7622/10788 [03:22<01:36, 32.87it/s]Training CobwebTree:  71%|   | 7626/10788 [03:22<01:37, 32.36it/s]Training CobwebTree:  71%|   | 7630/10788 [03:22<01:34, 33.55it/s]Training CobwebTree:  71%|   | 7634/10788 [03:22<01:35, 32.92it/s]Training CobwebTree:  71%|   | 7638/10788 [03:23<01:36, 32.60it/s]Training CobwebTree:  71%|   | 7642/10788 [03:23<01:31, 34.28it/s]Training CobwebTree:  71%|   | 7646/10788 [03:23<01:29, 35.10it/s]Training CobwebTree:  71%|   | 7650/10788 [03:23<01:32, 33.76it/s]Training CobwebTree:  71%|   | 7654/10788 [03:23<01:35, 32.90it/s]Training CobwebTree:  71%|   | 7658/10788 [03:23<01:33, 33.58it/s]Training CobwebTree:  71%|   | 7662/10788 [03:23<01:32, 33.65it/s]Training CobwebTree:  71%|   | 7666/10788 [03:23<01:32, 33.85it/s]Training CobwebTree:  71%|   | 7670/10788 [03:23<01:31, 34.09it/s]Training CobwebTree:  71%|   | 7674/10788 [03:24<01:29, 34.66it/s]Training CobwebTree:  71%|   | 7678/10788 [03:24<01:29, 34.77it/s]Training CobwebTree:  71%|   | 7682/10788 [03:24<01:26, 36.00it/s]Training CobwebTree:  71%|   | 7686/10788 [03:24<01:25, 36.25it/s]Training CobwebTree:  71%|  | 7690/10788 [03:24<01:27, 35.35it/s]Training CobwebTree:  71%|  | 7694/10788 [03:24<01:27, 35.29it/s]Training CobwebTree:  71%|  | 7698/10788 [03:24<01:31, 33.70it/s]Training CobwebTree:  71%|  | 7702/10788 [03:24<01:30, 34.29it/s]Training CobwebTree:  71%|  | 7706/10788 [03:25<01:27, 35.04it/s]Training CobwebTree:  71%|  | 7710/10788 [03:25<01:27, 35.15it/s]Training CobwebTree:  72%|  | 7714/10788 [03:25<01:29, 34.38it/s]Training CobwebTree:  72%|  | 7718/10788 [03:25<01:25, 35.89it/s]Training CobwebTree:  72%|  | 7722/10788 [03:25<01:26, 35.58it/s]Training CobwebTree:  72%|  | 7726/10788 [03:25<01:28, 34.69it/s]Training CobwebTree:  72%|  | 7730/10788 [03:25<01:29, 34.35it/s]Training CobwebTree:  72%|  | 7734/10788 [03:25<01:32, 33.19it/s]Training CobwebTree:  72%|  | 7738/10788 [03:25<01:31, 33.28it/s]Training CobwebTree:  72%|  | 7742/10788 [03:26<01:30, 33.73it/s]Training CobwebTree:  72%|  | 7746/10788 [03:26<01:33, 32.58it/s]Training CobwebTree:  72%|  | 7750/10788 [03:26<01:33, 32.59it/s]Training CobwebTree:  72%|  | 7754/10788 [03:26<01:32, 32.81it/s]Training CobwebTree:  72%|  | 7758/10788 [03:26<01:29, 33.82it/s]Training CobwebTree:  72%|  | 7762/10788 [03:26<01:29, 33.81it/s]Training CobwebTree:  72%|  | 7766/10788 [03:26<01:26, 35.03it/s]Training CobwebTree:  72%|  | 7770/10788 [03:26<01:28, 34.21it/s]Training CobwebTree:  72%|  | 7774/10788 [03:27<01:31, 33.06it/s]Training CobwebTree:  72%|  | 7778/10788 [03:27<01:33, 32.09it/s]Training CobwebTree:  72%|  | 7782/10788 [03:27<01:30, 33.31it/s]Training CobwebTree:  72%|  | 7786/10788 [03:27<01:28, 33.74it/s]Training CobwebTree:  72%|  | 7790/10788 [03:27<01:28, 33.83it/s]Training CobwebTree:  72%|  | 7794/10788 [03:27<01:26, 34.45it/s]Training CobwebTree:  72%|  | 7798/10788 [03:27<01:26, 34.65it/s]Training CobwebTree:  72%|  | 7802/10788 [03:27<01:27, 33.95it/s]Training CobwebTree:  72%|  | 7806/10788 [03:27<01:28, 33.85it/s]Training CobwebTree:  72%|  | 7810/10788 [03:28<01:38, 30.17it/s]Training CobwebTree:  72%|  | 7815/10788 [03:28<01:30, 32.73it/s]Training CobwebTree:  72%|  | 7819/10788 [03:28<01:27, 33.86it/s]Training CobwebTree:  73%|  | 7823/10788 [03:28<01:28, 33.46it/s]Training CobwebTree:  73%|  | 7827/10788 [03:28<01:31, 32.37it/s]Training CobwebTree:  73%|  | 7831/10788 [03:28<01:30, 32.68it/s]Training CobwebTree:  73%|  | 7835/10788 [03:28<01:34, 31.38it/s]Training CobwebTree:  73%|  | 7839/10788 [03:29<01:34, 31.14it/s]Training CobwebTree:  73%|  | 7843/10788 [03:29<01:35, 30.79it/s]Training CobwebTree:  73%|  | 7847/10788 [03:29<01:31, 32.03it/s]Training CobwebTree:  73%|  | 7851/10788 [03:29<01:31, 32.00it/s]Training CobwebTree:  73%|  | 7855/10788 [03:29<01:30, 32.31it/s]Training CobwebTree:  73%|  | 7860/10788 [03:29<01:27, 33.40it/s]Training CobwebTree:  73%|  | 7864/10788 [03:29<01:24, 34.53it/s]Training CobwebTree:  73%|  | 7868/10788 [03:29<01:25, 34.15it/s]Training CobwebTree:  73%|  | 7872/10788 [03:30<01:28, 32.91it/s]Training CobwebTree:  73%|  | 7876/10788 [03:30<01:29, 32.40it/s]Training CobwebTree:  73%|  | 7881/10788 [03:30<01:23, 34.62it/s]Training CobwebTree:  73%|  | 7885/10788 [03:30<01:23, 34.89it/s]Training CobwebTree:  73%|  | 7889/10788 [03:30<01:24, 34.24it/s]Training CobwebTree:  73%|  | 7893/10788 [03:30<01:26, 33.42it/s]Training CobwebTree:  73%|  | 7897/10788 [03:30<01:22, 34.90it/s]Training CobwebTree:  73%|  | 7901/10788 [03:30<01:25, 33.72it/s]Training CobwebTree:  73%|  | 7905/10788 [03:30<01:23, 34.42it/s]Training CobwebTree:  73%|  | 7909/10788 [03:31<01:26, 33.32it/s]Training CobwebTree:  73%|  | 7913/10788 [03:31<01:25, 33.70it/s]Training CobwebTree:  73%|  | 7917/10788 [03:31<01:24, 33.84it/s]Training CobwebTree:  73%|  | 7921/10788 [03:31<01:22, 34.56it/s]Training CobwebTree:  73%|  | 7925/10788 [03:31<01:21, 35.12it/s]Training CobwebTree:  73%|  | 7929/10788 [03:31<01:24, 33.67it/s]Training CobwebTree:  74%|  | 7933/10788 [03:31<01:24, 33.69it/s]Training CobwebTree:  74%|  | 7937/10788 [03:31<01:26, 32.98it/s]Training CobwebTree:  74%|  | 7941/10788 [03:32<01:27, 32.42it/s]Training CobwebTree:  74%|  | 7945/10788 [03:32<01:26, 33.00it/s]Training CobwebTree:  74%|  | 7949/10788 [03:32<01:25, 33.35it/s]Training CobwebTree:  74%|  | 7953/10788 [03:32<01:28, 32.16it/s]Training CobwebTree:  74%|  | 7957/10788 [03:32<01:24, 33.52it/s]Training CobwebTree:  74%|  | 7961/10788 [03:32<01:23, 33.66it/s]Training CobwebTree:  74%|  | 7965/10788 [03:32<01:26, 32.75it/s]Training CobwebTree:  74%|  | 7969/10788 [03:32<01:25, 32.93it/s]Training CobwebTree:  74%|  | 7973/10788 [03:33<01:29, 31.51it/s]Training CobwebTree:  74%|  | 7977/10788 [03:33<01:30, 30.98it/s]Training CobwebTree:  74%|  | 7981/10788 [03:33<01:29, 31.51it/s]Training CobwebTree:  74%|  | 7985/10788 [03:33<01:28, 31.72it/s]Training CobwebTree:  74%|  | 7989/10788 [03:33<01:28, 31.53it/s]Training CobwebTree:  74%|  | 7993/10788 [03:33<01:26, 32.16it/s]Training CobwebTree:  74%|  | 7997/10788 [03:33<01:24, 33.11it/s]Training CobwebTree:  74%|  | 8001/10788 [03:33<01:21, 34.14it/s]Training CobwebTree:  74%|  | 8005/10788 [03:34<01:20, 34.50it/s]Training CobwebTree:  74%|  | 8009/10788 [03:34<01:23, 33.20it/s]Training CobwebTree:  74%|  | 8013/10788 [03:34<01:22, 33.72it/s]Training CobwebTree:  74%|  | 8017/10788 [03:34<01:24, 32.98it/s]Training CobwebTree:  74%|  | 8021/10788 [03:34<01:23, 33.12it/s]Training CobwebTree:  74%|  | 8025/10788 [03:34<01:19, 34.55it/s]Training CobwebTree:  74%|  | 8029/10788 [03:34<01:21, 33.84it/s]Training CobwebTree:  74%|  | 8033/10788 [03:34<01:24, 32.70it/s]Training CobwebTree:  74%|  | 8037/10788 [03:34<01:22, 33.52it/s]Training CobwebTree:  75%|  | 8041/10788 [03:35<01:19, 34.57it/s]Training CobwebTree:  75%|  | 8045/10788 [03:35<01:17, 35.24it/s]Training CobwebTree:  75%|  | 8049/10788 [03:35<01:17, 35.32it/s]Training CobwebTree:  75%|  | 8053/10788 [03:35<01:21, 33.68it/s]Training CobwebTree:  75%|  | 8057/10788 [03:35<01:19, 34.48it/s]Training CobwebTree:  75%|  | 8062/10788 [03:35<01:16, 35.87it/s]Training CobwebTree:  75%|  | 8066/10788 [03:35<01:18, 34.77it/s]Training CobwebTree:  75%|  | 8070/10788 [03:35<01:24, 32.29it/s]Training CobwebTree:  75%|  | 8074/10788 [03:36<01:26, 31.32it/s]Training CobwebTree:  75%|  | 8078/10788 [03:36<01:26, 31.46it/s]Training CobwebTree:  75%|  | 8082/10788 [03:36<01:25, 31.72it/s]Training CobwebTree:  75%|  | 8086/10788 [03:36<01:23, 32.27it/s]Training CobwebTree:  75%|  | 8090/10788 [03:36<01:19, 33.75it/s]Training CobwebTree:  75%|  | 8094/10788 [03:36<01:18, 34.44it/s]Training CobwebTree:  75%|  | 8098/10788 [03:36<01:19, 33.75it/s]Training CobwebTree:  75%|  | 8102/10788 [03:36<01:19, 33.93it/s]Training CobwebTree:  75%|  | 8106/10788 [03:37<01:16, 35.27it/s]Training CobwebTree:  75%|  | 8110/10788 [03:37<01:19, 33.88it/s]Training CobwebTree:  75%|  | 8114/10788 [03:37<01:19, 33.68it/s]Training CobwebTree:  75%|  | 8118/10788 [03:37<01:18, 33.82it/s]Training CobwebTree:  75%|  | 8122/10788 [03:37<01:18, 33.76it/s]Training CobwebTree:  75%|  | 8126/10788 [03:37<01:16, 34.64it/s]Training CobwebTree:  75%|  | 8130/10788 [03:37<01:20, 32.94it/s]Training CobwebTree:  75%|  | 8134/10788 [03:37<01:19, 33.33it/s]Training CobwebTree:  75%|  | 8138/10788 [03:37<01:19, 33.39it/s]Training CobwebTree:  75%|  | 8142/10788 [03:38<01:19, 33.46it/s]Training CobwebTree:  76%|  | 8146/10788 [03:38<01:17, 33.88it/s]Training CobwebTree:  76%|  | 8150/10788 [03:38<01:17, 33.86it/s]Training CobwebTree:  76%|  | 8154/10788 [03:38<01:15, 34.93it/s]Training CobwebTree:  76%|  | 8158/10788 [03:38<01:16, 34.18it/s]Training CobwebTree:  76%|  | 8162/10788 [03:38<01:18, 33.51it/s]Training CobwebTree:  76%|  | 8166/10788 [03:38<01:19, 33.06it/s]Training CobwebTree:  76%|  | 8170/10788 [03:38<01:19, 32.81it/s]Training CobwebTree:  76%|  | 8174/10788 [03:39<01:21, 32.06it/s]Training CobwebTree:  76%|  | 8178/10788 [03:39<01:18, 33.37it/s]Training CobwebTree:  76%|  | 8182/10788 [03:39<01:21, 32.07it/s]Training CobwebTree:  76%|  | 8186/10788 [03:39<01:21, 32.07it/s]Training CobwebTree:  76%|  | 8190/10788 [03:39<01:20, 32.11it/s]Training CobwebTree:  76%|  | 8194/10788 [03:39<01:21, 31.80it/s]Training CobwebTree:  76%|  | 8198/10788 [03:39<01:20, 32.00it/s]Training CobwebTree:  76%|  | 8202/10788 [03:39<01:17, 33.21it/s]Training CobwebTree:  76%|  | 8206/10788 [03:40<01:18, 32.82it/s]Training CobwebTree:  76%|  | 8210/10788 [03:40<01:15, 34.26it/s]Training CobwebTree:  76%|  | 8214/10788 [03:40<01:15, 34.10it/s]Training CobwebTree:  76%|  | 8218/10788 [03:40<01:14, 34.28it/s]Training CobwebTree:  76%|  | 8222/10788 [03:40<01:16, 33.41it/s]Training CobwebTree:  76%|  | 8226/10788 [03:40<01:17, 33.03it/s]Training CobwebTree:  76%|  | 8230/10788 [03:40<01:16, 33.33it/s]Training CobwebTree:  76%|  | 8234/10788 [03:40<01:16, 33.43it/s]Training CobwebTree:  76%|  | 8238/10788 [03:40<01:19, 31.89it/s]Training CobwebTree:  76%|  | 8242/10788 [03:41<01:19, 31.97it/s]Training CobwebTree:  76%|  | 8246/10788 [03:41<01:17, 32.93it/s]Training CobwebTree:  76%|  | 8250/10788 [03:41<01:17, 32.88it/s]Training CobwebTree:  77%|  | 8254/10788 [03:41<01:17, 32.77it/s]Training CobwebTree:  77%|  | 8258/10788 [03:41<01:20, 31.34it/s]Training CobwebTree:  77%|  | 8262/10788 [03:41<01:16, 33.00it/s]Training CobwebTree:  77%|  | 8266/10788 [03:41<01:15, 33.28it/s]Training CobwebTree:  77%|  | 8270/10788 [03:41<01:14, 33.97it/s]Training CobwebTree:  77%|  | 8274/10788 [03:42<01:16, 32.85it/s]Training CobwebTree:  77%|  | 8278/10788 [03:42<01:12, 34.55it/s]Training CobwebTree:  77%|  | 8282/10788 [03:42<01:14, 33.61it/s]Training CobwebTree:  77%|  | 8286/10788 [03:42<01:16, 32.62it/s]Training CobwebTree:  77%|  | 8290/10788 [03:42<01:18, 31.70it/s]Training CobwebTree:  77%|  | 8294/10788 [03:42<01:19, 31.27it/s]Training CobwebTree:  77%|  | 8298/10788 [03:42<01:19, 31.34it/s]Training CobwebTree:  77%|  | 8302/10788 [03:42<01:17, 32.08it/s]Training CobwebTree:  77%|  | 8306/10788 [03:43<01:15, 32.72it/s]Training CobwebTree:  77%|  | 8310/10788 [03:43<01:22, 29.92it/s]Training CobwebTree:  77%|  | 8314/10788 [03:43<01:22, 29.84it/s]Training CobwebTree:  77%|  | 8318/10788 [03:43<01:20, 30.67it/s]Training CobwebTree:  77%|  | 8322/10788 [03:43<01:18, 31.22it/s]Training CobwebTree:  77%|  | 8326/10788 [03:43<01:18, 31.56it/s]Training CobwebTree:  77%|  | 8330/10788 [03:43<01:19, 30.85it/s]Training CobwebTree:  77%|  | 8334/10788 [03:44<01:18, 31.07it/s]Training CobwebTree:  77%|  | 8338/10788 [03:44<01:14, 32.74it/s]Training CobwebTree:  77%|  | 8342/10788 [03:44<01:14, 32.61it/s]Training CobwebTree:  77%|  | 8346/10788 [03:44<01:12, 33.71it/s]Training CobwebTree:  77%|  | 8350/10788 [03:44<01:14, 32.93it/s]Training CobwebTree:  77%|  | 8354/10788 [03:44<01:15, 32.24it/s]Training CobwebTree:  77%|  | 8358/10788 [03:44<01:14, 32.48it/s]Training CobwebTree:  78%|  | 8362/10788 [03:44<01:16, 31.81it/s]Training CobwebTree:  78%|  | 8366/10788 [03:44<01:19, 30.33it/s]Training CobwebTree:  78%|  | 8370/10788 [03:45<01:20, 30.11it/s]Training CobwebTree:  78%|  | 8374/10788 [03:45<01:19, 30.31it/s]Training CobwebTree:  78%|  | 8378/10788 [03:45<01:16, 31.52it/s]Training CobwebTree:  78%|  | 8382/10788 [03:45<01:18, 30.61it/s]Training CobwebTree:  78%|  | 8386/10788 [03:45<01:18, 30.74it/s]Training CobwebTree:  78%|  | 8390/10788 [03:45<01:15, 31.90it/s]Training CobwebTree:  78%|  | 8394/10788 [03:45<01:17, 30.72it/s]Training CobwebTree:  78%|  | 8398/10788 [03:46<01:18, 30.45it/s]Training CobwebTree:  78%|  | 8402/10788 [03:46<01:16, 31.08it/s]Training CobwebTree:  78%|  | 8406/10788 [03:46<01:16, 30.98it/s]Training CobwebTree:  78%|  | 8410/10788 [03:46<01:14, 31.97it/s]Training CobwebTree:  78%|  | 8414/10788 [03:46<01:16, 31.15it/s]Training CobwebTree:  78%|  | 8418/10788 [03:46<01:16, 31.11it/s]Training CobwebTree:  78%|  | 8422/10788 [03:46<01:12, 32.73it/s]Training CobwebTree:  78%|  | 8426/10788 [03:46<01:11, 33.14it/s]Training CobwebTree:  78%|  | 8430/10788 [03:47<01:14, 31.64it/s]Training CobwebTree:  78%|  | 8434/10788 [03:47<01:12, 32.48it/s]Training CobwebTree:  78%|  | 8438/10788 [03:47<01:10, 33.18it/s]Training CobwebTree:  78%|  | 8442/10788 [03:47<01:09, 33.99it/s]Training CobwebTree:  78%|  | 8446/10788 [03:47<01:05, 35.49it/s]Training CobwebTree:  78%|  | 8450/10788 [03:47<01:05, 35.51it/s]Training CobwebTree:  78%|  | 8454/10788 [03:47<01:08, 34.21it/s]Training CobwebTree:  78%|  | 8458/10788 [03:47<01:07, 34.53it/s]Training CobwebTree:  78%|  | 8462/10788 [03:47<01:06, 35.14it/s]Training CobwebTree:  78%|  | 8466/10788 [03:48<01:08, 33.90it/s]Training CobwebTree:  79%|  | 8470/10788 [03:48<01:07, 34.35it/s]Training CobwebTree:  79%|  | 8474/10788 [03:48<01:07, 34.15it/s]Training CobwebTree:  79%|  | 8478/10788 [03:48<01:06, 34.52it/s]Training CobwebTree:  79%|  | 8482/10788 [03:48<01:08, 33.77it/s]Training CobwebTree:  79%|  | 8486/10788 [03:48<01:09, 33.33it/s]Training CobwebTree:  79%|  | 8490/10788 [03:48<01:08, 33.77it/s]Training CobwebTree:  79%|  | 8494/10788 [03:48<01:09, 33.11it/s]Training CobwebTree:  79%|  | 8498/10788 [03:49<01:07, 34.08it/s]Training CobwebTree:  79%|  | 8502/10788 [03:49<01:09, 33.09it/s]Training CobwebTree:  79%|  | 8506/10788 [03:49<01:05, 34.58it/s]Training CobwebTree:  79%|  | 8510/10788 [03:49<01:08, 33.43it/s]Training CobwebTree:  79%|  | 8514/10788 [03:49<01:12, 31.33it/s]Training CobwebTree:  79%|  | 8518/10788 [03:49<01:09, 32.77it/s]Training CobwebTree:  79%|  | 8522/10788 [03:49<01:07, 33.67it/s]Training CobwebTree:  79%|  | 8526/10788 [03:49<01:05, 34.72it/s]Training CobwebTree:  79%|  | 8530/10788 [03:49<01:07, 33.63it/s]Training CobwebTree:  79%|  | 8534/10788 [03:50<01:08, 32.91it/s]Training CobwebTree:  79%|  | 8538/10788 [03:50<01:07, 33.11it/s]Training CobwebTree:  79%|  | 8542/10788 [03:50<01:06, 33.90it/s]Training CobwebTree:  79%|  | 8546/10788 [03:50<01:03, 35.23it/s]Training CobwebTree:  79%|  | 8550/10788 [03:50<01:06, 33.54it/s]Training CobwebTree:  79%|  | 8554/10788 [03:50<01:04, 34.76it/s]Training CobwebTree:  79%|  | 8558/10788 [03:50<01:03, 34.96it/s]Training CobwebTree:  79%|  | 8562/10788 [03:50<01:02, 35.63it/s]Training CobwebTree:  79%|  | 8566/10788 [03:51<01:02, 35.63it/s]Training CobwebTree:  79%|  | 8570/10788 [03:51<01:05, 33.87it/s]Training CobwebTree:  79%|  | 8574/10788 [03:51<01:05, 33.78it/s]Training CobwebTree:  80%|  | 8578/10788 [03:51<01:05, 33.91it/s]Training CobwebTree:  80%|  | 8582/10788 [03:51<01:04, 34.47it/s]Training CobwebTree:  80%|  | 8586/10788 [03:51<01:03, 34.71it/s]Training CobwebTree:  80%|  | 8590/10788 [03:51<01:06, 33.01it/s]Training CobwebTree:  80%|  | 8594/10788 [03:51<01:06, 32.99it/s]Training CobwebTree:  80%|  | 8598/10788 [03:51<01:05, 33.26it/s]Training CobwebTree:  80%|  | 8602/10788 [03:52<01:05, 33.51it/s]Training CobwebTree:  80%|  | 8606/10788 [03:52<01:04, 33.93it/s]Training CobwebTree:  80%|  | 8610/10788 [03:52<01:07, 32.35it/s]Training CobwebTree:  80%|  | 8614/10788 [03:52<01:08, 31.85it/s]Training CobwebTree:  80%|  | 8618/10788 [03:52<01:05, 32.94it/s]Training CobwebTree:  80%|  | 8622/10788 [03:52<01:07, 32.25it/s]Training CobwebTree:  80%|  | 8626/10788 [03:52<01:05, 32.98it/s]Training CobwebTree:  80%|  | 8630/10788 [03:52<01:05, 32.86it/s]Training CobwebTree:  80%|  | 8634/10788 [03:53<01:03, 34.06it/s]Training CobwebTree:  80%|  | 8638/10788 [03:53<01:05, 32.77it/s]Training CobwebTree:  80%|  | 8642/10788 [03:53<01:03, 33.61it/s]Training CobwebTree:  80%|  | 8646/10788 [03:53<01:05, 32.52it/s]Training CobwebTree:  80%|  | 8650/10788 [03:53<01:04, 33.16it/s]Training CobwebTree:  80%|  | 8654/10788 [03:53<01:05, 32.82it/s]Training CobwebTree:  80%|  | 8658/10788 [03:53<01:04, 33.12it/s]Training CobwebTree:  80%|  | 8662/10788 [03:53<01:04, 32.94it/s]Training CobwebTree:  80%|  | 8666/10788 [03:54<01:05, 32.49it/s]Training CobwebTree:  80%|  | 8670/10788 [03:54<01:03, 33.42it/s]Training CobwebTree:  80%|  | 8674/10788 [03:54<01:01, 34.14it/s]Training CobwebTree:  80%|  | 8678/10788 [03:54<01:01, 34.17it/s]Training CobwebTree:  80%|  | 8682/10788 [03:54<01:01, 34.37it/s]Training CobwebTree:  81%|  | 8686/10788 [03:54<01:02, 33.76it/s]Training CobwebTree:  81%|  | 8690/10788 [03:54<01:00, 34.72it/s]Training CobwebTree:  81%|  | 8694/10788 [03:54<01:00, 34.76it/s]Training CobwebTree:  81%|  | 8698/10788 [03:54<01:02, 33.25it/s]Training CobwebTree:  81%|  | 8702/10788 [03:55<01:00, 34.67it/s]Training CobwebTree:  81%|  | 8706/10788 [03:55<00:59, 34.86it/s]Training CobwebTree:  81%|  | 8710/10788 [03:55<00:59, 34.81it/s]Training CobwebTree:  81%|  | 8714/10788 [03:55<00:57, 36.12it/s]Training CobwebTree:  81%|  | 8718/10788 [03:55<00:57, 35.70it/s]Training CobwebTree:  81%|  | 8722/10788 [03:55<00:59, 34.57it/s]Training CobwebTree:  81%|  | 8726/10788 [03:55<01:01, 33.75it/s]Training CobwebTree:  81%|  | 8730/10788 [03:55<01:01, 33.71it/s]Training CobwebTree:  81%|  | 8734/10788 [03:56<01:00, 34.23it/s]Training CobwebTree:  81%|  | 8738/10788 [03:56<01:02, 32.60it/s]Training CobwebTree:  81%|  | 8742/10788 [03:56<01:03, 32.29it/s]Training CobwebTree:  81%|  | 8746/10788 [03:56<01:02, 32.87it/s]Training CobwebTree:  81%|  | 8750/10788 [03:56<01:00, 33.41it/s]Training CobwebTree:  81%|  | 8754/10788 [03:56<01:00, 33.53it/s]Training CobwebTree:  81%|  | 8758/10788 [03:56<01:04, 31.34it/s]Training CobwebTree:  81%|  | 8762/10788 [03:56<01:03, 31.77it/s]Training CobwebTree:  81%| | 8766/10788 [03:57<01:03, 31.64it/s]Training CobwebTree:  81%| | 8770/10788 [03:57<01:04, 31.28it/s]Training CobwebTree:  81%| | 8774/10788 [03:57<01:02, 32.20it/s]Training CobwebTree:  81%| | 8778/10788 [03:57<00:59, 34.01it/s]Training CobwebTree:  81%| | 8782/10788 [03:57<00:59, 33.98it/s]Training CobwebTree:  81%| | 8786/10788 [03:57<00:57, 34.73it/s]Training CobwebTree:  81%| | 8790/10788 [03:57<01:00, 33.17it/s]Training CobwebTree:  82%| | 8794/10788 [03:57<00:59, 33.64it/s]Training CobwebTree:  82%| | 8798/10788 [03:57<00:59, 33.55it/s]Training CobwebTree:  82%| | 8802/10788 [03:58<00:57, 34.40it/s]Training CobwebTree:  82%| | 8806/10788 [03:58<00:58, 33.77it/s]Training CobwebTree:  82%| | 8810/10788 [03:58<00:59, 33.33it/s]Training CobwebTree:  82%| | 8814/10788 [03:58<00:57, 34.19it/s]Training CobwebTree:  82%| | 8818/10788 [03:58<00:56, 34.69it/s]Training CobwebTree:  82%| | 8822/10788 [03:58<00:54, 35.83it/s]Training CobwebTree:  82%| | 8826/10788 [03:58<00:59, 32.84it/s]Training CobwebTree:  82%| | 8830/10788 [03:58<00:58, 33.38it/s]Training CobwebTree:  82%| | 8834/10788 [03:59<00:59, 32.98it/s]Training CobwebTree:  82%| | 8838/10788 [03:59<00:57, 33.84it/s]Training CobwebTree:  82%| | 8842/10788 [03:59<00:57, 33.98it/s]Training CobwebTree:  82%| | 8846/10788 [03:59<00:56, 34.24it/s]Training CobwebTree:  82%| | 8851/10788 [03:59<00:53, 36.36it/s]Training CobwebTree:  82%| | 8855/10788 [03:59<00:57, 33.52it/s]Training CobwebTree:  82%| | 8859/10788 [03:59<00:55, 34.84it/s]Training CobwebTree:  82%| | 8863/10788 [03:59<00:57, 33.76it/s]Training CobwebTree:  82%| | 8867/10788 [03:59<00:56, 34.14it/s]Training CobwebTree:  82%| | 8871/10788 [04:00<00:54, 34.91it/s]Training CobwebTree:  82%| | 8875/10788 [04:00<01:00, 31.56it/s]Training CobwebTree:  82%| | 8879/10788 [04:00<00:59, 32.13it/s]Training CobwebTree:  82%| | 8883/10788 [04:00<00:59, 32.16it/s]Training CobwebTree:  82%| | 8887/10788 [04:00<00:58, 32.53it/s]Training CobwebTree:  82%| | 8891/10788 [04:00<00:57, 32.96it/s]Training CobwebTree:  82%| | 8895/10788 [04:00<00:55, 33.89it/s]Training CobwebTree:  82%| | 8899/10788 [04:00<00:55, 34.19it/s]Training CobwebTree:  83%| | 8903/10788 [04:01<00:59, 31.80it/s]Training CobwebTree:  83%| | 8907/10788 [04:01<00:57, 32.87it/s]Training CobwebTree:  83%| | 8911/10788 [04:01<00:56, 33.12it/s]Training CobwebTree:  83%| | 8915/10788 [04:01<00:55, 33.93it/s]Training CobwebTree:  83%| | 8919/10788 [04:01<00:53, 35.11it/s]Training CobwebTree:  83%| | 8923/10788 [04:01<00:52, 35.44it/s]Training CobwebTree:  83%| | 8927/10788 [04:01<00:55, 33.80it/s]Training CobwebTree:  83%| | 8931/10788 [04:01<00:58, 31.97it/s]Training CobwebTree:  83%| | 8935/10788 [04:02<00:56, 32.73it/s]Training CobwebTree:  83%| | 8939/10788 [04:02<00:55, 33.15it/s]Training CobwebTree:  83%| | 8943/10788 [04:02<00:55, 33.17it/s]Training CobwebTree:  83%| | 8947/10788 [04:02<00:55, 32.92it/s]Training CobwebTree:  83%| | 8951/10788 [04:02<00:55, 32.90it/s]Training CobwebTree:  83%| | 8955/10788 [04:02<00:57, 32.06it/s]Training CobwebTree:  83%| | 8959/10788 [04:02<00:57, 31.55it/s]Training CobwebTree:  83%| | 8963/10788 [04:02<00:55, 33.11it/s]Training CobwebTree:  83%| | 8967/10788 [04:03<00:55, 32.94it/s]Training CobwebTree:  83%| | 8971/10788 [04:03<00:55, 32.90it/s]Training CobwebTree:  83%| | 8975/10788 [04:03<00:55, 32.77it/s]Training CobwebTree:  83%| | 8979/10788 [04:03<00:58, 31.13it/s]Training CobwebTree:  83%| | 8983/10788 [04:03<00:56, 31.90it/s]Training CobwebTree:  83%| | 8987/10788 [04:03<00:56, 31.90it/s]Training CobwebTree:  83%| | 8991/10788 [04:03<00:58, 30.81it/s]Training CobwebTree:  83%| | 8995/10788 [04:03<00:57, 30.98it/s]Training CobwebTree:  83%| | 8999/10788 [04:04<00:56, 31.49it/s]Training CobwebTree:  83%| | 9003/10788 [04:04<00:55, 32.07it/s]Training CobwebTree:  83%| | 9007/10788 [04:04<00:54, 32.94it/s]Training CobwebTree:  84%| | 9011/10788 [04:04<00:54, 32.50it/s]Training CobwebTree:  84%| | 9015/10788 [04:04<00:53, 33.32it/s]Training CobwebTree:  84%| | 9019/10788 [04:04<00:54, 32.41it/s]Training CobwebTree:  84%| | 9023/10788 [04:04<00:52, 33.87it/s]Training CobwebTree:  84%| | 9027/10788 [04:04<00:53, 33.16it/s]Training CobwebTree:  84%| | 9032/10788 [04:04<00:49, 35.12it/s]Training CobwebTree:  84%| | 9036/10788 [04:05<00:48, 35.91it/s]Training CobwebTree:  84%| | 9040/10788 [04:05<00:50, 34.46it/s]Training CobwebTree:  84%| | 9044/10788 [04:05<00:53, 32.89it/s]Training CobwebTree:  84%| | 9048/10788 [04:05<00:52, 32.86it/s]Training CobwebTree:  84%| | 9052/10788 [04:05<00:53, 32.34it/s]Training CobwebTree:  84%| | 9056/10788 [04:05<00:53, 32.44it/s]Training CobwebTree:  84%| | 9060/10788 [04:05<00:51, 33.41it/s]Training CobwebTree:  84%| | 9064/10788 [04:05<00:50, 34.17it/s]Training CobwebTree:  84%| | 9068/10788 [04:06<00:49, 34.88it/s]Training CobwebTree:  84%| | 9072/10788 [04:06<00:49, 34.63it/s]Training CobwebTree:  84%| | 9076/10788 [04:06<00:50, 34.19it/s]Training CobwebTree:  84%| | 9080/10788 [04:06<00:51, 33.34it/s]Training CobwebTree:  84%| | 9084/10788 [04:06<00:50, 33.88it/s]Training CobwebTree:  84%| | 9088/10788 [04:06<00:50, 33.63it/s]Training CobwebTree:  84%| | 9092/10788 [04:06<00:49, 34.56it/s]Training CobwebTree:  84%| | 9096/10788 [04:06<00:48, 34.53it/s]Training CobwebTree:  84%| | 9100/10788 [04:07<00:48, 34.57it/s]Training CobwebTree:  84%| | 9104/10788 [04:07<00:46, 35.95it/s]Training CobwebTree:  84%| | 9108/10788 [04:07<00:48, 34.45it/s]Training CobwebTree:  84%| | 9112/10788 [04:07<00:52, 31.94it/s]Training CobwebTree:  85%| | 9116/10788 [04:07<00:54, 30.59it/s]Training CobwebTree:  85%| | 9120/10788 [04:07<00:52, 31.62it/s]Training CobwebTree:  85%| | 9124/10788 [04:07<00:52, 31.89it/s]Training CobwebTree:  85%| | 9129/10788 [04:07<00:49, 33.85it/s]Training CobwebTree:  85%| | 9133/10788 [04:08<00:47, 34.84it/s]Training CobwebTree:  85%| | 9137/10788 [04:08<00:48, 34.35it/s]Training CobwebTree:  85%| | 9141/10788 [04:08<00:49, 32.98it/s]Training CobwebTree:  85%| | 9145/10788 [04:08<00:51, 31.76it/s]Training CobwebTree:  85%| | 9149/10788 [04:08<00:50, 32.64it/s]Training CobwebTree:  85%| | 9153/10788 [04:08<00:53, 30.37it/s]Training CobwebTree:  85%| | 9157/10788 [04:08<00:53, 30.74it/s]Training CobwebTree:  85%| | 9161/10788 [04:08<00:51, 31.55it/s]Training CobwebTree:  85%| | 9165/10788 [04:09<00:51, 31.52it/s]Training CobwebTree:  85%| | 9169/10788 [04:09<00:50, 32.16it/s]Training CobwebTree:  85%| | 9173/10788 [04:09<00:48, 33.04it/s]Training CobwebTree:  85%| | 9177/10788 [04:09<00:52, 30.95it/s]Training CobwebTree:  85%| | 9181/10788 [04:09<00:50, 31.78it/s]Training CobwebTree:  85%| | 9185/10788 [04:09<00:49, 32.38it/s]Training CobwebTree:  85%| | 9189/10788 [04:09<00:49, 32.44it/s]Training CobwebTree:  85%| | 9193/10788 [04:09<00:49, 32.40it/s]Training CobwebTree:  85%| | 9197/10788 [04:10<00:50, 31.81it/s]Training CobwebTree:  85%| | 9201/10788 [04:10<00:50, 31.36it/s]Training CobwebTree:  85%| | 9205/10788 [04:10<00:49, 32.28it/s]Training CobwebTree:  85%| | 9209/10788 [04:10<00:49, 32.02it/s]Training CobwebTree:  85%| | 9213/10788 [04:10<00:48, 32.68it/s]Training CobwebTree:  85%| | 9217/10788 [04:10<00:48, 32.34it/s]Training CobwebTree:  85%| | 9221/10788 [04:10<00:47, 33.10it/s]Training CobwebTree:  86%| | 9225/10788 [04:10<00:46, 33.95it/s]Training CobwebTree:  86%| | 9229/10788 [04:11<00:47, 32.93it/s]Training CobwebTree:  86%| | 9233/10788 [04:11<00:49, 31.53it/s]Training CobwebTree:  86%| | 9237/10788 [04:11<00:48, 31.87it/s]Training CobwebTree:  86%| | 9241/10788 [04:11<00:47, 32.44it/s]Training CobwebTree:  86%| | 9245/10788 [04:11<00:46, 32.94it/s]Training CobwebTree:  86%| | 9249/10788 [04:11<00:45, 34.10it/s]Training CobwebTree:  86%| | 9253/10788 [04:11<00:45, 33.89it/s]Training CobwebTree:  86%| | 9257/10788 [04:11<00:46, 33.02it/s]Training CobwebTree:  86%| | 9261/10788 [04:11<00:46, 32.72it/s]Training CobwebTree:  86%| | 9265/10788 [04:12<00:45, 33.27it/s]Training CobwebTree:  86%| | 9269/10788 [04:12<00:46, 32.70it/s]Training CobwebTree:  86%| | 9273/10788 [04:12<00:45, 32.95it/s]Training CobwebTree:  86%| | 9277/10788 [04:12<00:45, 33.02it/s]Training CobwebTree:  86%| | 9281/10788 [04:12<00:46, 32.56it/s]Training CobwebTree:  86%| | 9285/10788 [04:12<00:45, 32.99it/s]Training CobwebTree:  86%| | 9289/10788 [04:12<00:46, 32.34it/s]Training CobwebTree:  86%| | 9293/10788 [04:12<00:46, 32.34it/s]Training CobwebTree:  86%| | 9297/10788 [04:13<00:47, 31.39it/s]Training CobwebTree:  86%| | 9301/10788 [04:13<00:44, 33.39it/s]Training CobwebTree:  86%| | 9305/10788 [04:13<00:42, 34.63it/s]Training CobwebTree:  86%| | 9309/10788 [04:13<00:42, 34.97it/s]Training CobwebTree:  86%| | 9313/10788 [04:13<00:40, 36.32it/s]Training CobwebTree:  86%| | 9317/10788 [04:13<00:39, 36.93it/s]Training CobwebTree:  86%| | 9321/10788 [04:13<00:40, 36.05it/s]Training CobwebTree:  86%| | 9325/10788 [04:13<00:43, 33.94it/s]Training CobwebTree:  86%| | 9329/10788 [04:13<00:44, 33.02it/s]Training CobwebTree:  87%| | 9333/10788 [04:14<00:46, 31.40it/s]Training CobwebTree:  87%| | 9338/10788 [04:14<00:42, 34.23it/s]Training CobwebTree:  87%| | 9342/10788 [04:14<00:43, 33.54it/s]Training CobwebTree:  87%| | 9346/10788 [04:14<00:45, 31.92it/s]Training CobwebTree:  87%| | 9350/10788 [04:14<00:43, 33.05it/s]Training CobwebTree:  87%| | 9354/10788 [04:14<00:42, 34.09it/s]Training CobwebTree:  87%| | 9358/10788 [04:14<00:41, 34.28it/s]Training CobwebTree:  87%| | 9362/10788 [04:14<00:42, 33.41it/s]Training CobwebTree:  87%| | 9366/10788 [04:15<00:42, 33.38it/s]Training CobwebTree:  87%| | 9370/10788 [04:15<00:41, 34.19it/s]Training CobwebTree:  87%| | 9374/10788 [04:15<00:40, 35.18it/s]Training CobwebTree:  87%| | 9378/10788 [04:15<00:40, 35.13it/s]Training CobwebTree:  87%| | 9382/10788 [04:15<00:41, 34.05it/s]Training CobwebTree:  87%| | 9386/10788 [04:15<00:42, 32.83it/s]Training CobwebTree:  87%| | 9390/10788 [04:15<00:41, 33.35it/s]Training CobwebTree:  87%| | 9394/10788 [04:15<00:42, 32.51it/s]Training CobwebTree:  87%| | 9398/10788 [04:16<00:46, 30.09it/s]Training CobwebTree:  87%| | 9402/10788 [04:16<00:47, 29.31it/s]Training CobwebTree:  87%| | 9406/10788 [04:16<00:46, 29.49it/s]Training CobwebTree:  87%| | 9410/10788 [04:16<00:44, 31.09it/s]Training CobwebTree:  87%| | 9414/10788 [04:16<00:42, 32.03it/s]Training CobwebTree:  87%| | 9418/10788 [04:16<00:42, 31.91it/s]Training CobwebTree:  87%| | 9422/10788 [04:16<00:43, 31.62it/s]Training CobwebTree:  87%| | 9426/10788 [04:16<00:42, 32.30it/s]Training CobwebTree:  87%| | 9430/10788 [04:17<00:39, 34.18it/s]Training CobwebTree:  87%| | 9434/10788 [04:17<00:39, 34.56it/s]Training CobwebTree:  87%| | 9438/10788 [04:17<00:39, 34.28it/s]Training CobwebTree:  88%| | 9442/10788 [04:17<00:38, 34.87it/s]Training CobwebTree:  88%| | 9446/10788 [04:17<00:37, 35.48it/s]Training CobwebTree:  88%| | 9450/10788 [04:17<00:37, 35.36it/s]Training CobwebTree:  88%| | 9454/10788 [04:17<00:37, 35.63it/s]Training CobwebTree:  88%| | 9458/10788 [04:17<00:38, 34.90it/s]Training CobwebTree:  88%| | 9462/10788 [04:17<00:38, 34.26it/s]Training CobwebTree:  88%| | 9466/10788 [04:18<00:38, 33.92it/s]Training CobwebTree:  88%| | 9470/10788 [04:18<00:39, 33.64it/s]Training CobwebTree:  88%| | 9474/10788 [04:18<00:39, 32.95it/s]Training CobwebTree:  88%| | 9478/10788 [04:18<00:41, 31.80it/s]Training CobwebTree:  88%| | 9482/10788 [04:18<00:40, 31.99it/s]Training CobwebTree:  88%| | 9486/10788 [04:18<00:41, 31.16it/s]Training CobwebTree:  88%| | 9490/10788 [04:18<00:40, 31.93it/s]Training CobwebTree:  88%| | 9494/10788 [04:18<00:40, 32.28it/s]Training CobwebTree:  88%| | 9498/10788 [04:19<00:39, 32.80it/s]Training CobwebTree:  88%| | 9502/10788 [04:19<00:38, 33.08it/s]Training CobwebTree:  88%| | 9506/10788 [04:19<00:38, 33.62it/s]Training CobwebTree:  88%| | 9511/10788 [04:19<00:36, 34.92it/s]Training CobwebTree:  88%| | 9515/10788 [04:19<00:38, 33.47it/s]Training CobwebTree:  88%| | 9519/10788 [04:19<00:38, 33.11it/s]Training CobwebTree:  88%| | 9523/10788 [04:19<00:36, 34.55it/s]Training CobwebTree:  88%| | 9527/10788 [04:19<00:36, 34.67it/s]Training CobwebTree:  88%| | 9532/10788 [04:20<00:34, 36.86it/s]Training CobwebTree:  88%| | 9536/10788 [04:20<00:35, 35.11it/s]Training CobwebTree:  88%| | 9540/10788 [04:20<00:37, 33.42it/s]Training CobwebTree:  88%| | 9544/10788 [04:20<00:39, 31.35it/s]Training CobwebTree:  89%| | 9548/10788 [04:20<00:38, 32.21it/s]Training CobwebTree:  89%| | 9552/10788 [04:20<00:39, 31.47it/s]Training CobwebTree:  89%| | 9556/10788 [04:20<00:37, 32.79it/s]Training CobwebTree:  89%| | 9560/10788 [04:20<00:36, 33.78it/s]Training CobwebTree:  89%| | 9564/10788 [04:21<00:36, 33.43it/s]Training CobwebTree:  89%| | 9569/10788 [04:21<00:33, 35.86it/s]Training CobwebTree:  89%| | 9573/10788 [04:21<00:36, 33.67it/s]Training CobwebTree:  89%| | 9577/10788 [04:21<00:35, 33.93it/s]Training CobwebTree:  89%| | 9581/10788 [04:21<00:36, 33.36it/s]Training CobwebTree:  89%| | 9585/10788 [04:21<00:34, 34.67it/s]Training CobwebTree:  89%| | 9589/10788 [04:21<00:35, 33.39it/s]Training CobwebTree:  89%| | 9593/10788 [04:21<00:37, 32.25it/s]Training CobwebTree:  89%| | 9597/10788 [04:22<00:36, 32.54it/s]Training CobwebTree:  89%| | 9601/10788 [04:22<00:37, 32.03it/s]Training CobwebTree:  89%| | 9605/10788 [04:22<00:36, 32.02it/s]Training CobwebTree:  89%| | 9609/10788 [04:22<00:38, 30.90it/s]Training CobwebTree:  89%| | 9613/10788 [04:22<00:37, 31.47it/s]Training CobwebTree:  89%| | 9617/10788 [04:22<00:36, 32.49it/s]Training CobwebTree:  89%| | 9621/10788 [04:22<00:34, 33.76it/s]Training CobwebTree:  89%| | 9625/10788 [04:22<00:35, 32.91it/s]Training CobwebTree:  89%| | 9629/10788 [04:23<00:34, 33.18it/s]Training CobwebTree:  89%| | 9633/10788 [04:23<00:33, 34.35it/s]Training CobwebTree:  89%| | 9637/10788 [04:23<00:34, 33.23it/s]Training CobwebTree:  89%| | 9641/10788 [04:23<00:35, 32.44it/s]Training CobwebTree:  89%| | 9645/10788 [04:23<00:35, 32.01it/s]Training CobwebTree:  89%| | 9649/10788 [04:23<00:36, 31.42it/s]Training CobwebTree:  89%| | 9653/10788 [04:23<00:36, 31.26it/s]Training CobwebTree:  90%| | 9657/10788 [04:23<00:35, 32.18it/s]Training CobwebTree:  90%| | 9661/10788 [04:24<00:37, 30.35it/s]Training CobwebTree:  90%| | 9665/10788 [04:24<00:36, 30.95it/s]Training CobwebTree:  90%| | 9669/10788 [04:24<00:34, 32.32it/s]Training CobwebTree:  90%| | 9673/10788 [04:24<00:34, 32.17it/s]Training CobwebTree:  90%| | 9677/10788 [04:24<00:34, 32.04it/s]Training CobwebTree:  90%| | 9681/10788 [04:24<00:33, 33.06it/s]Training CobwebTree:  90%| | 9685/10788 [04:24<00:32, 34.42it/s]Training CobwebTree:  90%| | 9689/10788 [04:24<00:32, 34.05it/s]Training CobwebTree:  90%| | 9693/10788 [04:25<00:32, 33.44it/s]Training CobwebTree:  90%| | 9697/10788 [04:25<00:31, 34.42it/s]Training CobwebTree:  90%| | 9701/10788 [04:25<00:32, 33.86it/s]Training CobwebTree:  90%| | 9705/10788 [04:25<00:33, 32.80it/s]Training CobwebTree:  90%| | 9709/10788 [04:25<00:33, 32.64it/s]Training CobwebTree:  90%| | 9713/10788 [04:25<00:31, 33.75it/s]Training CobwebTree:  90%| | 9717/10788 [04:25<00:32, 32.82it/s]Training CobwebTree:  90%| | 9721/10788 [04:25<00:33, 32.32it/s]Training CobwebTree:  90%| | 9725/10788 [04:25<00:32, 33.05it/s]Training CobwebTree:  90%| | 9729/10788 [04:26<00:32, 32.42it/s]Training CobwebTree:  90%| | 9733/10788 [04:26<00:33, 31.80it/s]Training CobwebTree:  90%| | 9737/10788 [04:26<00:31, 33.53it/s]Training CobwebTree:  90%| | 9741/10788 [04:26<00:31, 33.57it/s]Training CobwebTree:  90%| | 9745/10788 [04:26<00:31, 33.57it/s]Training CobwebTree:  90%| | 9749/10788 [04:26<00:32, 31.78it/s]Training CobwebTree:  90%| | 9753/10788 [04:26<00:33, 31.22it/s]Training CobwebTree:  90%| | 9757/10788 [04:26<00:32, 31.88it/s]Training CobwebTree:  90%| | 9761/10788 [04:27<00:31, 32.63it/s]Training CobwebTree:  91%| | 9765/10788 [04:27<00:31, 32.09it/s]Training CobwebTree:  91%| | 9769/10788 [04:27<00:32, 31.68it/s]Training CobwebTree:  91%| | 9773/10788 [04:27<00:31, 32.34it/s]Training CobwebTree:  91%| | 9777/10788 [04:27<00:31, 32.12it/s]Training CobwebTree:  91%| | 9781/10788 [04:27<00:30, 33.42it/s]Training CobwebTree:  91%| | 9785/10788 [04:27<00:31, 31.74it/s]Training CobwebTree:  91%| | 9789/10788 [04:27<00:31, 32.20it/s]Training CobwebTree:  91%| | 9793/10788 [04:28<00:31, 32.02it/s]Training CobwebTree:  91%| | 9797/10788 [04:28<00:31, 31.76it/s]Training CobwebTree:  91%| | 9801/10788 [04:28<00:31, 31.78it/s]Training CobwebTree:  91%| | 9805/10788 [04:28<00:29, 33.77it/s]Training CobwebTree:  91%| | 9809/10788 [04:28<00:28, 34.06it/s]Training CobwebTree:  91%| | 9813/10788 [04:28<00:28, 33.83it/s]Training CobwebTree:  91%| | 9817/10788 [04:28<00:28, 33.77it/s]Training CobwebTree:  91%| | 9821/10788 [04:28<00:29, 32.69it/s]Training CobwebTree:  91%| | 9825/10788 [04:29<00:30, 31.39it/s]Training CobwebTree:  91%| | 9829/10788 [04:29<00:30, 31.50it/s]Training CobwebTree:  91%| | 9833/10788 [04:29<00:29, 32.58it/s]Training CobwebTree:  91%| | 9837/10788 [04:29<00:29, 32.25it/s]Training CobwebTree:  91%| | 9841/10788 [04:29<00:29, 31.65it/s]Training CobwebTree:  91%|| 9845/10788 [04:29<00:29, 31.81it/s]Training CobwebTree:  91%|| 9849/10788 [04:29<00:28, 32.45it/s]Training CobwebTree:  91%|| 9853/10788 [04:29<00:28, 32.73it/s]Training CobwebTree:  91%|| 9857/10788 [04:30<00:28, 32.35it/s]Training CobwebTree:  91%|| 9861/10788 [04:30<00:28, 33.08it/s]Training CobwebTree:  91%|| 9865/10788 [04:30<00:28, 32.80it/s]Training CobwebTree:  91%|| 9869/10788 [04:30<00:27, 33.66it/s]Training CobwebTree:  92%|| 9873/10788 [04:30<00:28, 32.38it/s]Training CobwebTree:  92%|| 9877/10788 [04:30<00:27, 32.72it/s]Training CobwebTree:  92%|| 9881/10788 [04:30<00:27, 32.67it/s]Training CobwebTree:  92%|| 9885/10788 [04:30<00:26, 33.86it/s]Training CobwebTree:  92%|| 9889/10788 [04:31<00:26, 34.23it/s]Training CobwebTree:  92%|| 9893/10788 [04:31<00:27, 32.81it/s]Training CobwebTree:  92%|| 9897/10788 [04:31<00:26, 33.51it/s]Training CobwebTree:  92%|| 9901/10788 [04:31<00:27, 32.57it/s]Training CobwebTree:  92%|| 9905/10788 [04:31<00:26, 33.24it/s]Training CobwebTree:  92%|| 9909/10788 [04:31<00:26, 33.73it/s]Training CobwebTree:  92%|| 9913/10788 [04:31<00:25, 34.25it/s]Training CobwebTree:  92%|| 9917/10788 [04:31<00:26, 33.08it/s]Training CobwebTree:  92%|| 9921/10788 [04:31<00:26, 32.98it/s]Training CobwebTree:  92%|| 9925/10788 [04:32<00:25, 33.29it/s]Training CobwebTree:  92%|| 9929/10788 [04:32<00:26, 32.24it/s]Training CobwebTree:  92%|| 9933/10788 [04:32<00:25, 33.26it/s]Training CobwebTree:  92%|| 9937/10788 [04:32<00:25, 33.93it/s]Training CobwebTree:  92%|| 9941/10788 [04:32<00:26, 32.02it/s]Training CobwebTree:  92%|| 9945/10788 [04:32<00:25, 33.11it/s]Training CobwebTree:  92%|| 9949/10788 [04:32<00:26, 31.55it/s]Training CobwebTree:  92%|| 9953/10788 [04:32<00:25, 32.54it/s]Training CobwebTree:  92%|| 9957/10788 [04:33<00:25, 33.03it/s]Training CobwebTree:  92%|| 9961/10788 [04:33<00:25, 31.84it/s]Training CobwebTree:  92%|| 9965/10788 [04:33<00:24, 33.00it/s]Training CobwebTree:  92%|| 9969/10788 [04:33<00:25, 32.45it/s]Training CobwebTree:  92%|| 9973/10788 [04:33<00:24, 33.48it/s]Training CobwebTree:  92%|| 9977/10788 [04:33<00:23, 34.38it/s]Training CobwebTree:  93%|| 9981/10788 [04:33<00:22, 35.23it/s]Training CobwebTree:  93%|| 9985/10788 [04:33<00:23, 34.50it/s]Training CobwebTree:  93%|| 9989/10788 [04:34<00:23, 34.04it/s]Training CobwebTree:  93%|| 9993/10788 [04:34<00:22, 34.68it/s]Training CobwebTree:  93%|| 9997/10788 [04:34<00:23, 33.25it/s]Training CobwebTree:  93%|| 10001/10788 [04:34<00:23, 33.24it/s]Training CobwebTree:  93%|| 10005/10788 [04:34<00:22, 34.91it/s]Training CobwebTree:  93%|| 10009/10788 [04:34<00:22, 34.71it/s]Training CobwebTree:  93%|| 10013/10788 [04:34<00:21, 35.90it/s]Training CobwebTree:  93%|| 10017/10788 [04:34<00:22, 34.02it/s]Training CobwebTree:  93%|| 10021/10788 [04:34<00:23, 33.16it/s]Training CobwebTree:  93%|| 10025/10788 [04:35<00:22, 33.38it/s]Training CobwebTree:  93%|| 10029/10788 [04:35<00:23, 32.33it/s]Training CobwebTree:  93%|| 10033/10788 [04:35<00:22, 33.07it/s]Training CobwebTree:  93%|| 10037/10788 [04:35<00:23, 32.48it/s]Training CobwebTree:  93%|| 10041/10788 [04:35<00:23, 32.29it/s]Training CobwebTree:  93%|| 10046/10788 [04:35<00:20, 35.37it/s]Training CobwebTree:  93%|| 10050/10788 [04:35<00:20, 35.46it/s]Training CobwebTree:  93%|| 10054/10788 [04:35<00:20, 35.22it/s]Training CobwebTree:  93%|| 10058/10788 [04:36<00:20, 35.07it/s]Training CobwebTree:  93%|| 10062/10788 [04:36<00:21, 33.16it/s]Training CobwebTree:  93%|| 10066/10788 [04:36<00:22, 31.59it/s]Training CobwebTree:  93%|| 10070/10788 [04:36<00:22, 31.23it/s]Training CobwebTree:  93%|| 10074/10788 [04:36<00:22, 31.30it/s]Training CobwebTree:  93%|| 10078/10788 [04:36<00:22, 31.04it/s]Training CobwebTree:  93%|| 10082/10788 [04:36<00:22, 30.90it/s]Training CobwebTree:  93%|| 10086/10788 [04:36<00:21, 32.23it/s]Training CobwebTree:  94%|| 10090/10788 [04:37<00:20, 33.50it/s]Training CobwebTree:  94%|| 10094/10788 [04:37<00:19, 34.74it/s]Training CobwebTree:  94%|| 10098/10788 [04:37<00:20, 33.43it/s]Training CobwebTree:  94%|| 10102/10788 [04:37<00:21, 32.43it/s]Training CobwebTree:  94%|| 10106/10788 [04:37<00:20, 33.62it/s]Training CobwebTree:  94%|| 10110/10788 [04:37<00:20, 33.42it/s]Training CobwebTree:  94%|| 10114/10788 [04:37<00:20, 32.82it/s]Training CobwebTree:  94%|| 10118/10788 [04:37<00:20, 32.66it/s]Training CobwebTree:  94%|| 10122/10788 [04:38<00:21, 30.64it/s]Training CobwebTree:  94%|| 10126/10788 [04:38<00:21, 31.38it/s]Training CobwebTree:  94%|| 10130/10788 [04:38<00:21, 30.04it/s]Training CobwebTree:  94%|| 10134/10788 [04:38<00:21, 30.16it/s]Training CobwebTree:  94%|| 10138/10788 [04:38<00:21, 30.26it/s]Training CobwebTree:  94%|| 10142/10788 [04:38<00:21, 29.54it/s]Training CobwebTree:  94%|| 10146/10788 [04:38<00:21, 30.18it/s]Training CobwebTree:  94%|| 10150/10788 [04:38<00:21, 30.38it/s]Training CobwebTree:  94%|| 10154/10788 [04:39<00:20, 30.82it/s]Training CobwebTree:  94%|| 10158/10788 [04:39<00:19, 32.51it/s]Training CobwebTree:  94%|| 10162/10788 [04:39<00:19, 32.59it/s]Training CobwebTree:  94%|| 10166/10788 [04:39<00:19, 31.35it/s]Training CobwebTree:  94%|| 10170/10788 [04:39<00:19, 31.54it/s]Training CobwebTree:  94%|| 10174/10788 [04:39<00:19, 31.77it/s]Training CobwebTree:  94%|| 10178/10788 [04:39<00:18, 32.19it/s]Training CobwebTree:  94%|| 10182/10788 [04:39<00:18, 32.71it/s]Training CobwebTree:  94%|| 10186/10788 [04:40<00:18, 32.89it/s]Training CobwebTree:  94%|| 10190/10788 [04:40<00:18, 32.70it/s]Training CobwebTree:  94%|| 10194/10788 [04:40<00:18, 32.12it/s]Training CobwebTree:  95%|| 10198/10788 [04:40<00:18, 32.10it/s]Training CobwebTree:  95%|| 10202/10788 [04:40<00:18, 32.37it/s]Training CobwebTree:  95%|| 10206/10788 [04:40<00:18, 30.85it/s]Training CobwebTree:  95%|| 10210/10788 [04:40<00:18, 30.71it/s]Training CobwebTree:  95%|| 10214/10788 [04:41<00:19, 29.21it/s]Training CobwebTree:  95%|| 10218/10788 [04:41<00:18, 30.97it/s]Training CobwebTree:  95%|| 10222/10788 [04:41<00:18, 31.25it/s]Training CobwebTree:  95%|| 10226/10788 [04:41<00:17, 31.96it/s]Training CobwebTree:  95%|| 10230/10788 [04:41<00:16, 33.10it/s]Training CobwebTree:  95%|| 10234/10788 [04:41<00:16, 33.98it/s]Training CobwebTree:  95%|| 10238/10788 [04:41<00:16, 33.33it/s]Training CobwebTree:  95%|| 10242/10788 [04:41<00:16, 33.08it/s]Training CobwebTree:  95%|| 10246/10788 [04:41<00:16, 32.58it/s]Training CobwebTree:  95%|| 10250/10788 [04:42<00:15, 34.41it/s]Training CobwebTree:  95%|| 10254/10788 [04:42<00:15, 35.34it/s]Training CobwebTree:  95%|| 10258/10788 [04:42<00:15, 35.06it/s]Training CobwebTree:  95%|| 10262/10788 [04:42<00:15, 33.63it/s]Training CobwebTree:  95%|| 10266/10788 [04:42<00:15, 34.02it/s]Training CobwebTree:  95%|| 10270/10788 [04:42<00:15, 34.27it/s]Training CobwebTree:  95%|| 10274/10788 [04:42<00:14, 35.10it/s]Training CobwebTree:  95%|| 10278/10788 [04:42<00:15, 33.22it/s]Training CobwebTree:  95%|| 10282/10788 [04:43<00:16, 31.18it/s]Training CobwebTree:  95%|| 10286/10788 [04:43<00:15, 31.91it/s]Training CobwebTree:  95%|| 10290/10788 [04:43<00:15, 31.87it/s]Training CobwebTree:  95%|| 10294/10788 [04:43<00:14, 33.03it/s]Training CobwebTree:  95%|| 10299/10788 [04:43<00:13, 34.94it/s]Training CobwebTree:  96%|| 10303/10788 [04:43<00:14, 32.81it/s]Training CobwebTree:  96%|| 10307/10788 [04:43<00:14, 32.30it/s]Training CobwebTree:  96%|| 10311/10788 [04:43<00:14, 32.45it/s]Training CobwebTree:  96%|| 10315/10788 [04:44<00:14, 33.38it/s]Training CobwebTree:  96%|| 10319/10788 [04:44<00:14, 32.50it/s]Training CobwebTree:  96%|| 10323/10788 [04:44<00:14, 32.59it/s]Training CobwebTree:  96%|| 10327/10788 [04:44<00:13, 34.10it/s]Training CobwebTree:  96%|| 10331/10788 [04:44<00:13, 34.83it/s]Training CobwebTree:  96%|| 10335/10788 [04:44<00:13, 33.87it/s]Training CobwebTree:  96%|| 10339/10788 [04:44<00:13, 33.73it/s]Training CobwebTree:  96%|| 10343/10788 [04:44<00:13, 33.12it/s]Training CobwebTree:  96%|| 10347/10788 [04:44<00:13, 33.04it/s]Training CobwebTree:  96%|| 10351/10788 [04:45<00:12, 34.15it/s]Training CobwebTree:  96%|| 10355/10788 [04:45<00:12, 34.22it/s]Training CobwebTree:  96%|| 10359/10788 [04:45<00:12, 33.60it/s]Training CobwebTree:  96%|| 10363/10788 [04:45<00:13, 32.59it/s]Training CobwebTree:  96%|| 10367/10788 [04:45<00:13, 32.23it/s]Training CobwebTree:  96%|| 10371/10788 [04:45<00:13, 31.90it/s]Training CobwebTree:  96%|| 10375/10788 [04:45<00:13, 31.52it/s]Training CobwebTree:  96%|| 10379/10788 [04:45<00:13, 31.17it/s]Training CobwebTree:  96%|| 10383/10788 [04:46<00:12, 31.77it/s]Training CobwebTree:  96%|| 10387/10788 [04:46<00:12, 32.75it/s]Training CobwebTree:  96%|| 10391/10788 [04:46<00:11, 33.16it/s]Training CobwebTree:  96%|| 10395/10788 [04:46<00:12, 31.89it/s]Training CobwebTree:  96%|| 10399/10788 [04:46<00:11, 32.76it/s]Training CobwebTree:  96%|| 10403/10788 [04:46<00:11, 32.44it/s]Training CobwebTree:  96%|| 10407/10788 [04:46<00:11, 32.62it/s]Training CobwebTree:  97%|| 10411/10788 [04:46<00:11, 31.44it/s]Training CobwebTree:  97%|| 10415/10788 [04:47<00:11, 31.44it/s]Training CobwebTree:  97%|| 10419/10788 [04:47<00:11, 32.06it/s]Training CobwebTree:  97%|| 10423/10788 [04:47<00:11, 31.43it/s]Training CobwebTree:  97%|| 10427/10788 [04:47<00:10, 33.32it/s]Training CobwebTree:  97%|| 10431/10788 [04:47<00:10, 32.82it/s]Training CobwebTree:  97%|| 10435/10788 [04:47<00:10, 32.84it/s]Training CobwebTree:  97%|| 10439/10788 [04:47<00:11, 30.76it/s]Training CobwebTree:  97%|| 10443/10788 [04:47<00:11, 30.61it/s]Training CobwebTree:  97%|| 10447/10788 [04:48<00:10, 31.96it/s]Training CobwebTree:  97%|| 10451/10788 [04:48<00:10, 32.91it/s]Training CobwebTree:  97%|| 10455/10788 [04:48<00:09, 33.64it/s]Training CobwebTree:  97%|| 10459/10788 [04:48<00:09, 34.56it/s]Training CobwebTree:  97%|| 10463/10788 [04:48<00:10, 32.49it/s]Training CobwebTree:  97%|| 10467/10788 [04:48<00:09, 32.29it/s]Training CobwebTree:  97%|| 10471/10788 [04:48<00:09, 33.21it/s]Training CobwebTree:  97%|| 10475/10788 [04:48<00:09, 33.59it/s]Training CobwebTree:  97%|| 10479/10788 [04:49<00:09, 33.21it/s]Training CobwebTree:  97%|| 10483/10788 [04:49<00:09, 33.45it/s]Training CobwebTree:  97%|| 10487/10788 [04:49<00:08, 33.68it/s]Training CobwebTree:  97%|| 10491/10788 [04:49<00:08, 33.47it/s]Training CobwebTree:  97%|| 10495/10788 [04:49<00:08, 34.31it/s]Training CobwebTree:  97%|| 10499/10788 [04:49<00:08, 34.38it/s]Training CobwebTree:  97%|| 10503/10788 [04:49<00:08, 34.54it/s]Training CobwebTree:  97%|| 10507/10788 [04:49<00:08, 32.98it/s]Training CobwebTree:  97%|| 10511/10788 [04:50<00:08, 32.02it/s]Training CobwebTree:  97%|| 10515/10788 [04:50<00:08, 31.71it/s]Training CobwebTree:  98%|| 10519/10788 [04:50<00:08, 31.18it/s]Training CobwebTree:  98%|| 10524/10788 [04:50<00:07, 34.02it/s]Training CobwebTree:  98%|| 10528/10788 [04:50<00:07, 34.30it/s]Training CobwebTree:  98%|| 10532/10788 [04:50<00:07, 34.02it/s]Training CobwebTree:  98%|| 10536/10788 [04:50<00:07, 34.68it/s]Training CobwebTree:  98%|| 10540/10788 [04:50<00:07, 33.47it/s]Training CobwebTree:  98%|| 10544/10788 [04:51<00:07, 32.54it/s]Training CobwebTree:  98%|| 10548/10788 [04:51<00:07, 33.65it/s]Training CobwebTree:  98%|| 10552/10788 [04:51<00:06, 34.01it/s]Training CobwebTree:  98%|| 10556/10788 [04:51<00:06, 33.35it/s]Training CobwebTree:  98%|| 10560/10788 [04:51<00:07, 32.38it/s]Training CobwebTree:  98%|| 10564/10788 [04:51<00:06, 32.45it/s]Training CobwebTree:  98%|| 10568/10788 [04:51<00:06, 31.93it/s]Training CobwebTree:  98%|| 10572/10788 [04:51<00:06, 31.28it/s]Training CobwebTree:  98%|| 10576/10788 [04:51<00:06, 31.55it/s]Training CobwebTree:  98%|| 10580/10788 [04:52<00:06, 30.67it/s]Training CobwebTree:  98%|| 10584/10788 [04:52<00:06, 31.01it/s]Training CobwebTree:  98%|| 10588/10788 [04:52<00:06, 32.18it/s]Training CobwebTree:  98%|| 10592/10788 [04:52<00:05, 33.57it/s]Training CobwebTree:  98%|| 10596/10788 [04:52<00:05, 32.87it/s]Training CobwebTree:  98%|| 10600/10788 [04:52<00:05, 34.21it/s]Training CobwebTree:  98%|| 10604/10788 [04:52<00:05, 32.76it/s]Training CobwebTree:  98%|| 10608/10788 [04:52<00:05, 32.92it/s]Training CobwebTree:  98%|| 10612/10788 [04:53<00:05, 33.42it/s]Training CobwebTree:  98%|| 10616/10788 [04:53<00:05, 33.36it/s]Training CobwebTree:  98%|| 10620/10788 [04:53<00:05, 33.54it/s]Training CobwebTree:  98%|| 10624/10788 [04:53<00:04, 34.32it/s]Training CobwebTree:  99%|| 10628/10788 [04:53<00:04, 33.70it/s]Training CobwebTree:  99%|| 10632/10788 [04:53<00:04, 33.31it/s]Training CobwebTree:  99%|| 10636/10788 [04:53<00:04, 33.76it/s]Training CobwebTree:  99%|| 10640/10788 [04:53<00:04, 32.02it/s]Training CobwebTree:  99%|| 10644/10788 [04:54<00:04, 31.60it/s]Training CobwebTree:  99%|| 10648/10788 [04:54<00:04, 32.75it/s]Training CobwebTree:  99%|| 10652/10788 [04:54<00:04, 32.37it/s]Training CobwebTree:  99%|| 10656/10788 [04:54<00:04, 32.24it/s]Training CobwebTree:  99%|| 10660/10788 [04:54<00:03, 33.09it/s]Training CobwebTree:  99%|| 10664/10788 [04:54<00:03, 32.01it/s]Training CobwebTree:  99%|| 10668/10788 [04:54<00:03, 32.78it/s]Training CobwebTree:  99%|| 10672/10788 [04:54<00:03, 33.35it/s]Training CobwebTree:  99%|| 10677/10788 [04:55<00:03, 36.19it/s]Training CobwebTree:  99%|| 10681/10788 [04:55<00:03, 34.20it/s]Training CobwebTree:  99%|| 10685/10788 [04:55<00:03, 34.01it/s]Training CobwebTree:  99%|| 10689/10788 [04:55<00:03, 32.91it/s]Training CobwebTree:  99%|| 10693/10788 [04:55<00:02, 33.55it/s]Training CobwebTree:  99%|| 10697/10788 [04:55<00:02, 31.95it/s]Training CobwebTree:  99%|| 10701/10788 [04:55<00:02, 33.50it/s]Training CobwebTree:  99%|| 10705/10788 [04:55<00:02, 32.63it/s]Training CobwebTree:  99%|| 10709/10788 [04:56<00:02, 33.03it/s]Training CobwebTree:  99%|| 10713/10788 [04:56<00:02, 32.48it/s]Training CobwebTree:  99%|| 10717/10788 [04:56<00:02, 31.68it/s]Training CobwebTree:  99%|| 10721/10788 [04:56<00:01, 33.73it/s]Training CobwebTree:  99%|| 10725/10788 [04:56<00:01, 33.68it/s]Training CobwebTree:  99%|| 10729/10788 [04:56<00:01, 34.48it/s]Training CobwebTree:  99%|| 10733/10788 [04:56<00:01, 33.37it/s]Training CobwebTree: 100%|| 10737/10788 [04:56<00:01, 32.95it/s]Training CobwebTree: 100%|| 10741/10788 [04:56<00:01, 32.95it/s]Training CobwebTree: 100%|| 10745/10788 [04:57<00:01, 32.31it/s]Training CobwebTree: 100%|| 10749/10788 [04:57<00:01, 31.74it/s]Training CobwebTree: 100%|| 10753/10788 [04:57<00:01, 33.55it/s]Training CobwebTree: 100%|| 10757/10788 [04:57<00:00, 32.05it/s]Training CobwebTree: 100%|| 10761/10788 [04:57<00:00, 32.82it/s]Training CobwebTree: 100%|| 10765/10788 [04:57<00:00, 31.81it/s]Training CobwebTree: 100%|| 10769/10788 [04:57<00:00, 31.79it/s]Training CobwebTree: 100%|| 10773/10788 [04:57<00:00, 32.33it/s]Training CobwebTree: 100%|| 10777/10788 [04:58<00:00, 31.99it/s]Training CobwebTree: 100%|| 10781/10788 [04:58<00:00, 32.50it/s]Training CobwebTree: 100%|| 10785/10788 [04:58<00:00, 33.96it/s]Training CobwebTree: 100%|| 10788/10788 [04:58<00:00, 36.15it/s]
2025-12-23 00:37:14,505 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 00:37:18,040 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-23 00:37:18,053 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-23 00:37:18,062 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-23 00:37:18,083 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-23 00:37:18,243 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-23 00:37:18,288 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-23 00:37:18,350 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-23 00:37:19,759 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-23 00:37:19,911 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-23 00:37:20,234 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-23 00:37:20,237 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-23 00:37:20,390 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-23 00:37:21,090 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-23 00:37:21,490 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-23 00:37:23,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,253 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,269 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:23,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:23,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,079 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,144 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,155 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,171 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,239 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,481 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,718 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,785 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,910 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:24,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:24,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,016 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:25,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:25,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:25,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:25,395 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:25,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:25,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:25,774 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:25,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:26,070 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:26,081 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:26,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:26,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:26,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:26,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:27,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:27,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:27,139 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:27,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:27,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:27,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:27,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:27,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:27,240 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:27,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:27,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:27,260 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:27,359 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:27,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:28,821 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-23 00:37:29,094 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-23 00:37:30,019 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 00:37:33,170 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-23 00:37:33,173 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-23 00:37:33,174 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-23 00:37:33,177 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-23 00:37:33,179 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-23 00:37:33,182 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-23 00:37:33,184 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-23 00:37:33,186 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-23 00:37:33,189 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-23 00:37:33,192 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-23 00:37:33,194 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-23 00:37:33,197 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-23 00:37:33,199 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-23 00:37:33,202 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-23 00:37:33,204 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-23 00:37:33,206 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-23 00:37:33,208 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-23 00:37:33,211 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-23 00:37:33,213 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-23 00:37:33,215 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-23 00:37:33,217 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-23 00:37:33,219 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-23 00:37:33,223 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-23 00:37:33,226 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-23 00:37:33,230 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-23 00:37:33,232 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-23 00:37:33,235 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-23 00:37:33,237 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-23 00:37:33,240 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-23 00:37:33,243 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-23 00:37:33,245 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-23 00:37:33,248 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-23 00:37:33,250 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-23 00:37:33,252 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-23 00:37:33,254 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-23 00:37:33,257 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-23 00:37:33,260 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-23 00:37:33,263 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-23 00:37:33,265 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-23 00:37:33,268 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-23 00:37:33,270 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-23 00:37:33,272 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-23 00:37:33,274 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-23 00:37:33,275 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-23 00:37:33,277 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-23 00:37:33,280 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-23 00:37:33,282 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-23 00:37:33,284 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-23 00:37:33,286 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-23 00:37:33,288 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-23 00:37:33,290 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-23 00:37:33,293 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-23 00:37:33,295 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-23 00:37:33,297 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-23 00:37:33,300 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-23 00:37:33,375 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-23 00:37:33,378 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-23 00:37:33,409 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-23 00:37:33,411 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-23 00:37:33,429 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-23 00:37:33,431 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-23 00:37:33,433 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-23 00:37:33,435 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-23 00:37:33,456 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-23 00:37:33,469 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-23 00:37:33,485 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-23 00:37:33,500 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-23 00:37:33,576 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-23 00:37:33,578 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-23 00:37:33,580 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-23 00:37:33,676 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-23 00:37:33,679 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-23 00:37:33,681 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-23 00:37:33,704 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-23 00:37:33,745 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-23 00:37:33,801 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-23 00:37:33,813 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-23 00:37:33,877 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-23 00:37:33,889 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-23 00:37:33,945 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-23 00:37:33,985 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-23 00:37:34,012 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-23 00:37:34,064 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-23 00:37:34,080 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-23 00:37:34,129 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-23 00:37:34,148 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-23 00:37:34,193 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-23 00:37:34,217 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-23 00:37:34,269 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-23 00:37:34,317 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-23 00:37:34,345 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-23 00:37:34,397 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-23 00:37:34,445 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-23 00:37:34,469 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-23 00:37:34,512 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-23 00:37:34,537 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-23 00:37:34,588 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-23 00:37:34,605 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-23 00:37:34,657 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-23 00:37:34,705 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-23 00:37:34,728 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-23 00:37:34,778 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-23 00:37:34,865 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-23 00:37:34,868 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-23 00:37:34,973 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-23 00:37:34,975 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-23 00:37:34,977 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-23 00:37:35,052 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-23 00:37:35,055 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-23 00:37:35,061 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-23 00:37:35,173 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-23 00:37:35,175 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-23 00:37:35,177 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-23 00:37:35,208 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-23 00:37:35,313 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-23 00:37:35,315 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-23 00:37:35,317 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-23 00:37:35,425 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-23 00:37:35,430 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-23 00:37:35,513 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-23 00:37:35,517 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-23 00:37:35,599 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-23 00:37:35,601 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-23 00:37:35,604 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-23 00:37:35,606 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-23 00:37:35,628 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-23 00:37:35,764 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-23 00:37:35,767 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-23 00:37:35,769 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-23 00:37:35,781 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-23 00:37:35,813 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-23 00:37:35,845 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-23 00:37:35,881 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-23 00:37:35,924 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-23 00:37:35,957 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-23 00:37:36,065 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-23 00:37:36,067 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-23 00:37:36,069 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-23 00:37:36,153 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-23 00:37:36,155 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-23 00:37:36,229 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-23 00:37:36,231 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-23 00:37:36,236 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-23 00:37:36,265 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-23 00:37:36,305 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-23 00:37:36,400 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-23 00:37:36,402 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-23 00:37:36,404 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-23 00:37:36,484 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-23 00:37:36,488 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-23 00:37:36,513 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-23 00:37:36,548 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-23 00:37:36,640 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-23 00:37:36,643 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-23 00:37:36,645 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-23 00:37:36,685 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-23 00:37:36,717 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-23 00:37:36,765 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-23 00:37:36,809 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-23 00:37:36,841 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-23 00:37:36,877 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-23 00:37:36,991 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-23 00:37:36,993 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-23 00:37:36,995 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-23 00:37:37,000 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-23 00:37:37,045 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-23 00:37:37,069 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-23 00:37:37,128 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-23 00:37:37,156 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-23 00:37:37,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,423 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,453 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,975 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:37,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:37,997 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,117 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,420 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,530 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,555 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,627 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,682 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,715 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,698 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,765 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,766 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,758 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,867 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:38,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:38,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,003 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,133 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,532 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:39,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 00:37:39,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 00:37:42,033 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-23 00:37:42,202 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-23 00:37:42,831 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.6531240577773645, 'coherence_npmi': 0.16911447252736764, 'topic_diversity': 0.6195121951219512, 'inter_topic_similarity': 0.4969416558742523}
2025-12-23 00:37:42,831 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6618886241093723, 'coherence_npmi': 0.16574415307361756, 'topic_diversity': 0.554, 'inter_topic_similarity': 0.5136188864707947}
2025-12-23 00:37:42,831 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6141344406464962, 'coherence_npmi': 0.12920022805297127, 'topic_diversity': 0.5019230769230769, 'inter_topic_similarity': 0.42815566062927246}
2025-12-23 00:37:42,831 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model HDBSCAN for hierarchical conversion
2025-12-23 00:37:54,207 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model KMeans for hierarchical conversion
2025-12-23 00:38:05,221 INFO src.utils.hierarchical_utils: Reusing already-fitted BERTopic model BERTopicCobwebWrapper for hierarchical conversion
2025-12-23 00:38:20,139 INFO __main__: Hierarchical Model 0 (HDBSCAN) metrics: {'hier_coherence_npmi': 0.15856990766554724, 'hier_topic_uniqueness': 0.6893001382320137, 'hier_topic_diversity': 0.6893001382320137, 'hier_topic_specialization': 0.32533369066066836, 'hier_affinity_child': 0.8374383449554443, 'hier_affinity_non_child': 0.307855486869812, 'hier_coherence_clnpmi': 0.010489890312148684, 'hier_PC_TD': 0.23321339330334837, 'hier_PnonC_TD': 0.7302944211395479, 'hier_sibling_TD': 0.9190311986863712, 'hier_sibling_clnpmi': 0.08125958540145037}
2025-12-23 00:38:20,139 INFO __main__: Hierarchical Model 1 (KMeans) metrics: {'hier_coherence_npmi': 0.1648606907761457, 'hier_topic_uniqueness': 0.7373315508021391, 'hier_topic_diversity': 0.7373315508021391, 'hier_topic_specialization': 0.3139707185463209, 'hier_affinity_child': 0.8575320839881897, 'hier_affinity_non_child': 0.31092706322669983, 'hier_coherence_clnpmi': -0.02014213259225455, 'hier_PC_TD': 0.15418024132730013, 'hier_PnonC_TD': 0.7074881238175856, 'hier_sibling_TD': 0.8943725490196078, 'hier_sibling_clnpmi': 0.08828036121393304}
2025-12-23 00:38:20,139 INFO __main__: Hierarchical Model 2 (BERTopicCobwebWrapper) metrics: {'hier_coherence_npmi': 0.1254623262019132, 'hier_topic_uniqueness': 0.7486929736929737, 'hier_topic_diversity': 0.7486929736929737, 'hier_topic_specialization': 0.3459712623504996, 'hier_affinity_child': 0.8632500171661377, 'hier_affinity_non_child': 0.22436431050300598, 'hier_coherence_clnpmi': 0.024962965320535043, 'hier_PC_TD': 0.19130584515199903, 'hier_PnonC_TD': 0.7687159819356179, 'hier_sibling_TD': 0.9358346781423705, 'hier_sibling_clnpmi': 0.08166956954265224}
