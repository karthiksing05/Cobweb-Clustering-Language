2025-12-20 23:05:36,777 INFO __main__: Starting benchmark for dataset=reuters
2025-12-20 23:05:43,500 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-20 23:05:43,847 INFO gensim.corpora.dictionary: adding document #10000 to Dictionary<29631 unique tokens: ['10', '15', '17', '1985', '30']...>
2025-12-20 23:05:43,876 INFO gensim.corpora.dictionary: built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)
2025-12-20 23:05:43,881 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)", 'datetime': '2025-12-20T23:05:43.876482', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-192-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-20 23:05:45,853 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-20 23:05:45,853 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-20 23:05:51,600 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10788 docs
2025-12-20 23:07:42,425 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:07:49,384 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-20 23:07:49,392 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-20 23:07:49,398 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-20 23:07:49,412 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-20 23:07:49,463 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-20 23:07:49,469 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-20 23:07:49,491 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-20 23:07:50,384 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-20 23:07:50,717 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-20 23:07:50,996 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-20 23:07:51,161 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-20 23:07:51,289 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-20 23:07:52,144 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-20 23:07:52,502 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-20 23:07:52,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,714 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,717 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,718 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,721 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,722 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,722 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,722 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,722 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,725 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,760 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,769 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,773 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,777 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,781 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,797 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,797 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,801 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,801 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,801 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,805 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,805 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,805 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,809 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,809 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,809 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,813 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,813 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,813 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,817 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,817 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,817 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,817 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,833 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,833 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,833 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,837 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,837 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,837 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,845 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,849 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,849 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,849 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,853 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,857 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,861 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,861 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,865 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:52,961 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:52,969 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,001 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,036 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,111 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,124 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,135 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,149 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,162 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,237 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,241 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,277 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,287 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,365 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,473 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,549 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,794 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,845 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,865 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,901 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,921 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:53,923 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:53,929 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,017 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,025 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,044 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,134 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,136 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,148 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,169 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,202 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,253 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,254 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,260 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,265 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,341 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,385 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,419 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,513 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,549 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,581 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,661 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,704 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,809 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,858 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,933 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:54,948 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:54,969 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,013 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,040 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,080 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,119 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,177 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,254 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,297 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,365 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,430 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,463 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,501 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,505 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,689 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,741 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,777 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,840 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,849 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:55,881 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:55,901 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:56,035 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:56,057 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:56,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:56,285 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:56,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:56,373 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:57,050 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:57,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:58,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:58,157 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:58,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:58,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:58,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:58,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:58,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:58,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:07:58,394 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:07:58,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:01,441 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:08:01,760 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-20 23:08:02,721 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:08:10,098 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-20 23:08:10,100 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-20 23:08:10,101 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-20 23:08:10,102 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-20 23:08:10,103 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-20 23:08:10,104 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-20 23:08:10,105 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-20 23:08:10,106 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-20 23:08:10,108 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-20 23:08:10,110 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-20 23:08:10,112 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-20 23:08:10,114 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-20 23:08:10,115 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-20 23:08:10,117 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-20 23:08:10,118 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-20 23:08:10,120 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-20 23:08:10,121 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-20 23:08:10,123 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-20 23:08:10,124 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-20 23:08:10,126 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-20 23:08:10,127 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-20 23:08:10,129 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-20 23:08:10,131 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-20 23:08:10,132 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-20 23:08:10,134 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-20 23:08:10,136 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-20 23:08:10,137 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-20 23:08:10,138 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-20 23:08:10,140 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-20 23:08:10,142 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-20 23:08:10,143 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-20 23:08:10,144 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-20 23:08:10,146 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-20 23:08:10,147 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-20 23:08:10,148 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-20 23:08:10,150 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-20 23:08:10,243 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-20 23:08:10,244 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-20 23:08:10,246 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-20 23:08:10,247 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-20 23:08:10,254 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-20 23:08:10,427 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-20 23:08:10,429 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-20 23:08:10,430 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-20 23:08:10,521 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-20 23:08:10,523 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-20 23:08:10,526 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-20 23:08:10,613 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-20 23:08:10,615 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-20 23:08:10,617 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-20 23:08:10,700 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-20 23:08:10,703 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-20 23:08:10,705 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-20 23:08:10,773 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-20 23:08:10,775 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-20 23:08:10,777 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-20 23:08:10,865 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-20 23:08:10,867 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-20 23:08:10,869 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-20 23:08:10,948 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-20 23:08:10,950 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-20 23:08:10,952 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-20 23:08:11,025 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-20 23:08:11,027 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-20 23:08:11,029 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-20 23:08:11,096 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-20 23:08:11,099 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-20 23:08:11,100 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-20 23:08:11,105 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-20 23:08:11,193 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-20 23:08:11,195 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-20 23:08:11,197 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-20 23:08:11,259 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-20 23:08:11,262 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-20 23:08:11,264 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-20 23:08:11,325 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-20 23:08:11,339 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-20 23:08:11,341 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-20 23:08:11,445 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-20 23:08:11,447 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-20 23:08:11,449 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-20 23:08:11,450 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-20 23:08:11,452 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-20 23:08:11,517 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-20 23:08:11,531 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-20 23:08:11,537 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-20 23:08:11,609 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-20 23:08:11,623 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-20 23:08:11,686 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-20 23:08:11,699 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-20 23:08:11,765 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-20 23:08:11,779 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-20 23:08:11,829 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-20 23:08:11,831 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-20 23:08:11,833 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-20 23:08:11,901 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-20 23:08:11,915 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-20 23:08:11,917 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-20 23:08:11,969 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-20 23:08:11,972 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-20 23:08:11,974 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-20 23:08:11,975 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-20 23:08:12,028 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-20 23:08:12,080 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-20 23:08:12,082 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-20 23:08:12,084 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-20 23:08:12,086 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-20 23:08:12,141 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-20 23:08:12,155 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-20 23:08:12,161 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-20 23:08:12,225 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-20 23:08:12,239 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-20 23:08:12,286 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-20 23:08:12,299 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-20 23:08:12,305 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-20 23:08:12,357 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-20 23:08:12,359 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-20 23:08:12,361 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-20 23:08:12,398 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-20 23:08:12,411 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-20 23:08:12,473 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-20 23:08:12,475 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-20 23:08:12,477 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-20 23:08:12,513 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-20 23:08:12,527 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-20 23:08:12,564 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-20 23:08:12,609 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-20 23:08:12,623 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-20 23:08:12,639 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-20 23:08:12,701 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-20 23:08:12,703 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-20 23:08:12,705 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-20 23:08:12,757 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-20 23:08:12,771 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-20 23:08:12,773 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-20 23:08:12,847 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-20 23:08:12,850 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-20 23:08:12,851 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-20 23:08:12,857 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-20 23:08:12,859 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-20 23:08:12,865 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-20 23:08:12,867 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-20 23:08:12,890 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-20 23:08:12,902 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-20 23:08:12,910 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-20 23:08:12,993 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-20 23:08:13,006 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-20 23:08:13,008 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-20 23:08:13,049 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-20 23:08:13,063 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-20 23:08:13,069 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-20 23:08:13,116 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-20 23:08:13,152 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-20 23:08:13,153 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-20 23:08:13,156 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-20 23:08:13,158 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-20 23:08:13,253 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-20 23:08:13,255 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-20 23:08:13,257 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-20 23:08:13,309 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-20 23:08:13,311 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-20 23:08:13,313 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-20 23:08:13,314 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-20 23:08:13,381 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-20 23:08:13,383 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-20 23:08:13,385 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-20 23:08:13,469 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-20 23:08:13,483 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-20 23:08:13,484 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-20 23:08:13,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,500 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,504 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,505 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,505 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,505 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,505 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,510 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,511 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,512 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,512 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,512 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,512 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,541 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,549 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,549 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,549 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,553 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,553 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,553 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,557 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,557 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,557 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,558 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,565 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,565 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,565 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,569 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,569 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,573 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,573 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,573 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,573 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,581 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,581 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,581 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,582 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,585 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,585 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,585 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,585 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,589 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,589 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,589 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,593 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,593 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,593 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,597 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,597 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,597 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,601 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,629 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,629 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,769 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,773 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,893 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:13,994 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:13,996 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,034 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,122 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,141 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,249 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,145 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,273 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,281 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,291 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,300 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,509 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,667 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,720 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,778 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,869 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:14,897 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,909 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,965 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,069 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:14,949 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,105 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,109 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,126 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,193 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,193 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,320 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,333 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,431 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,490 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,541 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,682 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,685 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,877 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,925 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:15,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:08:15,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:08:20,895 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:08:21,136 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-20 23:08:21,582 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10788 docs
2025-12-20 23:09:55,742 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:10:02,923 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-20 23:10:02,932 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-20 23:10:02,936 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-20 23:10:02,949 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-20 23:10:02,987 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-20 23:10:02,993 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-20 23:10:03,225 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-20 23:10:04,445 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-20 23:10:04,545 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-20 23:10:04,583 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-20 23:10:04,607 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-20 23:10:04,743 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-20 23:10:05,216 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-20 23:10:05,435 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-20 23:10:05,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,638 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,640 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,644 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,649 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,652 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,655 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,656 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,661 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,677 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,681 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,681 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,701 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,701 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,701 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,701 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,705 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,705 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,705 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,705 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,709 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,709 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,709 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,709 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,709 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,729 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,729 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,729 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,741 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,741 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,741 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,741 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,745 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,753 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,753 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,753 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,753 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,761 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,761 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,761 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,761 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,761 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,765 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,765 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,769 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,773 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,777 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,777 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,781 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,797 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,798 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,859 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:05,913 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,921 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,929 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,945 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:05,985 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,122 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,152 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,154 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,161 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,173 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,173 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,249 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,261 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,273 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,301 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,353 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,507 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,537 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,597 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,703 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,729 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,785 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:06,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,873 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:06,905 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,212 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,293 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,533 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,585 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,812 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,903 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:07,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:07,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:12,682 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:10:12,858 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-20 23:10:13,650 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:10:21,154 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-20 23:10:21,156 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-20 23:10:21,157 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-20 23:10:21,159 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-20 23:10:21,161 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-20 23:10:21,163 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-20 23:10:21,164 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-20 23:10:21,166 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-20 23:10:21,168 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-20 23:10:21,169 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-20 23:10:21,171 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-20 23:10:21,173 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-20 23:10:21,175 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-20 23:10:21,176 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-20 23:10:21,177 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-20 23:10:21,179 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-20 23:10:21,180 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-20 23:10:21,182 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-20 23:10:21,183 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-20 23:10:21,185 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-20 23:10:21,186 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-20 23:10:21,188 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-20 23:10:21,190 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-20 23:10:21,192 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-20 23:10:21,193 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-20 23:10:21,195 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-20 23:10:21,196 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-20 23:10:21,198 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-20 23:10:21,215 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-20 23:10:21,216 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-20 23:10:21,217 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-20 23:10:21,219 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-20 23:10:21,221 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-20 23:10:21,222 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-20 23:10:21,224 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-20 23:10:21,225 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-20 23:10:21,227 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-20 23:10:21,228 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-20 23:10:21,230 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-20 23:10:21,231 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-20 23:10:21,233 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-20 23:10:21,246 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-20 23:10:21,248 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-20 23:10:21,249 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-20 23:10:21,251 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-20 23:10:21,252 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-20 23:10:21,254 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-20 23:10:21,266 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-20 23:10:21,268 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-20 23:10:21,270 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-20 23:10:21,272 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-20 23:10:21,274 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-20 23:10:21,275 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-20 23:10:21,277 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-20 23:10:21,279 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-20 23:10:21,285 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-20 23:10:21,287 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-20 23:10:21,288 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-20 23:10:21,290 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-20 23:10:21,292 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-20 23:10:21,293 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-20 23:10:21,294 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-20 23:10:21,296 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-20 23:10:21,297 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-20 23:10:21,299 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-20 23:10:21,300 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-20 23:10:21,302 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-20 23:10:21,303 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-20 23:10:21,304 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-20 23:10:21,306 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-20 23:10:21,307 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-20 23:10:21,309 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-20 23:10:21,310 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-20 23:10:21,311 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-20 23:10:21,313 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-20 23:10:21,314 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-20 23:10:21,316 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-20 23:10:21,317 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-20 23:10:21,319 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-20 23:10:21,320 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-20 23:10:21,322 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-20 23:10:21,323 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-20 23:10:21,324 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-20 23:10:21,326 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-20 23:10:21,327 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-20 23:10:21,328 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-20 23:10:21,330 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-20 23:10:21,332 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-20 23:10:21,333 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-20 23:10:21,335 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-20 23:10:21,337 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-20 23:10:21,338 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-20 23:10:21,340 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-20 23:10:21,342 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-20 23:10:21,343 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-20 23:10:21,345 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-20 23:10:21,346 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-20 23:10:21,348 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-20 23:10:21,349 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-20 23:10:21,351 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-20 23:10:21,352 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-20 23:10:21,370 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-20 23:10:21,371 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-20 23:10:21,373 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-20 23:10:21,675 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-20 23:10:21,676 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-20 23:10:21,678 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-20 23:10:21,830 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-20 23:10:21,833 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-20 23:10:21,834 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-20 23:10:21,963 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-20 23:10:21,999 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-20 23:10:22,090 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-20 23:10:22,114 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-20 23:10:22,243 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-20 23:10:22,269 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-20 23:10:22,283 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-20 23:10:22,389 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-20 23:10:22,403 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-20 23:10:22,521 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-20 23:10:22,535 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-20 23:10:22,641 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-20 23:10:22,643 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-20 23:10:22,659 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-20 23:10:22,811 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-20 23:10:22,814 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-20 23:10:22,953 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-20 23:10:22,967 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-20 23:10:22,968 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-20 23:10:23,097 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-20 23:10:23,099 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-20 23:10:23,101 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-20 23:10:23,233 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-20 23:10:23,247 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-20 23:10:23,356 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-20 23:10:23,358 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-20 23:10:23,360 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-20 23:10:23,362 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-20 23:10:23,509 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-20 23:10:23,511 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-20 23:10:23,513 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-20 23:10:23,601 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-20 23:10:23,603 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-20 23:10:23,605 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-20 23:10:23,693 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-20 23:10:23,695 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-20 23:10:23,696 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-20 23:10:23,698 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-20 23:10:23,781 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-20 23:10:23,795 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-20 23:10:23,797 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-20 23:10:23,871 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-20 23:10:23,874 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-20 23:10:23,876 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-20 23:10:23,944 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-20 23:10:23,957 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-20 23:10:23,961 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-20 23:10:24,024 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-20 23:10:24,060 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-20 23:10:24,062 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-20 23:10:24,064 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-20 23:10:24,069 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-20 23:10:24,161 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-20 23:10:24,175 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-20 23:10:24,212 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-20 23:10:24,214 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-20 23:10:24,217 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-20 23:10:24,264 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-20 23:10:24,309 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-20 23:10:24,323 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,325 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,326 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,326 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,327 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,327 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,330 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,330 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,332 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,334 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,335 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,345 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,346 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,353 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,357 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,365 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,373 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,373 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,373 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,373 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,381 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,381 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,385 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,385 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,385 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,401 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,406 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,409 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,413 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,413 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,413 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,441 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,450 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,457 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,457 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,469 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,581 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,581 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,681 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,717 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,845 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,852 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,857 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,877 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,893 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,901 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,932 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,963 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:24,973 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,985 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:24,985 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,021 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,049 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,065 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,089 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,129 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,245 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,255 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,296 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,341 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,345 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,449 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,514 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,525 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,537 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,684 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,805 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,922 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:25,973 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:25,993 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,001 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,029 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,115 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,170 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,285 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,377 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,421 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,422 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:26,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:10:26,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:10:31,263 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:10:31,390 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-20 23:10:31,712 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10788 docs
Training CobwebTree:   0%|          | 0/10788 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 25/10788 [00:00<00:47, 228.91it/s]Training CobwebTree:   0%|          | 48/10788 [00:00<01:11, 150.20it/s]Training CobwebTree:   1%|          | 65/10788 [00:00<01:17, 138.30it/s]Training CobwebTree:   1%|          | 80/10788 [00:00<01:24, 126.73it/s]Training CobwebTree:   1%|          | 94/10788 [00:00<01:33, 114.01it/s]Training CobwebTree:   1%|          | 106/10788 [00:00<01:37, 109.54it/s]Training CobwebTree:   1%|          | 118/10788 [00:00<01:40, 105.83it/s]Training CobwebTree:   1%|          | 129/10788 [00:01<01:46, 99.94it/s] Training CobwebTree:   1%|         | 140/10788 [00:01<01:47, 99.13it/s]Training CobwebTree:   1%|         | 150/10788 [00:01<01:54, 92.85it/s]Training CobwebTree:   1%|         | 160/10788 [00:01<01:56, 90.99it/s]Training CobwebTree:   2%|         | 170/10788 [00:01<02:01, 87.61it/s]Training CobwebTree:   2%|         | 179/10788 [00:01<02:02, 86.58it/s]Training CobwebTree:   2%|         | 189/10788 [00:01<02:02, 86.50it/s]Training CobwebTree:   2%|         | 198/10788 [00:01<02:09, 81.72it/s]Training CobwebTree:   2%|         | 207/10788 [00:02<02:10, 80.79it/s]Training CobwebTree:   2%|         | 216/10788 [00:02<02:09, 81.94it/s]Training CobwebTree:   2%|         | 225/10788 [00:02<02:08, 82.03it/s]Training CobwebTree:   2%|         | 234/10788 [00:02<02:13, 79.03it/s]Training CobwebTree:   2%|         | 242/10788 [00:02<02:16, 77.19it/s]Training CobwebTree:   2%|         | 250/10788 [00:02<02:17, 76.91it/s]Training CobwebTree:   2%|         | 258/10788 [00:02<02:21, 74.33it/s]Training CobwebTree:   2%|         | 266/10788 [00:02<02:29, 70.49it/s]Training CobwebTree:   3%|         | 274/10788 [00:02<02:25, 72.45it/s]Training CobwebTree:   3%|         | 283/10788 [00:03<02:18, 76.00it/s]Training CobwebTree:   3%|         | 292/10788 [00:03<02:13, 78.84it/s]Training CobwebTree:   3%|         | 300/10788 [00:03<02:18, 75.53it/s]Training CobwebTree:   3%|         | 309/10788 [00:03<02:13, 78.72it/s]Training CobwebTree:   3%|         | 318/10788 [00:03<02:10, 80.39it/s]Training CobwebTree:   3%|         | 327/10788 [00:03<02:08, 81.59it/s]Training CobwebTree:   3%|         | 336/10788 [00:03<02:11, 79.58it/s]Training CobwebTree:   3%|         | 344/10788 [00:03<02:14, 77.65it/s]Training CobwebTree:   3%|         | 353/10788 [00:03<02:15, 77.12it/s]Training CobwebTree:   3%|         | 361/10788 [00:04<02:15, 76.92it/s]Training CobwebTree:   3%|         | 370/10788 [00:04<02:10, 80.12it/s]Training CobwebTree:   4%|         | 380/10788 [00:04<02:04, 83.56it/s]Training CobwebTree:   4%|         | 389/10788 [00:04<02:08, 80.81it/s]Training CobwebTree:   4%|         | 398/10788 [00:04<02:08, 80.83it/s]Training CobwebTree:   4%|         | 407/10788 [00:04<02:12, 78.40it/s]Training CobwebTree:   4%|         | 416/10788 [00:04<02:09, 80.39it/s]Training CobwebTree:   4%|         | 425/10788 [00:04<02:10, 79.58it/s]Training CobwebTree:   4%|         | 433/10788 [00:04<02:10, 79.52it/s]Training CobwebTree:   4%|         | 442/10788 [00:05<02:09, 80.16it/s]Training CobwebTree:   4%|         | 451/10788 [00:05<02:14, 77.11it/s]Training CobwebTree:   4%|         | 459/10788 [00:05<02:13, 77.22it/s]Training CobwebTree:   4%|         | 467/10788 [00:05<02:21, 72.97it/s]Training CobwebTree:   4%|         | 475/10788 [00:05<02:22, 72.60it/s]Training CobwebTree:   4%|         | 483/10788 [00:05<02:25, 70.75it/s]Training CobwebTree:   5%|         | 491/10788 [00:05<02:25, 70.57it/s]Training CobwebTree:   5%|         | 499/10788 [00:05<02:25, 70.83it/s]Training CobwebTree:   5%|         | 507/10788 [00:05<02:20, 72.98it/s]Training CobwebTree:   5%|         | 515/10788 [00:06<02:20, 73.36it/s]Training CobwebTree:   5%|         | 523/10788 [00:06<02:23, 71.53it/s]Training CobwebTree:   5%|         | 531/10788 [00:06<02:22, 72.10it/s]Training CobwebTree:   5%|         | 539/10788 [00:06<02:24, 71.10it/s]Training CobwebTree:   5%|         | 547/10788 [00:06<02:31, 67.55it/s]Training CobwebTree:   5%|         | 555/10788 [00:06<02:29, 68.28it/s]Training CobwebTree:   5%|         | 562/10788 [00:06<02:36, 65.23it/s]Training CobwebTree:   5%|         | 569/10788 [00:06<02:49, 60.26it/s]Training CobwebTree:   5%|         | 576/10788 [00:07<02:55, 58.22it/s]Training CobwebTree:   5%|         | 583/10788 [00:07<02:49, 60.35it/s]Training CobwebTree:   5%|         | 590/10788 [00:07<02:42, 62.76it/s]Training CobwebTree:   6%|         | 597/10788 [00:07<02:37, 64.71it/s]Training CobwebTree:   6%|         | 604/10788 [00:07<02:39, 63.99it/s]Training CobwebTree:   6%|         | 611/10788 [00:07<02:40, 63.33it/s]Training CobwebTree:   6%|         | 618/10788 [00:07<02:42, 62.60it/s]Training CobwebTree:   6%|         | 627/10788 [00:07<02:26, 69.20it/s]Training CobwebTree:   6%|         | 635/10788 [00:07<02:21, 71.55it/s]Training CobwebTree:   6%|         | 643/10788 [00:08<02:22, 71.09it/s]Training CobwebTree:   6%|         | 651/10788 [00:08<02:27, 68.62it/s]Training CobwebTree:   6%|         | 660/10788 [00:08<02:20, 71.87it/s]Training CobwebTree:   6%|         | 668/10788 [00:08<02:20, 71.88it/s]Training CobwebTree:   6%|         | 676/10788 [00:08<02:25, 69.71it/s]Training CobwebTree:   6%|         | 684/10788 [00:08<02:21, 71.44it/s]Training CobwebTree:   6%|         | 692/10788 [00:08<02:21, 71.46it/s]Training CobwebTree:   6%|         | 700/10788 [00:08<02:26, 68.85it/s]Training CobwebTree:   7%|         | 707/10788 [00:08<02:30, 66.96it/s]Training CobwebTree:   7%|         | 714/10788 [00:09<02:33, 65.60it/s]Training CobwebTree:   7%|         | 722/10788 [00:09<02:28, 67.82it/s]Training CobwebTree:   7%|         | 730/10788 [00:09<02:23, 70.04it/s]Training CobwebTree:   7%|         | 738/10788 [00:09<02:26, 68.42it/s]Training CobwebTree:   7%|         | 745/10788 [00:09<02:28, 67.81it/s]Training CobwebTree:   7%|         | 753/10788 [00:09<02:24, 69.30it/s]Training CobwebTree:   7%|         | 760/10788 [00:09<02:31, 66.23it/s]Training CobwebTree:   7%|         | 767/10788 [00:09<02:31, 65.99it/s]Training CobwebTree:   7%|         | 774/10788 [00:09<02:30, 66.48it/s]Training CobwebTree:   7%|         | 782/10788 [00:10<02:27, 67.70it/s]Training CobwebTree:   7%|         | 789/10788 [00:10<02:30, 66.24it/s]Training CobwebTree:   7%|         | 797/10788 [00:10<02:24, 68.92it/s]Training CobwebTree:   7%|         | 805/10788 [00:10<02:21, 70.33it/s]Training CobwebTree:   8%|         | 813/10788 [00:10<02:26, 68.16it/s]Training CobwebTree:   8%|         | 821/10788 [00:10<02:24, 69.16it/s]Training CobwebTree:   8%|         | 828/10788 [00:10<02:24, 69.16it/s]Training CobwebTree:   8%|         | 835/10788 [00:10<02:26, 67.88it/s]Training CobwebTree:   8%|         | 844/10788 [00:10<02:17, 72.45it/s]Training CobwebTree:   8%|         | 852/10788 [00:11<02:24, 68.94it/s]Training CobwebTree:   8%|         | 859/10788 [00:11<02:24, 68.72it/s]Training CobwebTree:   8%|         | 866/10788 [00:11<02:28, 66.76it/s]Training CobwebTree:   8%|         | 873/10788 [00:11<02:29, 66.20it/s]Training CobwebTree:   8%|         | 880/10788 [00:11<02:28, 66.54it/s]Training CobwebTree:   8%|         | 887/10788 [00:11<02:31, 65.40it/s]Training CobwebTree:   8%|         | 895/10788 [00:11<02:24, 68.42it/s]Training CobwebTree:   8%|         | 903/10788 [00:11<02:19, 71.00it/s]Training CobwebTree:   8%|         | 911/10788 [00:11<02:22, 69.49it/s]Training CobwebTree:   9%|         | 918/10788 [00:12<02:22, 69.25it/s]Training CobwebTree:   9%|         | 925/10788 [00:12<02:24, 68.10it/s]Training CobwebTree:   9%|         | 932/10788 [00:12<02:24, 68.01it/s]Training CobwebTree:   9%|         | 939/10788 [00:12<02:25, 67.69it/s]Training CobwebTree:   9%|         | 947/10788 [00:12<02:18, 71.05it/s]Training CobwebTree:   9%|         | 955/10788 [00:12<02:24, 67.98it/s]Training CobwebTree:   9%|         | 962/10788 [00:12<02:27, 66.66it/s]Training CobwebTree:   9%|         | 969/10788 [00:12<02:25, 67.34it/s]Training CobwebTree:   9%|         | 976/10788 [00:12<02:31, 64.76it/s]Training CobwebTree:   9%|         | 983/10788 [00:13<02:30, 65.24it/s]Training CobwebTree:   9%|         | 990/10788 [00:13<02:32, 64.43it/s]Training CobwebTree:   9%|         | 997/10788 [00:13<02:36, 62.41it/s]Training CobwebTree:   9%|         | 1004/10788 [00:13<02:43, 59.89it/s]Training CobwebTree:   9%|         | 1012/10788 [00:13<02:32, 64.01it/s]Training CobwebTree:   9%|         | 1019/10788 [00:13<02:31, 64.46it/s]Training CobwebTree:  10%|         | 1027/10788 [00:13<02:27, 66.04it/s]Training CobwebTree:  10%|         | 1034/10788 [00:13<02:29, 65.44it/s]Training CobwebTree:  10%|         | 1042/10788 [00:13<02:24, 67.24it/s]Training CobwebTree:  10%|         | 1049/10788 [00:14<02:29, 65.32it/s]Training CobwebTree:  10%|         | 1056/10788 [00:14<02:32, 63.86it/s]Training CobwebTree:  10%|         | 1063/10788 [00:14<02:32, 63.60it/s]Training CobwebTree:  10%|         | 1070/10788 [00:14<02:36, 62.28it/s]Training CobwebTree:  10%|         | 1077/10788 [00:14<02:41, 60.20it/s]Training CobwebTree:  10%|         | 1084/10788 [00:14<02:43, 59.48it/s]Training CobwebTree:  10%|         | 1090/10788 [00:14<02:42, 59.50it/s]Training CobwebTree:  10%|         | 1096/10788 [00:14<02:43, 59.34it/s]Training CobwebTree:  10%|         | 1103/10788 [00:14<02:40, 60.48it/s]Training CobwebTree:  10%|         | 1110/10788 [00:15<02:35, 62.38it/s]Training CobwebTree:  10%|         | 1117/10788 [00:15<02:36, 61.83it/s]Training CobwebTree:  10%|         | 1124/10788 [00:15<02:36, 61.94it/s]Training CobwebTree:  10%|         | 1131/10788 [00:15<02:36, 61.67it/s]Training CobwebTree:  11%|         | 1138/10788 [00:15<02:40, 60.14it/s]Training CobwebTree:  11%|         | 1145/10788 [00:15<02:49, 57.03it/s]Training CobwebTree:  11%|         | 1151/10788 [00:15<02:47, 57.49it/s]Training CobwebTree:  11%|         | 1157/10788 [00:15<02:48, 57.06it/s]Training CobwebTree:  11%|         | 1163/10788 [00:15<02:48, 56.97it/s]Training CobwebTree:  11%|         | 1171/10788 [00:16<02:39, 60.25it/s]Training CobwebTree:  11%|         | 1178/10788 [00:16<02:39, 60.08it/s]Training CobwebTree:  11%|         | 1185/10788 [00:16<02:34, 62.06it/s]Training CobwebTree:  11%|         | 1192/10788 [00:16<02:40, 59.77it/s]Training CobwebTree:  11%|         | 1199/10788 [00:16<02:42, 59.01it/s]Training CobwebTree:  11%|         | 1205/10788 [00:16<02:42, 58.84it/s]Training CobwebTree:  11%|         | 1212/10788 [00:16<02:40, 59.56it/s]Training CobwebTree:  11%|        | 1219/10788 [00:16<02:38, 60.48it/s]Training CobwebTree:  11%|        | 1227/10788 [00:16<02:30, 63.45it/s]Training CobwebTree:  11%|        | 1234/10788 [00:17<02:31, 62.94it/s]Training CobwebTree:  12%|        | 1241/10788 [00:17<02:36, 61.00it/s]Training CobwebTree:  12%|        | 1248/10788 [00:17<02:34, 61.94it/s]Training CobwebTree:  12%|        | 1255/10788 [00:17<02:29, 63.79it/s]Training CobwebTree:  12%|        | 1262/10788 [00:17<02:33, 61.93it/s]Training CobwebTree:  12%|        | 1269/10788 [00:17<02:35, 61.20it/s]Training CobwebTree:  12%|        | 1276/10788 [00:17<02:35, 61.17it/s]Training CobwebTree:  12%|        | 1284/10788 [00:17<02:25, 65.21it/s]Training CobwebTree:  12%|        | 1291/10788 [00:18<02:28, 63.98it/s]Training CobwebTree:  12%|        | 1298/10788 [00:18<02:30, 62.98it/s]Training CobwebTree:  12%|        | 1305/10788 [00:18<02:29, 63.29it/s]Training CobwebTree:  12%|        | 1312/10788 [00:18<02:26, 64.48it/s]Training CobwebTree:  12%|        | 1319/10788 [00:18<02:32, 62.23it/s]Training CobwebTree:  12%|        | 1326/10788 [00:18<02:36, 60.38it/s]Training CobwebTree:  12%|        | 1333/10788 [00:18<02:34, 61.35it/s]Training CobwebTree:  12%|        | 1340/10788 [00:18<02:33, 61.57it/s]Training CobwebTree:  12%|        | 1347/10788 [00:18<02:31, 62.46it/s]Training CobwebTree:  13%|        | 1354/10788 [00:19<02:31, 62.36it/s]Training CobwebTree:  13%|        | 1361/10788 [00:19<02:28, 63.54it/s]Training CobwebTree:  13%|        | 1368/10788 [00:19<02:27, 63.83it/s]Training CobwebTree:  13%|        | 1375/10788 [00:19<02:27, 63.60it/s]Training CobwebTree:  13%|        | 1382/10788 [00:19<02:28, 63.31it/s]Training CobwebTree:  13%|        | 1389/10788 [00:19<02:30, 62.30it/s]Training CobwebTree:  13%|        | 1396/10788 [00:19<02:33, 61.16it/s]Training CobwebTree:  13%|        | 1403/10788 [00:19<02:40, 58.50it/s]Training CobwebTree:  13%|        | 1410/10788 [00:19<02:35, 60.25it/s]Training CobwebTree:  13%|        | 1417/10788 [00:20<02:31, 61.98it/s]Training CobwebTree:  13%|        | 1424/10788 [00:20<02:31, 61.86it/s]Training CobwebTree:  13%|        | 1431/10788 [00:20<02:28, 62.80it/s]Training CobwebTree:  13%|        | 1438/10788 [00:20<02:31, 61.85it/s]Training CobwebTree:  13%|        | 1445/10788 [00:20<02:28, 63.01it/s]Training CobwebTree:  13%|        | 1452/10788 [00:20<02:26, 63.86it/s]Training CobwebTree:  14%|        | 1459/10788 [00:20<02:26, 63.61it/s]Training CobwebTree:  14%|        | 1466/10788 [00:20<02:33, 60.93it/s]Training CobwebTree:  14%|        | 1473/10788 [00:20<02:32, 61.09it/s]Training CobwebTree:  14%|        | 1480/10788 [00:21<02:27, 63.17it/s]Training CobwebTree:  14%|        | 1487/10788 [00:21<02:25, 64.11it/s]Training CobwebTree:  14%|        | 1494/10788 [00:21<02:24, 64.24it/s]Training CobwebTree:  14%|        | 1501/10788 [00:21<02:27, 63.17it/s]Training CobwebTree:  14%|        | 1508/10788 [00:21<02:25, 63.83it/s]Training CobwebTree:  14%|        | 1515/10788 [00:21<02:34, 60.04it/s]Training CobwebTree:  14%|        | 1522/10788 [00:21<02:36, 59.24it/s]Training CobwebTree:  14%|        | 1528/10788 [00:21<02:38, 58.59it/s]Training CobwebTree:  14%|        | 1534/10788 [00:21<02:37, 58.88it/s]Training CobwebTree:  14%|        | 1540/10788 [00:22<02:36, 59.07it/s]Training CobwebTree:  14%|        | 1546/10788 [00:22<02:36, 59.07it/s]Training CobwebTree:  14%|        | 1553/10788 [00:22<02:31, 60.84it/s]Training CobwebTree:  14%|        | 1560/10788 [00:22<02:31, 60.76it/s]Training CobwebTree:  15%|        | 1567/10788 [00:22<02:33, 59.89it/s]Training CobwebTree:  15%|        | 1575/10788 [00:22<02:24, 63.82it/s]Training CobwebTree:  15%|        | 1582/10788 [00:22<02:28, 62.18it/s]Training CobwebTree:  15%|        | 1589/10788 [00:22<02:26, 62.89it/s]Training CobwebTree:  15%|        | 1596/10788 [00:22<02:24, 63.80it/s]Training CobwebTree:  15%|        | 1603/10788 [00:23<02:27, 62.26it/s]Training CobwebTree:  15%|        | 1610/10788 [00:23<02:28, 61.75it/s]Training CobwebTree:  15%|        | 1617/10788 [00:23<02:24, 63.37it/s]Training CobwebTree:  15%|        | 1624/10788 [00:23<02:28, 61.65it/s]Training CobwebTree:  15%|        | 1631/10788 [00:23<02:31, 60.56it/s]Training CobwebTree:  15%|        | 1638/10788 [00:23<02:28, 61.50it/s]Training CobwebTree:  15%|        | 1645/10788 [00:23<02:29, 61.20it/s]Training CobwebTree:  15%|        | 1652/10788 [00:23<02:34, 59.28it/s]Training CobwebTree:  15%|        | 1659/10788 [00:23<02:32, 59.98it/s]Training CobwebTree:  15%|        | 1666/10788 [00:24<02:28, 61.38it/s]Training CobwebTree:  16%|        | 1673/10788 [00:24<02:34, 59.05it/s]Training CobwebTree:  16%|        | 1679/10788 [00:24<02:36, 58.21it/s]Training CobwebTree:  16%|        | 1685/10788 [00:24<02:37, 57.63it/s]Training CobwebTree:  16%|        | 1692/10788 [00:24<02:31, 59.98it/s]Training CobwebTree:  16%|        | 1699/10788 [00:24<02:37, 57.80it/s]Training CobwebTree:  16%|        | 1705/10788 [00:24<02:40, 56.71it/s]Training CobwebTree:  16%|        | 1711/10788 [00:24<02:38, 57.41it/s]Training CobwebTree:  16%|        | 1718/10788 [00:24<02:33, 59.26it/s]Training CobwebTree:  16%|        | 1724/10788 [00:25<02:33, 58.93it/s]Training CobwebTree:  16%|        | 1730/10788 [00:25<02:37, 57.66it/s]Training CobwebTree:  16%|        | 1736/10788 [00:25<02:36, 57.69it/s]Training CobwebTree:  16%|        | 1742/10788 [00:25<02:41, 55.95it/s]Training CobwebTree:  16%|        | 1748/10788 [00:25<02:39, 56.72it/s]Training CobwebTree:  16%|        | 1755/10788 [00:25<02:32, 59.16it/s]Training CobwebTree:  16%|        | 1762/10788 [00:25<02:31, 59.63it/s]Training CobwebTree:  16%|        | 1769/10788 [00:25<02:29, 60.27it/s]Training CobwebTree:  16%|        | 1776/10788 [00:25<02:33, 58.82it/s]Training CobwebTree:  17%|        | 1782/10788 [00:26<02:38, 56.68it/s]Training CobwebTree:  17%|        | 1788/10788 [00:26<02:36, 57.38it/s]Training CobwebTree:  17%|        | 1794/10788 [00:26<02:39, 56.55it/s]Training CobwebTree:  17%|        | 1800/10788 [00:26<02:44, 54.71it/s]Training CobwebTree:  17%|        | 1806/10788 [00:26<02:40, 55.89it/s]Training CobwebTree:  17%|        | 1812/10788 [00:26<02:39, 56.32it/s]Training CobwebTree:  17%|        | 1819/10788 [00:26<02:29, 59.87it/s]Training CobwebTree:  17%|        | 1826/10788 [00:26<02:35, 57.81it/s]Training CobwebTree:  17%|        | 1833/10788 [00:26<02:33, 58.15it/s]Training CobwebTree:  17%|        | 1839/10788 [00:27<02:35, 57.49it/s]Training CobwebTree:  17%|        | 1845/10788 [00:27<02:34, 57.87it/s]Training CobwebTree:  17%|        | 1852/10788 [00:27<02:28, 60.07it/s]Training CobwebTree:  17%|        | 1859/10788 [00:27<02:28, 60.31it/s]Training CobwebTree:  17%|        | 1866/10788 [00:27<02:35, 57.39it/s]Training CobwebTree:  17%|        | 1872/10788 [00:27<02:36, 57.00it/s]Training CobwebTree:  17%|        | 1878/10788 [00:27<02:35, 57.45it/s]Training CobwebTree:  17%|        | 1884/10788 [00:27<02:34, 57.56it/s]Training CobwebTree:  18%|        | 1890/10788 [00:27<02:33, 57.82it/s]Training CobwebTree:  18%|        | 1896/10788 [00:28<02:40, 55.55it/s]Training CobwebTree:  18%|        | 1902/10788 [00:28<02:37, 56.43it/s]Training CobwebTree:  18%|        | 1909/10788 [00:28<02:34, 57.50it/s]Training CobwebTree:  18%|        | 1916/10788 [00:28<02:28, 59.85it/s]Training CobwebTree:  18%|        | 1922/10788 [00:28<02:29, 59.20it/s]Training CobwebTree:  18%|        | 1928/10788 [00:28<02:29, 59.35it/s]Training CobwebTree:  18%|        | 1935/10788 [00:28<02:27, 60.16it/s]Training CobwebTree:  18%|        | 1942/10788 [00:28<02:29, 59.08it/s]Training CobwebTree:  18%|        | 1948/10788 [00:28<02:35, 56.84it/s]Training CobwebTree:  18%|        | 1954/10788 [00:29<02:34, 57.18it/s]Training CobwebTree:  18%|        | 1960/10788 [00:29<02:33, 57.57it/s]Training CobwebTree:  18%|        | 1966/10788 [00:29<02:35, 56.73it/s]Training CobwebTree:  18%|        | 1973/10788 [00:29<02:32, 57.85it/s]Training CobwebTree:  18%|        | 1979/10788 [00:29<02:35, 56.74it/s]Training CobwebTree:  18%|        | 1986/10788 [00:29<02:26, 59.89it/s]Training CobwebTree:  18%|        | 1993/10788 [00:29<02:29, 58.90it/s]Training CobwebTree:  19%|        | 1999/10788 [00:29<02:31, 57.84it/s]Training CobwebTree:  19%|        | 2005/10788 [00:29<02:31, 58.00it/s]Training CobwebTree:  19%|        | 2011/10788 [00:30<02:31, 58.03it/s]Training CobwebTree:  19%|        | 2017/10788 [00:30<02:35, 56.36it/s]Training CobwebTree:  19%|        | 2023/10788 [00:30<02:36, 56.12it/s]Training CobwebTree:  19%|        | 2030/10788 [00:30<02:29, 58.72it/s]Training CobwebTree:  19%|        | 2036/10788 [00:30<02:30, 58.31it/s]Training CobwebTree:  19%|        | 2043/10788 [00:30<02:26, 59.69it/s]Training CobwebTree:  19%|        | 2049/10788 [00:30<02:27, 59.42it/s]Training CobwebTree:  19%|        | 2055/10788 [00:30<02:39, 54.87it/s]Training CobwebTree:  19%|        | 2061/10788 [00:30<02:38, 55.09it/s]Training CobwebTree:  19%|        | 2067/10788 [00:31<02:37, 55.32it/s]Training CobwebTree:  19%|        | 2073/10788 [00:31<02:35, 56.16it/s]Training CobwebTree:  19%|        | 2079/10788 [00:31<02:36, 55.65it/s]Training CobwebTree:  19%|        | 2085/10788 [00:31<02:36, 55.54it/s]Training CobwebTree:  19%|        | 2092/10788 [00:31<02:31, 57.34it/s]Training CobwebTree:  19%|        | 2098/10788 [00:31<02:34, 56.17it/s]Training CobwebTree:  20%|        | 2104/10788 [00:31<02:36, 55.51it/s]Training CobwebTree:  20%|        | 2110/10788 [00:31<02:40, 53.91it/s]Training CobwebTree:  20%|        | 2116/10788 [00:31<02:39, 54.49it/s]Training CobwebTree:  20%|        | 2122/10788 [00:32<02:43, 53.02it/s]Training CobwebTree:  20%|        | 2128/10788 [00:32<02:39, 54.45it/s]Training CobwebTree:  20%|        | 2135/10788 [00:32<02:33, 56.56it/s]Training CobwebTree:  20%|        | 2142/10788 [00:32<02:27, 58.64it/s]Training CobwebTree:  20%|        | 2148/10788 [00:32<02:30, 57.46it/s]Training CobwebTree:  20%|        | 2154/10788 [00:32<02:32, 56.69it/s]Training CobwebTree:  20%|        | 2160/10788 [00:32<02:34, 55.94it/s]Training CobwebTree:  20%|        | 2166/10788 [00:32<02:32, 56.68it/s]Training CobwebTree:  20%|        | 2172/10788 [00:32<02:37, 54.74it/s]Training CobwebTree:  20%|        | 2178/10788 [00:33<02:39, 53.95it/s]Training CobwebTree:  20%|        | 2184/10788 [00:33<02:35, 55.23it/s]Training CobwebTree:  20%|        | 2190/10788 [00:33<02:36, 54.80it/s]Training CobwebTree:  20%|        | 2196/10788 [00:33<02:35, 55.09it/s]Training CobwebTree:  20%|        | 2202/10788 [00:33<02:36, 54.99it/s]Training CobwebTree:  20%|        | 2208/10788 [00:33<02:32, 56.35it/s]Training CobwebTree:  21%|        | 2214/10788 [00:33<02:33, 55.73it/s]Training CobwebTree:  21%|        | 2220/10788 [00:33<02:31, 56.71it/s]Training CobwebTree:  21%|        | 2227/10788 [00:33<02:24, 59.38it/s]Training CobwebTree:  21%|        | 2233/10788 [00:34<02:24, 59.06it/s]Training CobwebTree:  21%|        | 2239/10788 [00:34<02:25, 58.79it/s]Training CobwebTree:  21%|        | 2245/10788 [00:34<02:27, 57.93it/s]Training CobwebTree:  21%|        | 2251/10788 [00:34<02:28, 57.44it/s]Training CobwebTree:  21%|        | 2257/10788 [00:34<02:31, 56.42it/s]Training CobwebTree:  21%|        | 2264/10788 [00:34<02:29, 57.20it/s]Training CobwebTree:  21%|        | 2270/10788 [00:34<02:30, 56.63it/s]Training CobwebTree:  21%|        | 2276/10788 [00:34<02:33, 55.27it/s]Training CobwebTree:  21%|        | 2282/10788 [00:34<02:33, 55.33it/s]Training CobwebTree:  21%|        | 2288/10788 [00:34<02:34, 54.84it/s]Training CobwebTree:  21%|       | 2294/10788 [00:35<02:39, 53.30it/s]Training CobwebTree:  21%|       | 2300/10788 [00:35<02:38, 53.66it/s]Training CobwebTree:  21%|       | 2306/10788 [00:35<02:34, 55.06it/s]Training CobwebTree:  21%|       | 2312/10788 [00:35<02:33, 55.16it/s]Training CobwebTree:  21%|       | 2318/10788 [00:35<02:34, 54.89it/s]Training CobwebTree:  22%|       | 2324/10788 [00:35<02:38, 53.40it/s]Training CobwebTree:  22%|       | 2331/10788 [00:35<02:32, 55.44it/s]Training CobwebTree:  22%|       | 2337/10788 [00:35<02:31, 55.75it/s]Training CobwebTree:  22%|       | 2343/10788 [00:36<02:33, 54.88it/s]Training CobwebTree:  22%|       | 2349/10788 [00:36<02:31, 55.85it/s]Training CobwebTree:  22%|       | 2355/10788 [00:36<02:28, 56.80it/s]Training CobwebTree:  22%|       | 2361/10788 [00:36<02:35, 54.16it/s]Training CobwebTree:  22%|       | 2367/10788 [00:36<02:38, 53.14it/s]Training CobwebTree:  22%|       | 2373/10788 [00:36<02:38, 52.95it/s]Training CobwebTree:  22%|       | 2379/10788 [00:36<02:41, 52.02it/s]Training CobwebTree:  22%|       | 2385/10788 [00:36<02:44, 51.12it/s]Training CobwebTree:  22%|       | 2391/10788 [00:36<02:48, 49.93it/s]Training CobwebTree:  22%|       | 2397/10788 [00:37<02:44, 50.93it/s]Training CobwebTree:  22%|       | 2403/10788 [00:37<02:38, 52.80it/s]Training CobwebTree:  22%|       | 2409/10788 [00:37<02:40, 52.30it/s]Training CobwebTree:  22%|       | 2415/10788 [00:37<02:35, 53.86it/s]Training CobwebTree:  22%|       | 2421/10788 [00:37<02:37, 53.07it/s]Training CobwebTree:  22%|       | 2427/10788 [00:37<02:40, 52.18it/s]Training CobwebTree:  23%|       | 2433/10788 [00:37<02:37, 53.17it/s]Training CobwebTree:  23%|       | 2440/10788 [00:37<02:26, 57.09it/s]Training CobwebTree:  23%|       | 2446/10788 [00:37<02:29, 55.88it/s]Training CobwebTree:  23%|       | 2452/10788 [00:38<02:27, 56.45it/s]Training CobwebTree:  23%|       | 2458/10788 [00:38<02:26, 56.84it/s]Training CobwebTree:  23%|       | 2464/10788 [00:38<02:32, 54.67it/s]Training CobwebTree:  23%|       | 2471/10788 [00:38<02:24, 57.46it/s]Training CobwebTree:  23%|       | 2477/10788 [00:38<02:28, 56.06it/s]Training CobwebTree:  23%|       | 2483/10788 [00:38<02:32, 54.30it/s]Training CobwebTree:  23%|       | 2489/10788 [00:38<02:28, 55.86it/s]Training CobwebTree:  23%|       | 2495/10788 [00:38<02:27, 56.25it/s]Training CobwebTree:  23%|       | 2501/10788 [00:38<02:28, 55.97it/s]Training CobwebTree:  23%|       | 2507/10788 [00:39<02:26, 56.39it/s]Training CobwebTree:  23%|       | 2514/10788 [00:39<02:22, 58.01it/s]Training CobwebTree:  23%|       | 2520/10788 [00:39<02:26, 56.41it/s]Training CobwebTree:  23%|       | 2526/10788 [00:39<02:28, 55.73it/s]Training CobwebTree:  23%|       | 2532/10788 [00:39<02:26, 56.32it/s]Training CobwebTree:  24%|       | 2538/10788 [00:39<02:28, 55.42it/s]Training CobwebTree:  24%|       | 2544/10788 [00:39<02:29, 55.22it/s]Training CobwebTree:  24%|       | 2550/10788 [00:39<02:28, 55.57it/s]Training CobwebTree:  24%|       | 2557/10788 [00:39<02:22, 57.61it/s]Training CobwebTree:  24%|       | 2564/10788 [00:40<02:16, 60.30it/s]Training CobwebTree:  24%|       | 2571/10788 [00:40<02:19, 59.05it/s]Training CobwebTree:  24%|       | 2577/10788 [00:40<02:21, 57.99it/s]Training CobwebTree:  24%|       | 2583/10788 [00:40<02:24, 56.93it/s]Training CobwebTree:  24%|       | 2589/10788 [00:40<02:23, 57.09it/s]Training CobwebTree:  24%|       | 2595/10788 [00:40<02:27, 55.51it/s]Training CobwebTree:  24%|       | 2601/10788 [00:40<02:27, 55.55it/s]Training CobwebTree:  24%|       | 2607/10788 [00:40<02:25, 56.27it/s]Training CobwebTree:  24%|       | 2614/10788 [00:40<02:21, 57.87it/s]Training CobwebTree:  24%|       | 2620/10788 [00:41<02:25, 56.06it/s]Training CobwebTree:  24%|       | 2626/10788 [00:41<02:24, 56.34it/s]Training CobwebTree:  24%|       | 2633/10788 [00:41<02:22, 57.38it/s]Training CobwebTree:  24%|       | 2639/10788 [00:41<02:22, 57.06it/s]Training CobwebTree:  25%|       | 2645/10788 [00:41<02:23, 56.68it/s]Training CobwebTree:  25%|       | 2651/10788 [00:41<02:26, 55.59it/s]Training CobwebTree:  25%|       | 2657/10788 [00:41<02:28, 54.84it/s]Training CobwebTree:  25%|       | 2663/10788 [00:41<02:29, 54.49it/s]Training CobwebTree:  25%|       | 2669/10788 [00:41<02:25, 55.65it/s]Training CobwebTree:  25%|       | 2675/10788 [00:42<02:32, 53.06it/s]Training CobwebTree:  25%|       | 2681/10788 [00:42<02:28, 54.55it/s]Training CobwebTree:  25%|       | 2687/10788 [00:42<02:26, 55.27it/s]Training CobwebTree:  25%|       | 2693/10788 [00:42<02:25, 55.78it/s]Training CobwebTree:  25%|       | 2699/10788 [00:42<02:23, 56.36it/s]Training CobwebTree:  25%|       | 2705/10788 [00:42<02:22, 56.82it/s]Training CobwebTree:  25%|       | 2711/10788 [00:42<02:28, 54.27it/s]Training CobwebTree:  25%|       | 2717/10788 [00:42<02:37, 51.29it/s]Training CobwebTree:  25%|       | 2723/10788 [00:42<02:33, 52.48it/s]Training CobwebTree:  25%|       | 2729/10788 [00:43<02:33, 52.56it/s]Training CobwebTree:  25%|       | 2735/10788 [00:43<02:32, 52.77it/s]Training CobwebTree:  25%|       | 2741/10788 [00:43<02:29, 53.88it/s]Training CobwebTree:  25%|       | 2748/10788 [00:43<02:26, 55.05it/s]Training CobwebTree:  26%|       | 2754/10788 [00:43<02:32, 52.65it/s]Training CobwebTree:  26%|       | 2760/10788 [00:43<02:32, 52.62it/s]Training CobwebTree:  26%|       | 2766/10788 [00:43<02:30, 53.14it/s]Training CobwebTree:  26%|       | 2772/10788 [00:43<02:31, 52.80it/s]Training CobwebTree:  26%|       | 2778/10788 [00:43<02:27, 54.47it/s]Training CobwebTree:  26%|       | 2784/10788 [00:44<02:23, 55.95it/s]Training CobwebTree:  26%|       | 2790/10788 [00:44<02:26, 54.56it/s]Training CobwebTree:  26%|       | 2796/10788 [00:44<02:27, 54.01it/s]Training CobwebTree:  26%|       | 2803/10788 [00:44<02:22, 56.18it/s]Training CobwebTree:  26%|       | 2809/10788 [00:44<02:27, 54.09it/s]Training CobwebTree:  26%|       | 2815/10788 [00:44<02:25, 54.78it/s]Training CobwebTree:  26%|       | 2821/10788 [00:44<02:22, 56.06it/s]Training CobwebTree:  26%|       | 2828/10788 [00:44<02:17, 57.93it/s]Training CobwebTree:  26%|       | 2834/10788 [00:44<02:24, 54.86it/s]Training CobwebTree:  26%|       | 2840/10788 [00:45<02:35, 51.10it/s]Training CobwebTree:  26%|       | 2846/10788 [00:45<02:35, 51.07it/s]Training CobwebTree:  26%|       | 2852/10788 [00:45<02:31, 52.43it/s]Training CobwebTree:  26%|       | 2858/10788 [00:45<02:33, 51.63it/s]Training CobwebTree:  27%|       | 2864/10788 [00:45<02:29, 53.11it/s]Training CobwebTree:  27%|       | 2871/10788 [00:45<02:23, 55.22it/s]Training CobwebTree:  27%|       | 2877/10788 [00:45<02:25, 54.42it/s]Training CobwebTree:  27%|       | 2883/10788 [00:45<02:25, 54.40it/s]Training CobwebTree:  27%|       | 2889/10788 [00:45<02:30, 52.61it/s]Training CobwebTree:  27%|       | 2895/10788 [00:46<02:33, 51.44it/s]Training CobwebTree:  27%|       | 2901/10788 [00:46<02:36, 50.46it/s]Training CobwebTree:  27%|       | 2907/10788 [00:46<02:36, 50.43it/s]Training CobwebTree:  27%|       | 2913/10788 [00:46<02:35, 50.74it/s]Training CobwebTree:  27%|       | 2919/10788 [00:46<02:32, 51.73it/s]Training CobwebTree:  27%|       | 2925/10788 [00:46<02:27, 53.41it/s]Training CobwebTree:  27%|       | 2931/10788 [00:46<02:25, 53.85it/s]Training CobwebTree:  27%|       | 2937/10788 [00:46<02:25, 53.87it/s]Training CobwebTree:  27%|       | 2943/10788 [00:47<02:29, 52.63it/s]Training CobwebTree:  27%|       | 2949/10788 [00:47<02:30, 52.19it/s]Training CobwebTree:  27%|       | 2955/10788 [00:47<02:32, 51.53it/s]Training CobwebTree:  27%|       | 2961/10788 [00:47<02:28, 52.84it/s]Training CobwebTree:  28%|       | 2967/10788 [00:47<02:26, 53.29it/s]Training CobwebTree:  28%|       | 2974/10788 [00:47<02:22, 54.96it/s]Training CobwebTree:  28%|       | 2980/10788 [00:47<02:26, 53.48it/s]Training CobwebTree:  28%|       | 2986/10788 [00:47<02:21, 55.09it/s]Training CobwebTree:  28%|       | 2992/10788 [00:47<02:24, 53.98it/s]Training CobwebTree:  28%|       | 2998/10788 [00:48<02:25, 53.65it/s]Training CobwebTree:  28%|       | 3004/10788 [00:48<02:23, 54.39it/s]Training CobwebTree:  28%|       | 3010/10788 [00:48<02:21, 55.05it/s]Training CobwebTree:  28%|       | 3016/10788 [00:48<02:20, 55.49it/s]Training CobwebTree:  28%|       | 3022/10788 [00:48<02:25, 53.33it/s]Training CobwebTree:  28%|       | 3029/10788 [00:48<02:20, 55.16it/s]Training CobwebTree:  28%|       | 3035/10788 [00:48<02:28, 52.30it/s]Training CobwebTree:  28%|       | 3042/10788 [00:48<02:21, 54.58it/s]Training CobwebTree:  28%|       | 3048/10788 [00:48<02:23, 53.88it/s]Training CobwebTree:  28%|       | 3055/10788 [00:49<02:19, 55.60it/s]Training CobwebTree:  28%|       | 3061/10788 [00:49<02:19, 55.30it/s]Training CobwebTree:  28%|       | 3067/10788 [00:49<02:18, 55.89it/s]Training CobwebTree:  28%|       | 3073/10788 [00:49<02:16, 56.45it/s]Training CobwebTree:  29%|       | 3079/10788 [00:49<02:14, 57.20it/s]Training CobwebTree:  29%|       | 3085/10788 [00:49<02:22, 54.09it/s]Training CobwebTree:  29%|       | 3091/10788 [00:49<02:20, 54.61it/s]Training CobwebTree:  29%|       | 3097/10788 [00:49<02:17, 55.87it/s]Training CobwebTree:  29%|       | 3103/10788 [00:49<02:20, 54.59it/s]Training CobwebTree:  29%|       | 3109/10788 [00:50<02:17, 55.95it/s]Training CobwebTree:  29%|       | 3115/10788 [00:50<02:16, 56.22it/s]Training CobwebTree:  29%|       | 3121/10788 [00:50<02:15, 56.76it/s]Training CobwebTree:  29%|       | 3127/10788 [00:50<02:15, 56.62it/s]Training CobwebTree:  29%|       | 3133/10788 [00:50<02:21, 54.16it/s]Training CobwebTree:  29%|       | 3139/10788 [00:50<02:19, 54.98it/s]Training CobwebTree:  29%|       | 3145/10788 [00:50<02:17, 55.53it/s]Training CobwebTree:  29%|       | 3151/10788 [00:50<02:16, 55.81it/s]Training CobwebTree:  29%|       | 3158/10788 [00:50<02:14, 56.55it/s]Training CobwebTree:  29%|       | 3164/10788 [00:51<02:17, 55.59it/s]Training CobwebTree:  29%|       | 3170/10788 [00:51<02:14, 56.49it/s]Training CobwebTree:  29%|       | 3176/10788 [00:51<02:14, 56.71it/s]Training CobwebTree:  30%|       | 3183/10788 [00:51<02:13, 57.13it/s]Training CobwebTree:  30%|       | 3189/10788 [00:51<02:13, 56.84it/s]Training CobwebTree:  30%|       | 3195/10788 [00:51<02:12, 57.43it/s]Training CobwebTree:  30%|       | 3201/10788 [00:51<02:12, 57.37it/s]Training CobwebTree:  30%|       | 3208/10788 [00:51<02:10, 57.89it/s]Training CobwebTree:  30%|       | 3214/10788 [00:51<02:15, 56.01it/s]Training CobwebTree:  30%|       | 3220/10788 [00:52<02:14, 56.23it/s]Training CobwebTree:  30%|       | 3226/10788 [00:52<02:13, 56.51it/s]Training CobwebTree:  30%|       | 3232/10788 [00:52<02:16, 55.29it/s]Training CobwebTree:  30%|       | 3238/10788 [00:52<02:14, 56.30it/s]Training CobwebTree:  30%|       | 3244/10788 [00:52<02:17, 54.69it/s]Training CobwebTree:  30%|       | 3250/10788 [00:52<02:15, 55.46it/s]Training CobwebTree:  30%|       | 3256/10788 [00:52<02:13, 56.21it/s]Training CobwebTree:  30%|       | 3263/10788 [00:52<02:10, 57.59it/s]Training CobwebTree:  30%|       | 3269/10788 [00:52<02:09, 58.25it/s]Training CobwebTree:  30%|       | 3275/10788 [00:52<02:12, 56.57it/s]Training CobwebTree:  30%|       | 3283/10788 [00:53<02:03, 60.68it/s]Training CobwebTree:  30%|       | 3290/10788 [00:53<02:05, 59.93it/s]Training CobwebTree:  31%|       | 3296/10788 [00:53<02:09, 57.67it/s]Training CobwebTree:  31%|       | 3302/10788 [00:53<02:13, 56.27it/s]Training CobwebTree:  31%|       | 3308/10788 [00:53<02:15, 55.04it/s]Training CobwebTree:  31%|       | 3314/10788 [00:53<02:15, 55.20it/s]Training CobwebTree:  31%|       | 3320/10788 [00:53<02:13, 56.13it/s]Training CobwebTree:  31%|       | 3326/10788 [00:53<02:14, 55.58it/s]Training CobwebTree:  31%|       | 3332/10788 [00:54<02:15, 55.22it/s]Training CobwebTree:  31%|       | 3338/10788 [00:54<02:14, 55.25it/s]Training CobwebTree:  31%|       | 3344/10788 [00:54<02:13, 55.68it/s]Training CobwebTree:  31%|       | 3351/10788 [00:54<02:09, 57.32it/s]Training CobwebTree:  31%|       | 3358/10788 [00:54<02:08, 57.96it/s]Training CobwebTree:  31%|       | 3365/10788 [00:54<02:03, 60.20it/s]Training CobwebTree:  31%|      | 3372/10788 [00:54<02:07, 57.98it/s]Training CobwebTree:  31%|      | 3378/10788 [00:54<02:10, 56.89it/s]Training CobwebTree:  31%|      | 3384/10788 [00:54<02:14, 54.94it/s]Training CobwebTree:  31%|      | 3390/10788 [00:55<02:20, 52.57it/s]Training CobwebTree:  31%|      | 3396/10788 [00:55<02:15, 54.38it/s]Training CobwebTree:  32%|      | 3402/10788 [00:55<02:12, 55.63it/s]Training CobwebTree:  32%|      | 3408/10788 [00:55<02:12, 55.84it/s]Training CobwebTree:  32%|      | 3414/10788 [00:55<02:13, 55.16it/s]Training CobwebTree:  32%|      | 3421/10788 [00:55<02:07, 57.83it/s]Training CobwebTree:  32%|      | 3427/10788 [00:55<02:07, 57.90it/s]Training CobwebTree:  32%|      | 3433/10788 [00:55<02:08, 57.12it/s]Training CobwebTree:  32%|      | 3439/10788 [00:55<02:06, 57.93it/s]Training CobwebTree:  32%|      | 3445/10788 [00:55<02:08, 57.21it/s]Training CobwebTree:  32%|      | 3451/10788 [00:56<02:12, 55.35it/s]Training CobwebTree:  32%|      | 3457/10788 [00:56<02:20, 52.23it/s]Training CobwebTree:  32%|      | 3463/10788 [00:56<02:16, 53.51it/s]Training CobwebTree:  32%|      | 3469/10788 [00:56<02:24, 50.69it/s]Training CobwebTree:  32%|      | 3475/10788 [00:56<02:24, 50.66it/s]Training CobwebTree:  32%|      | 3481/10788 [00:56<02:20, 52.18it/s]Training CobwebTree:  32%|      | 3488/10788 [00:56<02:12, 55.18it/s]Training CobwebTree:  32%|      | 3494/10788 [00:56<02:14, 54.14it/s]Training CobwebTree:  32%|      | 3500/10788 [00:57<02:16, 53.55it/s]Training CobwebTree:  33%|      | 3507/10788 [00:57<02:12, 55.08it/s]Training CobwebTree:  33%|      | 3513/10788 [00:57<02:16, 53.15it/s]Training CobwebTree:  33%|      | 3520/10788 [00:57<02:12, 55.00it/s]Training CobwebTree:  33%|      | 3526/10788 [00:57<02:10, 55.44it/s]Training CobwebTree:  33%|      | 3532/10788 [00:57<02:09, 56.15it/s]Training CobwebTree:  33%|      | 3538/10788 [00:57<02:07, 56.68it/s]Training CobwebTree:  33%|      | 3544/10788 [00:57<02:10, 55.65it/s]Training CobwebTree:  33%|      | 3551/10788 [00:57<02:09, 55.74it/s]Training CobwebTree:  33%|      | 3557/10788 [00:58<02:15, 53.21it/s]Training CobwebTree:  33%|      | 3563/10788 [00:58<02:14, 53.57it/s]Training CobwebTree:  33%|      | 3569/10788 [00:58<02:16, 53.05it/s]Training CobwebTree:  33%|      | 3575/10788 [00:58<02:14, 53.81it/s]Training CobwebTree:  33%|      | 3581/10788 [00:58<02:17, 52.48it/s]Training CobwebTree:  33%|      | 3587/10788 [00:58<02:22, 50.49it/s]Training CobwebTree:  33%|      | 3593/10788 [00:58<02:22, 50.38it/s]Training CobwebTree:  33%|      | 3599/10788 [00:58<02:17, 52.09it/s]Training CobwebTree:  33%|      | 3605/10788 [00:59<02:20, 51.09it/s]Training CobwebTree:  33%|      | 3611/10788 [00:59<02:17, 52.38it/s]Training CobwebTree:  34%|      | 3617/10788 [00:59<02:17, 51.99it/s]Training CobwebTree:  34%|      | 3623/10788 [00:59<02:17, 52.26it/s]Training CobwebTree:  34%|      | 3629/10788 [00:59<02:12, 53.92it/s]Training CobwebTree:  34%|      | 3635/10788 [00:59<02:13, 53.51it/s]Training CobwebTree:  34%|      | 3641/10788 [00:59<02:12, 53.82it/s]Training CobwebTree:  34%|      | 3647/10788 [00:59<02:12, 53.84it/s]Training CobwebTree:  34%|      | 3653/10788 [00:59<02:13, 53.27it/s]Training CobwebTree:  34%|      | 3659/10788 [01:00<02:13, 53.24it/s]Training CobwebTree:  34%|      | 3666/10788 [01:00<02:06, 56.25it/s]Training CobwebTree:  34%|      | 3673/10788 [01:00<02:04, 57.18it/s]Training CobwebTree:  34%|      | 3679/10788 [01:00<02:10, 54.28it/s]Training CobwebTree:  34%|      | 3685/10788 [01:00<02:12, 53.61it/s]Training CobwebTree:  34%|      | 3691/10788 [01:00<02:11, 54.01it/s]Training CobwebTree:  34%|      | 3697/10788 [01:00<02:08, 55.33it/s]Training CobwebTree:  34%|      | 3703/10788 [01:00<02:08, 55.35it/s]Training CobwebTree:  34%|      | 3709/10788 [01:00<02:05, 56.41it/s]Training CobwebTree:  34%|      | 3715/10788 [01:01<02:05, 56.51it/s]Training CobwebTree:  34%|      | 3721/10788 [01:01<02:05, 56.26it/s]Training CobwebTree:  35%|      | 3727/10788 [01:01<02:09, 54.57it/s]Training CobwebTree:  35%|      | 3733/10788 [01:01<02:09, 54.50it/s]Training CobwebTree:  35%|      | 3739/10788 [01:01<02:08, 54.99it/s]Training CobwebTree:  35%|      | 3745/10788 [01:01<02:10, 53.92it/s]Training CobwebTree:  35%|      | 3751/10788 [01:01<02:08, 54.75it/s]Training CobwebTree:  35%|      | 3758/10788 [01:01<02:01, 57.64it/s]Training CobwebTree:  35%|      | 3764/10788 [01:01<02:04, 56.36it/s]Training CobwebTree:  35%|      | 3771/10788 [01:02<02:02, 57.36it/s]Training CobwebTree:  35%|      | 3777/10788 [01:02<02:01, 57.74it/s]Training CobwebTree:  35%|      | 3784/10788 [01:02<01:57, 59.63it/s]Training CobwebTree:  35%|      | 3790/10788 [01:02<02:01, 57.77it/s]Training CobwebTree:  35%|      | 3796/10788 [01:02<02:03, 56.54it/s]Training CobwebTree:  35%|      | 3802/10788 [01:02<02:06, 55.35it/s]Training CobwebTree:  35%|      | 3809/10788 [01:02<02:04, 56.20it/s]Training CobwebTree:  35%|      | 3816/10788 [01:02<02:00, 57.76it/s]Training CobwebTree:  35%|      | 3822/10788 [01:02<02:04, 55.89it/s]Training CobwebTree:  35%|      | 3828/10788 [01:03<02:02, 56.86it/s]Training CobwebTree:  36%|      | 3834/10788 [01:03<02:01, 57.14it/s]Training CobwebTree:  36%|      | 3840/10788 [01:03<02:05, 55.24it/s]Training CobwebTree:  36%|      | 3846/10788 [01:03<02:03, 56.12it/s]Training CobwebTree:  36%|      | 3852/10788 [01:03<02:02, 56.75it/s]Training CobwebTree:  36%|      | 3858/10788 [01:03<02:06, 54.72it/s]Training CobwebTree:  36%|      | 3864/10788 [01:03<02:08, 53.80it/s]Training CobwebTree:  36%|      | 3871/10788 [01:03<02:04, 55.40it/s]Training CobwebTree:  36%|      | 3877/10788 [01:03<02:04, 55.70it/s]Training CobwebTree:  36%|      | 3883/10788 [01:04<02:01, 56.85it/s]Training CobwebTree:  36%|      | 3889/10788 [01:04<02:03, 55.93it/s]Training CobwebTree:  36%|      | 3895/10788 [01:04<02:04, 55.44it/s]Training CobwebTree:  36%|      | 3901/10788 [01:04<02:07, 53.94it/s]Training CobwebTree:  36%|      | 3907/10788 [01:04<02:05, 54.79it/s]Training CobwebTree:  36%|      | 3913/10788 [01:04<02:02, 56.19it/s]Training CobwebTree:  36%|      | 3919/10788 [01:04<02:00, 56.91it/s]Training CobwebTree:  36%|      | 3925/10788 [01:04<01:59, 57.62it/s]Training CobwebTree:  36%|      | 3931/10788 [01:04<02:07, 53.99it/s]Training CobwebTree:  37%|      | 3938/10788 [01:04<02:01, 56.56it/s]Training CobwebTree:  37%|      | 3945/10788 [01:05<01:56, 58.54it/s]Training CobwebTree:  37%|      | 3951/10788 [01:05<02:03, 55.40it/s]Training CobwebTree:  37%|      | 3957/10788 [01:05<04:19, 26.35it/s]Training CobwebTree:  37%|      | 3963/10788 [01:05<03:38, 31.23it/s]Training CobwebTree:  37%|      | 3969/10788 [01:05<03:11, 35.53it/s]Training CobwebTree:  37%|      | 3975/10788 [01:06<02:50, 39.99it/s]Training CobwebTree:  37%|      | 3981/10788 [01:06<02:39, 42.72it/s]Training CobwebTree:  37%|      | 3987/10788 [01:06<02:28, 45.70it/s]Training CobwebTree:  37%|      | 3993/10788 [01:06<02:26, 46.40it/s]Training CobwebTree:  37%|      | 3999/10788 [01:06<02:22, 47.65it/s]Training CobwebTree:  37%|      | 4006/10788 [01:06<02:12, 51.28it/s]Training CobwebTree:  37%|      | 4012/10788 [01:06<02:12, 51.32it/s]Training CobwebTree:  37%|      | 4018/10788 [01:06<02:09, 52.30it/s]Training CobwebTree:  37%|      | 4024/10788 [01:07<02:08, 52.68it/s]Training CobwebTree:  37%|      | 4030/10788 [01:07<02:07, 53.06it/s]Training CobwebTree:  37%|      | 4036/10788 [01:07<02:05, 53.94it/s]Training CobwebTree:  37%|      | 4043/10788 [01:07<02:00, 56.08it/s]Training CobwebTree:  38%|      | 4049/10788 [01:07<01:59, 56.58it/s]Training CobwebTree:  38%|      | 4055/10788 [01:07<01:59, 56.14it/s]Training CobwebTree:  38%|      | 4061/10788 [01:07<01:59, 56.24it/s]Training CobwebTree:  38%|      | 4067/10788 [01:07<02:04, 54.03it/s]Training CobwebTree:  38%|      | 4073/10788 [01:07<02:06, 53.29it/s]Training CobwebTree:  38%|      | 4079/10788 [01:07<02:03, 54.20it/s]Training CobwebTree:  38%|      | 4085/10788 [01:08<02:06, 53.10it/s]Training CobwebTree:  38%|      | 4091/10788 [01:08<02:06, 53.15it/s]Training CobwebTree:  38%|      | 4097/10788 [01:08<02:04, 53.94it/s]Training CobwebTree:  38%|      | 4103/10788 [01:08<02:04, 53.66it/s]Training CobwebTree:  38%|      | 4109/10788 [01:08<02:00, 55.31it/s]Training CobwebTree:  38%|      | 4115/10788 [01:08<02:00, 55.45it/s]Training CobwebTree:  38%|      | 4121/10788 [01:08<02:05, 53.26it/s]Training CobwebTree:  38%|      | 4127/10788 [01:08<02:05, 53.12it/s]Training CobwebTree:  38%|      | 4133/10788 [01:09<02:04, 53.39it/s]Training CobwebTree:  38%|      | 4139/10788 [01:09<02:03, 53.78it/s]Training CobwebTree:  38%|      | 4145/10788 [01:09<02:01, 54.75it/s]Training CobwebTree:  38%|      | 4152/10788 [01:09<01:55, 57.34it/s]Training CobwebTree:  39%|      | 4158/10788 [01:09<02:00, 55.17it/s]Training CobwebTree:  39%|      | 4164/10788 [01:09<02:03, 53.45it/s]Training CobwebTree:  39%|      | 4170/10788 [01:09<02:03, 53.49it/s]Training CobwebTree:  39%|      | 4176/10788 [01:09<02:04, 52.91it/s]Training CobwebTree:  39%|      | 4182/10788 [01:09<02:01, 54.43it/s]Training CobwebTree:  39%|      | 4188/10788 [01:10<02:09, 50.85it/s]Training CobwebTree:  39%|      | 4194/10788 [01:10<02:07, 51.87it/s]Training CobwebTree:  39%|      | 4200/10788 [01:10<02:02, 53.56it/s]Training CobwebTree:  39%|      | 4206/10788 [01:10<02:10, 50.55it/s]Training CobwebTree:  39%|      | 4212/10788 [01:10<02:15, 48.55it/s]Training CobwebTree:  39%|      | 4217/10788 [01:10<02:15, 48.52it/s]Training CobwebTree:  39%|      | 4223/10788 [01:10<02:09, 50.65it/s]Training CobwebTree:  39%|      | 4229/10788 [01:10<02:13, 49.27it/s]Training CobwebTree:  39%|      | 4235/10788 [01:10<02:12, 49.62it/s]Training CobwebTree:  39%|      | 4241/10788 [01:11<02:08, 50.79it/s]Training CobwebTree:  39%|      | 4247/10788 [01:11<02:05, 52.11it/s]Training CobwebTree:  39%|      | 4253/10788 [01:11<02:08, 51.04it/s]Training CobwebTree:  39%|      | 4259/10788 [01:11<02:04, 52.64it/s]Training CobwebTree:  40%|      | 4265/10788 [01:11<02:05, 51.99it/s]Training CobwebTree:  40%|      | 4271/10788 [01:11<02:08, 50.85it/s]Training CobwebTree:  40%|      | 4277/10788 [01:11<02:05, 51.85it/s]Training CobwebTree:  40%|      | 4283/10788 [01:11<02:04, 52.20it/s]Training CobwebTree:  40%|      | 4289/10788 [01:12<02:05, 51.84it/s]Training CobwebTree:  40%|      | 4295/10788 [01:12<02:07, 50.97it/s]Training CobwebTree:  40%|      | 4301/10788 [01:12<02:04, 52.18it/s]Training CobwebTree:  40%|      | 4307/10788 [01:12<02:03, 52.56it/s]Training CobwebTree:  40%|      | 4313/10788 [01:12<01:59, 54.35it/s]Training CobwebTree:  40%|      | 4319/10788 [01:12<02:07, 50.90it/s]Training CobwebTree:  40%|      | 4325/10788 [01:12<02:03, 52.18it/s]Training CobwebTree:  40%|      | 4331/10788 [01:12<02:08, 50.25it/s]Training CobwebTree:  40%|      | 4337/10788 [01:12<02:08, 50.38it/s]Training CobwebTree:  40%|      | 4343/10788 [01:13<02:09, 49.95it/s]Training CobwebTree:  40%|      | 4349/10788 [01:13<02:09, 49.81it/s]Training CobwebTree:  40%|      | 4355/10788 [01:13<02:08, 50.21it/s]Training CobwebTree:  40%|      | 4361/10788 [01:13<02:04, 51.51it/s]Training CobwebTree:  40%|      | 4367/10788 [01:13<02:07, 50.39it/s]Training CobwebTree:  41%|      | 4373/10788 [01:13<02:03, 52.08it/s]Training CobwebTree:  41%|      | 4379/10788 [01:13<01:59, 53.78it/s]Training CobwebTree:  41%|      | 4385/10788 [01:13<01:58, 53.90it/s]Training CobwebTree:  41%|      | 4391/10788 [01:13<02:02, 52.07it/s]Training CobwebTree:  41%|      | 4398/10788 [01:14<01:55, 55.13it/s]Training CobwebTree:  41%|      | 4404/10788 [01:14<01:56, 54.97it/s]Training CobwebTree:  41%|      | 4410/10788 [01:14<02:00, 52.95it/s]Training CobwebTree:  41%|      | 4416/10788 [01:14<01:59, 53.37it/s]Training CobwebTree:  41%|      | 4422/10788 [01:14<02:02, 51.81it/s]Training CobwebTree:  41%|      | 4428/10788 [01:14<02:02, 51.71it/s]Training CobwebTree:  41%|      | 4434/10788 [01:14<02:02, 52.00it/s]Training CobwebTree:  41%|      | 4440/10788 [01:14<01:57, 54.03it/s]Training CobwebTree:  41%|      | 4446/10788 [01:15<01:59, 53.09it/s]Training CobwebTree:  41%|     | 4452/10788 [01:15<02:03, 51.30it/s]Training CobwebTree:  41%|     | 4458/10788 [01:15<02:04, 50.69it/s]Training CobwebTree:  41%|     | 4464/10788 [01:15<02:01, 52.09it/s]Training CobwebTree:  41%|     | 4470/10788 [01:15<01:57, 53.94it/s]Training CobwebTree:  41%|     | 4476/10788 [01:15<01:58, 53.33it/s]Training CobwebTree:  42%|     | 4482/10788 [01:15<01:58, 53.33it/s]Training CobwebTree:  42%|     | 4488/10788 [01:15<01:59, 52.84it/s]Training CobwebTree:  42%|     | 4494/10788 [01:15<01:59, 52.47it/s]Training CobwebTree:  42%|     | 4500/10788 [01:16<01:59, 52.42it/s]Training CobwebTree:  42%|     | 4506/10788 [01:16<01:56, 53.79it/s]Training CobwebTree:  42%|     | 4512/10788 [01:16<02:00, 52.23it/s]Training CobwebTree:  42%|     | 4518/10788 [01:16<01:59, 52.45it/s]Training CobwebTree:  42%|     | 4524/10788 [01:16<02:05, 49.82it/s]Training CobwebTree:  42%|     | 4530/10788 [01:16<02:06, 49.38it/s]Training CobwebTree:  42%|     | 4536/10788 [01:16<02:06, 49.40it/s]Training CobwebTree:  42%|     | 4542/10788 [01:16<02:02, 51.05it/s]Training CobwebTree:  42%|     | 4548/10788 [01:16<02:02, 50.93it/s]Training CobwebTree:  42%|     | 4554/10788 [01:17<01:59, 52.18it/s]Training CobwebTree:  42%|     | 4560/10788 [01:17<02:01, 51.28it/s]Training CobwebTree:  42%|     | 4566/10788 [01:17<02:00, 51.70it/s]Training CobwebTree:  42%|     | 4572/10788 [01:17<02:04, 49.84it/s]Training CobwebTree:  42%|     | 4578/10788 [01:17<02:03, 50.16it/s]Training CobwebTree:  42%|     | 4584/10788 [01:17<01:59, 51.79it/s]Training CobwebTree:  43%|     | 4590/10788 [01:17<01:56, 53.05it/s]Training CobwebTree:  43%|     | 4596/10788 [01:17<01:58, 52.30it/s]Training CobwebTree:  43%|     | 4602/10788 [01:18<02:01, 50.84it/s]Training CobwebTree:  43%|     | 4608/10788 [01:18<02:03, 49.95it/s]Training CobwebTree:  43%|     | 4614/10788 [01:18<01:58, 51.89it/s]Training CobwebTree:  43%|     | 4620/10788 [01:18<01:59, 51.55it/s]Training CobwebTree:  43%|     | 4626/10788 [01:18<02:05, 49.07it/s]Training CobwebTree:  43%|     | 4631/10788 [01:18<02:05, 48.98it/s]Training CobwebTree:  43%|     | 4636/10788 [01:18<02:05, 49.21it/s]Training CobwebTree:  43%|     | 4641/10788 [01:18<02:07, 48.16it/s]Training CobwebTree:  43%|     | 4646/10788 [01:18<02:06, 48.65it/s]Training CobwebTree:  43%|     | 4651/10788 [01:19<02:09, 47.27it/s]Training CobwebTree:  43%|     | 4657/10788 [01:19<02:04, 49.31it/s]Training CobwebTree:  43%|     | 4663/10788 [01:19<02:05, 48.92it/s]Training CobwebTree:  43%|     | 4668/10788 [01:19<02:09, 47.14it/s]Training CobwebTree:  43%|     | 4674/10788 [01:19<02:03, 49.50it/s]Training CobwebTree:  43%|     | 4679/10788 [01:19<02:03, 49.58it/s]Training CobwebTree:  43%|     | 4685/10788 [01:19<02:00, 50.78it/s]Training CobwebTree:  43%|     | 4691/10788 [01:19<02:01, 50.36it/s]Training CobwebTree:  44%|     | 4697/10788 [01:19<02:05, 48.61it/s]Training CobwebTree:  44%|     | 4703/10788 [01:20<01:58, 51.23it/s]Training CobwebTree:  44%|     | 4709/10788 [01:20<01:56, 52.12it/s]Training CobwebTree:  44%|     | 4715/10788 [01:20<01:59, 50.65it/s]Training CobwebTree:  44%|     | 4721/10788 [01:20<01:58, 51.07it/s]Training CobwebTree:  44%|     | 4727/10788 [01:20<02:02, 49.37it/s]Training CobwebTree:  44%|     | 4733/10788 [01:20<02:02, 49.47it/s]Training CobwebTree:  44%|     | 4738/10788 [01:20<02:05, 48.28it/s]Training CobwebTree:  44%|     | 4743/10788 [01:20<02:05, 48.04it/s]Training CobwebTree:  44%|     | 4748/10788 [01:21<02:08, 46.87it/s]Training CobwebTree:  44%|     | 4753/10788 [01:21<02:11, 45.80it/s]Training CobwebTree:  44%|     | 4758/10788 [01:21<02:13, 45.29it/s]Training CobwebTree:  44%|     | 4763/10788 [01:21<02:13, 45.09it/s]Training CobwebTree:  44%|     | 4768/10788 [01:21<02:12, 45.42it/s]Training CobwebTree:  44%|     | 4773/10788 [01:21<02:10, 46.11it/s]Training CobwebTree:  44%|     | 4778/10788 [01:21<02:08, 46.86it/s]Training CobwebTree:  44%|     | 4783/10788 [01:21<02:06, 47.52it/s]Training CobwebTree:  44%|     | 4788/10788 [01:21<02:04, 48.11it/s]Training CobwebTree:  44%|     | 4793/10788 [01:21<02:03, 48.64it/s]Training CobwebTree:  44%|     | 4799/10788 [01:22<01:59, 50.07it/s]Training CobwebTree:  45%|     | 4805/10788 [01:22<02:03, 48.44it/s]Training CobwebTree:  45%|     | 4811/10788 [01:22<01:58, 50.25it/s]Training CobwebTree:  45%|     | 4817/10788 [01:22<02:00, 49.48it/s]Training CobwebTree:  45%|     | 4823/10788 [01:22<01:59, 49.94it/s]Training CobwebTree:  45%|     | 4829/10788 [01:22<01:55, 51.75it/s]Training CobwebTree:  45%|     | 4835/10788 [01:22<01:54, 51.88it/s]Training CobwebTree:  45%|     | 4841/10788 [01:22<01:52, 52.82it/s]Training CobwebTree:  45%|     | 4847/10788 [01:23<01:58, 49.94it/s]Training CobwebTree:  45%|     | 4853/10788 [01:23<02:00, 49.42it/s]Training CobwebTree:  45%|     | 4859/10788 [01:23<01:55, 51.53it/s]Training CobwebTree:  45%|     | 4865/10788 [01:23<01:56, 50.94it/s]Training CobwebTree:  45%|     | 4871/10788 [01:23<01:53, 51.96it/s]Training CobwebTree:  45%|     | 4877/10788 [01:23<01:52, 52.68it/s]Training CobwebTree:  45%|     | 4883/10788 [01:23<01:56, 50.72it/s]Training CobwebTree:  45%|     | 4890/10788 [01:23<01:47, 54.62it/s]Training CobwebTree:  45%|     | 4896/10788 [01:23<01:50, 53.19it/s]Training CobwebTree:  45%|     | 4902/10788 [01:24<01:51, 52.95it/s]Training CobwebTree:  45%|     | 4908/10788 [01:24<01:50, 53.01it/s]Training CobwebTree:  46%|     | 4914/10788 [01:24<01:51, 52.83it/s]Training CobwebTree:  46%|     | 4920/10788 [01:24<01:59, 49.31it/s]Training CobwebTree:  46%|     | 4926/10788 [01:24<01:54, 51.26it/s]Training CobwebTree:  46%|     | 4932/10788 [01:24<01:51, 52.75it/s]Training CobwebTree:  46%|     | 4938/10788 [01:24<01:54, 51.21it/s]Training CobwebTree:  46%|     | 4944/10788 [01:24<01:59, 48.84it/s]Training CobwebTree:  46%|     | 4949/10788 [01:25<02:02, 47.77it/s]Training CobwebTree:  46%|     | 4956/10788 [01:25<01:53, 51.19it/s]Training CobwebTree:  46%|     | 4962/10788 [01:25<01:51, 52.13it/s]Training CobwebTree:  46%|     | 4968/10788 [01:25<01:51, 52.41it/s]Training CobwebTree:  46%|     | 4974/10788 [01:25<01:56, 50.00it/s]Training CobwebTree:  46%|     | 4980/10788 [01:25<01:58, 48.94it/s]Training CobwebTree:  46%|     | 4986/10788 [01:25<01:57, 49.25it/s]Training CobwebTree:  46%|     | 4991/10788 [01:25<01:58, 48.92it/s]Training CobwebTree:  46%|     | 4996/10788 [01:25<01:58, 49.04it/s]Training CobwebTree:  46%|     | 5002/10788 [01:26<01:56, 49.77it/s]Training CobwebTree:  46%|     | 5008/10788 [01:26<01:52, 51.42it/s]Training CobwebTree:  46%|     | 5014/10788 [01:26<01:50, 52.06it/s]Training CobwebTree:  47%|     | 5020/10788 [01:26<01:56, 49.39it/s]Training CobwebTree:  47%|     | 5026/10788 [01:26<01:55, 49.88it/s]Training CobwebTree:  47%|     | 5032/10788 [01:26<01:53, 50.92it/s]Training CobwebTree:  47%|     | 5038/10788 [01:26<01:56, 49.21it/s]Training CobwebTree:  47%|     | 5044/10788 [01:26<01:54, 50.16it/s]Training CobwebTree:  47%|     | 5050/10788 [01:27<01:52, 50.85it/s]Training CobwebTree:  47%|     | 5056/10788 [01:27<01:55, 49.47it/s]Training CobwebTree:  47%|     | 5061/10788 [01:27<01:56, 49.14it/s]Training CobwebTree:  47%|     | 5066/10788 [01:27<01:56, 49.33it/s]Training CobwebTree:  47%|     | 5072/10788 [01:27<01:55, 49.48it/s]Training CobwebTree:  47%|     | 5077/10788 [01:27<01:59, 47.76it/s]Training CobwebTree:  47%|     | 5082/10788 [01:27<01:59, 47.90it/s]Training CobwebTree:  47%|     | 5088/10788 [01:27<01:56, 49.13it/s]Training CobwebTree:  47%|     | 5094/10788 [01:27<01:51, 50.88it/s]Training CobwebTree:  47%|     | 5100/10788 [01:28<01:49, 51.76it/s]Training CobwebTree:  47%|     | 5106/10788 [01:28<01:46, 53.17it/s]Training CobwebTree:  47%|     | 5112/10788 [01:28<01:43, 54.68it/s]Training CobwebTree:  47%|     | 5118/10788 [01:28<01:43, 54.53it/s]Training CobwebTree:  47%|     | 5124/10788 [01:28<01:44, 54.28it/s]Training CobwebTree:  48%|     | 5130/10788 [01:28<01:44, 54.30it/s]Training CobwebTree:  48%|     | 5136/10788 [01:28<01:42, 54.94it/s]Training CobwebTree:  48%|     | 5142/10788 [01:28<01:46, 53.01it/s]Training CobwebTree:  48%|     | 5148/10788 [01:28<01:50, 51.15it/s]Training CobwebTree:  48%|     | 5154/10788 [01:29<01:48, 52.10it/s]Training CobwebTree:  48%|     | 5160/10788 [01:29<01:51, 50.53it/s]Training CobwebTree:  48%|     | 5166/10788 [01:29<01:49, 51.17it/s]Training CobwebTree:  48%|     | 5172/10788 [01:29<01:49, 51.35it/s]Training CobwebTree:  48%|     | 5178/10788 [01:29<01:46, 52.77it/s]Training CobwebTree:  48%|     | 5184/10788 [01:29<01:46, 52.48it/s]Training CobwebTree:  48%|     | 5191/10788 [01:29<01:41, 55.12it/s]Training CobwebTree:  48%|     | 5197/10788 [01:29<01:40, 55.61it/s]Training CobwebTree:  48%|     | 5203/10788 [01:29<01:39, 56.23it/s]Training CobwebTree:  48%|     | 5209/10788 [01:30<01:39, 55.80it/s]Training CobwebTree:  48%|     | 5216/10788 [01:30<01:36, 57.79it/s]Training CobwebTree:  48%|     | 5223/10788 [01:30<01:34, 58.77it/s]Training CobwebTree:  48%|     | 5230/10788 [01:30<01:33, 59.40it/s]Training CobwebTree:  49%|     | 5236/10788 [01:30<01:37, 56.99it/s]Training CobwebTree:  49%|     | 5242/10788 [01:30<01:38, 56.13it/s]Training CobwebTree:  49%|     | 5248/10788 [01:30<01:40, 54.94it/s]Training CobwebTree:  49%|     | 5254/10788 [01:30<01:43, 53.29it/s]Training CobwebTree:  49%|     | 5260/10788 [01:30<01:48, 51.03it/s]Training CobwebTree:  49%|     | 5266/10788 [01:31<01:50, 49.81it/s]Training CobwebTree:  49%|     | 5272/10788 [01:31<01:55, 47.80it/s]Training CobwebTree:  49%|     | 5278/10788 [01:31<01:54, 48.19it/s]Training CobwebTree:  49%|     | 5284/10788 [01:31<01:48, 50.92it/s]Training CobwebTree:  49%|     | 5290/10788 [01:31<01:44, 52.59it/s]Training CobwebTree:  49%|     | 5297/10788 [01:31<01:39, 55.11it/s]Training CobwebTree:  49%|     | 5304/10788 [01:31<01:35, 57.61it/s]Training CobwebTree:  49%|     | 5310/10788 [01:31<01:34, 58.09it/s]Training CobwebTree:  49%|     | 5316/10788 [01:32<01:40, 54.27it/s]Training CobwebTree:  49%|     | 5322/10788 [01:32<01:40, 54.17it/s]Training CobwebTree:  49%|     | 5329/10788 [01:32<01:39, 54.91it/s]Training CobwebTree:  49%|     | 5336/10788 [01:32<01:33, 58.46it/s]Training CobwebTree:  50%|     | 5342/10788 [01:32<01:34, 57.47it/s]Training CobwebTree:  50%|     | 5348/10788 [01:32<01:41, 53.82it/s]Training CobwebTree:  50%|     | 5354/10788 [01:32<01:44, 51.92it/s]Training CobwebTree:  50%|     | 5360/10788 [01:32<01:46, 50.83it/s]Training CobwebTree:  50%|     | 5366/10788 [01:32<01:43, 52.38it/s]Training CobwebTree:  50%|     | 5373/10788 [01:33<01:39, 54.68it/s]Training CobwebTree:  50%|     | 5379/10788 [01:33<01:39, 54.12it/s]Training CobwebTree:  50%|     | 5385/10788 [01:33<01:38, 54.64it/s]Training CobwebTree:  50%|     | 5391/10788 [01:33<01:42, 52.53it/s]Training CobwebTree:  50%|     | 5397/10788 [01:33<01:44, 51.38it/s]Training CobwebTree:  50%|     | 5403/10788 [01:33<01:44, 51.48it/s]Training CobwebTree:  50%|     | 5409/10788 [01:33<01:42, 52.46it/s]Training CobwebTree:  50%|     | 5415/10788 [01:33<01:45, 50.69it/s]Training CobwebTree:  50%|     | 5421/10788 [01:34<01:45, 51.08it/s]Training CobwebTree:  50%|     | 5427/10788 [01:34<01:45, 50.79it/s]Training CobwebTree:  50%|     | 5433/10788 [01:34<01:44, 51.19it/s]Training CobwebTree:  50%|     | 5439/10788 [01:34<01:46, 50.16it/s]Training CobwebTree:  50%|     | 5445/10788 [01:34<01:45, 50.65it/s]Training CobwebTree:  51%|     | 5451/10788 [01:34<01:44, 51.23it/s]Training CobwebTree:  51%|     | 5457/10788 [01:34<01:44, 51.23it/s]Training CobwebTree:  51%|     | 5463/10788 [01:34<01:42, 51.86it/s]Training CobwebTree:  51%|     | 5469/10788 [01:34<01:41, 52.30it/s]Training CobwebTree:  51%|     | 5475/10788 [01:35<01:43, 51.17it/s]Training CobwebTree:  51%|     | 5481/10788 [01:35<01:43, 51.35it/s]Training CobwebTree:  51%|     | 5487/10788 [01:35<01:39, 53.41it/s]Training CobwebTree:  51%|     | 5493/10788 [01:35<01:39, 53.11it/s]Training CobwebTree:  51%|     | 5499/10788 [01:35<01:42, 51.48it/s]Training CobwebTree:  51%|     | 5505/10788 [01:35<01:43, 50.95it/s]Training CobwebTree:  51%|     | 5511/10788 [01:35<01:41, 51.76it/s]Training CobwebTree:  51%|     | 5517/10788 [01:35<01:40, 52.52it/s]Training CobwebTree:  51%|     | 5523/10788 [01:35<01:38, 53.70it/s]Training CobwebTree:  51%|    | 5529/10788 [01:36<01:35, 54.82it/s]Training CobwebTree:  51%|    | 5535/10788 [01:36<01:36, 54.59it/s]Training CobwebTree:  51%|    | 5541/10788 [01:36<01:40, 52.07it/s]Training CobwebTree:  51%|    | 5547/10788 [01:36<01:38, 53.01it/s]Training CobwebTree:  51%|    | 5553/10788 [01:36<01:37, 53.81it/s]Training CobwebTree:  52%|    | 5559/10788 [01:36<01:37, 53.85it/s]Training CobwebTree:  52%|    | 5565/10788 [01:36<01:41, 51.56it/s]Training CobwebTree:  52%|    | 5571/10788 [01:36<01:41, 51.43it/s]Training CobwebTree:  52%|    | 5577/10788 [01:37<01:38, 52.75it/s]Training CobwebTree:  52%|    | 5583/10788 [01:37<01:43, 50.32it/s]Training CobwebTree:  52%|    | 5589/10788 [01:37<01:41, 51.42it/s]Training CobwebTree:  52%|    | 5595/10788 [01:37<01:40, 51.53it/s]Training CobwebTree:  52%|    | 5601/10788 [01:37<01:40, 51.55it/s]Training CobwebTree:  52%|    | 5607/10788 [01:37<01:47, 48.05it/s]Training CobwebTree:  52%|    | 5613/10788 [01:37<01:43, 49.86it/s]Training CobwebTree:  52%|    | 5619/10788 [01:37<01:44, 49.27it/s]Training CobwebTree:  52%|    | 5625/10788 [01:37<01:41, 51.04it/s]Training CobwebTree:  52%|    | 5631/10788 [01:38<01:41, 50.77it/s]Training CobwebTree:  52%|    | 5637/10788 [01:38<01:39, 51.52it/s]Training CobwebTree:  52%|    | 5643/10788 [01:38<01:38, 52.19it/s]Training CobwebTree:  52%|    | 5649/10788 [01:38<01:42, 50.16it/s]Training CobwebTree:  52%|    | 5655/10788 [01:38<01:40, 50.96it/s]Training CobwebTree:  52%|    | 5661/10788 [01:38<01:36, 52.87it/s]Training CobwebTree:  53%|    | 5667/10788 [01:38<01:37, 52.38it/s]Training CobwebTree:  53%|    | 5673/10788 [01:38<01:35, 53.52it/s]Training CobwebTree:  53%|    | 5679/10788 [01:39<01:41, 50.48it/s]Training CobwebTree:  53%|    | 5685/10788 [01:39<01:42, 49.66it/s]Training CobwebTree:  53%|    | 5691/10788 [01:39<01:42, 49.89it/s]Training CobwebTree:  53%|    | 5697/10788 [01:39<01:42, 49.72it/s]Training CobwebTree:  53%|    | 5702/10788 [01:39<01:44, 48.72it/s]Training CobwebTree:  53%|    | 5708/10788 [01:39<01:43, 49.00it/s]Training CobwebTree:  53%|    | 5714/10788 [01:39<01:40, 50.64it/s]Training CobwebTree:  53%|    | 5720/10788 [01:39<01:39, 50.87it/s]Training CobwebTree:  53%|    | 5726/10788 [01:39<01:38, 51.54it/s]Training CobwebTree:  53%|    | 5732/10788 [01:40<01:37, 51.68it/s]Training CobwebTree:  53%|    | 5738/10788 [01:40<01:37, 51.94it/s]Training CobwebTree:  53%|    | 5744/10788 [01:40<01:33, 53.73it/s]Training CobwebTree:  53%|    | 5750/10788 [01:40<01:34, 53.51it/s]Training CobwebTree:  53%|    | 5756/10788 [01:40<01:33, 53.93it/s]Training CobwebTree:  53%|    | 5762/10788 [01:40<01:34, 52.99it/s]Training CobwebTree:  53%|    | 5768/10788 [01:40<01:33, 53.73it/s]Training CobwebTree:  54%|    | 5774/10788 [01:40<01:36, 51.73it/s]Training CobwebTree:  54%|    | 5780/10788 [01:40<01:39, 50.21it/s]Training CobwebTree:  54%|    | 5786/10788 [01:41<01:41, 49.32it/s]Training CobwebTree:  54%|    | 5792/10788 [01:41<01:39, 50.21it/s]Training CobwebTree:  54%|    | 5798/10788 [01:41<01:39, 50.04it/s]Training CobwebTree:  54%|    | 5804/10788 [01:41<01:41, 49.34it/s]Training CobwebTree:  54%|    | 5810/10788 [01:41<01:39, 49.95it/s]Training CobwebTree:  54%|    | 5816/10788 [01:41<01:40, 49.71it/s]Training CobwebTree:  54%|    | 5822/10788 [01:41<01:37, 50.78it/s]Training CobwebTree:  54%|    | 5828/10788 [01:41<01:35, 52.20it/s]Training CobwebTree:  54%|    | 5834/10788 [01:42<01:32, 53.70it/s]Training CobwebTree:  54%|    | 5840/10788 [01:42<01:32, 53.35it/s]Training CobwebTree:  54%|    | 5846/10788 [01:42<01:32, 53.50it/s]Training CobwebTree:  54%|    | 5852/10788 [01:42<01:30, 54.30it/s]Training CobwebTree:  54%|    | 5858/10788 [01:42<01:30, 54.75it/s]Training CobwebTree:  54%|    | 5864/10788 [01:42<01:31, 53.71it/s]Training CobwebTree:  54%|    | 5870/10788 [01:42<01:32, 53.33it/s]Training CobwebTree:  54%|    | 5876/10788 [01:42<01:33, 52.74it/s]Training CobwebTree:  55%|    | 5882/10788 [01:42<01:31, 53.37it/s]Training CobwebTree:  55%|    | 5888/10788 [01:43<01:34, 51.98it/s]Training CobwebTree:  55%|    | 5894/10788 [01:43<01:33, 52.30it/s]Training CobwebTree:  55%|    | 5900/10788 [01:43<01:32, 52.76it/s]Training CobwebTree:  55%|    | 5906/10788 [01:43<01:33, 52.09it/s]Training CobwebTree:  55%|    | 5912/10788 [01:43<01:35, 51.19it/s]Training CobwebTree:  55%|    | 5919/10788 [01:43<01:30, 53.81it/s]Training CobwebTree:  55%|    | 5925/10788 [01:43<01:33, 52.10it/s]Training CobwebTree:  55%|    | 5932/10788 [01:43<01:27, 55.20it/s]Training CobwebTree:  55%|    | 5938/10788 [01:43<01:32, 52.35it/s]Training CobwebTree:  55%|    | 5944/10788 [01:44<01:33, 51.69it/s]Training CobwebTree:  55%|    | 5950/10788 [01:44<01:33, 51.76it/s]Training CobwebTree:  55%|    | 5956/10788 [01:44<01:35, 50.68it/s]Training CobwebTree:  55%|    | 5962/10788 [01:44<01:35, 50.36it/s]Training CobwebTree:  55%|    | 5968/10788 [01:44<01:33, 51.41it/s]Training CobwebTree:  55%|    | 5974/10788 [01:44<01:33, 51.35it/s]Training CobwebTree:  55%|    | 5980/10788 [01:44<01:32, 52.21it/s]Training CobwebTree:  55%|    | 5986/10788 [01:44<01:32, 51.95it/s]Training CobwebTree:  56%|    | 5992/10788 [01:45<01:33, 51.15it/s]Training CobwebTree:  56%|    | 5998/10788 [01:45<01:33, 51.01it/s]Training CobwebTree:  56%|    | 6004/10788 [01:45<01:32, 51.62it/s]Training CobwebTree:  56%|    | 6010/10788 [01:45<01:34, 50.78it/s]Training CobwebTree:  56%|    | 6016/10788 [01:45<01:33, 51.24it/s]Training CobwebTree:  56%|    | 6022/10788 [01:45<01:35, 50.16it/s]Training CobwebTree:  56%|    | 6028/10788 [01:45<01:32, 51.50it/s]Training CobwebTree:  56%|    | 6034/10788 [01:45<01:30, 52.54it/s]Training CobwebTree:  56%|    | 6040/10788 [01:45<01:32, 51.32it/s]Training CobwebTree:  56%|    | 6047/10788 [01:46<01:25, 55.38it/s]Training CobwebTree:  56%|    | 6053/10788 [01:46<01:28, 53.31it/s]Training CobwebTree:  56%|    | 6059/10788 [01:46<01:26, 54.52it/s]Training CobwebTree:  56%|    | 6065/10788 [01:46<01:28, 53.11it/s]Training CobwebTree:  56%|    | 6071/10788 [01:46<01:29, 52.97it/s]Training CobwebTree:  56%|    | 6077/10788 [01:46<01:31, 51.38it/s]Training CobwebTree:  56%|    | 6083/10788 [01:46<01:30, 52.24it/s]Training CobwebTree:  56%|    | 6089/10788 [01:46<01:29, 52.64it/s]Training CobwebTree:  56%|    | 6095/10788 [01:47<01:29, 52.17it/s]Training CobwebTree:  57%|    | 6101/10788 [01:47<01:27, 53.54it/s]Training CobwebTree:  57%|    | 6107/10788 [01:47<01:29, 52.27it/s]Training CobwebTree:  57%|    | 6113/10788 [01:47<01:33, 50.17it/s]Training CobwebTree:  57%|    | 6119/10788 [01:47<01:35, 49.10it/s]Training CobwebTree:  57%|    | 6125/10788 [01:47<01:33, 49.80it/s]Training CobwebTree:  57%|    | 6131/10788 [01:47<01:32, 50.57it/s]Training CobwebTree:  57%|    | 6137/10788 [01:47<01:33, 49.62it/s]Training CobwebTree:  57%|    | 6142/10788 [01:47<01:33, 49.66it/s]Training CobwebTree:  57%|    | 6148/10788 [01:48<01:31, 50.93it/s]Training CobwebTree:  57%|    | 6154/10788 [01:48<01:33, 49.60it/s]Training CobwebTree:  57%|    | 6159/10788 [01:48<01:35, 48.30it/s]Training CobwebTree:  57%|    | 6165/10788 [01:48<01:32, 49.82it/s]Training CobwebTree:  57%|    | 6171/10788 [01:48<01:29, 51.82it/s]Training CobwebTree:  57%|    | 6177/10788 [01:48<01:27, 52.88it/s]Training CobwebTree:  57%|    | 6183/10788 [01:48<01:28, 51.74it/s]Training CobwebTree:  57%|    | 6189/10788 [01:48<01:28, 52.15it/s]Training CobwebTree:  57%|    | 6195/10788 [01:48<01:24, 54.22it/s]Training CobwebTree:  57%|    | 6201/10788 [01:49<01:27, 52.23it/s]Training CobwebTree:  58%|    | 6207/10788 [01:49<01:25, 53.82it/s]Training CobwebTree:  58%|    | 6213/10788 [01:49<01:28, 51.69it/s]Training CobwebTree:  58%|    | 6219/10788 [01:49<01:31, 50.14it/s]Training CobwebTree:  58%|    | 6225/10788 [01:49<01:34, 48.53it/s]Training CobwebTree:  58%|    | 6231/10788 [01:49<01:30, 50.25it/s]Training CobwebTree:  58%|    | 6237/10788 [01:49<01:35, 47.55it/s]Training CobwebTree:  58%|    | 6242/10788 [01:49<01:34, 48.00it/s]Training CobwebTree:  58%|    | 6248/10788 [01:50<01:35, 47.74it/s]Training CobwebTree:  58%|    | 6253/10788 [01:50<01:36, 47.05it/s]Training CobwebTree:  58%|    | 6258/10788 [01:50<01:39, 45.69it/s]Training CobwebTree:  58%|    | 6263/10788 [01:50<01:43, 43.66it/s]Training CobwebTree:  58%|    | 6269/10788 [01:50<01:37, 46.50it/s]Training CobwebTree:  58%|    | 6275/10788 [01:50<01:30, 50.02it/s]Training CobwebTree:  58%|    | 6281/10788 [01:50<01:27, 51.35it/s]Training CobwebTree:  58%|    | 6287/10788 [01:50<01:27, 51.45it/s]Training CobwebTree:  58%|    | 6293/10788 [01:50<01:26, 52.02it/s]Training CobwebTree:  58%|    | 6299/10788 [01:51<01:26, 51.80it/s]Training CobwebTree:  58%|    | 6305/10788 [01:51<01:26, 51.67it/s]Training CobwebTree:  59%|    | 6311/10788 [01:51<01:26, 52.05it/s]Training CobwebTree:  59%|    | 6317/10788 [01:51<01:22, 54.15it/s]Training CobwebTree:  59%|    | 6324/10788 [01:51<01:19, 56.30it/s]Training CobwebTree:  59%|    | 6330/10788 [01:51<01:19, 55.81it/s]Training CobwebTree:  59%|    | 6336/10788 [01:51<01:19, 55.84it/s]Training CobwebTree:  59%|    | 6342/10788 [01:51<01:18, 56.45it/s]Training CobwebTree:  59%|    | 6348/10788 [01:51<01:20, 55.08it/s]Training CobwebTree:  59%|    | 6354/10788 [01:52<01:23, 53.23it/s]Training CobwebTree:  59%|    | 6360/10788 [01:52<01:24, 52.50it/s]Training CobwebTree:  59%|    | 6366/10788 [01:52<01:25, 51.75it/s]Training CobwebTree:  59%|    | 6372/10788 [01:52<01:26, 50.81it/s]Training CobwebTree:  59%|    | 6378/10788 [01:52<01:27, 50.12it/s]Training CobwebTree:  59%|    | 6384/10788 [01:52<01:26, 51.09it/s]Training CobwebTree:  59%|    | 6390/10788 [01:52<01:25, 51.60it/s]Training CobwebTree:  59%|    | 6396/10788 [01:52<01:22, 53.06it/s]Training CobwebTree:  59%|    | 6402/10788 [01:53<01:22, 52.98it/s]Training CobwebTree:  59%|    | 6408/10788 [01:53<01:22, 53.04it/s]Training CobwebTree:  59%|    | 6414/10788 [01:53<01:23, 52.17it/s]Training CobwebTree:  60%|    | 6420/10788 [01:53<01:26, 50.43it/s]Training CobwebTree:  60%|    | 6426/10788 [01:53<01:28, 49.56it/s]Training CobwebTree:  60%|    | 6431/10788 [01:53<01:28, 49.37it/s]Training CobwebTree:  60%|    | 6437/10788 [01:53<01:27, 49.87it/s]Training CobwebTree:  60%|    | 6442/10788 [01:53<01:28, 48.95it/s]Training CobwebTree:  60%|    | 6448/10788 [01:53<01:26, 50.39it/s]Training CobwebTree:  60%|    | 6454/10788 [01:54<01:27, 49.49it/s]Training CobwebTree:  60%|    | 6460/10788 [01:54<01:25, 50.38it/s]Training CobwebTree:  60%|    | 6466/10788 [01:54<01:27, 49.44it/s]Training CobwebTree:  60%|    | 6472/10788 [01:54<01:24, 50.98it/s]Training CobwebTree:  60%|    | 6478/10788 [01:54<01:23, 51.66it/s]Training CobwebTree:  60%|    | 6484/10788 [01:54<01:26, 49.64it/s]Training CobwebTree:  60%|    | 6490/10788 [01:54<01:25, 50.20it/s]Training CobwebTree:  60%|    | 6496/10788 [01:54<01:32, 46.63it/s]Training CobwebTree:  60%|    | 6501/10788 [01:55<01:32, 46.49it/s]Training CobwebTree:  60%|    | 6506/10788 [01:55<01:30, 47.12it/s]Training CobwebTree:  60%|    | 6512/10788 [01:55<01:27, 49.10it/s]Training CobwebTree:  60%|    | 6518/10788 [01:55<01:23, 51.41it/s]Training CobwebTree:  60%|    | 6524/10788 [01:55<01:26, 49.43it/s]Training CobwebTree:  61%|    | 6530/10788 [01:55<01:25, 49.98it/s]Training CobwebTree:  61%|    | 6536/10788 [01:55<01:25, 50.02it/s]Training CobwebTree:  61%|    | 6542/10788 [01:55<01:25, 49.66it/s]Training CobwebTree:  61%|    | 6547/10788 [01:55<01:26, 48.78it/s]Training CobwebTree:  61%|    | 6552/10788 [01:56<01:27, 48.65it/s]Training CobwebTree:  61%|    | 6558/10788 [01:56<01:23, 50.92it/s]Training CobwebTree:  61%|    | 6564/10788 [01:56<01:24, 49.98it/s]Training CobwebTree:  61%|    | 6570/10788 [01:56<01:23, 50.39it/s]Training CobwebTree:  61%|    | 6576/10788 [01:56<01:22, 50.99it/s]Training CobwebTree:  61%|    | 6582/10788 [01:56<01:22, 50.90it/s]Training CobwebTree:  61%|    | 6588/10788 [01:56<01:20, 52.29it/s]Training CobwebTree:  61%|    | 6594/10788 [01:56<01:19, 52.76it/s]Training CobwebTree:  61%|    | 6600/10788 [01:56<01:17, 53.97it/s]Training CobwebTree:  61%|    | 6606/10788 [01:57<01:18, 53.17it/s]Training CobwebTree:  61%|   | 6612/10788 [01:57<01:16, 54.44it/s]Training CobwebTree:  61%|   | 6618/10788 [01:57<01:17, 53.88it/s]Training CobwebTree:  61%|   | 6625/10788 [01:57<01:13, 56.45it/s]Training CobwebTree:  61%|   | 6631/10788 [01:57<01:15, 55.42it/s]Training CobwebTree:  62%|   | 6637/10788 [01:57<01:18, 52.77it/s]Training CobwebTree:  62%|   | 6643/10788 [01:57<01:17, 53.15it/s]Training CobwebTree:  62%|   | 6649/10788 [01:57<01:21, 51.07it/s]Training CobwebTree:  62%|   | 6655/10788 [01:58<01:17, 53.09it/s]Training CobwebTree:  62%|   | 6661/10788 [01:58<01:18, 52.64it/s]Training CobwebTree:  62%|   | 6667/10788 [01:58<01:17, 53.48it/s]Training CobwebTree:  62%|   | 6673/10788 [01:58<01:16, 53.57it/s]Training CobwebTree:  62%|   | 6679/10788 [01:58<01:19, 51.86it/s]Training CobwebTree:  62%|   | 6685/10788 [01:58<01:18, 52.40it/s]Training CobwebTree:  62%|   | 6691/10788 [01:58<01:21, 50.31it/s]Training CobwebTree:  62%|   | 6697/10788 [01:58<01:21, 50.33it/s]Training CobwebTree:  62%|   | 6703/10788 [01:58<01:22, 49.32it/s]Training CobwebTree:  62%|   | 6709/10788 [01:59<01:21, 50.24it/s]Training CobwebTree:  62%|   | 6715/10788 [01:59<01:22, 49.63it/s]Training CobwebTree:  62%|   | 6720/10788 [01:59<01:27, 46.28it/s]Training CobwebTree:  62%|   | 6725/10788 [01:59<01:29, 45.17it/s]Training CobwebTree:  62%|   | 6730/10788 [01:59<01:31, 44.16it/s]Training CobwebTree:  62%|   | 6735/10788 [01:59<01:31, 44.20it/s]Training CobwebTree:  62%|   | 6740/10788 [01:59<01:30, 44.84it/s]Training CobwebTree:  63%|   | 6746/10788 [01:59<01:24, 47.60it/s]Training CobwebTree:  63%|   | 6751/10788 [01:59<01:26, 46.81it/s]Training CobwebTree:  63%|   | 6756/10788 [02:00<01:29, 44.91it/s]Training CobwebTree:  63%|   | 6761/10788 [02:00<01:28, 45.55it/s]Training CobwebTree:  63%|   | 6767/10788 [02:00<01:26, 46.70it/s]Training CobwebTree:  63%|   | 6773/10788 [02:00<01:23, 48.30it/s]Training CobwebTree:  63%|   | 6779/10788 [02:00<01:19, 50.24it/s]Training CobwebTree:  63%|   | 6785/10788 [02:00<01:21, 49.39it/s]Training CobwebTree:  63%|   | 6790/10788 [02:00<01:21, 49.19it/s]Training CobwebTree:  63%|   | 6795/10788 [02:00<01:21, 49.02it/s]Training CobwebTree:  63%|   | 6801/10788 [02:01<01:19, 50.39it/s]Training CobwebTree:  63%|   | 6807/10788 [02:01<01:19, 50.35it/s]Training CobwebTree:  63%|   | 6813/10788 [02:01<01:20, 49.43it/s]Training CobwebTree:  63%|   | 6818/10788 [02:01<01:21, 48.86it/s]Training CobwebTree:  63%|   | 6824/10788 [02:01<01:19, 50.15it/s]Training CobwebTree:  63%|   | 6830/10788 [02:01<01:18, 50.72it/s]Training CobwebTree:  63%|   | 6836/10788 [02:01<01:18, 50.20it/s]Training CobwebTree:  63%|   | 6842/10788 [02:01<01:15, 52.12it/s]Training CobwebTree:  63%|   | 6848/10788 [02:01<01:17, 50.95it/s]Training CobwebTree:  64%|   | 6854/10788 [02:02<01:13, 53.36it/s]Training CobwebTree:  64%|   | 6860/10788 [02:02<01:15, 51.75it/s]Training CobwebTree:  64%|   | 6866/10788 [02:02<01:17, 50.65it/s]Training CobwebTree:  64%|   | 6873/10788 [02:02<01:14, 52.87it/s]Training CobwebTree:  64%|   | 6879/10788 [02:02<01:13, 53.12it/s]Training CobwebTree:  64%|   | 6885/10788 [02:02<01:12, 53.49it/s]Training CobwebTree:  64%|   | 6891/10788 [02:02<01:15, 51.74it/s]Training CobwebTree:  64%|   | 6897/10788 [02:02<01:17, 50.35it/s]Training CobwebTree:  64%|   | 6903/10788 [02:03<01:17, 50.43it/s]Training CobwebTree:  64%|   | 6909/10788 [02:03<01:18, 49.42it/s]Training CobwebTree:  64%|   | 6914/10788 [02:03<01:18, 49.13it/s]Training CobwebTree:  64%|   | 6919/10788 [02:03<01:20, 48.28it/s]Training CobwebTree:  64%|   | 6925/10788 [02:03<01:17, 49.84it/s]Training CobwebTree:  64%|   | 6930/10788 [02:03<01:20, 48.07it/s]Training CobwebTree:  64%|   | 6936/10788 [02:03<01:18, 48.92it/s]Training CobwebTree:  64%|   | 6942/10788 [02:03<01:18, 48.68it/s]Training CobwebTree:  64%|   | 6947/10788 [02:03<01:20, 47.45it/s]Training CobwebTree:  64%|   | 6952/10788 [02:04<01:21, 47.25it/s]Training CobwebTree:  64%|   | 6957/10788 [02:04<01:21, 47.11it/s]Training CobwebTree:  65%|   | 6963/10788 [02:04<01:18, 48.43it/s]Training CobwebTree:  65%|   | 6968/10788 [02:04<01:21, 46.69it/s]Training CobwebTree:  65%|   | 6973/10788 [02:04<01:21, 47.09it/s]Training CobwebTree:  65%|   | 6978/10788 [02:04<01:20, 47.54it/s]Training CobwebTree:  65%|   | 6983/10788 [02:04<01:19, 48.01it/s]Training CobwebTree:  65%|   | 6989/10788 [02:04<01:15, 50.12it/s]Training CobwebTree:  65%|   | 6995/10788 [02:04<01:16, 49.32it/s]Training CobwebTree:  65%|   | 7000/10788 [02:05<01:19, 47.55it/s]Training CobwebTree:  65%|   | 7005/10788 [02:05<01:20, 47.13it/s]Training CobwebTree:  65%|   | 7011/10788 [02:05<01:16, 49.21it/s]Training CobwebTree:  65%|   | 7016/10788 [02:05<01:16, 49.31it/s]Training CobwebTree:  65%|   | 7022/10788 [02:05<01:14, 50.41it/s]Training CobwebTree:  65%|   | 7028/10788 [02:05<01:14, 50.30it/s]Training CobwebTree:  65%|   | 7034/10788 [02:05<01:14, 50.54it/s]Training CobwebTree:  65%|   | 7040/10788 [02:05<01:12, 51.67it/s]Training CobwebTree:  65%|   | 7046/10788 [02:05<01:10, 53.15it/s]Training CobwebTree:  65%|   | 7052/10788 [02:06<01:10, 53.30it/s]Training CobwebTree:  65%|   | 7058/10788 [02:06<01:09, 53.86it/s]Training CobwebTree:  65%|   | 7064/10788 [02:06<01:12, 51.25it/s]Training CobwebTree:  66%|   | 7070/10788 [02:06<01:09, 53.16it/s]Training CobwebTree:  66%|   | 7076/10788 [02:06<01:10, 52.69it/s]Training CobwebTree:  66%|   | 7083/10788 [02:06<01:09, 53.50it/s]Training CobwebTree:  66%|   | 7089/10788 [02:06<01:11, 51.39it/s]Training CobwebTree:  66%|   | 7095/10788 [02:06<01:10, 52.64it/s]Training CobwebTree:  66%|   | 7101/10788 [02:06<01:09, 53.13it/s]Training CobwebTree:  66%|   | 7107/10788 [02:07<01:08, 53.42it/s]Training CobwebTree:  66%|   | 7113/10788 [02:07<01:11, 51.04it/s]Training CobwebTree:  66%|   | 7119/10788 [02:07<01:10, 51.78it/s]Training CobwebTree:  66%|   | 7125/10788 [02:07<01:11, 51.06it/s]Training CobwebTree:  66%|   | 7131/10788 [02:07<01:12, 50.14it/s]Training CobwebTree:  66%|   | 7137/10788 [02:07<01:10, 51.48it/s]Training CobwebTree:  66%|   | 7143/10788 [02:07<01:09, 52.43it/s]Training CobwebTree:  66%|   | 7149/10788 [02:07<01:14, 49.18it/s]Training CobwebTree:  66%|   | 7155/10788 [02:08<01:12, 50.30it/s]Training CobwebTree:  66%|   | 7161/10788 [02:08<01:10, 51.14it/s]Training CobwebTree:  66%|   | 7167/10788 [02:08<01:13, 49.09it/s]Training CobwebTree:  66%|   | 7172/10788 [02:08<01:13, 49.19it/s]Training CobwebTree:  67%|   | 7177/10788 [02:08<01:13, 49.20it/s]Training CobwebTree:  67%|   | 7183/10788 [02:08<01:11, 50.34it/s]Training CobwebTree:  67%|   | 7189/10788 [02:08<01:13, 48.82it/s]Training CobwebTree:  67%|   | 7195/10788 [02:08<01:14, 48.09it/s]Training CobwebTree:  67%|   | 7201/10788 [02:08<01:12, 49.78it/s]Training CobwebTree:  67%|   | 7207/10788 [02:09<01:12, 49.30it/s]Training CobwebTree:  67%|   | 7212/10788 [02:09<01:15, 47.64it/s]Training CobwebTree:  67%|   | 7217/10788 [02:09<01:15, 47.33it/s]Training CobwebTree:  67%|   | 7223/10788 [02:09<01:11, 49.63it/s]Training CobwebTree:  67%|   | 7229/10788 [02:09<01:09, 51.17it/s]Training CobwebTree:  67%|   | 7235/10788 [02:09<01:10, 50.12it/s]Training CobwebTree:  67%|   | 7241/10788 [02:09<01:12, 48.66it/s]Training CobwebTree:  67%|   | 7247/10788 [02:09<01:09, 50.62it/s]Training CobwebTree:  67%|   | 7253/10788 [02:10<01:11, 49.20it/s]Training CobwebTree:  67%|   | 7258/10788 [02:10<01:15, 46.77it/s]Training CobwebTree:  67%|   | 7264/10788 [02:10<01:12, 48.37it/s]Training CobwebTree:  67%|   | 7269/10788 [02:10<01:13, 47.63it/s]Training CobwebTree:  67%|   | 7275/10788 [02:10<01:11, 49.09it/s]Training CobwebTree:  67%|   | 7281/10788 [02:10<01:11, 49.03it/s]Training CobwebTree:  68%|   | 7288/10788 [02:10<01:06, 52.47it/s]Training CobwebTree:  68%|   | 7294/10788 [02:10<01:06, 52.77it/s]Training CobwebTree:  68%|   | 7300/10788 [02:10<01:04, 53.82it/s]Training CobwebTree:  68%|   | 7306/10788 [02:11<01:06, 52.15it/s]Training CobwebTree:  68%|   | 7312/10788 [02:11<01:06, 52.10it/s]Training CobwebTree:  68%|   | 7318/10788 [02:11<01:06, 52.12it/s]Training CobwebTree:  68%|   | 7324/10788 [02:11<01:07, 51.04it/s]Training CobwebTree:  68%|   | 7330/10788 [02:11<01:07, 51.42it/s]Training CobwebTree:  68%|   | 7336/10788 [02:11<01:08, 50.65it/s]Training CobwebTree:  68%|   | 7342/10788 [02:11<01:04, 53.05it/s]Training CobwebTree:  68%|   | 7348/10788 [02:11<01:06, 51.97it/s]Training CobwebTree:  68%|   | 7354/10788 [02:11<01:05, 52.28it/s]Training CobwebTree:  68%|   | 7360/10788 [02:12<01:04, 53.28it/s]Training CobwebTree:  68%|   | 7367/10788 [02:12<01:01, 55.33it/s]Training CobwebTree:  68%|   | 7373/10788 [02:12<01:02, 54.49it/s]Training CobwebTree:  68%|   | 7379/10788 [02:12<01:05, 52.30it/s]Training CobwebTree:  68%|   | 7385/10788 [02:12<01:04, 52.39it/s]Training CobwebTree:  69%|   | 7391/10788 [02:12<01:06, 50.70it/s]Training CobwebTree:  69%|   | 7397/10788 [02:12<01:07, 50.54it/s]Training CobwebTree:  69%|   | 7403/10788 [02:12<01:08, 49.34it/s]Training CobwebTree:  69%|   | 7408/10788 [02:13<01:09, 48.97it/s]Training CobwebTree:  69%|   | 7413/10788 [02:13<01:09, 48.74it/s]Training CobwebTree:  69%|   | 7419/10788 [02:13<01:07, 50.04it/s]Training CobwebTree:  69%|   | 7425/10788 [02:13<01:05, 51.38it/s]Training CobwebTree:  69%|   | 7431/10788 [02:13<01:06, 50.34it/s]Training CobwebTree:  69%|   | 7437/10788 [02:13<01:07, 49.89it/s]Training CobwebTree:  69%|   | 7442/10788 [02:13<01:08, 48.76it/s]Training CobwebTree:  69%|   | 7448/10788 [02:13<01:06, 50.51it/s]Training CobwebTree:  69%|   | 7454/10788 [02:13<01:06, 49.84it/s]Training CobwebTree:  69%|   | 7459/10788 [02:14<01:07, 49.50it/s]Training CobwebTree:  69%|   | 7464/10788 [02:14<01:07, 49.13it/s]Training CobwebTree:  69%|   | 7469/10788 [02:14<01:11, 46.19it/s]Training CobwebTree:  69%|   | 7475/10788 [02:14<01:09, 47.79it/s]Training CobwebTree:  69%|   | 7481/10788 [02:14<01:05, 50.17it/s]Training CobwebTree:  69%|   | 7487/10788 [02:14<01:04, 51.52it/s]Training CobwebTree:  69%|   | 7493/10788 [02:14<01:05, 50.25it/s]Training CobwebTree:  70%|   | 7499/10788 [02:14<01:07, 48.96it/s]Training CobwebTree:  70%|   | 7504/10788 [02:15<01:09, 47.54it/s]Training CobwebTree:  70%|   | 7510/10788 [02:15<01:06, 49.54it/s]Training CobwebTree:  70%|   | 7516/10788 [02:15<01:06, 49.40it/s]Training CobwebTree:  70%|   | 7521/10788 [02:15<01:06, 48.93it/s]Training CobwebTree:  70%|   | 7526/10788 [02:15<01:07, 48.37it/s]Training CobwebTree:  70%|   | 7531/10788 [02:15<01:08, 47.45it/s]Training CobwebTree:  70%|   | 7537/10788 [02:15<01:07, 48.45it/s]Training CobwebTree:  70%|   | 7542/10788 [02:15<01:07, 47.94it/s]Training CobwebTree:  70%|   | 7547/10788 [02:15<01:07, 47.72it/s]Training CobwebTree:  70%|   | 7553/10788 [02:15<01:03, 50.56it/s]Training CobwebTree:  70%|   | 7559/10788 [02:16<01:02, 51.25it/s]Training CobwebTree:  70%|   | 7565/10788 [02:16<01:02, 51.84it/s]Training CobwebTree:  70%|   | 7571/10788 [02:16<01:02, 51.44it/s]Training CobwebTree:  70%|   | 7577/10788 [02:16<01:00, 53.18it/s]Training CobwebTree:  70%|   | 7583/10788 [02:16<01:01, 52.37it/s]Training CobwebTree:  70%|   | 7589/10788 [02:16<01:01, 51.94it/s]Training CobwebTree:  70%|   | 7595/10788 [02:16<01:01, 51.61it/s]Training CobwebTree:  70%|   | 7601/10788 [02:16<01:02, 50.62it/s]Training CobwebTree:  71%|   | 7607/10788 [02:17<01:01, 51.55it/s]Training CobwebTree:  71%|   | 7613/10788 [02:17<01:01, 51.99it/s]Training CobwebTree:  71%|   | 7619/10788 [02:17<01:00, 52.27it/s]Training CobwebTree:  71%|   | 7625/10788 [02:17<00:59, 52.87it/s]Training CobwebTree:  71%|   | 7631/10788 [02:17<01:02, 50.64it/s]Training CobwebTree:  71%|   | 7637/10788 [02:17<01:04, 49.01it/s]Training CobwebTree:  71%|   | 7642/10788 [02:17<01:04, 48.45it/s]Training CobwebTree:  71%|   | 7647/10788 [02:17<01:04, 48.75it/s]Training CobwebTree:  71%|   | 7652/10788 [02:17<01:05, 47.74it/s]Training CobwebTree:  71%|   | 7657/10788 [02:18<01:05, 48.06it/s]Training CobwebTree:  71%|   | 7662/10788 [02:18<01:06, 47.27it/s]Training CobwebTree:  71%|   | 7667/10788 [02:18<01:05, 47.68it/s]Training CobwebTree:  71%|   | 7672/10788 [02:18<01:06, 46.77it/s]Training CobwebTree:  71%|   | 7678/10788 [02:18<01:05, 47.57it/s]Training CobwebTree:  71%|   | 7683/10788 [02:18<01:05, 47.49it/s]Training CobwebTree:  71%|  | 7688/10788 [02:18<01:04, 47.90it/s]Training CobwebTree:  71%|  | 7694/10788 [02:18<01:04, 48.31it/s]Training CobwebTree:  71%|  | 7699/10788 [02:18<01:05, 47.09it/s]Training CobwebTree:  71%|  | 7704/10788 [02:19<01:05, 46.81it/s]Training CobwebTree:  71%|  | 7710/10788 [02:19<01:03, 48.76it/s]Training CobwebTree:  72%|  | 7715/10788 [02:19<01:02, 49.04it/s]Training CobwebTree:  72%|  | 7721/10788 [02:19<01:00, 50.65it/s]Training CobwebTree:  72%|  | 7727/10788 [02:19<01:00, 50.42it/s]Training CobwebTree:  72%|  | 7733/10788 [02:19<01:00, 50.62it/s]Training CobwebTree:  72%|  | 7739/10788 [02:19<00:59, 50.88it/s]Training CobwebTree:  72%|  | 7745/10788 [02:19<01:00, 50.55it/s]Training CobwebTree:  72%|  | 7751/10788 [02:19<01:03, 48.03it/s]Training CobwebTree:  72%|  | 7757/10788 [02:20<01:01, 49.55it/s]Training CobwebTree:  72%|  | 7763/10788 [02:20<01:00, 50.38it/s]Training CobwebTree:  72%|  | 7769/10788 [02:20<01:00, 50.05it/s]Training CobwebTree:  72%|  | 7775/10788 [02:20<01:00, 49.85it/s]Training CobwebTree:  72%|  | 7780/10788 [02:20<01:00, 49.85it/s]Training CobwebTree:  72%|  | 7785/10788 [02:20<01:00, 49.55it/s]Training CobwebTree:  72%|  | 7790/10788 [02:20<01:02, 47.83it/s]Training CobwebTree:  72%|  | 7795/10788 [02:20<01:01, 48.36it/s]Training CobwebTree:  72%|  | 7800/10788 [02:20<01:02, 47.74it/s]Training CobwebTree:  72%|  | 7806/10788 [02:21<01:01, 48.41it/s]Training CobwebTree:  72%|  | 7811/10788 [02:21<01:04, 46.33it/s]Training CobwebTree:  72%|  | 7817/10788 [02:21<01:01, 48.04it/s]Training CobwebTree:  73%|  | 7822/10788 [02:21<01:03, 47.06it/s]Training CobwebTree:  73%|  | 7827/10788 [02:21<01:02, 47.13it/s]Training CobwebTree:  73%|  | 7833/10788 [02:21<00:58, 50.39it/s]Training CobwebTree:  73%|  | 7839/10788 [02:21<00:57, 50.97it/s]Training CobwebTree:  73%|  | 7845/10788 [02:21<00:58, 50.62it/s]Training CobwebTree:  73%|  | 7851/10788 [02:21<00:56, 51.71it/s]Training CobwebTree:  73%|  | 7857/10788 [02:22<00:56, 51.77it/s]Training CobwebTree:  73%|  | 7863/10788 [02:22<00:56, 51.77it/s]Training CobwebTree:  73%|  | 7869/10788 [02:22<00:57, 50.51it/s]Training CobwebTree:  73%|  | 7875/10788 [02:22<01:00, 48.54it/s]Training CobwebTree:  73%|  | 7880/10788 [02:22<01:00, 48.24it/s]Training CobwebTree:  73%|  | 7886/10788 [02:22<00:59, 49.13it/s]Training CobwebTree:  73%|  | 7892/10788 [02:22<00:58, 49.79it/s]Training CobwebTree:  73%|  | 7898/10788 [02:22<00:57, 50.66it/s]Training CobwebTree:  73%|  | 7904/10788 [02:23<00:58, 49.08it/s]Training CobwebTree:  73%|  | 7909/10788 [02:23<01:00, 47.56it/s]Training CobwebTree:  73%|  | 7914/10788 [02:23<01:00, 47.64it/s]Training CobwebTree:  73%|  | 7920/10788 [02:23<00:58, 49.30it/s]Training CobwebTree:  73%|  | 7926/10788 [02:23<00:57, 50.03it/s]Training CobwebTree:  74%|  | 7932/10788 [02:23<00:58, 48.94it/s]Training CobwebTree:  74%|  | 7937/10788 [02:23<01:01, 46.63it/s]Training CobwebTree:  74%|  | 7942/10788 [02:23<01:01, 46.43it/s]Training CobwebTree:  74%|  | 7948/10788 [02:23<00:58, 48.31it/s]Training CobwebTree:  74%|  | 7953/10788 [02:24<00:59, 47.76it/s]Training CobwebTree:  74%|  | 7958/10788 [02:24<00:59, 47.18it/s]Training CobwebTree:  74%|  | 7963/10788 [02:24<01:00, 47.01it/s]Training CobwebTree:  74%|  | 7969/10788 [02:24<00:59, 47.38it/s]Training CobwebTree:  74%|  | 7974/10788 [02:24<00:58, 48.06it/s]Training CobwebTree:  74%|  | 7980/10788 [02:24<00:57, 48.48it/s]Training CobwebTree:  74%|  | 7986/10788 [02:24<00:55, 50.93it/s]Training CobwebTree:  74%|  | 7992/10788 [02:24<00:56, 49.43it/s]Training CobwebTree:  74%|  | 7997/10788 [02:25<00:57, 48.96it/s]Training CobwebTree:  74%|  | 8002/10788 [02:25<00:56, 49.09it/s]Training CobwebTree:  74%|  | 8007/10788 [02:25<00:56, 49.15it/s]Training CobwebTree:  74%|  | 8012/10788 [02:25<00:56, 49.08it/s]Training CobwebTree:  74%|  | 8018/10788 [02:25<00:55, 49.63it/s]Training CobwebTree:  74%|  | 8023/10788 [02:25<00:56, 49.20it/s]Training CobwebTree:  74%|  | 8029/10788 [02:25<00:55, 50.13it/s]Training CobwebTree:  74%|  | 8035/10788 [02:25<00:55, 49.64it/s]Training CobwebTree:  75%|  | 8041/10788 [02:25<00:53, 51.28it/s]Training CobwebTree:  75%|  | 8047/10788 [02:25<00:53, 51.30it/s]Training CobwebTree:  75%|  | 8053/10788 [02:26<00:52, 51.94it/s]Training CobwebTree:  75%|  | 8059/10788 [02:26<00:52, 52.00it/s]Training CobwebTree:  75%|  | 8065/10788 [02:26<00:53, 51.26it/s]Training CobwebTree:  75%|  | 8071/10788 [02:26<00:54, 50.20it/s]Training CobwebTree:  75%|  | 8077/10788 [02:26<00:51, 52.59it/s]Training CobwebTree:  75%|  | 8083/10788 [02:26<00:51, 52.36it/s]Training CobwebTree:  75%|  | 8089/10788 [02:26<00:51, 52.06it/s]Training CobwebTree:  75%|  | 8095/10788 [02:26<00:53, 50.16it/s]Training CobwebTree:  75%|  | 8101/10788 [02:27<00:55, 48.84it/s]Training CobwebTree:  75%|  | 8107/10788 [02:27<00:54, 49.45it/s]Training CobwebTree:  75%|  | 8112/10788 [02:27<00:55, 48.10it/s]Training CobwebTree:  75%|  | 8117/10788 [02:27<00:56, 47.50it/s]Training CobwebTree:  75%|  | 8123/10788 [02:27<00:54, 49.23it/s]Training CobwebTree:  75%|  | 8128/10788 [02:27<00:54, 49.15it/s]Training CobwebTree:  75%|  | 8134/10788 [02:27<00:53, 49.35it/s]Training CobwebTree:  75%|  | 8140/10788 [02:27<00:51, 51.71it/s]Training CobwebTree:  76%|  | 8146/10788 [02:27<00:51, 51.40it/s]Training CobwebTree:  76%|  | 8152/10788 [02:28<00:52, 50.67it/s]Training CobwebTree:  76%|  | 8158/10788 [02:28<00:52, 50.18it/s]Training CobwebTree:  76%|  | 8164/10788 [02:28<00:52, 50.20it/s]Training CobwebTree:  76%|  | 8170/10788 [02:28<00:50, 52.30it/s]Training CobwebTree:  76%|  | 8176/10788 [02:28<00:51, 51.02it/s]Training CobwebTree:  76%|  | 8182/10788 [02:28<00:53, 49.06it/s]Training CobwebTree:  76%|  | 8187/10788 [02:28<00:55, 47.24it/s]Training CobwebTree:  76%|  | 8192/10788 [02:28<00:54, 47.35it/s]Training CobwebTree:  76%|  | 8197/10788 [02:29<00:54, 47.87it/s]Training CobwebTree:  76%|  | 8202/10788 [02:29<00:55, 46.99it/s]Training CobwebTree:  76%|  | 8207/10788 [02:29<00:54, 47.23it/s]Training CobwebTree:  76%|  | 8213/10788 [02:29<00:52, 48.94it/s]Training CobwebTree:  76%|  | 8219/10788 [02:29<00:52, 49.10it/s]Training CobwebTree:  76%|  | 8225/10788 [02:29<00:52, 49.13it/s]Training CobwebTree:  76%|  | 8230/10788 [02:29<00:53, 48.13it/s]Training CobwebTree:  76%|  | 8235/10788 [02:29<00:52, 48.21it/s]Training CobwebTree:  76%|  | 8241/10788 [02:29<00:51, 49.18it/s]Training CobwebTree:  76%|  | 8246/10788 [02:30<00:52, 48.41it/s]Training CobwebTree:  76%|  | 8251/10788 [02:30<00:55, 46.05it/s]Training CobwebTree:  77%|  | 8256/10788 [02:30<00:56, 44.81it/s]Training CobwebTree:  77%|  | 8261/10788 [02:30<00:57, 44.17it/s]Training CobwebTree:  77%|  | 8267/10788 [02:30<00:54, 46.38it/s]Training CobwebTree:  77%|  | 8272/10788 [02:30<00:54, 45.76it/s]Training CobwebTree:  77%|  | 8277/10788 [02:30<00:55, 45.18it/s]Training CobwebTree:  77%|  | 8282/10788 [02:30<00:57, 43.38it/s]Training CobwebTree:  77%|  | 8287/10788 [02:30<00:57, 43.74it/s]Training CobwebTree:  77%|  | 8292/10788 [02:31<00:59, 41.77it/s]Training CobwebTree:  77%|  | 8297/10788 [02:31<01:00, 41.04it/s]Training CobwebTree:  77%|  | 8302/10788 [02:31<00:59, 41.58it/s]Training CobwebTree:  77%|  | 8307/10788 [02:31<01:00, 41.20it/s]Training CobwebTree:  77%|  | 8312/10788 [02:31<01:02, 39.87it/s]Training CobwebTree:  77%|  | 8317/10788 [02:31<00:58, 42.44it/s]Training CobwebTree:  77%|  | 8323/10788 [02:31<00:55, 44.80it/s]Training CobwebTree:  77%|  | 8328/10788 [02:31<00:53, 45.88it/s]Training CobwebTree:  77%|  | 8333/10788 [02:32<00:53, 46.09it/s]Training CobwebTree:  77%|  | 8339/10788 [02:32<00:52, 46.23it/s]Training CobwebTree:  77%|  | 8345/10788 [02:32<00:49, 49.70it/s]Training CobwebTree:  77%|  | 8351/10788 [02:32<00:51, 47.31it/s]Training CobwebTree:  77%|  | 8356/10788 [02:32<00:51, 47.46it/s]Training CobwebTree:  78%|  | 8361/10788 [02:32<00:50, 48.04it/s]Training CobwebTree:  78%|  | 8366/10788 [02:32<00:51, 47.06it/s]Training CobwebTree:  78%|  | 8371/10788 [02:32<00:52, 45.98it/s]Training CobwebTree:  78%|  | 8376/10788 [02:32<00:52, 46.26it/s]Training CobwebTree:  78%|  | 8382/10788 [02:33<00:51, 46.34it/s]Training CobwebTree:  78%|  | 8387/10788 [02:33<00:52, 45.39it/s]Training CobwebTree:  78%|  | 8393/10788 [02:33<00:49, 48.19it/s]Training CobwebTree:  78%|  | 8399/10788 [02:33<00:48, 49.14it/s]Training CobwebTree:  78%|  | 8405/10788 [02:33<00:47, 50.57it/s]Training CobwebTree:  78%|  | 8411/10788 [02:33<00:47, 49.65it/s]Training CobwebTree:  78%|  | 8416/10788 [02:33<00:48, 48.57it/s]Training CobwebTree:  78%|  | 8422/10788 [02:33<00:45, 51.63it/s]Training CobwebTree:  78%|  | 8428/10788 [02:33<00:45, 51.89it/s]Training CobwebTree:  78%|  | 8434/10788 [02:34<00:47, 49.71it/s]Training CobwebTree:  78%|  | 8440/10788 [02:34<00:45, 51.94it/s]Training CobwebTree:  78%|  | 8446/10788 [02:34<00:43, 53.93it/s]Training CobwebTree:  78%|  | 8452/10788 [02:34<00:46, 50.77it/s]Training CobwebTree:  78%|  | 8458/10788 [02:34<00:45, 51.74it/s]Training CobwebTree:  78%|  | 8464/10788 [02:34<00:46, 49.45it/s]Training CobwebTree:  79%|  | 8470/10788 [02:34<00:50, 46.35it/s]Training CobwebTree:  79%|  | 8475/10788 [02:34<00:49, 46.41it/s]Training CobwebTree:  79%|  | 8481/10788 [02:35<00:48, 47.91it/s]Training CobwebTree:  79%|  | 8487/10788 [02:35<00:46, 49.67it/s]Training CobwebTree:  79%|  | 8493/10788 [02:35<00:46, 49.14it/s]Training CobwebTree:  79%|  | 8500/10788 [02:35<00:43, 52.41it/s]Training CobwebTree:  79%|  | 8506/10788 [02:35<00:42, 53.43it/s]Training CobwebTree:  79%|  | 8512/10788 [02:35<00:43, 52.70it/s]Training CobwebTree:  79%|  | 8518/10788 [02:35<00:44, 50.60it/s]Training CobwebTree:  79%|  | 8524/10788 [02:35<00:44, 51.40it/s]Training CobwebTree:  79%|  | 8530/10788 [02:35<00:43, 51.80it/s]Training CobwebTree:  79%|  | 8536/10788 [02:36<00:44, 51.05it/s]Training CobwebTree:  79%|  | 8542/10788 [02:36<00:45, 49.45it/s]Training CobwebTree:  79%|  | 8547/10788 [02:36<00:46, 48.29it/s]Training CobwebTree:  79%|  | 8552/10788 [02:36<00:46, 48.49it/s]Training CobwebTree:  79%|  | 8557/10788 [02:36<00:46, 48.39it/s]Training CobwebTree:  79%|  | 8563/10788 [02:36<00:45, 48.71it/s]Training CobwebTree:  79%|  | 8568/10788 [02:36<00:46, 47.88it/s]Training CobwebTree:  79%|  | 8573/10788 [02:36<00:46, 47.57it/s]Training CobwebTree:  80%|  | 8578/10788 [02:36<00:45, 48.23it/s]Training CobwebTree:  80%|  | 8584/10788 [02:37<00:44, 49.76it/s]Training CobwebTree:  80%|  | 8589/10788 [02:37<00:45, 48.17it/s]Training CobwebTree:  80%|  | 8595/10788 [02:37<00:44, 49.21it/s]Training CobwebTree:  80%|  | 8601/10788 [02:37<00:44, 49.21it/s]Training CobwebTree:  80%|  | 8607/10788 [02:37<00:42, 50.99it/s]Training CobwebTree:  80%|  | 8613/10788 [02:37<00:43, 50.33it/s]Training CobwebTree:  80%|  | 8619/10788 [02:37<00:43, 50.31it/s]Training CobwebTree:  80%|  | 8625/10788 [02:37<00:43, 50.16it/s]Training CobwebTree:  80%|  | 8631/10788 [02:38<00:43, 49.22it/s]Training CobwebTree:  80%|  | 8636/10788 [02:38<00:43, 48.91it/s]Training CobwebTree:  80%|  | 8641/10788 [02:38<00:45, 47.09it/s]Training CobwebTree:  80%|  | 8646/10788 [02:38<00:46, 46.35it/s]Training CobwebTree:  80%|  | 8652/10788 [02:38<00:44, 48.03it/s]Training CobwebTree:  80%|  | 8658/10788 [02:38<00:43, 49.15it/s]Training CobwebTree:  80%|  | 8663/10788 [02:38<00:44, 47.82it/s]Training CobwebTree:  80%|  | 8668/10788 [02:38<00:44, 47.34it/s]Training CobwebTree:  80%|  | 8674/10788 [02:38<00:42, 49.33it/s]Training CobwebTree:  80%|  | 8679/10788 [02:39<00:42, 49.45it/s]Training CobwebTree:  81%|  | 8685/10788 [02:39<00:41, 50.50it/s]Training CobwebTree:  81%|  | 8691/10788 [02:39<00:40, 51.58it/s]Training CobwebTree:  81%|  | 8697/10788 [02:39<00:43, 48.33it/s]Training CobwebTree:  81%|  | 8703/10788 [02:39<00:41, 50.54it/s]Training CobwebTree:  81%|  | 8709/10788 [02:39<00:40, 50.99it/s]Training CobwebTree:  81%|  | 8715/10788 [02:39<00:39, 51.92it/s]Training CobwebTree:  81%|  | 8721/10788 [02:39<00:39, 51.81it/s]Training CobwebTree:  81%|  | 8727/10788 [02:39<00:40, 50.88it/s]Training CobwebTree:  81%|  | 8733/10788 [02:40<00:41, 49.00it/s]Training CobwebTree:  81%|  | 8739/10788 [02:40<00:41, 49.30it/s]Training CobwebTree:  81%|  | 8745/10788 [02:40<00:41, 49.26it/s]Training CobwebTree:  81%|  | 8750/10788 [02:40<00:41, 48.81it/s]Training CobwebTree:  81%|  | 8756/10788 [02:40<00:40, 50.20it/s]Training CobwebTree:  81%|  | 8762/10788 [02:40<00:39, 50.91it/s]Training CobwebTree:  81%| | 8768/10788 [02:40<00:39, 50.91it/s]Training CobwebTree:  81%| | 8774/10788 [02:40<00:40, 50.13it/s]Training CobwebTree:  81%| | 8780/10788 [02:41<00:39, 50.46it/s]Training CobwebTree:  81%| | 8786/10788 [02:41<00:38, 52.01it/s]Training CobwebTree:  81%| | 8792/10788 [02:41<00:38, 51.86it/s]Training CobwebTree:  82%| | 8798/10788 [02:41<00:38, 51.65it/s]Training CobwebTree:  82%| | 8804/10788 [02:41<00:37, 52.46it/s]Training CobwebTree:  82%| | 8810/10788 [02:41<00:38, 51.42it/s]Training CobwebTree:  82%| | 8816/10788 [02:41<00:39, 50.29it/s]Training CobwebTree:  82%| | 8822/10788 [02:41<00:39, 49.73it/s]Training CobwebTree:  82%| | 8827/10788 [02:41<00:40, 48.89it/s]Training CobwebTree:  82%| | 8832/10788 [02:42<00:39, 49.02it/s]Training CobwebTree:  82%| | 8838/10788 [02:42<00:38, 50.75it/s]Training CobwebTree:  82%| | 8844/10788 [02:42<00:38, 50.43it/s]Training CobwebTree:  82%| | 8850/10788 [02:42<00:37, 51.18it/s]Training CobwebTree:  82%| | 8856/10788 [02:42<00:38, 50.06it/s]Training CobwebTree:  82%| | 8862/10788 [02:42<00:39, 48.56it/s]Training CobwebTree:  82%| | 8867/10788 [02:42<00:40, 47.81it/s]Training CobwebTree:  82%| | 8873/10788 [02:42<00:39, 48.65it/s]Training CobwebTree:  82%| | 8878/10788 [02:43<00:39, 48.47it/s]Training CobwebTree:  82%| | 8884/10788 [02:43<00:37, 50.30it/s]Training CobwebTree:  82%| | 8890/10788 [02:43<00:37, 51.24it/s]Training CobwebTree:  82%| | 8896/10788 [02:43<00:37, 50.85it/s]Training CobwebTree:  83%| | 8902/10788 [02:43<00:37, 50.68it/s]Training CobwebTree:  83%| | 8908/10788 [02:43<00:37, 50.75it/s]Training CobwebTree:  83%| | 8914/10788 [02:43<00:36, 51.32it/s]Training CobwebTree:  83%| | 8920/10788 [02:43<00:36, 50.90it/s]Training CobwebTree:  83%| | 8926/10788 [02:43<00:37, 49.72it/s]Training CobwebTree:  83%| | 8932/10788 [02:44<00:37, 49.30it/s]Training CobwebTree:  83%| | 8937/10788 [02:44<00:37, 48.85it/s]Training CobwebTree:  83%| | 8942/10788 [02:44<00:37, 48.64it/s]Training CobwebTree:  83%| | 8948/10788 [02:44<00:36, 50.52it/s]Training CobwebTree:  83%| | 8954/10788 [02:44<00:37, 48.99it/s]Training CobwebTree:  83%| | 8959/10788 [02:44<00:38, 47.03it/s]Training CobwebTree:  83%| | 8965/10788 [02:44<00:37, 48.17it/s]Training CobwebTree:  83%| | 8971/10788 [02:44<00:36, 49.94it/s]Training CobwebTree:  83%| | 8977/10788 [02:44<00:36, 50.23it/s]Training CobwebTree:  83%| | 8983/10788 [02:45<00:37, 47.59it/s]Training CobwebTree:  83%| | 8989/10788 [02:45<00:37, 48.42it/s]Training CobwebTree:  83%| | 8995/10788 [02:45<00:36, 48.66it/s]Training CobwebTree:  83%| | 9000/10788 [02:45<00:37, 47.56it/s]Training CobwebTree:  83%| | 9006/10788 [02:45<00:35, 49.51it/s]Training CobwebTree:  84%| | 9011/10788 [02:45<00:37, 47.79it/s]Training CobwebTree:  84%| | 9017/10788 [02:45<00:36, 48.55it/s]Training CobwebTree:  84%| | 9023/10788 [02:45<00:35, 49.84it/s]Training CobwebTree:  84%| | 9029/10788 [02:46<00:34, 51.43it/s]Training CobwebTree:  84%| | 9035/10788 [02:46<00:33, 52.66it/s]Training CobwebTree:  84%| | 9041/10788 [02:46<00:34, 51.26it/s]Training CobwebTree:  84%| | 9047/10788 [02:46<00:33, 52.37it/s]Training CobwebTree:  84%| | 9053/10788 [02:46<00:35, 49.50it/s]Training CobwebTree:  84%| | 9058/10788 [02:46<00:35, 49.31it/s]Training CobwebTree:  84%| | 9063/10788 [02:46<00:34, 49.34it/s]Training CobwebTree:  84%| | 9069/10788 [02:46<00:34, 49.81it/s]Training CobwebTree:  84%| | 9075/10788 [02:46<00:34, 50.37it/s]Training CobwebTree:  84%| | 9081/10788 [02:47<00:34, 49.10it/s]Training CobwebTree:  84%| | 9087/10788 [02:47<00:34, 49.45it/s]Training CobwebTree:  84%| | 9092/10788 [02:47<00:34, 48.82it/s]Training CobwebTree:  84%| | 9097/10788 [02:47<00:35, 47.61it/s]Training CobwebTree:  84%| | 9103/10788 [02:47<00:34, 48.74it/s]Training CobwebTree:  84%| | 9109/10788 [02:47<00:34, 48.11it/s]Training CobwebTree:  84%| | 9114/10788 [02:47<00:35, 47.73it/s]Training CobwebTree:  85%| | 9119/10788 [02:47<00:34, 48.19it/s]Training CobwebTree:  85%| | 9125/10788 [02:48<00:34, 48.41it/s]Training CobwebTree:  85%| | 9131/10788 [02:48<00:32, 50.72it/s]Training CobwebTree:  85%| | 9137/10788 [02:48<00:33, 49.91it/s]Training CobwebTree:  85%| | 9143/10788 [02:48<00:33, 48.63it/s]Training CobwebTree:  85%| | 9149/10788 [02:48<00:33, 49.12it/s]Training CobwebTree:  85%| | 9154/10788 [02:48<00:33, 48.49it/s]Training CobwebTree:  85%| | 9160/10788 [02:48<00:32, 49.45it/s]Training CobwebTree:  85%| | 9165/10788 [02:48<00:32, 49.45it/s]Training CobwebTree:  85%| | 9171/10788 [02:48<00:31, 51.12it/s]Training CobwebTree:  85%| | 9177/10788 [02:49<00:32, 50.22it/s]Training CobwebTree:  85%| | 9183/10788 [02:49<00:31, 51.35it/s]Training CobwebTree:  85%| | 9189/10788 [02:49<00:31, 50.68it/s]Training CobwebTree:  85%| | 9195/10788 [02:49<00:30, 51.44it/s]Training CobwebTree:  85%| | 9201/10788 [02:49<00:30, 52.21it/s]Training CobwebTree:  85%| | 9207/10788 [02:49<00:30, 51.98it/s]Training CobwebTree:  85%| | 9213/10788 [02:49<00:29, 53.28it/s]Training CobwebTree:  85%| | 9219/10788 [02:49<00:28, 54.36it/s]Training CobwebTree:  86%| | 9225/10788 [02:49<00:29, 53.84it/s]Training CobwebTree:  86%| | 9231/10788 [02:50<00:29, 51.94it/s]Training CobwebTree:  86%| | 9237/10788 [02:50<00:30, 50.80it/s]Training CobwebTree:  86%| | 9243/10788 [02:50<00:31, 48.67it/s]Training CobwebTree:  86%| | 9250/10788 [02:50<00:29, 52.51it/s]Training CobwebTree:  86%| | 9256/10788 [02:50<00:28, 53.01it/s]Training CobwebTree:  86%| | 9262/10788 [02:50<00:28, 53.57it/s]Training CobwebTree:  86%| | 9268/10788 [02:50<00:28, 53.61it/s]Training CobwebTree:  86%| | 9274/10788 [02:50<00:29, 51.84it/s]Training CobwebTree:  86%| | 9280/10788 [02:51<00:28, 52.11it/s]Training CobwebTree:  86%| | 9286/10788 [02:51<00:30, 48.74it/s]Training CobwebTree:  86%| | 9292/10788 [02:51<00:30, 49.02it/s]Training CobwebTree:  86%| | 9297/10788 [02:51<00:30, 48.85it/s]Training CobwebTree:  86%| | 9303/10788 [02:51<00:29, 49.50it/s]Training CobwebTree:  86%| | 9309/10788 [02:51<00:29, 50.69it/s]Training CobwebTree:  86%| | 9315/10788 [02:51<00:28, 51.67it/s]Training CobwebTree:  86%| | 9321/10788 [02:51<00:27, 53.20it/s]Training CobwebTree:  86%| | 9327/10788 [02:51<00:28, 51.60it/s]Training CobwebTree:  87%| | 9333/10788 [02:52<00:28, 51.50it/s]Training CobwebTree:  87%| | 9339/10788 [02:52<00:27, 52.46it/s]Training CobwebTree:  87%| | 9345/10788 [02:52<00:27, 53.20it/s]Training CobwebTree:  87%| | 9351/10788 [02:52<00:28, 51.25it/s]Training CobwebTree:  87%| | 9357/10788 [02:52<00:28, 50.06it/s]Training CobwebTree:  87%| | 9363/10788 [02:52<00:29, 47.72it/s]Training CobwebTree:  87%| | 9368/10788 [02:52<00:30, 46.98it/s]Training CobwebTree:  87%| | 9373/10788 [02:52<00:29, 47.65it/s]Training CobwebTree:  87%| | 9379/10788 [02:52<00:28, 48.93it/s]Training CobwebTree:  87%| | 9384/10788 [02:53<00:29, 47.85it/s]Training CobwebTree:  87%| | 9389/10788 [02:53<00:29, 47.00it/s]Training CobwebTree:  87%| | 9394/10788 [02:53<00:29, 47.41it/s]Training CobwebTree:  87%| | 9399/10788 [02:53<00:29, 47.60it/s]Training CobwebTree:  87%| | 9404/10788 [02:53<00:29, 46.83it/s]Training CobwebTree:  87%| | 9410/10788 [02:53<00:27, 49.30it/s]Training CobwebTree:  87%| | 9415/10788 [02:53<00:28, 47.82it/s]Training CobwebTree:  87%| | 9420/10788 [02:53<00:28, 48.16it/s]Training CobwebTree:  87%| | 9425/10788 [02:53<00:28, 48.34it/s]Training CobwebTree:  87%| | 9430/10788 [02:54<00:27, 48.80it/s]Training CobwebTree:  87%| | 9435/10788 [02:54<00:27, 48.39it/s]Training CobwebTree:  88%| | 9441/10788 [02:54<00:27, 49.28it/s]Training CobwebTree:  88%| | 9447/10788 [02:54<00:26, 50.36it/s]Training CobwebTree:  88%| | 9453/10788 [02:54<00:25, 52.56it/s]Training CobwebTree:  88%| | 9459/10788 [02:54<00:25, 51.26it/s]Training CobwebTree:  88%| | 9465/10788 [02:54<00:25, 51.64it/s]Training CobwebTree:  88%| | 9471/10788 [02:54<00:25, 51.35it/s]Training CobwebTree:  88%| | 9477/10788 [02:54<00:25, 51.49it/s]Training CobwebTree:  88%| | 9483/10788 [02:55<00:25, 52.11it/s]Training CobwebTree:  88%| | 9489/10788 [02:55<00:25, 49.99it/s]Training CobwebTree:  88%| | 9495/10788 [02:55<00:26, 49.63it/s]Training CobwebTree:  88%| | 9500/10788 [02:55<00:25, 49.55it/s]Training CobwebTree:  88%| | 9506/10788 [02:55<00:25, 50.25it/s]Training CobwebTree:  88%| | 9512/10788 [02:55<00:24, 51.66it/s]Training CobwebTree:  88%| | 9518/10788 [02:55<00:25, 50.73it/s]Training CobwebTree:  88%| | 9524/10788 [02:55<00:25, 50.54it/s]Training CobwebTree:  88%| | 9530/10788 [02:56<00:25, 49.94it/s]Training CobwebTree:  88%| | 9536/10788 [02:56<00:25, 48.25it/s]Training CobwebTree:  88%| | 9541/10788 [02:56<00:26, 47.80it/s]Training CobwebTree:  88%| | 9546/10788 [02:56<00:26, 46.15it/s]Training CobwebTree:  89%| | 9551/10788 [02:56<00:26, 46.77it/s]Training CobwebTree:  89%| | 9557/10788 [02:56<00:25, 47.99it/s]Training CobwebTree:  89%| | 9563/10788 [02:56<00:24, 49.16it/s]Training CobwebTree:  89%| | 9568/10788 [02:56<00:25, 48.39it/s]Training CobwebTree:  89%| | 9574/10788 [02:56<00:24, 48.83it/s]Training CobwebTree:  89%| | 9579/10788 [02:57<00:25, 48.29it/s]Training CobwebTree:  89%| | 9585/10788 [02:57<00:24, 49.30it/s]Training CobwebTree:  89%| | 9591/10788 [02:57<00:24, 48.88it/s]Training CobwebTree:  89%| | 9596/10788 [02:57<00:24, 47.71it/s]Training CobwebTree:  89%| | 9601/10788 [02:57<00:24, 47.91it/s]Training CobwebTree:  89%| | 9606/10788 [02:57<00:25, 46.21it/s]Training CobwebTree:  89%| | 9611/10788 [02:57<00:25, 46.57it/s]Training CobwebTree:  89%| | 9616/10788 [02:57<00:25, 46.86it/s]Training CobwebTree:  89%| | 9622/10788 [02:57<00:24, 47.55it/s]Training CobwebTree:  89%| | 9628/10788 [02:58<00:23, 50.02it/s]Training CobwebTree:  89%| | 9634/10788 [02:58<00:24, 46.48it/s]Training CobwebTree:  89%| | 9639/10788 [02:58<00:24, 46.37it/s]Training CobwebTree:  89%| | 9644/10788 [02:58<00:24, 46.35it/s]Training CobwebTree:  89%| | 9650/10788 [02:58<00:23, 47.83it/s]Training CobwebTree:  90%| | 9656/10788 [02:58<00:23, 48.62it/s]Training CobwebTree:  90%| | 9661/10788 [02:58<00:23, 48.60it/s]Training CobwebTree:  90%| | 9667/10788 [02:58<00:22, 49.83it/s]Training CobwebTree:  90%| | 9673/10788 [02:59<00:22, 49.75it/s]Training CobwebTree:  90%| | 9678/10788 [02:59<00:22, 49.52it/s]Training CobwebTree:  90%| | 9683/10788 [02:59<00:22, 48.96it/s]Training CobwebTree:  90%| | 9689/10788 [02:59<00:21, 51.46it/s]Training CobwebTree:  90%| | 9695/10788 [02:59<00:20, 52.05it/s]Training CobwebTree:  90%| | 9701/10788 [02:59<00:21, 51.57it/s]Training CobwebTree:  90%| | 9707/10788 [02:59<00:21, 49.66it/s]Training CobwebTree:  90%| | 9713/10788 [02:59<00:20, 51.38it/s]Training CobwebTree:  90%| | 9719/10788 [02:59<00:21, 49.46it/s]Training CobwebTree:  90%| | 9725/10788 [03:00<00:21, 50.25it/s]Training CobwebTree:  90%| | 9731/10788 [03:00<00:21, 49.02it/s]Training CobwebTree:  90%| | 9737/10788 [03:00<00:21, 49.36it/s]Training CobwebTree:  90%| | 9742/10788 [03:00<00:21, 49.36it/s]Training CobwebTree:  90%| | 9747/10788 [03:00<00:21, 48.46it/s]Training CobwebTree:  90%| | 9752/10788 [03:00<00:22, 46.72it/s]Training CobwebTree:  90%| | 9758/10788 [03:00<00:21, 47.06it/s]Training CobwebTree:  90%| | 9763/10788 [03:00<00:22, 46.05it/s]Training CobwebTree:  91%| | 9768/10788 [03:00<00:22, 44.89it/s]Training CobwebTree:  91%| | 9773/10788 [03:01<00:23, 43.68it/s]Training CobwebTree:  91%| | 9778/10788 [03:01<00:23, 42.33it/s]Training CobwebTree:  91%| | 9783/10788 [03:01<00:22, 44.24it/s]Training CobwebTree:  91%| | 9788/10788 [03:01<00:22, 44.22it/s]Training CobwebTree:  91%| | 9793/10788 [03:01<00:22, 44.48it/s]Training CobwebTree:  91%| | 9798/10788 [03:01<00:21, 45.33it/s]Training CobwebTree:  91%| | 9803/10788 [03:01<00:21, 44.94it/s]Training CobwebTree:  91%| | 9809/10788 [03:01<00:20, 46.68it/s]Training CobwebTree:  91%| | 9814/10788 [03:01<00:20, 47.06it/s]Training CobwebTree:  91%| | 9819/10788 [03:02<00:21, 45.10it/s]Training CobwebTree:  91%| | 9824/10788 [03:02<00:21, 44.86it/s]Training CobwebTree:  91%| | 9829/10788 [03:02<00:21, 44.55it/s]Training CobwebTree:  91%| | 9834/10788 [03:02<00:22, 41.64it/s]Training CobwebTree:  91%| | 9839/10788 [03:02<00:23, 40.73it/s]Training CobwebTree:  91%| | 9844/10788 [03:02<00:22, 41.71it/s]Training CobwebTree:  91%|| 9849/10788 [03:02<00:22, 41.32it/s]Training CobwebTree:  91%|| 9855/10788 [03:02<00:20, 44.81it/s]Training CobwebTree:  91%|| 9860/10788 [03:03<00:20, 45.44it/s]Training CobwebTree:  91%|| 9865/10788 [03:03<00:20, 46.02it/s]Training CobwebTree:  91%|| 9870/10788 [03:03<00:19, 46.76it/s]Training CobwebTree:  92%|| 9875/10788 [03:03<00:19, 46.71it/s]Training CobwebTree:  92%|| 9880/10788 [03:03<00:19, 46.84it/s]Training CobwebTree:  92%|| 9886/10788 [03:03<00:18, 47.62it/s]Training CobwebTree:  92%|| 9892/10788 [03:03<00:17, 49.85it/s]Training CobwebTree:  92%|| 9897/10788 [03:03<00:18, 48.97it/s]Training CobwebTree:  92%|| 9902/10788 [03:03<00:18, 48.91it/s]Training CobwebTree:  92%|| 9907/10788 [03:04<00:18, 48.26it/s]Training CobwebTree:  92%|| 9912/10788 [03:04<00:18, 47.21it/s]Training CobwebTree:  92%|| 9917/10788 [03:04<00:19, 44.49it/s]Training CobwebTree:  92%|| 9922/10788 [03:04<00:19, 45.33it/s]Training CobwebTree:  92%|| 9928/10788 [03:04<00:17, 47.82it/s]Training CobwebTree:  92%|| 9933/10788 [03:04<00:17, 48.17it/s]Training CobwebTree:  92%|| 9939/10788 [03:04<00:17, 48.53it/s]Training CobwebTree:  92%|| 9944/10788 [03:04<00:17, 47.88it/s]Training CobwebTree:  92%|| 9950/10788 [03:04<00:16, 49.41it/s]Training CobwebTree:  92%|| 9956/10788 [03:05<00:16, 49.87it/s]Training CobwebTree:  92%|| 9962/10788 [03:05<00:16, 50.13it/s]Training CobwebTree:  92%|| 9968/10788 [03:05<00:16, 48.88it/s]Training CobwebTree:  92%|| 9974/10788 [03:05<00:16, 49.43it/s]Training CobwebTree:  93%|| 9980/10788 [03:05<00:16, 50.21it/s]Training CobwebTree:  93%|| 9986/10788 [03:05<00:16, 50.10it/s]Training CobwebTree:  93%|| 9992/10788 [03:05<00:15, 50.58it/s]Training CobwebTree:  93%|| 9998/10788 [03:05<00:15, 50.26it/s]Training CobwebTree:  93%|| 10004/10788 [03:06<00:15, 49.86it/s]Training CobwebTree:  93%|| 10009/10788 [03:06<00:15, 49.71it/s]Training CobwebTree:  93%|| 10015/10788 [03:06<00:15, 50.49it/s]Training CobwebTree:  93%|| 10021/10788 [03:06<00:14, 52.39it/s]Training CobwebTree:  93%|| 10027/10788 [03:06<00:14, 51.82it/s]Training CobwebTree:  93%|| 10033/10788 [03:06<00:15, 49.96it/s]Training CobwebTree:  93%|| 10039/10788 [03:06<00:15, 48.44it/s]Training CobwebTree:  93%|| 10045/10788 [03:06<00:14, 49.62it/s]Training CobwebTree:  93%|| 10050/10788 [03:06<00:15, 48.77it/s]Training CobwebTree:  93%|| 10056/10788 [03:07<00:15, 48.80it/s]Training CobwebTree:  93%|| 10061/10788 [03:07<00:15, 48.45it/s]Training CobwebTree:  93%|| 10066/10788 [03:07<00:15, 47.81it/s]Training CobwebTree:  93%|| 10071/10788 [03:07<00:15, 46.70it/s]Training CobwebTree:  93%|| 10077/10788 [03:07<00:14, 48.16it/s]Training CobwebTree:  93%|| 10082/10788 [03:07<00:15, 46.72it/s]Training CobwebTree:  94%|| 10087/10788 [03:07<00:14, 47.37it/s]Training CobwebTree:  94%|| 10092/10788 [03:07<00:14, 47.17it/s]Training CobwebTree:  94%|| 10097/10788 [03:07<00:14, 46.75it/s]Training CobwebTree:  94%|| 10103/10788 [03:08<00:14, 47.53it/s]Training CobwebTree:  94%|| 10108/10788 [03:08<00:14, 47.76it/s]Training CobwebTree:  94%|| 10113/10788 [03:08<00:14, 46.09it/s]Training CobwebTree:  94%|| 10119/10788 [03:08<00:14, 47.62it/s]Training CobwebTree:  94%|| 10124/10788 [03:08<00:14, 45.37it/s]Training CobwebTree:  94%|| 10130/10788 [03:08<00:14, 46.93it/s]Training CobwebTree:  94%|| 10135/10788 [03:08<00:13, 47.37it/s]Training CobwebTree:  94%|| 10140/10788 [03:08<00:13, 47.16it/s]Training CobwebTree:  94%|| 10145/10788 [03:08<00:13, 47.26it/s]Training CobwebTree:  94%|| 10151/10788 [03:09<00:12, 49.55it/s]Training CobwebTree:  94%|| 10157/10788 [03:09<00:12, 49.79it/s]Training CobwebTree:  94%|| 10163/10788 [03:09<00:12, 51.13it/s]Training CobwebTree:  94%|| 10169/10788 [03:09<00:12, 51.34it/s]Training CobwebTree:  94%|| 10175/10788 [03:09<00:12, 49.42it/s]Training CobwebTree:  94%|| 10180/10788 [03:09<00:12, 48.24it/s]Training CobwebTree:  94%|| 10185/10788 [03:09<00:12, 48.34it/s]Training CobwebTree:  94%|| 10190/10788 [03:09<00:12, 47.54it/s]Training CobwebTree:  95%|| 10195/10788 [03:09<00:12, 47.84it/s]Training CobwebTree:  95%|| 10200/10788 [03:10<00:12, 47.02it/s]Training CobwebTree:  95%|| 10205/10788 [03:10<00:12, 47.50it/s]Training CobwebTree:  95%|| 10211/10788 [03:10<00:11, 49.45it/s]Training CobwebTree:  95%|| 10217/10788 [03:10<00:11, 50.69it/s]Training CobwebTree:  95%|| 10223/10788 [03:10<00:10, 51.58it/s]Training CobwebTree:  95%|| 10229/10788 [03:10<00:11, 49.79it/s]Training CobwebTree:  95%|| 10234/10788 [03:10<00:11, 49.14it/s]Training CobwebTree:  95%|| 10240/10788 [03:10<00:10, 51.27it/s]Training CobwebTree:  95%|| 10246/10788 [03:10<00:10, 51.09it/s]Training CobwebTree:  95%|| 10252/10788 [03:11<00:10, 51.20it/s]Training CobwebTree:  95%|| 10258/10788 [03:11<00:10, 52.03it/s]Training CobwebTree:  95%|| 10264/10788 [03:11<00:10, 49.57it/s]Training CobwebTree:  95%|| 10269/10788 [03:11<00:10, 49.00it/s]Training CobwebTree:  95%|| 10274/10788 [03:11<00:10, 48.52it/s]Training CobwebTree:  95%|| 10279/10788 [03:11<00:10, 48.57it/s]Training CobwebTree:  95%|| 10284/10788 [03:11<00:10, 48.13it/s]Training CobwebTree:  95%|| 10289/10788 [03:11<00:10, 47.13it/s]Training CobwebTree:  95%|| 10295/10788 [03:11<00:10, 48.35it/s]Training CobwebTree:  95%|| 10300/10788 [03:12<00:10, 47.96it/s]Training CobwebTree:  96%|| 10306/10788 [03:12<00:09, 48.83it/s]Training CobwebTree:  96%|| 10312/10788 [03:12<00:09, 50.30it/s]Training CobwebTree:  96%|| 10318/10788 [03:12<00:09, 49.34it/s]Training CobwebTree:  96%|| 10323/10788 [03:12<00:09, 49.49it/s]Training CobwebTree:  96%|| 10329/10788 [03:12<00:09, 50.34it/s]Training CobwebTree:  96%|| 10335/10788 [03:12<00:09, 49.63it/s]Training CobwebTree:  96%|| 10340/10788 [03:12<00:09, 47.89it/s]Training CobwebTree:  96%|| 10345/10788 [03:13<00:09, 47.64it/s]Training CobwebTree:  96%|| 10351/10788 [03:13<00:08, 49.35it/s]Training CobwebTree:  96%|| 10356/10788 [03:13<00:08, 48.32it/s]Training CobwebTree:  96%|| 10362/10788 [03:13<00:08, 48.75it/s]Training CobwebTree:  96%|| 10367/10788 [03:13<00:08, 48.04it/s]Training CobwebTree:  96%|| 10373/10788 [03:13<00:08, 49.73it/s]Training CobwebTree:  96%|| 10378/10788 [03:13<00:08, 49.09it/s]Training CobwebTree:  96%|| 10384/10788 [03:13<00:07, 51.43it/s]Training CobwebTree:  96%|| 10390/10788 [03:13<00:07, 51.89it/s]Training CobwebTree:  96%|| 10396/10788 [03:14<00:07, 50.19it/s]Training CobwebTree:  96%|| 10402/10788 [03:14<00:07, 49.29it/s]Training CobwebTree:  96%|| 10407/10788 [03:14<00:07, 49.11it/s]Training CobwebTree:  97%|| 10412/10788 [03:14<00:07, 49.26it/s]Training CobwebTree:  97%|| 10417/10788 [03:14<00:07, 48.91it/s]Training CobwebTree:  97%|| 10423/10788 [03:14<00:07, 50.30it/s]Training CobwebTree:  97%|| 10429/10788 [03:14<00:07, 50.63it/s]Training CobwebTree:  97%|| 10435/10788 [03:14<00:06, 51.70it/s]Training CobwebTree:  97%|| 10441/10788 [03:14<00:06, 50.35it/s]Training CobwebTree:  97%|| 10447/10788 [03:15<00:06, 49.56it/s]Training CobwebTree:  97%|| 10452/10788 [03:15<00:06, 49.32it/s]Training CobwebTree:  97%|| 10458/10788 [03:15<00:06, 51.53it/s]Training CobwebTree:  97%|| 10464/10788 [03:15<00:06, 48.71it/s]Training CobwebTree:  97%|| 10470/10788 [03:15<00:06, 49.51it/s]Training CobwebTree:  97%|| 10476/10788 [03:15<00:06, 50.36it/s]Training CobwebTree:  97%|| 10482/10788 [03:15<00:06, 50.89it/s]Training CobwebTree:  97%|| 10488/10788 [03:15<00:05, 50.23it/s]Training CobwebTree:  97%|| 10494/10788 [03:16<00:06, 47.99it/s]Training CobwebTree:  97%|| 10499/10788 [03:16<00:06, 47.45it/s]Training CobwebTree:  97%|| 10504/10788 [03:16<00:05, 47.37it/s]Training CobwebTree:  97%|| 10510/10788 [03:16<00:05, 48.93it/s]Training CobwebTree:  97%|| 10515/10788 [03:16<00:05, 48.52it/s]Training CobwebTree:  98%|| 10520/10788 [03:16<00:05, 48.25it/s]Training CobwebTree:  98%|| 10527/10788 [03:16<00:05, 51.76it/s]Training CobwebTree:  98%|| 10533/10788 [03:16<00:04, 52.07it/s]Training CobwebTree:  98%|| 10539/10788 [03:16<00:04, 51.21it/s]Training CobwebTree:  98%|| 10545/10788 [03:17<00:04, 49.67it/s]Training CobwebTree:  98%|| 10550/10788 [03:17<00:04, 49.72it/s]Training CobwebTree:  98%|| 10555/10788 [03:17<00:04, 47.37it/s]Training CobwebTree:  98%|| 10560/10788 [03:17<00:04, 46.64it/s]Training CobwebTree:  98%|| 10565/10788 [03:17<00:04, 46.25it/s]Training CobwebTree:  98%|| 10571/10788 [03:17<00:04, 47.76it/s]Training CobwebTree:  98%|| 10577/10788 [03:17<00:04, 48.51it/s]Training CobwebTree:  98%|| 10582/10788 [03:17<00:04, 48.00it/s]Training CobwebTree:  98%|| 10588/10788 [03:17<00:04, 49.79it/s]Training CobwebTree:  98%|| 10594/10788 [03:18<00:03, 50.37it/s]Training CobwebTree:  98%|| 10600/10788 [03:18<00:03, 50.35it/s]Training CobwebTree:  98%|| 10606/10788 [03:18<00:03, 52.54it/s]Training CobwebTree:  98%|| 10612/10788 [03:18<00:03, 52.96it/s]Training CobwebTree:  98%|| 10618/10788 [03:18<00:03, 50.89it/s]Training CobwebTree:  98%|| 10624/10788 [03:18<00:03, 50.74it/s]Training CobwebTree:  99%|| 10630/10788 [03:18<00:03, 48.12it/s]Training CobwebTree:  99%|| 10635/10788 [03:18<00:03, 48.37it/s]Training CobwebTree:  99%|| 10641/10788 [03:18<00:03, 48.96it/s]Training CobwebTree:  99%|| 10647/10788 [03:19<00:02, 50.13it/s]Training CobwebTree:  99%|| 10653/10788 [03:19<00:02, 50.07it/s]Training CobwebTree:  99%|| 10659/10788 [03:19<00:02, 49.13it/s]Training CobwebTree:  99%|| 10665/10788 [03:19<00:02, 50.34it/s]Training CobwebTree:  99%|| 10671/10788 [03:19<00:02, 50.56it/s]Training CobwebTree:  99%|| 10677/10788 [03:19<00:02, 52.63it/s]Training CobwebTree:  99%|| 10683/10788 [03:19<00:02, 52.40it/s]Training CobwebTree:  99%|| 10689/10788 [03:19<00:01, 51.04it/s]Training CobwebTree:  99%|| 10695/10788 [03:20<00:01, 53.08it/s]Training CobwebTree:  99%|| 10701/10788 [03:20<00:01, 52.07it/s]Training CobwebTree:  99%|| 10707/10788 [03:20<00:01, 51.87it/s]Training CobwebTree:  99%|| 10713/10788 [03:20<00:01, 52.59it/s]Training CobwebTree:  99%|| 10719/10788 [03:20<00:01, 51.44it/s]Training CobwebTree:  99%|| 10725/10788 [03:20<00:01, 49.46it/s]Training CobwebTree:  99%|| 10730/10788 [03:20<00:01, 49.16it/s]Training CobwebTree: 100%|| 10736/10788 [03:20<00:01, 49.86it/s]Training CobwebTree: 100%|| 10741/10788 [03:20<00:00, 49.74it/s]Training CobwebTree: 100%|| 10747/10788 [03:21<00:00, 50.24it/s]Training CobwebTree: 100%|| 10753/10788 [03:21<00:00, 49.66it/s]Training CobwebTree: 100%|| 10758/10788 [03:21<00:00, 49.06it/s]Training CobwebTree: 100%|| 10764/10788 [03:21<00:00, 49.67it/s]Training CobwebTree: 100%|| 10769/10788 [03:21<00:00, 49.47it/s]Training CobwebTree: 100%|| 10775/10788 [03:21<00:00, 49.92it/s]Training CobwebTree: 100%|| 10780/10788 [03:21<00:00, 49.91it/s]Training CobwebTree: 100%|| 10785/10788 [03:21<00:00, 49.48it/s]Training CobwebTree: 100%|| 10788/10788 [03:21<00:00, 53.44it/s]
2025-12-20 23:15:31,482 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:15:38,875 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-20 23:15:38,884 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-20 23:15:38,887 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-20 23:15:38,903 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-20 23:15:38,943 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-20 23:15:39,150 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-20 23:15:39,238 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-20 23:15:40,360 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-20 23:15:40,387 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-20 23:15:40,660 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-20 23:15:40,662 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-20 23:15:40,813 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-20 23:15:41,236 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-20 23:15:41,469 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-20 23:15:41,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,571 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,571 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,573 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,573 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,573 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,573 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,585 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,585 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,585 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,587 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,587 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,587 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,589 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,591 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,596 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,629 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,629 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,629 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,629 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,641 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,641 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,641 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,641 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,645 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,645 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,645 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,649 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,649 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,649 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,649 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,657 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,657 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,657 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,657 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,661 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,661 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,661 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,661 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,666 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,669 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,669 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,669 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,669 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,673 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,673 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,673 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,677 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,677 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,677 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,681 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,681 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,697 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,701 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,701 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,705 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,711 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,729 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,758 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,765 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,832 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,849 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,860 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,861 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,867 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,769 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,906 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,945 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,969 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,970 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:41,985 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:41,997 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,085 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,086 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,096 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,125 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,213 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,338 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,340 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,453 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,513 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,520 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,521 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,641 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,660 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,673 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,753 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:42,937 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:42,988 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,107 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,357 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,496 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,528 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,597 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,713 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,836 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,838 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,858 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,858 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,881 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:43,928 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:15:43,937 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:15:49,254 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:15:49,416 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-20 23:15:49,827 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-20 23:15:58,136 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-20 23:15:58,138 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-20 23:15:58,139 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-20 23:15:58,141 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-20 23:15:58,143 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-20 23:15:58,145 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-20 23:15:58,146 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-20 23:15:58,148 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-20 23:15:58,150 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-20 23:15:58,152 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-20 23:15:58,154 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-20 23:15:58,156 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-20 23:15:58,158 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-20 23:15:58,160 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-20 23:15:58,161 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-20 23:15:58,163 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-20 23:15:58,164 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-20 23:15:58,166 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-20 23:15:58,168 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-20 23:15:58,170 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-20 23:15:58,171 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-20 23:15:58,173 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-20 23:15:58,175 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-20 23:15:58,177 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-20 23:15:58,178 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-20 23:15:58,180 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-20 23:15:58,182 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-20 23:15:58,183 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-20 23:15:58,185 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-20 23:15:58,187 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-20 23:15:58,189 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-20 23:15:58,190 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-20 23:15:58,192 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-20 23:15:58,193 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-20 23:15:58,195 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-20 23:15:58,197 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-20 23:15:58,199 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-20 23:15:58,200 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-20 23:15:58,202 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-20 23:15:58,203 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-20 23:15:58,205 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-20 23:15:58,207 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-20 23:15:58,208 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-20 23:15:58,209 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-20 23:15:58,211 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-20 23:15:58,213 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-20 23:15:58,215 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-20 23:15:58,216 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-20 23:15:58,218 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-20 23:15:58,220 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-20 23:15:58,222 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-20 23:15:58,224 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-20 23:15:58,226 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-20 23:15:58,227 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-20 23:15:58,229 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-20 23:15:58,232 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-20 23:15:58,233 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-20 23:15:58,235 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-20 23:15:58,237 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-20 23:15:58,240 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-20 23:15:58,241 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-20 23:15:58,243 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-20 23:15:58,245 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-20 23:15:58,246 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-20 23:15:58,248 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-20 23:15:58,250 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-20 23:15:58,252 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-20 23:15:58,253 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-20 23:15:58,255 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-20 23:15:58,257 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-20 23:15:58,259 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-20 23:15:58,261 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-20 23:15:58,262 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-20 23:15:58,264 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-20 23:15:58,266 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-20 23:15:58,268 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-20 23:15:58,270 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-20 23:15:58,272 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-20 23:15:58,273 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-20 23:15:58,275 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-20 23:15:58,277 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-20 23:15:58,278 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-20 23:15:58,280 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-20 23:15:58,281 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-20 23:15:58,283 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-20 23:15:58,285 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-20 23:15:58,287 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-20 23:15:58,289 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-20 23:15:58,291 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-20 23:15:58,293 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-20 23:15:58,295 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-20 23:15:58,297 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-20 23:15:58,298 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-20 23:15:58,300 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-20 23:15:58,302 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-20 23:15:58,304 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-20 23:15:58,306 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-20 23:15:58,308 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-20 23:15:58,310 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-20 23:15:58,312 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-20 23:15:58,313 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-20 23:15:58,315 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-20 23:15:58,317 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-20 23:15:58,319 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-20 23:15:58,321 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-20 23:15:58,323 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-20 23:15:58,324 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-20 23:15:58,326 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-20 23:15:58,328 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-20 23:15:58,330 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-20 23:15:58,332 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-20 23:15:58,334 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-20 23:15:58,335 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-20 23:15:58,337 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-20 23:15:58,338 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-20 23:15:58,341 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-20 23:15:58,342 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-20 23:15:58,344 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-20 23:15:58,346 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-20 23:15:58,348 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-20 23:15:58,350 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-20 23:15:58,352 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-20 23:15:58,353 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-20 23:15:58,355 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-20 23:15:58,357 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-20 23:15:58,358 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-20 23:15:58,360 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-20 23:15:58,362 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-20 23:15:58,364 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-20 23:15:58,365 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-20 23:15:58,367 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-20 23:15:58,369 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-20 23:15:58,370 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-20 23:15:58,372 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-20 23:15:58,374 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-20 23:15:58,376 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-20 23:15:58,377 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-20 23:15:58,379 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-20 23:15:58,381 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-20 23:15:58,382 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-20 23:15:58,384 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-20 23:15:58,385 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-20 23:15:58,387 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-20 23:15:58,389 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-20 23:15:58,390 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-20 23:15:58,391 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-20 23:15:58,393 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-20 23:15:58,742 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-20 23:15:58,745 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-20 23:15:58,747 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-20 23:15:58,783 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-20 23:15:58,798 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-20 23:15:58,854 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-20 23:15:58,863 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-20 23:15:58,875 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-20 23:15:58,931 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-20 23:15:58,943 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-20 23:15:59,003 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-20 23:15:59,014 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-20 23:15:59,075 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-20 23:15:59,076 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-20 23:15:59,086 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-20 23:15:59,146 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-20 23:15:59,163 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-20 23:15:59,215 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-20 23:15:59,223 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-20 23:15:59,239 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-20 23:15:59,290 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-20 23:15:59,310 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-20 23:16:00,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:00,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:00,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:00,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:00,941 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:00,945 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:00,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:00,966 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:00,969 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,017 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,033 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,046 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,049 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,052 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,052 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,055 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,073 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,081 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,108 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,117 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,119 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,129 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,130 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,134 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,143 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,185 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,186 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,188 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,189 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,209 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,211 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,253 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,258 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,265 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,272 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,285 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,293 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,297 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,302 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,313 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,321 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,321 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,322 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,345 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,347 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,358 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,371 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,385 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,410 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,415 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,416 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,417 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,422 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,465 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,479 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,485 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,495 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,513 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,513 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,517 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,525 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,533 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,544 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,545 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,546 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,565 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,566 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,566 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,609 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,642 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,643 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,653 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,656 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,659 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,669 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,672 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,672 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,673 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,674 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,682 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,685 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,686 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,693 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,709 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,716 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,725 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,729 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,738 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,765 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,773 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,777 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,781 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,786 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,793 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,794 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,797 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,829 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,830 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,861 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,870 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,893 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,897 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,913 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,917 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,921 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,929 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,937 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,938 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,948 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:01,972 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,973 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,985 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:01,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,013 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,013 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,017 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,021 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,025 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,049 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,077 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,098 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,109 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,125 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,178 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,188 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,189 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,192 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,193 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,197 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,197 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,273 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,281 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,285 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,285 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,309 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,429 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,461 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,508 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,535 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,625 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,633 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,665 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,681 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,757 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,825 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:02,881 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:02,953 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:03,045 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:03,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:03,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:03,113 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:03,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:03,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-20 23:16:03,617 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-20 23:16:08,299 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-20 23:16:08,414 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-20 23:16:08,710 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.6536614993433586, 'coherence_npmi': 0.18363377086496685, 'topic_diversity': 0.5876543209876544, 'inter_topic_similarity': 0.49001601338386536}
2025-12-20 23:16:08,710 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6673239110831066, 'coherence_npmi': 0.1725020595109562, 'topic_diversity': 0.54, 'inter_topic_similarity': 0.517137348651886}
2025-12-20 23:16:08,710 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6535226871990745, 'coherence_npmi': 0.157543807106644, 'topic_diversity': 0.5644444444444444, 'inter_topic_similarity': 0.4625857472419739}
