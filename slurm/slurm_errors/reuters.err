2025-12-23 23:34:39,125 INFO __main__: Starting benchmark for dataset=reuters
2025-12-23 23:34:42,215 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-23 23:34:42,646 INFO gensim.corpora.dictionary: built Dictionary<29631 unique tokens: ['10', '15', '17', '1985', '30']...> from 10000 documents (total 834996 corpus positions)
2025-12-23 23:34:42,650 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<29631 unique tokens: ['10', '15', '17', '1985', '30']...> from 10000 documents (total 834996 corpus positions)", 'datetime': '2025-12-23T23:34:42.646609', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-155-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-23 23:34:43,877 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-23 23:34:43,877 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-23 23:34:49,180 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10000 docs
2025-12-23 23:38:09,516 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:38:18,311 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-23 23:38:18,322 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-23 23:38:18,328 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-23 23:38:18,347 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-23 23:38:18,779 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-23 23:38:18,991 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-23 23:38:19,118 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-23 23:38:21,051 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-23 23:38:21,210 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-23 23:38:21,809 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-23 23:38:21,937 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-23 23:38:22,110 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-23 23:38:23,318 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-23 23:38:23,797 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (-255004 virtual)
2025-12-23 23:38:23,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,894 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,902 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,902 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,902 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,906 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,906 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,906 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,910 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,910 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,914 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,914 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,929 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:23,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,986 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:23,994 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,018 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,018 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,074 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,080 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,097 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,120 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,177 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,218 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,241 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,339 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,428 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,442 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,455 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,537 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,546 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,721 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:24,982 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:24,998 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,002 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,066 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,165 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,221 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,374 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,654 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,697 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,698 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,774 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,790 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,693 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,822 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,866 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,902 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,921 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:25,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:25,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,014 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,042 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,109 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,160 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,167 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,214 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,269 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,285 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,422 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,550 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,569 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,681 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,734 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,762 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,838 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,849 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,851 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:26,874 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,950 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:26,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,000 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,062 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,074 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,157 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,182 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,402 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,426 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,521 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,638 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:27,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:27,957 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,002 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,065 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,091 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,094 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,116 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,125 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,166 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,174 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,194 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,200 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,247 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,353 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,586 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,682 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,683 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,694 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,703 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,746 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,782 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,885 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,894 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,911 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:28,969 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:28,970 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,034 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,044 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,047 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,069 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,102 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,126 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,138 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,217 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,293 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,298 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,305 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,385 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,405 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,408 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,418 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,622 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,693 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,722 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,727 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,730 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,765 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,789 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:29,954 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,966 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:30,026 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:29,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:30,086 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:30,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:30,317 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:30,362 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:30,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:30,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:30,793 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:30,814 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:30,842 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,046 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,060 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,097 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,137 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,288 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,334 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,465 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,489 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,569 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,605 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,606 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:31,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:31,770 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,262 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,558 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,860 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,860 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:32,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:32,878 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,039 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,066 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:33,083 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:33,084 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:37,340 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:38:39,873 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 200058 virtual documents
2025-12-23 23:38:44,945 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:38:54,542 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-23 23:38:54,545 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-23 23:38:54,546 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-23 23:38:54,547 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-23 23:38:54,549 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-23 23:38:54,550 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-23 23:38:54,552 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-23 23:38:54,554 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-23 23:38:54,557 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-23 23:38:54,559 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-23 23:38:54,562 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-23 23:38:54,564 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-23 23:38:54,566 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-23 23:38:54,568 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-23 23:38:54,569 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-23 23:38:54,571 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-23 23:38:54,573 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-23 23:38:54,575 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-23 23:38:54,577 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-23 23:38:54,579 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-23 23:38:54,581 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-23 23:38:54,583 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-23 23:38:54,585 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-23 23:38:54,588 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-23 23:38:54,590 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-23 23:38:54,592 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-23 23:38:54,594 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-23 23:38:54,597 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-23 23:38:54,599 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-23 23:38:54,601 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-23 23:38:54,603 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-23 23:38:54,605 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-23 23:38:54,607 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-23 23:38:54,609 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-23 23:38:54,610 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-23 23:38:54,612 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-23 23:38:54,615 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-23 23:38:54,616 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-23 23:38:54,618 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-23 23:38:54,620 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-23 23:38:54,622 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-23 23:38:54,639 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-23 23:38:54,641 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-23 23:38:54,643 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-23 23:38:54,644 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-23 23:38:54,646 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-23 23:38:54,648 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-23 23:38:54,660 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-23 23:38:54,662 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-23 23:38:54,664 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-23 23:38:54,667 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-23 23:38:54,669 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-23 23:38:54,680 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-23 23:38:54,707 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-23 23:38:54,709 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-23 23:38:54,711 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-23 23:38:54,713 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-23 23:38:54,715 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-23 23:38:54,719 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-23 23:38:54,721 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-23 23:38:54,728 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-23 23:38:54,729 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-23 23:38:54,736 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-23 23:38:54,737 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-23 23:38:54,744 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-23 23:38:54,782 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-23 23:38:54,784 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-23 23:38:54,786 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-23 23:38:54,789 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-23 23:38:54,908 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-23 23:38:54,989 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-23 23:38:55,004 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-23 23:38:55,096 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-23 23:38:55,137 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-23 23:38:55,152 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-23 23:38:55,253 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-23 23:38:55,268 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-23 23:38:55,360 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-23 23:38:55,405 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-23 23:38:55,420 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-23 23:38:55,532 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-23 23:38:55,584 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-23 23:38:55,645 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-23 23:38:55,660 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-23 23:38:55,765 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-23 23:38:55,768 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-23 23:38:55,770 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-23 23:38:55,905 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-23 23:38:55,957 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-23 23:38:56,033 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-23 23:38:56,048 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-23 23:38:56,157 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-23 23:38:56,172 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-23 23:38:56,281 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-23 23:38:56,295 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-23 23:38:56,393 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-23 23:38:56,408 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-23 23:38:56,533 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-23 23:38:56,537 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-23 23:38:56,693 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-23 23:38:56,696 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-23 23:38:56,698 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-23 23:38:56,861 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-23 23:38:56,868 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-23 23:38:56,977 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-23 23:38:56,992 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-23 23:38:57,104 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-23 23:38:57,181 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-23 23:38:57,196 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-23 23:38:57,333 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-23 23:38:57,336 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-23 23:38:57,429 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-23 23:38:57,432 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-23 23:38:57,434 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-23 23:38:57,557 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-23 23:38:57,572 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-23 23:38:57,577 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-23 23:38:57,705 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-23 23:38:57,708 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-23 23:38:57,801 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-23 23:38:57,805 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-23 23:38:57,904 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-23 23:38:57,907 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-23 23:38:57,911 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-23 23:38:58,001 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-23 23:38:58,004 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-23 23:38:58,081 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-23 23:38:58,084 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-23 23:38:58,145 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-23 23:38:58,160 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-23 23:38:58,265 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-23 23:38:58,280 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-23 23:38:58,385 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-23 23:38:58,388 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-23 23:38:58,436 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-23 23:38:58,493 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-23 23:38:58,508 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-23 23:38:58,589 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-23 23:38:58,604 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-23 23:38:58,673 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-23 23:38:58,676 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-23 23:38:58,678 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-23 23:38:58,773 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-23 23:38:58,788 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-23 23:38:58,845 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-23 23:38:58,859 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-23 23:38:58,865 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-23 23:38:58,953 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-23 23:38:58,956 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-23 23:38:59,013 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-23 23:38:59,016 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-23 23:38:59,018 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-23 23:38:59,101 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-23 23:38:59,116 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-23 23:38:59,177 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-23 23:38:59,192 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-23 23:38:59,209 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (744996 virtual)
2025-12-23 23:38:59,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,216 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,217 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,218 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,220 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,222 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,224 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,226 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,227 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,228 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,229 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,235 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,238 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,249 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,274 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,282 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,282 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,282 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,282 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,286 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,286 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,286 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,286 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,286 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,290 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,290 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,290 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,290 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,298 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,298 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,298 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,298 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,302 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,302 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,302 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,302 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,306 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,310 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,310 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,310 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,310 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,314 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,314 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,314 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,314 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,322 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,322 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,322 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,326 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,326 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,326 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,326 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,326 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,330 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,330 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,330 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,334 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,334 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,334 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,338 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,338 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,338 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,342 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,342 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,342 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,346 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,350 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,350 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,350 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,358 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,362 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,366 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,390 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,487 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,498 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,502 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,517 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,521 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,654 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,762 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,842 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,881 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,882 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,885 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:38:59,853 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:38:59,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,042 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,073 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,078 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,117 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,147 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,154 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,186 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,241 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,250 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,533 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,683 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,782 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,794 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,890 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:00,922 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:00,937 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,037 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,050 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,075 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,082 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,138 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,138 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,202 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,294 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,297 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,378 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,378 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,389 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,590 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,694 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,698 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,733 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,778 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,834 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,781 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:01,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:01,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,006 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,096 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,110 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,118 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,166 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,182 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,194 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,214 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,238 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,280 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,314 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,322 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,337 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,344 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,351 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,351 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,353 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,374 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,386 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,390 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,573 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,782 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,813 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:02,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:02,913 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:03,208 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:39:03,222 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:39:08,937 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:39:09,824 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 745134 virtual documents
2025-12-23 23:39:11,341 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10000 docs
2025-12-23 23:41:42,774 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:41:51,496 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-23 23:41:51,507 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-23 23:41:51,512 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-23 23:41:51,530 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-23 23:41:51,577 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-23 23:41:51,584 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-23 23:41:51,593 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-23 23:41:52,535 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-23 23:41:52,712 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-23 23:41:53,145 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-23 23:41:53,292 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-23 23:41:53,504 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-23 23:41:54,202 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-23 23:41:54,369 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (-255004 virtual)
2025-12-23 23:41:54,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,391 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,391 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,396 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,397 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,400 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,406 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,406 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,434 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,438 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,439 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,442 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,445 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,447 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,450 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,450 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,450 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,478 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,478 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,478 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,482 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,482 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,492 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,494 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,494 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,494 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,498 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,498 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,498 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,502 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,502 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,502 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,502 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,514 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,514 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,514 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,518 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,554 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,561 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,580 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,585 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,682 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,690 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,708 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,710 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,776 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,822 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,822 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,701 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,885 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:54,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:54,934 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,014 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,022 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,030 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,096 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,154 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,283 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,390 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,393 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,472 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,522 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,529 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,537 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,540 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,571 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,621 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,634 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,664 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,674 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,678 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,690 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,702 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,778 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,842 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,867 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,874 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,894 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,902 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,910 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,920 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,950 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,974 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:55,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:55,982 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,022 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,074 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,124 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,225 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,298 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,308 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,414 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,442 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,536 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,637 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,674 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,790 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:56,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:56,914 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:57,187 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:57,201 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:41:57,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:41:57,316 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:02,523 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:42:02,748 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 200058 virtual documents
2025-12-23 23:42:03,231 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:42:12,198 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-23 23:42:12,201 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-23 23:42:12,202 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-23 23:42:12,204 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-23 23:42:12,205 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-23 23:42:12,206 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-23 23:42:12,208 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-23 23:42:12,210 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-23 23:42:12,212 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-23 23:42:12,214 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-23 23:42:12,216 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-23 23:42:12,218 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-23 23:42:12,220 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-23 23:42:12,222 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-23 23:42:12,224 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-23 23:42:12,225 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-23 23:42:12,227 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-23 23:42:12,229 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-23 23:42:12,230 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-23 23:42:12,232 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-23 23:42:12,234 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-23 23:42:12,236 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-23 23:42:12,239 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-23 23:42:12,241 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-23 23:42:12,259 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-23 23:42:12,261 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-23 23:42:12,263 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-23 23:42:12,265 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-23 23:42:12,267 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-23 23:42:12,269 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-23 23:42:12,270 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-23 23:42:12,272 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-23 23:42:12,274 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-23 23:42:12,275 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-23 23:42:12,277 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-23 23:42:12,279 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-23 23:42:12,282 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-23 23:42:12,284 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-23 23:42:12,285 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-23 23:42:12,287 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-23 23:42:12,289 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-23 23:42:12,291 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-23 23:42:12,293 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-23 23:42:12,294 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-23 23:42:12,296 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-23 23:42:12,298 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-23 23:42:12,300 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-23 23:42:12,302 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-23 23:42:12,304 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-23 23:42:12,306 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-23 23:42:12,309 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-23 23:42:12,311 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-23 23:42:12,314 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-23 23:42:12,316 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-23 23:42:12,318 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-23 23:42:12,320 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-23 23:42:12,322 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-23 23:42:12,324 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-23 23:42:12,326 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-23 23:42:12,329 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-23 23:42:12,331 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-23 23:42:12,333 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-23 23:42:12,335 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-23 23:42:12,337 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-23 23:42:12,338 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-23 23:42:12,341 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-23 23:42:12,342 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-23 23:42:12,344 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-23 23:42:12,346 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-23 23:42:12,348 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-23 23:42:12,349 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-23 23:42:12,352 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-23 23:42:12,353 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-23 23:42:12,355 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-23 23:42:12,358 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-23 23:42:12,360 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-23 23:42:12,362 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-23 23:42:12,363 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-23 23:42:12,365 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-23 23:42:12,367 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-23 23:42:12,369 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-23 23:42:12,371 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-23 23:42:12,543 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-23 23:42:12,545 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-23 23:42:12,547 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-23 23:42:12,567 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-23 23:42:12,583 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-23 23:42:12,611 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-23 23:42:12,856 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-23 23:42:12,943 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-23 23:42:12,960 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-23 23:42:13,056 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-23 23:42:13,177 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-23 23:42:13,191 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-23 23:42:13,261 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-23 23:42:13,275 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-23 23:42:13,365 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-23 23:42:13,379 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-23 23:42:13,512 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-23 23:42:13,515 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-23 23:42:13,624 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-23 23:42:13,626 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-23 23:42:13,628 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-23 23:42:13,777 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-23 23:42:13,791 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-23 23:42:13,797 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-23 23:42:13,916 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-23 23:42:13,997 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-23 23:42:14,000 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-23 23:42:14,002 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-23 23:42:14,125 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-23 23:42:14,128 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-23 23:42:14,129 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-23 23:42:14,232 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-23 23:42:14,235 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-23 23:42:14,237 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-23 23:42:14,369 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-23 23:42:14,372 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-23 23:42:14,374 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-23 23:42:14,513 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-23 23:42:14,516 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-23 23:42:14,518 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-23 23:42:14,636 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-23 23:42:14,639 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-23 23:42:14,641 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-23 23:42:14,645 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-23 23:42:14,761 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-23 23:42:14,764 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-23 23:42:14,766 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-23 23:42:14,849 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-23 23:42:14,863 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-23 23:42:14,865 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-23 23:42:14,949 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-23 23:42:14,951 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-23 23:42:14,953 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-23 23:42:14,959 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-23 23:42:14,960 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-23 23:42:15,063 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-23 23:42:15,065 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-23 23:42:15,128 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-23 23:42:15,153 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-23 23:42:15,155 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-23 23:42:15,157 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-23 23:42:15,220 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-23 23:42:15,265 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-23 23:42:15,279 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-23 23:42:15,321 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-23 23:42:15,335 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-23 23:42:15,337 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-23 23:42:15,396 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-23 23:42:15,417 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-23 23:42:15,431 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-23 23:42:15,433 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-23 23:42:15,480 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-23 23:42:15,532 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-23 23:42:15,535 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-23 23:42:15,536 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (744996 virtual)
2025-12-23 23:42:15,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,542 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,545 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,546 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,546 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,546 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,547 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,554 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,557 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,560 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,561 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,561 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,561 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,561 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,562 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,562 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,562 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,565 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,566 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,573 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,574 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,586 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,590 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,594 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,594 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,594 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,594 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,598 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,598 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,598 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,606 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,606 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,606 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,606 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,614 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,622 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,622 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,622 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,622 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,626 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,630 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,630 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,630 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,630 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,630 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,634 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,634 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,634 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,637 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,638 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,638 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,638 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,646 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,646 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,646 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,646 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,650 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,654 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,654 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,654 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,658 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,662 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,662 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,662 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,666 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,666 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,669 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,670 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,670 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,674 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,674 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,678 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,678 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,678 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,683 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,684 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,690 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,694 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,694 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,700 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,786 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,908 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,912 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,926 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:15,934 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:15,996 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,009 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,021 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,062 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,106 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,185 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,308 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,329 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,364 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,462 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,483 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,501 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,506 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,577 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,638 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,712 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,713 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,796 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,822 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,838 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,841 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:16,953 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,954 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:16,966 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,004 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,005 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,026 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,078 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,098 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,111 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,009 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,150 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,163 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,165 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,194 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,206 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,230 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,230 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,234 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,262 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,266 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,407 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,485 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,514 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,522 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,610 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,793 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:17,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:17,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:18,101 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:42:18,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:42:23,038 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:42:23,184 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 745134 virtual documents
2025-12-23 23:42:23,595 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10000 docs
Training CobwebTree:   0%|          | 0/10000 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 20/10000 [00:00<00:51, 195.62it/s]Training CobwebTree:   0%|          | 40/10000 [00:00<01:21, 122.40it/s]Training CobwebTree:   1%|          | 54/10000 [00:00<01:39, 99.56it/s] Training CobwebTree:   1%|          | 65/10000 [00:00<01:38, 101.15it/s]Training CobwebTree:   1%|          | 76/10000 [00:00<01:45, 93.82it/s] Training CobwebTree:   1%|          | 86/10000 [00:00<01:56, 84.91it/s]Training CobwebTree:   1%|          | 95/10000 [00:01<02:03, 79.90it/s]Training CobwebTree:   1%|          | 104/10000 [00:01<02:07, 77.62it/s]Training CobwebTree:   1%|          | 112/10000 [00:01<02:09, 76.27it/s]Training CobwebTree:   1%|          | 120/10000 [00:01<02:10, 75.53it/s]Training CobwebTree:   1%|         | 128/10000 [00:01<02:21, 69.54it/s]Training CobwebTree:   1%|         | 136/10000 [00:01<02:24, 68.27it/s]Training CobwebTree:   1%|         | 143/10000 [00:01<02:31, 65.05it/s]Training CobwebTree:   2%|         | 150/10000 [00:01<02:39, 61.92it/s]Training CobwebTree:   2%|         | 157/10000 [00:01<02:44, 59.93it/s]Training CobwebTree:   2%|         | 164/10000 [00:02<02:45, 59.36it/s]Training CobwebTree:   2%|         | 170/10000 [00:02<02:50, 57.56it/s]Training CobwebTree:   2%|         | 176/10000 [00:02<02:49, 57.86it/s]Training CobwebTree:   2%|         | 182/10000 [00:02<02:55, 56.10it/s]Training CobwebTree:   2%|         | 189/10000 [00:02<02:48, 58.38it/s]Training CobwebTree:   2%|         | 195/10000 [00:02<02:49, 57.71it/s]Training CobwebTree:   2%|         | 201/10000 [00:02<02:50, 57.50it/s]Training CobwebTree:   2%|         | 207/10000 [00:02<02:49, 57.76it/s]Training CobwebTree:   2%|         | 214/10000 [00:02<02:42, 60.27it/s]Training CobwebTree:   2%|         | 221/10000 [00:03<02:41, 60.73it/s]Training CobwebTree:   2%|         | 228/10000 [00:03<02:43, 59.61it/s]Training CobwebTree:   2%|         | 234/10000 [00:03<02:44, 59.29it/s]Training CobwebTree:   2%|         | 240/10000 [00:03<02:59, 54.52it/s]Training CobwebTree:   2%|         | 247/10000 [00:03<02:55, 55.62it/s]Training CobwebTree:   3%|         | 253/10000 [00:03<02:57, 54.95it/s]Training CobwebTree:   3%|         | 260/10000 [00:03<02:51, 56.72it/s]Training CobwebTree:   3%|         | 266/10000 [00:03<03:04, 52.70it/s]Training CobwebTree:   3%|         | 272/10000 [00:04<03:00, 53.95it/s]Training CobwebTree:   3%|         | 278/10000 [00:04<02:58, 54.41it/s]Training CobwebTree:   3%|         | 284/10000 [00:04<02:55, 55.48it/s]Training CobwebTree:   3%|         | 290/10000 [00:04<02:51, 56.62it/s]Training CobwebTree:   3%|         | 296/10000 [00:04<02:49, 57.10it/s]Training CobwebTree:   3%|         | 302/10000 [00:04<02:58, 54.29it/s]Training CobwebTree:   3%|         | 309/10000 [00:04<02:53, 55.95it/s]Training CobwebTree:   3%|         | 315/10000 [00:04<02:56, 54.78it/s]Training CobwebTree:   3%|         | 322/10000 [00:04<02:46, 58.06it/s]Training CobwebTree:   3%|         | 328/10000 [00:05<02:50, 56.81it/s]Training CobwebTree:   3%|         | 334/10000 [00:05<03:03, 52.62it/s]Training CobwebTree:   3%|         | 340/10000 [00:05<02:59, 53.94it/s]Training CobwebTree:   3%|         | 346/10000 [00:05<03:05, 52.12it/s]Training CobwebTree:   4%|         | 352/10000 [00:05<02:58, 54.12it/s]Training CobwebTree:   4%|         | 358/10000 [00:05<03:05, 51.89it/s]Training CobwebTree:   4%|         | 364/10000 [00:05<03:07, 51.27it/s]Training CobwebTree:   4%|         | 370/10000 [00:05<03:07, 51.43it/s]Training CobwebTree:   4%|         | 376/10000 [00:05<03:00, 53.26it/s]Training CobwebTree:   4%|         | 382/10000 [00:06<03:03, 52.51it/s]Training CobwebTree:   4%|         | 389/10000 [00:06<02:56, 54.35it/s]Training CobwebTree:   4%|         | 395/10000 [00:06<02:54, 55.11it/s]Training CobwebTree:   4%|         | 401/10000 [00:06<02:59, 53.57it/s]Training CobwebTree:   4%|         | 407/10000 [00:06<03:08, 50.99it/s]Training CobwebTree:   4%|         | 414/10000 [00:06<02:56, 54.41it/s]Training CobwebTree:   4%|         | 420/10000 [00:06<02:54, 54.98it/s]Training CobwebTree:   4%|         | 426/10000 [00:06<02:55, 54.57it/s]Training CobwebTree:   4%|         | 432/10000 [00:06<02:56, 54.23it/s]Training CobwebTree:   4%|         | 438/10000 [00:07<03:04, 51.73it/s]Training CobwebTree:   4%|         | 444/10000 [00:07<03:00, 53.02it/s]Training CobwebTree:   4%|         | 450/10000 [00:07<03:09, 50.31it/s]Training CobwebTree:   5%|         | 456/10000 [00:07<03:07, 50.98it/s]Training CobwebTree:   5%|         | 462/10000 [00:07<03:14, 49.09it/s]Training CobwebTree:   5%|         | 467/10000 [00:07<03:17, 48.25it/s]Training CobwebTree:   5%|         | 473/10000 [00:07<03:14, 49.06it/s]Training CobwebTree:   5%|         | 478/10000 [00:07<03:23, 46.87it/s]Training CobwebTree:   5%|         | 484/10000 [00:08<03:11, 49.62it/s]Training CobwebTree:   5%|         | 490/10000 [00:08<03:09, 50.23it/s]Training CobwebTree:   5%|         | 496/10000 [00:08<03:11, 49.53it/s]Training CobwebTree:   5%|         | 501/10000 [00:08<03:21, 47.25it/s]Training CobwebTree:   5%|         | 507/10000 [00:08<03:17, 48.04it/s]Training CobwebTree:   5%|         | 513/10000 [00:08<03:10, 49.68it/s]Training CobwebTree:   5%|         | 519/10000 [00:08<03:08, 50.28it/s]Training CobwebTree:   5%|         | 525/10000 [00:08<03:13, 48.96it/s]Training CobwebTree:   5%|         | 530/10000 [00:09<03:16, 48.13it/s]Training CobwebTree:   5%|         | 535/10000 [00:09<03:21, 47.05it/s]Training CobwebTree:   5%|         | 541/10000 [00:09<03:19, 47.46it/s]Training CobwebTree:   5%|         | 546/10000 [00:09<03:23, 46.50it/s]Training CobwebTree:   6%|         | 551/10000 [00:09<03:22, 46.73it/s]Training CobwebTree:   6%|         | 556/10000 [00:09<03:19, 47.44it/s]Training CobwebTree:   6%|         | 561/10000 [00:09<03:18, 47.45it/s]Training CobwebTree:   6%|         | 566/10000 [00:09<03:20, 47.15it/s]Training CobwebTree:   6%|         | 571/10000 [00:09<03:33, 44.10it/s]Training CobwebTree:   6%|         | 576/10000 [00:10<03:37, 43.36it/s]Training CobwebTree:   6%|         | 581/10000 [00:10<03:36, 43.41it/s]Training CobwebTree:   6%|         | 586/10000 [00:10<03:38, 43.08it/s]Training CobwebTree:   6%|         | 592/10000 [00:10<03:25, 45.73it/s]Training CobwebTree:   6%|         | 598/10000 [00:10<03:20, 46.79it/s]Training CobwebTree:   6%|         | 603/10000 [00:10<03:26, 45.42it/s]Training CobwebTree:   6%|         | 608/10000 [00:10<03:24, 45.85it/s]Training CobwebTree:   6%|         | 613/10000 [00:10<03:27, 45.33it/s]Training CobwebTree:   6%|         | 618/10000 [00:10<03:31, 44.45it/s]Training CobwebTree:   6%|         | 624/10000 [00:11<03:17, 47.58it/s]Training CobwebTree:   6%|         | 631/10000 [00:11<03:02, 51.42it/s]Training CobwebTree:   6%|         | 637/10000 [00:11<03:04, 50.66it/s]Training CobwebTree:   6%|         | 643/10000 [00:11<03:10, 49.23it/s]Training CobwebTree:   6%|         | 648/10000 [00:11<03:09, 49.32it/s]Training CobwebTree:   7%|         | 654/10000 [00:11<03:05, 50.50it/s]Training CobwebTree:   7%|         | 660/10000 [00:11<03:05, 50.46it/s]Training CobwebTree:   7%|         | 666/10000 [00:11<03:03, 50.99it/s]Training CobwebTree:   7%|         | 672/10000 [00:11<03:05, 50.33it/s]Training CobwebTree:   7%|         | 678/10000 [00:12<03:13, 48.15it/s]Training CobwebTree:   7%|         | 683/10000 [00:12<03:14, 47.80it/s]Training CobwebTree:   7%|         | 689/10000 [00:12<03:06, 49.96it/s]Training CobwebTree:   7%|         | 695/10000 [00:12<03:08, 49.38it/s]Training CobwebTree:   7%|         | 701/10000 [00:12<03:05, 50.16it/s]Training CobwebTree:   7%|         | 707/10000 [00:12<03:10, 48.84it/s]Training CobwebTree:   7%|         | 712/10000 [00:12<03:09, 49.08it/s]Training CobwebTree:   7%|         | 718/10000 [00:12<03:02, 50.76it/s]Training CobwebTree:   7%|         | 724/10000 [00:13<03:08, 49.23it/s]Training CobwebTree:   7%|         | 730/10000 [00:13<03:04, 50.24it/s]Training CobwebTree:   7%|         | 736/10000 [00:13<03:05, 49.93it/s]Training CobwebTree:   7%|         | 742/10000 [00:13<03:07, 49.34it/s]Training CobwebTree:   7%|         | 747/10000 [00:13<03:07, 49.35it/s]Training CobwebTree:   8%|         | 753/10000 [00:13<03:04, 50.07it/s]Training CobwebTree:   8%|         | 759/10000 [00:13<03:10, 48.63it/s]Training CobwebTree:   8%|         | 764/10000 [00:13<03:11, 48.33it/s]Training CobwebTree:   8%|         | 769/10000 [00:13<03:11, 48.19it/s]Training CobwebTree:   8%|         | 774/10000 [00:14<03:11, 48.23it/s]Training CobwebTree:   8%|         | 780/10000 [00:14<03:04, 49.87it/s]Training CobwebTree:   8%|         | 785/10000 [00:14<03:24, 45.16it/s]Training CobwebTree:   8%|         | 790/10000 [00:14<03:22, 45.51it/s]Training CobwebTree:   8%|         | 795/10000 [00:14<03:17, 46.67it/s]Training CobwebTree:   8%|         | 801/10000 [00:14<03:09, 48.44it/s]Training CobwebTree:   8%|         | 807/10000 [00:14<03:03, 50.07it/s]Training CobwebTree:   8%|         | 813/10000 [00:14<03:14, 47.30it/s]Training CobwebTree:   8%|         | 818/10000 [00:15<03:22, 45.34it/s]Training CobwebTree:   8%|         | 824/10000 [00:15<03:17, 46.46it/s]Training CobwebTree:   8%|         | 830/10000 [00:15<03:10, 48.22it/s]Training CobwebTree:   8%|         | 835/10000 [00:15<03:16, 46.66it/s]Training CobwebTree:   8%|         | 841/10000 [00:15<03:05, 49.32it/s]Training CobwebTree:   8%|         | 846/10000 [00:15<03:10, 48.13it/s]Training CobwebTree:   9%|         | 851/10000 [00:15<03:10, 48.03it/s]Training CobwebTree:   9%|         | 856/10000 [00:15<03:09, 48.32it/s]Training CobwebTree:   9%|         | 861/10000 [00:15<03:19, 45.90it/s]Training CobwebTree:   9%|         | 866/10000 [00:16<03:24, 44.77it/s]Training CobwebTree:   9%|         | 871/10000 [00:16<03:19, 45.82it/s]Training CobwebTree:   9%|         | 876/10000 [00:16<03:32, 42.84it/s]Training CobwebTree:   9%|         | 881/10000 [00:16<03:31, 43.17it/s]Training CobwebTree:   9%|         | 886/10000 [00:16<03:24, 44.51it/s]Training CobwebTree:   9%|         | 892/10000 [00:16<03:12, 47.38it/s]Training CobwebTree:   9%|         | 897/10000 [00:16<03:10, 47.80it/s]Training CobwebTree:   9%|         | 902/10000 [00:16<03:08, 48.31it/s]Training CobwebTree:   9%|         | 907/10000 [00:16<03:12, 47.21it/s]Training CobwebTree:   9%|         | 912/10000 [00:17<03:18, 45.89it/s]Training CobwebTree:   9%|         | 917/10000 [00:17<03:21, 45.15it/s]Training CobwebTree:   9%|         | 922/10000 [00:17<03:29, 43.43it/s]Training CobwebTree:   9%|         | 927/10000 [00:17<03:24, 44.26it/s]Training CobwebTree:   9%|         | 932/10000 [00:17<03:23, 44.48it/s]Training CobwebTree:   9%|         | 937/10000 [00:17<03:28, 43.45it/s]Training CobwebTree:   9%|         | 945/10000 [00:17<02:57, 51.14it/s]Training CobwebTree:  10%|         | 951/10000 [00:17<03:03, 49.19it/s]Training CobwebTree:  10%|         | 956/10000 [00:17<03:03, 49.24it/s]Training CobwebTree:  10%|         | 961/10000 [00:18<03:13, 46.76it/s]Training CobwebTree:  10%|         | 966/10000 [00:18<03:23, 44.46it/s]Training CobwebTree:  10%|         | 971/10000 [00:18<03:19, 45.16it/s]Training CobwebTree:  10%|         | 976/10000 [00:18<03:28, 43.32it/s]Training CobwebTree:  10%|         | 981/10000 [00:18<03:20, 45.08it/s]Training CobwebTree:  10%|         | 986/10000 [00:18<03:20, 44.97it/s]Training CobwebTree:  10%|         | 991/10000 [00:18<03:15, 46.03it/s]Training CobwebTree:  10%|         | 996/10000 [00:18<03:24, 44.10it/s]Training CobwebTree:  10%|         | 1001/10000 [00:19<03:25, 43.83it/s]Training CobwebTree:  10%|         | 1006/10000 [00:19<03:22, 44.43it/s]Training CobwebTree:  10%|         | 1012/10000 [00:19<03:14, 46.24it/s]Training CobwebTree:  10%|         | 1017/10000 [00:19<03:18, 45.16it/s]Training CobwebTree:  10%|         | 1023/10000 [00:19<03:10, 47.03it/s]Training CobwebTree:  10%|         | 1028/10000 [00:19<03:13, 46.43it/s]Training CobwebTree:  10%|         | 1033/10000 [00:19<03:12, 46.60it/s]Training CobwebTree:  10%|         | 1038/10000 [00:19<03:09, 47.25it/s]Training CobwebTree:  10%|         | 1043/10000 [00:19<03:10, 47.08it/s]Training CobwebTree:  10%|         | 1048/10000 [00:20<03:12, 46.59it/s]Training CobwebTree:  11%|         | 1053/10000 [00:20<03:15, 45.72it/s]Training CobwebTree:  11%|         | 1058/10000 [00:20<03:22, 44.24it/s]Training CobwebTree:  11%|         | 1063/10000 [00:20<03:26, 43.25it/s]Training CobwebTree:  11%|         | 1068/10000 [00:20<03:26, 43.15it/s]Training CobwebTree:  11%|         | 1073/10000 [00:20<03:29, 42.58it/s]Training CobwebTree:  11%|         | 1078/10000 [00:20<03:23, 43.74it/s]Training CobwebTree:  11%|         | 1083/10000 [00:20<03:36, 41.12it/s]Training CobwebTree:  11%|         | 1088/10000 [00:20<03:32, 41.87it/s]Training CobwebTree:  11%|         | 1093/10000 [00:21<03:27, 43.01it/s]Training CobwebTree:  11%|         | 1098/10000 [00:21<03:25, 43.42it/s]Training CobwebTree:  11%|         | 1103/10000 [00:21<03:24, 43.41it/s]Training CobwebTree:  11%|         | 1108/10000 [00:21<03:19, 44.60it/s]Training CobwebTree:  11%|         | 1113/10000 [00:21<03:14, 45.75it/s]Training CobwebTree:  11%|         | 1119/10000 [00:21<03:07, 47.25it/s]Training CobwebTree:  11%|         | 1124/10000 [00:21<03:17, 44.91it/s]Training CobwebTree:  11%|        | 1129/10000 [00:21<03:22, 43.76it/s]Training CobwebTree:  11%|        | 1134/10000 [00:21<03:17, 44.92it/s]Training CobwebTree:  11%|        | 1139/10000 [00:22<03:25, 43.21it/s]Training CobwebTree:  11%|        | 1144/10000 [00:22<03:28, 42.56it/s]Training CobwebTree:  11%|        | 1149/10000 [00:22<03:33, 41.41it/s]Training CobwebTree:  12%|        | 1154/10000 [00:22<03:27, 42.57it/s]Training CobwebTree:  12%|        | 1159/10000 [00:22<03:19, 44.26it/s]Training CobwebTree:  12%|        | 1164/10000 [00:22<03:20, 44.05it/s]Training CobwebTree:  12%|        | 1169/10000 [00:22<03:21, 43.87it/s]Training CobwebTree:  12%|        | 1174/10000 [00:22<03:28, 42.41it/s]Training CobwebTree:  12%|        | 1179/10000 [00:23<03:19, 44.17it/s]Training CobwebTree:  12%|        | 1184/10000 [00:23<03:21, 43.79it/s]Training CobwebTree:  12%|        | 1189/10000 [00:23<03:19, 44.11it/s]Training CobwebTree:  12%|        | 1194/10000 [00:23<03:18, 44.25it/s]Training CobwebTree:  12%|        | 1199/10000 [00:23<03:33, 41.22it/s]Training CobwebTree:  12%|        | 1204/10000 [00:23<03:23, 43.28it/s]Training CobwebTree:  12%|        | 1209/10000 [00:23<03:20, 43.84it/s]Training CobwebTree:  12%|        | 1214/10000 [00:23<03:17, 44.57it/s]Training CobwebTree:  12%|        | 1219/10000 [00:23<03:16, 44.69it/s]Training CobwebTree:  12%|        | 1224/10000 [00:24<03:11, 45.75it/s]Training CobwebTree:  12%|        | 1229/10000 [00:24<03:10, 45.95it/s]Training CobwebTree:  12%|        | 1234/10000 [00:24<03:14, 45.14it/s]Training CobwebTree:  12%|        | 1239/10000 [00:24<03:16, 44.53it/s]Training CobwebTree:  12%|        | 1244/10000 [00:24<03:15, 44.74it/s]Training CobwebTree:  12%|        | 1249/10000 [00:24<03:20, 43.67it/s]Training CobwebTree:  13%|        | 1254/10000 [00:24<03:18, 43.96it/s]Training CobwebTree:  13%|        | 1259/10000 [00:24<03:18, 44.11it/s]Training CobwebTree:  13%|        | 1264/10000 [00:24<03:33, 41.01it/s]Training CobwebTree:  13%|        | 1269/10000 [00:25<03:25, 42.56it/s]Training CobwebTree:  13%|        | 1274/10000 [00:25<03:22, 43.04it/s]Training CobwebTree:  13%|        | 1279/10000 [00:25<03:22, 43.03it/s]Training CobwebTree:  13%|        | 1285/10000 [00:25<03:11, 45.40it/s]Training CobwebTree:  13%|        | 1290/10000 [00:25<03:14, 44.87it/s]Training CobwebTree:  13%|        | 1295/10000 [00:25<03:12, 45.33it/s]Training CobwebTree:  13%|        | 1300/10000 [00:25<03:13, 45.00it/s]Training CobwebTree:  13%|        | 1305/10000 [00:25<03:09, 45.92it/s]Training CobwebTree:  13%|        | 1310/10000 [00:25<03:11, 45.32it/s]Training CobwebTree:  13%|        | 1315/10000 [00:26<03:13, 44.83it/s]Training CobwebTree:  13%|        | 1320/10000 [00:26<03:22, 42.83it/s]Training CobwebTree:  13%|        | 1325/10000 [00:26<03:22, 42.93it/s]Training CobwebTree:  13%|        | 1330/10000 [00:26<03:16, 44.09it/s]Training CobwebTree:  13%|        | 1335/10000 [00:26<03:13, 44.72it/s]Training CobwebTree:  13%|        | 1340/10000 [00:26<03:16, 44.09it/s]Training CobwebTree:  13%|        | 1345/10000 [00:26<03:18, 43.54it/s]Training CobwebTree:  14%|        | 1350/10000 [00:26<03:16, 44.05it/s]Training CobwebTree:  14%|        | 1355/10000 [00:27<03:27, 41.56it/s]Training CobwebTree:  14%|        | 1360/10000 [00:27<03:18, 43.59it/s]Training CobwebTree:  14%|        | 1365/10000 [00:27<03:24, 42.22it/s]Training CobwebTree:  14%|        | 1370/10000 [00:27<03:25, 42.05it/s]Training CobwebTree:  14%|        | 1375/10000 [00:27<03:32, 40.61it/s]Training CobwebTree:  14%|        | 1380/10000 [00:27<03:37, 39.60it/s]Training CobwebTree:  14%|        | 1384/10000 [00:27<03:38, 39.36it/s]Training CobwebTree:  14%|        | 1389/10000 [00:27<03:29, 41.06it/s]Training CobwebTree:  14%|        | 1394/10000 [00:27<03:23, 42.26it/s]Training CobwebTree:  14%|        | 1399/10000 [00:28<03:17, 43.47it/s]Training CobwebTree:  14%|        | 1404/10000 [00:28<03:15, 43.95it/s]Training CobwebTree:  14%|        | 1409/10000 [00:28<03:28, 41.30it/s]Training CobwebTree:  14%|        | 1414/10000 [00:28<03:23, 42.14it/s]Training CobwebTree:  14%|        | 1419/10000 [00:28<03:20, 42.72it/s]Training CobwebTree:  14%|        | 1424/10000 [00:28<03:13, 44.28it/s]Training CobwebTree:  14%|        | 1429/10000 [00:28<03:08, 45.42it/s]Training CobwebTree:  14%|        | 1434/10000 [00:28<03:05, 46.23it/s]Training CobwebTree:  14%|        | 1439/10000 [00:28<03:08, 45.44it/s]Training CobwebTree:  14%|        | 1444/10000 [00:29<03:13, 44.13it/s]Training CobwebTree:  14%|        | 1449/10000 [00:29<03:17, 43.31it/s]Training CobwebTree:  15%|        | 1454/10000 [00:29<03:12, 44.28it/s]Training CobwebTree:  15%|        | 1459/10000 [00:29<03:08, 45.31it/s]Training CobwebTree:  15%|        | 1464/10000 [00:29<03:22, 42.13it/s]Training CobwebTree:  15%|        | 1469/10000 [00:29<03:19, 42.76it/s]Training CobwebTree:  15%|        | 1474/10000 [00:29<03:15, 43.55it/s]Training CobwebTree:  15%|        | 1479/10000 [00:29<03:18, 42.97it/s]Training CobwebTree:  15%|        | 1485/10000 [00:30<03:02, 46.65it/s]Training CobwebTree:  15%|        | 1490/10000 [00:30<03:15, 43.60it/s]Training CobwebTree:  15%|        | 1495/10000 [00:30<03:15, 43.50it/s]Training CobwebTree:  15%|        | 1500/10000 [00:30<03:14, 43.71it/s]Training CobwebTree:  15%|        | 1505/10000 [00:30<03:14, 43.58it/s]Training CobwebTree:  15%|        | 1510/10000 [00:30<03:24, 41.52it/s]Training CobwebTree:  15%|        | 1515/10000 [00:30<03:23, 41.60it/s]Training CobwebTree:  15%|        | 1520/10000 [00:30<03:15, 43.40it/s]Training CobwebTree:  15%|        | 1525/10000 [00:30<03:11, 44.15it/s]Training CobwebTree:  15%|        | 1530/10000 [00:31<03:14, 43.53it/s]Training CobwebTree:  15%|        | 1535/10000 [00:31<03:15, 43.27it/s]Training CobwebTree:  15%|        | 1540/10000 [00:31<03:14, 43.48it/s]Training CobwebTree:  15%|        | 1545/10000 [00:31<03:19, 42.37it/s]Training CobwebTree:  16%|        | 1550/10000 [00:31<03:21, 41.88it/s]Training CobwebTree:  16%|        | 1555/10000 [00:31<03:23, 41.58it/s]Training CobwebTree:  16%|        | 1560/10000 [00:31<03:23, 41.42it/s]Training CobwebTree:  16%|        | 1565/10000 [00:31<03:25, 40.96it/s]Training CobwebTree:  16%|        | 1570/10000 [00:32<03:20, 42.14it/s]Training CobwebTree:  16%|        | 1575/10000 [00:32<03:20, 41.92it/s]Training CobwebTree:  16%|        | 1580/10000 [00:32<03:18, 42.36it/s]Training CobwebTree:  16%|        | 1585/10000 [00:32<03:19, 42.14it/s]Training CobwebTree:  16%|        | 1591/10000 [00:32<03:09, 44.37it/s]Training CobwebTree:  16%|        | 1596/10000 [00:32<03:07, 44.84it/s]Training CobwebTree:  16%|        | 1601/10000 [00:32<03:18, 42.35it/s]Training CobwebTree:  16%|        | 1606/10000 [00:32<03:12, 43.50it/s]Training CobwebTree:  16%|        | 1611/10000 [00:32<03:10, 43.94it/s]Training CobwebTree:  16%|        | 1616/10000 [00:33<03:16, 42.66it/s]Training CobwebTree:  16%|        | 1621/10000 [00:33<03:25, 40.87it/s]Training CobwebTree:  16%|        | 1626/10000 [00:33<03:26, 40.62it/s]Training CobwebTree:  16%|        | 1631/10000 [00:33<03:25, 40.73it/s]Training CobwebTree:  16%|        | 1636/10000 [00:33<03:22, 41.40it/s]Training CobwebTree:  16%|        | 1641/10000 [00:33<03:23, 41.11it/s]Training CobwebTree:  16%|        | 1646/10000 [00:33<03:22, 41.25it/s]Training CobwebTree:  17%|        | 1651/10000 [00:33<03:16, 42.52it/s]Training CobwebTree:  17%|        | 1656/10000 [00:34<03:13, 43.11it/s]Training CobwebTree:  17%|        | 1661/10000 [00:34<03:13, 43.04it/s]Training CobwebTree:  17%|        | 1666/10000 [00:34<03:14, 42.85it/s]Training CobwebTree:  17%|        | 1671/10000 [00:34<03:15, 42.69it/s]Training CobwebTree:  17%|        | 1676/10000 [00:34<03:21, 41.30it/s]Training CobwebTree:  17%|        | 1681/10000 [00:34<03:19, 41.77it/s]Training CobwebTree:  17%|        | 1686/10000 [00:34<03:25, 40.47it/s]Training CobwebTree:  17%|        | 1691/10000 [00:34<03:17, 42.12it/s]Training CobwebTree:  17%|        | 1696/10000 [00:35<03:20, 41.50it/s]Training CobwebTree:  17%|        | 1701/10000 [00:35<03:22, 41.08it/s]Training CobwebTree:  17%|        | 1706/10000 [00:35<03:15, 42.44it/s]Training CobwebTree:  17%|        | 1711/10000 [00:35<03:06, 44.33it/s]Training CobwebTree:  17%|        | 1716/10000 [00:35<03:06, 44.34it/s]Training CobwebTree:  17%|        | 1721/10000 [00:35<03:08, 43.80it/s]Training CobwebTree:  17%|        | 1726/10000 [00:35<03:08, 43.89it/s]Training CobwebTree:  17%|        | 1731/10000 [00:35<03:28, 39.59it/s]Training CobwebTree:  17%|        | 1736/10000 [00:35<03:24, 40.38it/s]Training CobwebTree:  17%|        | 1741/10000 [00:36<03:21, 40.89it/s]Training CobwebTree:  17%|        | 1746/10000 [00:36<03:16, 41.92it/s]Training CobwebTree:  18%|        | 1751/10000 [00:36<03:19, 41.37it/s]Training CobwebTree:  18%|        | 1756/10000 [00:36<03:20, 41.03it/s]Training CobwebTree:  18%|        | 1761/10000 [00:36<03:14, 42.26it/s]Training CobwebTree:  18%|        | 1766/10000 [00:36<03:18, 41.40it/s]Training CobwebTree:  18%|        | 1771/10000 [00:36<03:19, 41.23it/s]Training CobwebTree:  18%|        | 1776/10000 [00:36<03:22, 40.65it/s]Training CobwebTree:  18%|        | 1781/10000 [00:37<03:21, 40.88it/s]Training CobwebTree:  18%|        | 1786/10000 [00:37<03:20, 41.02it/s]Training CobwebTree:  18%|        | 1791/10000 [00:37<03:21, 40.82it/s]Training CobwebTree:  18%|        | 1796/10000 [00:37<03:13, 42.41it/s]Training CobwebTree:  18%|        | 1801/10000 [00:37<03:15, 41.90it/s]Training CobwebTree:  18%|        | 1806/10000 [00:37<03:12, 42.50it/s]Training CobwebTree:  18%|        | 1811/10000 [00:37<03:19, 41.12it/s]Training CobwebTree:  18%|        | 1816/10000 [00:37<03:16, 41.59it/s]Training CobwebTree:  18%|        | 1821/10000 [00:38<03:14, 41.95it/s]Training CobwebTree:  18%|        | 1826/10000 [00:38<03:27, 39.39it/s]Training CobwebTree:  18%|        | 1830/10000 [00:38<03:28, 39.12it/s]Training CobwebTree:  18%|        | 1835/10000 [00:38<03:22, 40.32it/s]Training CobwebTree:  18%|        | 1840/10000 [00:38<03:18, 41.10it/s]Training CobwebTree:  18%|        | 1845/10000 [00:38<03:19, 40.91it/s]Training CobwebTree:  18%|        | 1850/10000 [00:38<03:12, 42.38it/s]Training CobwebTree:  19%|        | 1855/10000 [00:38<03:10, 42.82it/s]Training CobwebTree:  19%|        | 1860/10000 [00:38<03:11, 42.59it/s]Training CobwebTree:  19%|        | 1865/10000 [00:39<03:21, 40.46it/s]Training CobwebTree:  19%|        | 1870/10000 [00:39<03:19, 40.68it/s]Training CobwebTree:  19%|        | 1875/10000 [00:39<03:15, 41.61it/s]Training CobwebTree:  19%|        | 1880/10000 [00:39<03:16, 41.32it/s]Training CobwebTree:  19%|        | 1885/10000 [00:39<03:18, 40.85it/s]Training CobwebTree:  19%|        | 1890/10000 [00:39<03:19, 40.65it/s]Training CobwebTree:  19%|        | 1895/10000 [00:39<03:23, 39.79it/s]Training CobwebTree:  19%|        | 1899/10000 [00:39<03:23, 39.76it/s]Training CobwebTree:  19%|        | 1903/10000 [00:40<03:25, 39.42it/s]Training CobwebTree:  19%|        | 1908/10000 [00:40<03:18, 40.84it/s]Training CobwebTree:  19%|        | 1913/10000 [00:40<03:17, 40.98it/s]Training CobwebTree:  19%|        | 1918/10000 [00:40<03:16, 41.21it/s]Training CobwebTree:  19%|        | 1923/10000 [00:40<03:19, 40.51it/s]Training CobwebTree:  19%|        | 1928/10000 [00:40<03:12, 41.88it/s]Training CobwebTree:  19%|        | 1933/10000 [00:40<03:13, 41.78it/s]Training CobwebTree:  19%|        | 1938/10000 [00:40<03:18, 40.58it/s]Training CobwebTree:  19%|        | 1943/10000 [00:41<03:22, 39.83it/s]Training CobwebTree:  19%|        | 1948/10000 [00:41<03:22, 39.84it/s]Training CobwebTree:  20%|        | 1952/10000 [00:41<03:23, 39.54it/s]Training CobwebTree:  20%|        | 1957/10000 [00:41<03:21, 39.98it/s]Training CobwebTree:  20%|        | 1961/10000 [00:41<03:25, 39.15it/s]Training CobwebTree:  20%|        | 1966/10000 [00:41<03:21, 39.78it/s]Training CobwebTree:  20%|        | 1970/10000 [00:41<03:21, 39.77it/s]Training CobwebTree:  20%|        | 1974/10000 [00:41<03:26, 38.86it/s]Training CobwebTree:  20%|        | 1979/10000 [00:41<03:18, 40.39it/s]Training CobwebTree:  20%|        | 1984/10000 [00:42<03:09, 42.24it/s]Training CobwebTree:  20%|        | 1989/10000 [00:42<03:04, 43.53it/s]Training CobwebTree:  20%|        | 1994/10000 [00:42<03:10, 41.96it/s]Training CobwebTree:  20%|        | 1999/10000 [00:42<03:07, 42.73it/s]Training CobwebTree:  20%|        | 2004/10000 [00:42<03:12, 41.51it/s]Training CobwebTree:  20%|        | 2009/10000 [00:42<03:21, 39.69it/s]Training CobwebTree:  20%|        | 2014/10000 [00:42<03:16, 40.55it/s]Training CobwebTree:  20%|        | 2019/10000 [00:42<03:15, 40.88it/s]Training CobwebTree:  20%|        | 2024/10000 [00:43<03:17, 40.45it/s]Training CobwebTree:  20%|        | 2029/10000 [00:43<03:07, 42.45it/s]Training CobwebTree:  20%|        | 2034/10000 [00:43<03:12, 41.47it/s]Training CobwebTree:  20%|        | 2039/10000 [00:43<03:18, 40.01it/s]Training CobwebTree:  20%|        | 2044/10000 [00:43<03:27, 38.28it/s]Training CobwebTree:  20%|        | 2049/10000 [00:43<03:18, 40.16it/s]Training CobwebTree:  21%|        | 2054/10000 [00:43<03:12, 41.26it/s]Training CobwebTree:  21%|        | 2059/10000 [00:43<03:24, 38.86it/s]Training CobwebTree:  21%|        | 2063/10000 [00:44<03:26, 38.35it/s]Training CobwebTree:  21%|        | 2068/10000 [00:44<03:22, 39.21it/s]Training CobwebTree:  21%|        | 2073/10000 [00:44<03:13, 40.93it/s]Training CobwebTree:  21%|        | 2078/10000 [00:44<03:10, 41.69it/s]Training CobwebTree:  21%|        | 2083/10000 [00:44<03:10, 41.66it/s]Training CobwebTree:  21%|        | 2088/10000 [00:44<03:05, 42.71it/s]Training CobwebTree:  21%|        | 2093/10000 [00:44<03:02, 43.34it/s]Training CobwebTree:  21%|        | 2098/10000 [00:44<03:06, 42.27it/s]Training CobwebTree:  21%|        | 2103/10000 [00:44<03:06, 42.25it/s]Training CobwebTree:  21%|        | 2108/10000 [00:45<03:08, 41.83it/s]Training CobwebTree:  21%|        | 2113/10000 [00:45<03:16, 40.18it/s]Training CobwebTree:  21%|        | 2118/10000 [00:45<03:12, 40.92it/s]Training CobwebTree:  21%|        | 2123/10000 [00:45<03:22, 38.98it/s]Training CobwebTree:  21%|       | 2127/10000 [00:45<03:24, 38.41it/s]Training CobwebTree:  21%|       | 2132/10000 [00:45<03:16, 39.94it/s]Training CobwebTree:  21%|       | 2137/10000 [00:45<03:18, 39.66it/s]Training CobwebTree:  21%|       | 2142/10000 [00:45<03:13, 40.66it/s]Training CobwebTree:  21%|       | 2147/10000 [00:46<03:16, 39.95it/s]Training CobwebTree:  22%|       | 2152/10000 [00:46<03:16, 39.88it/s]Training CobwebTree:  22%|       | 2157/10000 [00:46<03:17, 39.65it/s]Training CobwebTree:  22%|       | 2162/10000 [00:46<03:14, 40.29it/s]Training CobwebTree:  22%|       | 2167/10000 [00:46<03:11, 40.83it/s]Training CobwebTree:  22%|       | 2172/10000 [00:46<03:15, 40.13it/s]Training CobwebTree:  22%|       | 2177/10000 [00:46<03:14, 40.15it/s]Training CobwebTree:  22%|       | 2182/10000 [00:46<03:09, 41.22it/s]Training CobwebTree:  22%|       | 2187/10000 [00:47<03:12, 40.66it/s]Training CobwebTree:  22%|       | 2192/10000 [00:47<03:10, 40.89it/s]Training CobwebTree:  22%|       | 2197/10000 [00:47<03:02, 42.71it/s]Training CobwebTree:  22%|       | 2202/10000 [00:47<03:07, 41.53it/s]Training CobwebTree:  22%|       | 2207/10000 [00:47<03:12, 40.52it/s]Training CobwebTree:  22%|       | 2212/10000 [00:47<03:12, 40.52it/s]Training CobwebTree:  22%|       | 2217/10000 [00:47<03:18, 39.28it/s]Training CobwebTree:  22%|       | 2222/10000 [00:47<03:11, 40.61it/s]Training CobwebTree:  22%|       | 2227/10000 [00:48<03:15, 39.79it/s]Training CobwebTree:  22%|       | 2232/10000 [00:48<03:10, 40.79it/s]Training CobwebTree:  22%|       | 2237/10000 [00:48<03:14, 39.88it/s]Training CobwebTree:  22%|       | 2242/10000 [00:48<03:09, 40.85it/s]Training CobwebTree:  22%|       | 2247/10000 [00:48<03:06, 41.52it/s]Training CobwebTree:  23%|       | 2252/10000 [00:48<03:15, 39.63it/s]Training CobwebTree:  23%|       | 2257/10000 [00:48<03:12, 40.23it/s]Training CobwebTree:  23%|       | 2262/10000 [00:48<03:04, 42.02it/s]Training CobwebTree:  23%|       | 2267/10000 [00:48<02:59, 43.15it/s]Training CobwebTree:  23%|       | 2272/10000 [00:49<02:56, 43.69it/s]Training CobwebTree:  23%|       | 2277/10000 [00:49<02:59, 43.01it/s]Training CobwebTree:  23%|       | 2282/10000 [00:49<03:04, 41.84it/s]Training CobwebTree:  23%|       | 2287/10000 [00:49<02:59, 42.91it/s]Training CobwebTree:  23%|       | 2292/10000 [00:49<02:57, 43.33it/s]Training CobwebTree:  23%|       | 2297/10000 [00:49<03:12, 40.04it/s]Training CobwebTree:  23%|       | 2302/10000 [00:49<03:09, 40.56it/s]Training CobwebTree:  23%|       | 2307/10000 [00:49<03:06, 41.32it/s]Training CobwebTree:  23%|       | 2312/10000 [00:50<03:08, 40.88it/s]Training CobwebTree:  23%|       | 2317/10000 [00:50<03:06, 41.21it/s]Training CobwebTree:  23%|       | 2322/10000 [00:50<03:12, 39.83it/s]Training CobwebTree:  23%|       | 2327/10000 [00:50<03:01, 42.39it/s]Training CobwebTree:  23%|       | 2332/10000 [00:50<02:59, 42.66it/s]Training CobwebTree:  23%|       | 2337/10000 [00:50<03:00, 42.55it/s]Training CobwebTree:  23%|       | 2342/10000 [00:50<03:07, 40.78it/s]Training CobwebTree:  23%|       | 2347/10000 [00:50<03:18, 38.63it/s]Training CobwebTree:  24%|       | 2351/10000 [00:51<03:20, 38.14it/s]Training CobwebTree:  24%|       | 2355/10000 [00:51<03:22, 37.77it/s]Training CobwebTree:  24%|       | 2359/10000 [00:51<03:23, 37.61it/s]Training CobwebTree:  24%|       | 2363/10000 [00:51<03:22, 37.79it/s]Training CobwebTree:  24%|       | 2368/10000 [00:51<03:17, 38.74it/s]Training CobwebTree:  24%|       | 2372/10000 [00:51<03:16, 38.88it/s]Training CobwebTree:  24%|       | 2376/10000 [00:51<03:16, 38.80it/s]Training CobwebTree:  24%|       | 2380/10000 [00:51<03:19, 38.11it/s]Training CobwebTree:  24%|       | 2385/10000 [00:51<03:19, 38.20it/s]Training CobwebTree:  24%|       | 2390/10000 [00:52<03:16, 38.78it/s]Training CobwebTree:  24%|       | 2395/10000 [00:52<03:10, 40.01it/s]Training CobwebTree:  24%|       | 2399/10000 [00:52<03:10, 39.95it/s]Training CobwebTree:  24%|       | 2403/10000 [00:52<03:12, 39.51it/s]Training CobwebTree:  24%|       | 2407/10000 [00:52<03:13, 39.20it/s]Training CobwebTree:  24%|       | 2411/10000 [00:52<03:17, 38.39it/s]Training CobwebTree:  24%|       | 2415/10000 [00:52<03:20, 37.79it/s]Training CobwebTree:  24%|       | 2419/10000 [00:52<03:19, 37.94it/s]Training CobwebTree:  24%|       | 2423/10000 [00:52<03:17, 38.40it/s]Training CobwebTree:  24%|       | 2427/10000 [00:53<03:17, 38.28it/s]Training CobwebTree:  24%|       | 2431/10000 [00:53<03:22, 37.36it/s]Training CobwebTree:  24%|       | 2436/10000 [00:53<03:18, 38.12it/s]Training CobwebTree:  24%|       | 2440/10000 [00:53<03:18, 38.03it/s]Training CobwebTree:  24%|       | 2444/10000 [00:53<03:16, 38.37it/s]Training CobwebTree:  24%|       | 2448/10000 [00:53<03:17, 38.29it/s]Training CobwebTree:  25%|       | 2452/10000 [00:53<03:23, 37.05it/s]Training CobwebTree:  25%|       | 2456/10000 [00:53<03:19, 37.77it/s]Training CobwebTree:  25%|       | 2460/10000 [00:53<03:18, 38.00it/s]Training CobwebTree:  25%|       | 2464/10000 [00:54<03:18, 37.91it/s]Training CobwebTree:  25%|       | 2468/10000 [00:54<03:17, 38.13it/s]Training CobwebTree:  25%|       | 2473/10000 [00:54<03:16, 38.37it/s]Training CobwebTree:  25%|       | 2477/10000 [00:54<03:17, 38.05it/s]Training CobwebTree:  25%|       | 2481/10000 [00:54<03:14, 38.57it/s]Training CobwebTree:  25%|       | 2486/10000 [00:54<03:14, 38.53it/s]Training CobwebTree:  25%|       | 2490/10000 [00:54<03:18, 37.93it/s]Training CobwebTree:  25%|       | 2495/10000 [00:54<03:10, 39.34it/s]Training CobwebTree:  25%|       | 2499/10000 [00:54<03:20, 37.32it/s]Training CobwebTree:  25%|       | 2503/10000 [00:55<03:27, 36.18it/s]Training CobwebTree:  25%|       | 2507/10000 [00:55<03:22, 37.08it/s]Training CobwebTree:  25%|       | 2511/10000 [00:55<03:22, 36.90it/s]Training CobwebTree:  25%|       | 2515/10000 [00:55<03:28, 35.88it/s]Training CobwebTree:  25%|       | 2519/10000 [00:55<03:30, 35.52it/s]Training CobwebTree:  25%|       | 2523/10000 [00:55<03:36, 34.51it/s]Training CobwebTree:  25%|       | 2528/10000 [00:55<03:27, 36.08it/s]Training CobwebTree:  25%|       | 2532/10000 [00:55<03:24, 36.56it/s]Training CobwebTree:  25%|       | 2536/10000 [00:55<03:27, 35.96it/s]Training CobwebTree:  25%|       | 2540/10000 [00:56<03:21, 37.04it/s]Training CobwebTree:  25%|       | 2544/10000 [00:56<03:21, 37.00it/s]Training CobwebTree:  25%|       | 2549/10000 [00:56<03:13, 38.46it/s]Training CobwebTree:  26%|       | 2553/10000 [00:56<03:11, 38.81it/s]Training CobwebTree:  26%|       | 2557/10000 [00:56<03:10, 38.97it/s]Training CobwebTree:  26%|       | 2562/10000 [00:56<03:02, 40.66it/s]Training CobwebTree:  26%|       | 2567/10000 [00:56<03:09, 39.23it/s]Training CobwebTree:  26%|       | 2572/10000 [00:56<03:09, 39.11it/s]Training CobwebTree:  26%|       | 2576/10000 [00:56<03:10, 39.02it/s]Training CobwebTree:  26%|       | 2580/10000 [00:57<03:11, 38.83it/s]Training CobwebTree:  26%|       | 2584/10000 [00:57<03:11, 38.65it/s]Training CobwebTree:  26%|       | 2588/10000 [00:57<03:10, 38.95it/s]Training CobwebTree:  26%|       | 2592/10000 [00:57<03:14, 38.15it/s]Training CobwebTree:  26%|       | 2596/10000 [00:57<03:18, 37.27it/s]Training CobwebTree:  26%|       | 2601/10000 [00:57<03:12, 38.46it/s]Training CobwebTree:  26%|       | 2606/10000 [00:57<03:10, 38.90it/s]Training CobwebTree:  26%|       | 2611/10000 [00:57<03:08, 39.23it/s]Training CobwebTree:  26%|       | 2616/10000 [00:57<03:04, 40.09it/s]Training CobwebTree:  26%|       | 2621/10000 [00:58<03:10, 38.78it/s]Training CobwebTree:  26%|       | 2625/10000 [00:58<03:11, 38.44it/s]Training CobwebTree:  26%|       | 2629/10000 [00:58<03:10, 38.70it/s]Training CobwebTree:  26%|       | 2633/10000 [00:58<03:17, 37.37it/s]Training CobwebTree:  26%|       | 2637/10000 [00:58<03:31, 34.87it/s]Training CobwebTree:  26%|       | 2641/10000 [00:58<03:37, 33.87it/s]Training CobwebTree:  26%|       | 2645/10000 [00:58<03:32, 34.66it/s]Training CobwebTree:  26%|       | 2649/10000 [00:58<03:27, 35.38it/s]Training CobwebTree:  27%|       | 2653/10000 [00:59<03:31, 34.73it/s]Training CobwebTree:  27%|       | 2657/10000 [00:59<03:25, 35.68it/s]Training CobwebTree:  27%|       | 2661/10000 [00:59<03:23, 36.12it/s]Training CobwebTree:  27%|       | 2665/10000 [00:59<03:25, 35.78it/s]Training CobwebTree:  27%|       | 2669/10000 [00:59<03:21, 36.39it/s]Training CobwebTree:  27%|       | 2673/10000 [00:59<03:18, 36.95it/s]Training CobwebTree:  27%|       | 2678/10000 [00:59<03:10, 38.45it/s]Training CobwebTree:  27%|       | 2682/10000 [00:59<03:13, 37.84it/s]Training CobwebTree:  27%|       | 2687/10000 [00:59<03:02, 40.10it/s]Training CobwebTree:  27%|       | 2692/10000 [01:00<03:05, 39.33it/s]Training CobwebTree:  27%|       | 2697/10000 [01:00<03:04, 39.69it/s]Training CobwebTree:  27%|       | 2701/10000 [01:00<03:06, 39.22it/s]Training CobwebTree:  27%|       | 2706/10000 [01:00<03:01, 40.26it/s]Training CobwebTree:  27%|       | 2711/10000 [01:00<03:11, 38.14it/s]Training CobwebTree:  27%|       | 2716/10000 [01:00<03:10, 38.21it/s]Training CobwebTree:  27%|       | 2720/10000 [01:00<03:12, 37.90it/s]Training CobwebTree:  27%|       | 2724/10000 [01:00<03:10, 38.12it/s]Training CobwebTree:  27%|       | 2728/10000 [01:00<03:09, 38.38it/s]Training CobwebTree:  27%|       | 2733/10000 [01:01<02:59, 40.44it/s]Training CobwebTree:  27%|       | 2738/10000 [01:01<03:02, 39.72it/s]Training CobwebTree:  27%|       | 2743/10000 [01:01<02:56, 41.14it/s]Training CobwebTree:  27%|       | 2748/10000 [01:01<03:04, 39.27it/s]Training CobwebTree:  28%|       | 2752/10000 [01:01<03:06, 38.83it/s]Training CobwebTree:  28%|       | 2756/10000 [01:01<03:10, 38.04it/s]Training CobwebTree:  28%|       | 2760/10000 [01:01<03:08, 38.39it/s]Training CobwebTree:  28%|       | 2765/10000 [01:01<03:02, 39.64it/s]Training CobwebTree:  28%|       | 2769/10000 [01:02<03:17, 36.63it/s]Training CobwebTree:  28%|       | 2773/10000 [01:02<03:13, 37.35it/s]Training CobwebTree:  28%|       | 2778/10000 [01:02<03:07, 38.47it/s]Training CobwebTree:  28%|       | 2782/10000 [01:02<03:22, 35.71it/s]Training CobwebTree:  28%|       | 2786/10000 [01:02<03:19, 36.18it/s]Training CobwebTree:  28%|       | 2790/10000 [01:02<03:24, 35.22it/s]Training CobwebTree:  28%|       | 2794/10000 [01:02<03:27, 34.77it/s]Training CobwebTree:  28%|       | 2798/10000 [01:02<03:23, 35.39it/s]Training CobwebTree:  28%|       | 2802/10000 [01:02<03:25, 35.03it/s]Training CobwebTree:  28%|       | 2806/10000 [01:03<03:25, 34.96it/s]Training CobwebTree:  28%|       | 2810/10000 [01:03<03:20, 35.84it/s]Training CobwebTree:  28%|       | 2815/10000 [01:03<03:09, 37.93it/s]Training CobwebTree:  28%|       | 2819/10000 [01:03<03:07, 38.23it/s]Training CobwebTree:  28%|       | 2823/10000 [01:03<03:13, 37.15it/s]Training CobwebTree:  28%|       | 2828/10000 [01:03<03:06, 38.47it/s]Training CobwebTree:  28%|       | 2832/10000 [01:03<03:15, 36.63it/s]Training CobwebTree:  28%|       | 2836/10000 [01:03<03:22, 35.45it/s]Training CobwebTree:  28%|       | 2840/10000 [01:04<03:28, 34.36it/s]Training CobwebTree:  28%|       | 2845/10000 [01:04<03:14, 36.87it/s]Training CobwebTree:  28%|       | 2849/10000 [01:04<03:11, 37.41it/s]Training CobwebTree:  29%|       | 2854/10000 [01:04<03:03, 38.93it/s]Training CobwebTree:  29%|       | 2858/10000 [01:04<03:06, 38.38it/s]Training CobwebTree:  29%|       | 2862/10000 [01:04<03:07, 38.05it/s]Training CobwebTree:  29%|       | 2867/10000 [01:04<03:01, 39.36it/s]Training CobwebTree:  29%|       | 2872/10000 [01:04<02:57, 40.07it/s]Training CobwebTree:  29%|       | 2877/10000 [01:04<03:02, 39.14it/s]Training CobwebTree:  29%|       | 2881/10000 [01:05<03:06, 38.16it/s]Training CobwebTree:  29%|       | 2885/10000 [01:05<03:07, 37.99it/s]Training CobwebTree:  29%|       | 2889/10000 [01:05<03:12, 37.02it/s]Training CobwebTree:  29%|       | 2893/10000 [01:05<03:12, 36.91it/s]Training CobwebTree:  29%|       | 2897/10000 [01:05<03:15, 36.28it/s]Training CobwebTree:  29%|       | 2901/10000 [01:05<03:10, 37.19it/s]Training CobwebTree:  29%|       | 2905/10000 [01:05<03:15, 36.37it/s]Training CobwebTree:  29%|       | 2910/10000 [01:05<03:07, 37.73it/s]Training CobwebTree:  29%|       | 2915/10000 [01:05<03:07, 37.78it/s]Training CobwebTree:  29%|       | 2919/10000 [01:06<03:05, 38.24it/s]Training CobwebTree:  29%|       | 2923/10000 [01:06<03:08, 37.56it/s]Training CobwebTree:  29%|       | 2927/10000 [01:06<03:05, 38.13it/s]Training CobwebTree:  29%|       | 2931/10000 [01:06<03:05, 38.12it/s]Training CobwebTree:  29%|       | 2935/10000 [01:06<03:12, 36.66it/s]Training CobwebTree:  29%|       | 2939/10000 [01:06<03:17, 35.83it/s]Training CobwebTree:  29%|       | 2943/10000 [01:06<03:17, 35.69it/s]Training CobwebTree:  29%|       | 2947/10000 [01:06<03:19, 35.35it/s]Training CobwebTree:  30%|       | 2951/10000 [01:06<03:18, 35.52it/s]Training CobwebTree:  30%|       | 2955/10000 [01:07<03:18, 35.57it/s]Training CobwebTree:  30%|       | 2959/10000 [01:07<03:15, 36.00it/s]Training CobwebTree:  30%|       | 2963/10000 [01:07<03:10, 37.02it/s]Training CobwebTree:  30%|       | 2968/10000 [01:07<03:05, 37.89it/s]Training CobwebTree:  30%|       | 2972/10000 [01:07<03:04, 38.03it/s]Training CobwebTree:  30%|       | 2976/10000 [01:07<03:09, 37.04it/s]Training CobwebTree:  30%|       | 2981/10000 [01:07<03:02, 38.45it/s]Training CobwebTree:  30%|       | 2986/10000 [01:07<02:55, 40.05it/s]Training CobwebTree:  30%|       | 2991/10000 [01:08<02:59, 39.02it/s]Training CobwebTree:  30%|       | 2995/10000 [01:08<03:07, 37.43it/s]Training CobwebTree:  30%|       | 3000/10000 [01:08<03:00, 38.76it/s]Training CobwebTree:  30%|       | 3005/10000 [01:08<02:52, 40.53it/s]Training CobwebTree:  30%|       | 3010/10000 [01:08<02:55, 39.92it/s]Training CobwebTree:  30%|       | 3015/10000 [01:08<03:02, 38.21it/s]Training CobwebTree:  30%|       | 3020/10000 [01:08<02:57, 39.39it/s]Training CobwebTree:  30%|       | 3025/10000 [01:08<02:55, 39.84it/s]Training CobwebTree:  30%|       | 3030/10000 [01:09<02:56, 39.60it/s]Training CobwebTree:  30%|       | 3034/10000 [01:09<03:03, 37.87it/s]Training CobwebTree:  30%|       | 3038/10000 [01:09<03:09, 36.76it/s]Training CobwebTree:  30%|       | 3043/10000 [01:09<03:05, 37.41it/s]Training CobwebTree:  30%|       | 3048/10000 [01:09<03:02, 38.16it/s]Training CobwebTree:  31%|       | 3053/10000 [01:09<02:53, 39.94it/s]Training CobwebTree:  31%|       | 3058/10000 [01:09<02:52, 40.20it/s]Training CobwebTree:  31%|       | 3063/10000 [01:09<02:46, 41.60it/s]Training CobwebTree:  31%|       | 3068/10000 [01:09<02:47, 41.51it/s]Training CobwebTree:  31%|       | 3073/10000 [01:10<02:49, 40.78it/s]Training CobwebTree:  31%|       | 3078/10000 [01:10<02:53, 39.89it/s]Training CobwebTree:  31%|       | 3083/10000 [01:10<02:55, 39.47it/s]Training CobwebTree:  31%|       | 3088/10000 [01:10<02:52, 39.97it/s]Training CobwebTree:  31%|       | 3093/10000 [01:10<02:54, 39.66it/s]Training CobwebTree:  31%|       | 3098/10000 [01:10<02:54, 39.58it/s]Training CobwebTree:  31%|       | 3102/10000 [01:10<02:58, 38.70it/s]Training CobwebTree:  31%|       | 3106/10000 [01:10<02:58, 38.54it/s]Training CobwebTree:  31%|       | 3111/10000 [01:11<02:56, 39.01it/s]Training CobwebTree:  31%|       | 3116/10000 [01:11<02:51, 40.21it/s]Training CobwebTree:  31%|       | 3122/10000 [01:11<02:37, 43.55it/s]Training CobwebTree:  31%|      | 3127/10000 [01:11<02:48, 40.69it/s]Training CobwebTree:  31%|      | 3132/10000 [01:11<02:51, 40.00it/s]Training CobwebTree:  31%|      | 3137/10000 [01:11<02:47, 40.93it/s]Training CobwebTree:  31%|      | 3142/10000 [01:11<02:44, 41.65it/s]Training CobwebTree:  31%|      | 3147/10000 [01:11<02:43, 41.92it/s]Training CobwebTree:  32%|      | 3152/10000 [01:12<02:42, 42.11it/s]Training CobwebTree:  32%|      | 3157/10000 [01:12<02:42, 42.06it/s]Training CobwebTree:  32%|      | 3162/10000 [01:12<02:49, 40.26it/s]Training CobwebTree:  32%|      | 3167/10000 [01:12<02:48, 40.63it/s]Training CobwebTree:  32%|      | 3172/10000 [01:12<02:49, 40.34it/s]Training CobwebTree:  32%|      | 3178/10000 [01:12<02:45, 41.14it/s]Training CobwebTree:  32%|      | 3183/10000 [01:12<02:55, 38.87it/s]Training CobwebTree:  32%|      | 3187/10000 [01:12<02:57, 38.35it/s]Training CobwebTree:  32%|      | 3192/10000 [01:13<02:53, 39.19it/s]Training CobwebTree:  32%|      | 3196/10000 [01:13<02:57, 38.43it/s]Training CobwebTree:  32%|      | 3200/10000 [01:13<02:56, 38.56it/s]Training CobwebTree:  32%|      | 3204/10000 [01:13<03:02, 37.29it/s]Training CobwebTree:  32%|      | 3208/10000 [01:13<03:02, 37.23it/s]Training CobwebTree:  32%|      | 3212/10000 [01:13<03:04, 36.71it/s]Training CobwebTree:  32%|      | 3217/10000 [01:13<02:53, 39.16it/s]Training CobwebTree:  32%|      | 3222/10000 [01:13<02:47, 40.35it/s]Training CobwebTree:  32%|      | 3227/10000 [01:13<02:46, 40.65it/s]Training CobwebTree:  32%|      | 3232/10000 [01:14<02:55, 38.54it/s]Training CobwebTree:  32%|      | 3237/10000 [01:14<02:51, 39.52it/s]Training CobwebTree:  32%|      | 3242/10000 [01:14<02:48, 40.05it/s]Training CobwebTree:  32%|      | 3247/10000 [01:14<02:45, 40.86it/s]Training CobwebTree:  33%|      | 3252/10000 [01:14<02:41, 41.67it/s]Training CobwebTree:  33%|      | 3257/10000 [01:14<02:43, 41.32it/s]Training CobwebTree:  33%|      | 3262/10000 [01:14<02:49, 39.76it/s]Training CobwebTree:  33%|      | 3267/10000 [01:14<02:46, 40.34it/s]Training CobwebTree:  33%|      | 3272/10000 [01:15<02:50, 39.52it/s]Training CobwebTree:  33%|      | 3277/10000 [01:15<02:44, 40.76it/s]Training CobwebTree:  33%|      | 3282/10000 [01:15<02:41, 41.57it/s]Training CobwebTree:  33%|      | 3287/10000 [01:15<02:43, 40.96it/s]Training CobwebTree:  33%|      | 3292/10000 [01:15<02:39, 41.94it/s]Training CobwebTree:  33%|      | 3297/10000 [01:15<02:37, 42.44it/s]Training CobwebTree:  33%|      | 3302/10000 [01:15<02:49, 39.47it/s]Training CobwebTree:  33%|      | 3307/10000 [01:15<02:49, 39.45it/s]Training CobwebTree:  33%|      | 3312/10000 [01:16<02:43, 40.80it/s]Training CobwebTree:  33%|      | 3317/10000 [01:16<02:48, 39.76it/s]Training CobwebTree:  33%|      | 3322/10000 [01:16<02:47, 39.93it/s]Training CobwebTree:  33%|      | 3327/10000 [01:16<02:42, 41.11it/s]Training CobwebTree:  33%|      | 3332/10000 [01:16<02:48, 39.60it/s]Training CobwebTree:  33%|      | 3336/10000 [01:16<02:54, 38.27it/s]Training CobwebTree:  33%|      | 3341/10000 [01:16<02:47, 39.77it/s]Training CobwebTree:  33%|      | 3345/10000 [01:16<02:52, 38.69it/s]Training CobwebTree:  34%|      | 3350/10000 [01:17<02:43, 40.76it/s]Training CobwebTree:  34%|      | 3355/10000 [01:17<02:44, 40.37it/s]Training CobwebTree:  34%|      | 3360/10000 [01:17<02:45, 40.17it/s]Training CobwebTree:  34%|      | 3365/10000 [01:17<02:40, 41.28it/s]Training CobwebTree:  34%|      | 3370/10000 [01:17<02:47, 39.48it/s]Training CobwebTree:  34%|      | 3375/10000 [01:17<02:41, 40.98it/s]Training CobwebTree:  34%|      | 3380/10000 [01:17<02:46, 39.77it/s]Training CobwebTree:  34%|      | 3385/10000 [01:17<02:48, 39.37it/s]Training CobwebTree:  34%|      | 3389/10000 [01:17<02:49, 39.11it/s]Training CobwebTree:  34%|      | 3394/10000 [01:18<02:41, 40.80it/s]Training CobwebTree:  34%|      | 3399/10000 [01:18<02:44, 40.19it/s]Training CobwebTree:  34%|      | 3404/10000 [01:18<02:51, 38.53it/s]Training CobwebTree:  34%|      | 3409/10000 [01:18<02:45, 39.85it/s]Training CobwebTree:  34%|      | 3414/10000 [01:18<02:42, 40.56it/s]Training CobwebTree:  34%|      | 3419/10000 [01:18<02:42, 40.57it/s]Training CobwebTree:  34%|      | 3424/10000 [01:18<02:41, 40.81it/s]Training CobwebTree:  34%|      | 3429/10000 [01:18<02:42, 40.38it/s]Training CobwebTree:  34%|      | 3434/10000 [01:19<02:41, 40.69it/s]Training CobwebTree:  34%|      | 3439/10000 [01:19<02:40, 40.96it/s]Training CobwebTree:  34%|      | 3444/10000 [01:19<02:45, 39.63it/s]Training CobwebTree:  34%|      | 3449/10000 [01:19<02:45, 39.56it/s]Training CobwebTree:  35%|      | 3453/10000 [01:19<02:46, 39.38it/s]Training CobwebTree:  35%|      | 3458/10000 [01:19<02:45, 39.56it/s]Training CobwebTree:  35%|      | 3462/10000 [01:19<02:45, 39.51it/s]Training CobwebTree:  35%|      | 3466/10000 [01:19<02:54, 37.48it/s]Training CobwebTree:  35%|      | 3470/10000 [01:20<02:52, 37.93it/s]Training CobwebTree:  35%|      | 3474/10000 [01:20<02:50, 38.21it/s]Training CobwebTree:  35%|      | 3479/10000 [01:20<02:42, 40.17it/s]Training CobwebTree:  35%|      | 3484/10000 [01:20<02:38, 41.12it/s]Training CobwebTree:  35%|      | 3489/10000 [01:20<02:37, 41.40it/s]Training CobwebTree:  35%|      | 3494/10000 [01:20<02:31, 42.88it/s]Training CobwebTree:  35%|      | 3499/10000 [01:20<02:34, 42.21it/s]Training CobwebTree:  35%|      | 3504/10000 [01:20<02:36, 41.50it/s]Training CobwebTree:  35%|      | 3509/10000 [01:20<02:36, 41.39it/s]Training CobwebTree:  35%|      | 3514/10000 [01:21<02:46, 38.99it/s]Training CobwebTree:  35%|      | 3519/10000 [01:21<02:43, 39.70it/s]Training CobwebTree:  35%|      | 3523/10000 [01:21<02:50, 37.91it/s]Training CobwebTree:  35%|      | 3527/10000 [01:21<02:53, 37.40it/s]Training CobwebTree:  35%|      | 3531/10000 [01:21<02:55, 36.77it/s]Training CobwebTree:  35%|      | 3535/10000 [01:21<02:56, 36.65it/s]Training CobwebTree:  35%|      | 3540/10000 [01:21<02:45, 39.05it/s]Training CobwebTree:  35%|      | 3544/10000 [01:21<02:45, 39.05it/s]Training CobwebTree:  35%|      | 3549/10000 [01:22<02:38, 40.82it/s]Training CobwebTree:  36%|      | 3554/10000 [01:22<02:43, 39.46it/s]Training CobwebTree:  36%|      | 3558/10000 [01:22<03:03, 35.15it/s]Training CobwebTree:  36%|      | 3562/10000 [01:22<02:57, 36.34it/s]Training CobwebTree:  36%|      | 3567/10000 [01:22<02:51, 37.58it/s]Training CobwebTree:  36%|      | 3571/10000 [01:22<02:53, 36.95it/s]Training CobwebTree:  36%|      | 3576/10000 [01:22<02:45, 38.88it/s]Training CobwebTree:  36%|      | 3580/10000 [01:22<02:51, 37.38it/s]Training CobwebTree:  36%|      | 3584/10000 [01:22<02:49, 37.88it/s]Training CobwebTree:  36%|      | 3588/10000 [01:23<02:50, 37.55it/s]Training CobwebTree:  36%|      | 3593/10000 [01:23<02:45, 38.68it/s]Training CobwebTree:  36%|      | 3598/10000 [01:23<02:42, 39.31it/s]Training CobwebTree:  36%|      | 3603/10000 [01:23<02:42, 39.46it/s]Training CobwebTree:  36%|      | 3607/10000 [01:23<02:42, 39.37it/s]Training CobwebTree:  36%|      | 3611/10000 [01:23<02:42, 39.35it/s]Training CobwebTree:  36%|      | 3615/10000 [01:23<02:46, 38.30it/s]Training CobwebTree:  36%|      | 3619/10000 [01:23<02:55, 36.28it/s]Training CobwebTree:  36%|      | 3623/10000 [01:23<02:55, 36.30it/s]Training CobwebTree:  36%|      | 3627/10000 [01:24<03:04, 34.53it/s]Training CobwebTree:  36%|      | 3631/10000 [01:24<03:00, 35.28it/s]Training CobwebTree:  36%|      | 3636/10000 [01:24<02:51, 37.15it/s]Training CobwebTree:  36%|      | 3640/10000 [01:24<02:56, 35.93it/s]Training CobwebTree:  36%|      | 3644/10000 [01:24<02:57, 35.77it/s]Training CobwebTree:  36%|      | 3648/10000 [01:24<02:52, 36.83it/s]Training CobwebTree:  37%|      | 3652/10000 [01:24<02:49, 37.43it/s]Training CobwebTree:  37%|      | 3656/10000 [01:24<02:55, 36.25it/s]Training CobwebTree:  37%|      | 3660/10000 [01:25<02:52, 36.70it/s]Training CobwebTree:  37%|      | 3665/10000 [01:25<02:47, 37.79it/s]Training CobwebTree:  37%|      | 3670/10000 [01:25<02:46, 38.00it/s]Training CobwebTree:  37%|      | 3674/10000 [01:25<02:47, 37.81it/s]Training CobwebTree:  37%|      | 3679/10000 [01:25<02:44, 38.47it/s]Training CobwebTree:  37%|      | 3683/10000 [01:25<02:51, 36.77it/s]Training CobwebTree:  37%|      | 3687/10000 [01:25<02:51, 36.91it/s]Training CobwebTree:  37%|      | 3691/10000 [01:25<02:51, 36.80it/s]Training CobwebTree:  37%|      | 3695/10000 [01:25<02:55, 35.87it/s]Training CobwebTree:  37%|      | 3699/10000 [01:26<02:52, 36.53it/s]Training CobwebTree:  37%|      | 3703/10000 [01:26<02:54, 36.05it/s]Training CobwebTree:  37%|      | 3707/10000 [01:26<02:50, 36.85it/s]Training CobwebTree:  37%|      | 3712/10000 [01:26<02:44, 38.28it/s]Training CobwebTree:  37%|      | 3717/10000 [01:26<02:40, 39.09it/s]Training CobwebTree:  37%|      | 3722/10000 [01:26<02:37, 39.75it/s]Training CobwebTree:  37%|      | 3726/10000 [01:26<02:48, 37.30it/s]Training CobwebTree:  37%|      | 3730/10000 [01:26<02:49, 36.91it/s]Training CobwebTree:  37%|      | 3734/10000 [01:26<02:50, 36.85it/s]Training CobwebTree:  37%|      | 3738/10000 [01:27<02:49, 36.95it/s]Training CobwebTree:  37%|      | 3742/10000 [01:27<02:49, 36.93it/s]Training CobwebTree:  37%|      | 3746/10000 [01:27<02:49, 37.00it/s]Training CobwebTree:  38%|      | 3750/10000 [01:27<02:50, 36.55it/s]Training CobwebTree:  38%|      | 3755/10000 [01:27<02:41, 38.72it/s]Training CobwebTree:  38%|      | 3759/10000 [01:27<02:43, 38.18it/s]Training CobwebTree:  38%|      | 3763/10000 [01:27<02:49, 36.82it/s]Training CobwebTree:  38%|      | 3768/10000 [01:27<02:39, 39.00it/s]Training CobwebTree:  38%|      | 3772/10000 [01:27<02:39, 38.93it/s]Training CobwebTree:  38%|      | 3776/10000 [01:28<02:39, 39.09it/s]Training CobwebTree:  38%|      | 3781/10000 [01:28<02:33, 40.63it/s]Training CobwebTree:  38%|      | 3786/10000 [01:28<02:30, 41.38it/s]Training CobwebTree:  38%|      | 3791/10000 [01:28<02:32, 40.60it/s]Training CobwebTree:  38%|      | 3796/10000 [01:28<02:31, 40.90it/s]Training CobwebTree:  38%|      | 3801/10000 [01:28<02:39, 38.95it/s]Training CobwebTree:  38%|      | 3806/10000 [01:28<02:37, 39.23it/s]Training CobwebTree:  38%|      | 3811/10000 [01:28<02:35, 39.70it/s]Training CobwebTree:  38%|      | 3815/10000 [01:29<02:39, 38.86it/s]Training CobwebTree:  38%|      | 3820/10000 [01:29<02:40, 38.47it/s]Training CobwebTree:  38%|      | 3825/10000 [01:29<02:35, 39.70it/s]Training CobwebTree:  38%|      | 3829/10000 [01:29<02:36, 39.47it/s]Training CobwebTree:  38%|      | 3833/10000 [01:29<02:45, 37.31it/s]Training CobwebTree:  38%|      | 3837/10000 [01:29<02:45, 37.32it/s]Training CobwebTree:  38%|      | 3842/10000 [01:29<02:37, 39.08it/s]Training CobwebTree:  38%|      | 3846/10000 [01:29<02:41, 37.99it/s]Training CobwebTree:  39%|      | 3851/10000 [01:30<02:42, 37.90it/s]Training CobwebTree:  39%|      | 3855/10000 [01:30<02:42, 37.73it/s]Training CobwebTree:  39%|      | 3860/10000 [01:30<02:43, 37.54it/s]Training CobwebTree:  39%|      | 3864/10000 [01:30<02:52, 35.50it/s]Training CobwebTree:  39%|      | 3869/10000 [01:30<02:43, 37.53it/s]Training CobwebTree:  39%|      | 3874/10000 [01:30<02:39, 38.30it/s]Training CobwebTree:  39%|      | 3878/10000 [01:30<02:46, 36.69it/s]Training CobwebTree:  39%|      | 3882/10000 [01:30<02:47, 36.45it/s]Training CobwebTree:  39%|      | 3886/10000 [01:30<02:47, 36.60it/s]Training CobwebTree:  39%|      | 3890/10000 [01:31<02:48, 36.24it/s]Training CobwebTree:  39%|      | 3894/10000 [01:31<02:45, 36.87it/s]Training CobwebTree:  39%|      | 3898/10000 [01:31<02:44, 37.05it/s]Training CobwebTree:  39%|      | 3902/10000 [01:31<02:41, 37.67it/s]Training CobwebTree:  39%|      | 3907/10000 [01:31<02:41, 37.74it/s]Training CobwebTree:  39%|      | 3912/10000 [01:31<02:35, 39.12it/s]Training CobwebTree:  39%|      | 3916/10000 [01:31<02:38, 38.46it/s]Training CobwebTree:  39%|      | 3920/10000 [01:31<02:41, 37.55it/s]Training CobwebTree:  39%|      | 3925/10000 [01:31<02:41, 37.70it/s]Training CobwebTree:  39%|      | 3929/10000 [01:32<02:42, 37.41it/s]Training CobwebTree:  39%|      | 3933/10000 [01:32<02:44, 36.87it/s]Training CobwebTree:  39%|      | 3938/10000 [01:32<02:33, 39.61it/s]Training CobwebTree:  39%|      | 3943/10000 [01:32<02:26, 41.36it/s]Training CobwebTree:  39%|      | 3948/10000 [01:32<02:32, 39.75it/s]Training CobwebTree:  40%|      | 3953/10000 [01:32<02:27, 40.90it/s]Training CobwebTree:  40%|      | 3958/10000 [01:32<02:27, 41.08it/s]Training CobwebTree:  40%|      | 3963/10000 [01:32<02:31, 39.72it/s]Training CobwebTree:  40%|      | 3968/10000 [01:33<02:31, 39.79it/s]Training CobwebTree:  40%|      | 3972/10000 [01:33<02:38, 37.96it/s]Training CobwebTree:  40%|      | 3976/10000 [01:33<02:37, 38.24it/s]Training CobwebTree:  40%|      | 3981/10000 [01:33<02:33, 39.16it/s]Training CobwebTree:  40%|      | 3985/10000 [01:33<02:36, 38.43it/s]Training CobwebTree:  40%|      | 3989/10000 [01:33<02:39, 37.62it/s]Training CobwebTree:  40%|      | 3994/10000 [01:33<02:38, 37.87it/s]Training CobwebTree:  40%|      | 3998/10000 [01:33<02:39, 37.66it/s]Training CobwebTree:  40%|      | 4003/10000 [01:33<02:33, 39.14it/s]Training CobwebTree:  40%|      | 4007/10000 [01:34<02:35, 38.51it/s]Training CobwebTree:  40%|      | 4012/10000 [01:34<02:36, 38.36it/s]Training CobwebTree:  40%|      | 4016/10000 [01:34<02:46, 35.86it/s]Training CobwebTree:  40%|      | 4020/10000 [01:34<02:46, 35.88it/s]Training CobwebTree:  40%|      | 4024/10000 [01:34<02:46, 35.86it/s]Training CobwebTree:  40%|      | 4028/10000 [01:34<02:48, 35.45it/s]Training CobwebTree:  40%|      | 4032/10000 [01:34<02:52, 34.66it/s]Training CobwebTree:  40%|      | 4036/10000 [01:34<02:53, 34.39it/s]Training CobwebTree:  40%|      | 4041/10000 [01:35<02:42, 36.77it/s]Training CobwebTree:  40%|      | 4046/10000 [01:35<02:35, 38.22it/s]Training CobwebTree:  40%|      | 4050/10000 [01:35<02:34, 38.57it/s]Training CobwebTree:  41%|      | 4054/10000 [01:35<02:40, 37.15it/s]Training CobwebTree:  41%|      | 4058/10000 [01:35<02:38, 37.57it/s]Training CobwebTree:  41%|      | 4062/10000 [01:35<02:36, 37.82it/s]Training CobwebTree:  41%|      | 4066/10000 [01:35<02:37, 37.57it/s]Training CobwebTree:  41%|      | 4071/10000 [01:35<02:32, 38.89it/s]Training CobwebTree:  41%|      | 4075/10000 [01:35<02:32, 38.84it/s]Training CobwebTree:  41%|      | 4079/10000 [01:36<02:34, 38.32it/s]Training CobwebTree:  41%|      | 4083/10000 [01:36<02:37, 37.62it/s]Training CobwebTree:  41%|      | 4087/10000 [01:36<02:41, 36.70it/s]Training CobwebTree:  41%|      | 4091/10000 [01:36<02:39, 37.08it/s]Training CobwebTree:  41%|      | 4096/10000 [01:36<02:28, 39.85it/s]Training CobwebTree:  41%|      | 4100/10000 [01:36<02:28, 39.76it/s]Training CobwebTree:  41%|      | 4104/10000 [01:36<02:38, 37.23it/s]Training CobwebTree:  41%|      | 4108/10000 [01:36<02:36, 37.55it/s]Training CobwebTree:  41%|      | 4112/10000 [01:36<02:35, 37.87it/s]Training CobwebTree:  41%|      | 4116/10000 [01:37<02:33, 38.31it/s]Training CobwebTree:  41%|      | 4120/10000 [01:37<02:41, 36.34it/s]Training CobwebTree:  41%|      | 4124/10000 [01:37<02:44, 35.73it/s]Training CobwebTree:  41%|     | 4129/10000 [01:37<02:38, 36.98it/s]Training CobwebTree:  41%|     | 4134/10000 [01:37<02:33, 38.20it/s]Training CobwebTree:  41%|     | 4139/10000 [01:37<02:31, 38.65it/s]Training CobwebTree:  41%|     | 4143/10000 [01:37<02:34, 37.98it/s]Training CobwebTree:  41%|     | 4148/10000 [01:37<02:32, 38.33it/s]Training CobwebTree:  42%|     | 4152/10000 [01:37<02:36, 37.36it/s]Training CobwebTree:  42%|     | 4156/10000 [01:38<02:42, 36.07it/s]Training CobwebTree:  42%|     | 4160/10000 [01:38<02:41, 36.18it/s]Training CobwebTree:  42%|     | 4164/10000 [01:38<02:44, 35.52it/s]Training CobwebTree:  42%|     | 4169/10000 [01:38<02:33, 37.93it/s]Training CobwebTree:  42%|     | 4173/10000 [01:38<02:41, 36.05it/s]Training CobwebTree:  42%|     | 4177/10000 [01:38<02:41, 35.96it/s]Training CobwebTree:  42%|     | 4182/10000 [01:38<02:36, 37.28it/s]Training CobwebTree:  42%|     | 4186/10000 [01:38<02:38, 36.77it/s]Training CobwebTree:  42%|     | 4190/10000 [01:39<02:36, 37.06it/s]Training CobwebTree:  42%|     | 4195/10000 [01:39<02:32, 38.11it/s]Training CobwebTree:  42%|     | 4200/10000 [01:39<02:26, 39.46it/s]Training CobwebTree:  42%|     | 4204/10000 [01:39<02:28, 39.08it/s]Training CobwebTree:  42%|     | 4208/10000 [01:39<02:29, 38.78it/s]Training CobwebTree:  42%|     | 4212/10000 [01:39<02:32, 37.89it/s]Training CobwebTree:  42%|     | 4216/10000 [01:39<02:33, 37.65it/s]Training CobwebTree:  42%|     | 4220/10000 [01:39<02:35, 37.21it/s]Training CobwebTree:  42%|     | 4224/10000 [01:39<02:33, 37.60it/s]Training CobwebTree:  42%|     | 4228/10000 [01:40<02:33, 37.70it/s]Training CobwebTree:  42%|     | 4233/10000 [01:40<02:29, 38.70it/s]Training CobwebTree:  42%|     | 4237/10000 [01:40<02:33, 37.50it/s]Training CobwebTree:  42%|     | 4241/10000 [01:40<02:33, 37.42it/s]Training CobwebTree:  42%|     | 4245/10000 [01:40<02:31, 37.98it/s]Training CobwebTree:  42%|     | 4249/10000 [01:40<02:32, 37.82it/s]Training CobwebTree:  43%|     | 4253/10000 [01:40<02:38, 36.30it/s]Training CobwebTree:  43%|     | 4257/10000 [01:40<02:34, 37.11it/s]Training CobwebTree:  43%|     | 4262/10000 [01:40<02:31, 37.90it/s]Training CobwebTree:  43%|     | 4266/10000 [01:41<02:29, 38.30it/s]Training CobwebTree:  43%|     | 4270/10000 [01:41<02:34, 37.21it/s]Training CobwebTree:  43%|     | 4274/10000 [01:41<02:38, 36.17it/s]Training CobwebTree:  43%|     | 4279/10000 [01:41<02:28, 38.40it/s]Training CobwebTree:  43%|     | 4284/10000 [01:41<02:23, 39.95it/s]Training CobwebTree:  43%|     | 4288/10000 [01:41<02:27, 38.66it/s]Training CobwebTree:  43%|     | 4292/10000 [01:41<02:27, 38.72it/s]Training CobwebTree:  43%|     | 4296/10000 [01:41<02:36, 36.47it/s]Training CobwebTree:  43%|     | 4300/10000 [01:41<02:35, 36.74it/s]Training CobwebTree:  43%|     | 4304/10000 [01:42<02:32, 37.25it/s]Training CobwebTree:  43%|     | 4308/10000 [01:42<02:31, 37.67it/s]Training CobwebTree:  43%|     | 4312/10000 [01:42<02:33, 37.16it/s]Training CobwebTree:  43%|     | 4316/10000 [01:42<02:30, 37.78it/s]Training CobwebTree:  43%|     | 4320/10000 [01:42<02:33, 37.02it/s]Training CobwebTree:  43%|     | 4324/10000 [01:42<02:36, 36.31it/s]Training CobwebTree:  43%|     | 4328/10000 [01:42<02:37, 35.92it/s]Training CobwebTree:  43%|     | 4332/10000 [01:42<02:36, 36.21it/s]Training CobwebTree:  43%|     | 4336/10000 [01:42<02:33, 36.97it/s]Training CobwebTree:  43%|     | 4340/10000 [01:43<02:34, 36.70it/s]Training CobwebTree:  43%|     | 4345/10000 [01:43<02:26, 38.48it/s]Training CobwebTree:  43%|     | 4349/10000 [01:43<02:28, 38.03it/s]Training CobwebTree:  44%|     | 4354/10000 [01:43<02:26, 38.57it/s]Training CobwebTree:  44%|     | 4358/10000 [01:43<02:31, 37.23it/s]Training CobwebTree:  44%|     | 4362/10000 [01:43<02:28, 37.85it/s]Training CobwebTree:  44%|     | 4366/10000 [01:43<02:35, 36.29it/s]Training CobwebTree:  44%|     | 4371/10000 [01:43<02:29, 37.75it/s]Training CobwebTree:  44%|     | 4376/10000 [01:43<02:27, 38.17it/s]Training CobwebTree:  44%|     | 4380/10000 [01:44<02:28, 37.78it/s]Training CobwebTree:  44%|     | 4384/10000 [01:44<02:26, 38.30it/s]Training CobwebTree:  44%|     | 4388/10000 [01:44<02:26, 38.26it/s]Training CobwebTree:  44%|     | 4392/10000 [01:44<02:24, 38.68it/s]Training CobwebTree:  44%|     | 4396/10000 [01:44<02:23, 39.00it/s]Training CobwebTree:  44%|     | 4400/10000 [01:44<02:27, 37.85it/s]Training CobwebTree:  44%|     | 4404/10000 [01:44<02:29, 37.35it/s]Training CobwebTree:  44%|     | 4408/10000 [01:44<02:29, 37.39it/s]Training CobwebTree:  44%|     | 4412/10000 [01:44<02:35, 36.05it/s]Training CobwebTree:  44%|     | 4416/10000 [01:45<02:36, 35.67it/s]Training CobwebTree:  44%|     | 4420/10000 [01:45<02:32, 36.49it/s]Training CobwebTree:  44%|     | 4424/10000 [01:45<02:35, 35.95it/s]Training CobwebTree:  44%|     | 4428/10000 [01:45<02:35, 35.78it/s]Training CobwebTree:  44%|     | 4432/10000 [01:45<02:36, 35.64it/s]Training CobwebTree:  44%|     | 4437/10000 [01:45<02:31, 36.81it/s]Training CobwebTree:  44%|     | 4441/10000 [01:45<02:29, 37.30it/s]Training CobwebTree:  44%|     | 4445/10000 [01:45<02:29, 37.26it/s]Training CobwebTree:  44%|     | 4450/10000 [01:45<02:27, 37.71it/s]Training CobwebTree:  45%|     | 4454/10000 [01:46<02:33, 36.11it/s]Training CobwebTree:  45%|     | 4458/10000 [01:46<02:39, 34.80it/s]Training CobwebTree:  45%|     | 4462/10000 [01:46<02:37, 35.27it/s]Training CobwebTree:  45%|     | 4466/10000 [01:46<02:38, 34.82it/s]Training CobwebTree:  45%|     | 4471/10000 [01:46<02:29, 37.04it/s]Training CobwebTree:  45%|     | 4475/10000 [01:46<02:30, 36.81it/s]Training CobwebTree:  45%|     | 4479/10000 [01:46<02:30, 36.71it/s]Training CobwebTree:  45%|     | 4483/10000 [01:46<02:32, 36.21it/s]Training CobwebTree:  45%|     | 4487/10000 [01:46<02:31, 36.50it/s]Training CobwebTree:  45%|     | 4491/10000 [01:47<02:33, 35.78it/s]Training CobwebTree:  45%|     | 4495/10000 [01:47<02:35, 35.29it/s]Training CobwebTree:  45%|     | 4500/10000 [01:47<02:31, 36.27it/s]Training CobwebTree:  45%|     | 4504/10000 [01:47<02:31, 36.35it/s]Training CobwebTree:  45%|     | 4508/10000 [01:47<02:27, 37.31it/s]Training CobwebTree:  45%|     | 4512/10000 [01:47<02:29, 36.72it/s]Training CobwebTree:  45%|     | 4517/10000 [01:47<02:29, 36.78it/s]Training CobwebTree:  45%|     | 4521/10000 [01:47<02:27, 37.10it/s]Training CobwebTree:  45%|     | 4525/10000 [01:48<02:32, 35.81it/s]Training CobwebTree:  45%|     | 4529/10000 [01:48<02:34, 35.50it/s]Training CobwebTree:  45%|     | 4534/10000 [01:48<02:27, 37.18it/s]Training CobwebTree:  45%|     | 4538/10000 [01:48<02:28, 36.70it/s]Training CobwebTree:  45%|     | 4542/10000 [01:48<02:26, 37.21it/s]Training CobwebTree:  45%|     | 4546/10000 [01:48<02:24, 37.85it/s]Training CobwebTree:  46%|     | 4550/10000 [01:48<02:30, 36.31it/s]Training CobwebTree:  46%|     | 4554/10000 [01:48<02:28, 36.71it/s]Training CobwebTree:  46%|     | 4559/10000 [01:48<02:23, 38.03it/s]Training CobwebTree:  46%|     | 4563/10000 [01:49<02:24, 37.64it/s]Training CobwebTree:  46%|     | 4567/10000 [01:49<02:30, 36.09it/s]Training CobwebTree:  46%|     | 4572/10000 [01:49<02:25, 37.28it/s]Training CobwebTree:  46%|     | 4577/10000 [01:49<02:22, 37.97it/s]Training CobwebTree:  46%|     | 4581/10000 [01:49<02:26, 36.99it/s]Training CobwebTree:  46%|     | 4586/10000 [01:49<02:16, 39.77it/s]Training CobwebTree:  46%|     | 4591/10000 [01:49<02:13, 40.63it/s]Training CobwebTree:  46%|     | 4596/10000 [01:49<02:16, 39.52it/s]Training CobwebTree:  46%|     | 4600/10000 [01:50<02:18, 39.05it/s]Training CobwebTree:  46%|     | 4604/10000 [01:50<02:18, 39.03it/s]Training CobwebTree:  46%|     | 4608/10000 [01:50<02:21, 38.13it/s]Training CobwebTree:  46%|     | 4613/10000 [01:50<02:16, 39.53it/s]Training CobwebTree:  46%|     | 4618/10000 [01:50<02:15, 39.63it/s]Training CobwebTree:  46%|     | 4622/10000 [01:50<02:17, 38.97it/s]Training CobwebTree:  46%|     | 4626/10000 [01:50<02:18, 38.83it/s]Training CobwebTree:  46%|     | 4630/10000 [01:50<02:21, 37.93it/s]Training CobwebTree:  46%|     | 4634/10000 [01:50<02:22, 37.70it/s]Training CobwebTree:  46%|     | 4638/10000 [01:51<02:24, 37.17it/s]Training CobwebTree:  46%|     | 4642/10000 [01:51<02:24, 37.18it/s]Training CobwebTree:  46%|     | 4646/10000 [01:51<02:22, 37.68it/s]Training CobwebTree:  46%|     | 4650/10000 [01:51<02:24, 36.94it/s]Training CobwebTree:  47%|     | 4655/10000 [01:51<02:17, 38.91it/s]Training CobwebTree:  47%|     | 4659/10000 [01:51<02:23, 37.10it/s]Training CobwebTree:  47%|     | 4663/10000 [01:51<02:31, 35.31it/s]Training CobwebTree:  47%|     | 4667/10000 [01:51<02:36, 34.14it/s]Training CobwebTree:  47%|     | 4672/10000 [01:51<02:22, 37.29it/s]Training CobwebTree:  47%|     | 4676/10000 [01:52<02:20, 37.93it/s]Training CobwebTree:  47%|     | 4680/10000 [01:52<02:22, 37.39it/s]Training CobwebTree:  47%|     | 4685/10000 [01:52<02:16, 38.95it/s]Training CobwebTree:  47%|     | 4690/10000 [01:52<02:14, 39.52it/s]Training CobwebTree:  47%|     | 4694/10000 [01:52<02:24, 36.74it/s]Training CobwebTree:  47%|     | 4698/10000 [01:52<02:22, 37.09it/s]Training CobwebTree:  47%|     | 4703/10000 [01:52<02:16, 38.82it/s]Training CobwebTree:  47%|     | 4707/10000 [01:52<02:21, 37.29it/s]Training CobwebTree:  47%|     | 4711/10000 [01:52<02:19, 37.79it/s]Training CobwebTree:  47%|     | 4715/10000 [01:53<02:21, 37.47it/s]Training CobwebTree:  47%|     | 4720/10000 [01:53<02:17, 38.44it/s]Training CobwebTree:  47%|     | 4724/10000 [01:53<02:17, 38.47it/s]Training CobwebTree:  47%|     | 4729/10000 [01:53<02:13, 39.51it/s]Training CobwebTree:  47%|     | 4733/10000 [01:53<02:16, 38.62it/s]Training CobwebTree:  47%|     | 4737/10000 [01:53<02:18, 38.00it/s]Training CobwebTree:  47%|     | 4742/10000 [01:53<02:15, 38.76it/s]Training CobwebTree:  47%|     | 4746/10000 [01:53<02:18, 37.96it/s]Training CobwebTree:  48%|     | 4750/10000 [01:53<02:17, 38.11it/s]Training CobwebTree:  48%|     | 4754/10000 [01:54<02:19, 37.71it/s]Training CobwebTree:  48%|     | 4758/10000 [01:54<02:19, 37.60it/s]Training CobwebTree:  48%|     | 4762/10000 [01:54<02:23, 36.49it/s]Training CobwebTree:  48%|     | 4766/10000 [01:54<02:24, 36.22it/s]Training CobwebTree:  48%|     | 4770/10000 [01:54<02:24, 36.07it/s]Training CobwebTree:  48%|     | 4775/10000 [01:54<02:21, 36.99it/s]Training CobwebTree:  48%|     | 4780/10000 [01:54<02:18, 37.62it/s]Training CobwebTree:  48%|     | 4784/10000 [01:54<02:19, 37.41it/s]Training CobwebTree:  48%|     | 4788/10000 [01:54<02:20, 37.23it/s]Training CobwebTree:  48%|     | 4792/10000 [01:55<02:27, 35.41it/s]Training CobwebTree:  48%|     | 4796/10000 [01:55<02:22, 36.63it/s]Training CobwebTree:  48%|     | 4800/10000 [01:55<02:21, 36.75it/s]Training CobwebTree:  48%|     | 4804/10000 [01:55<02:18, 37.49it/s]Training CobwebTree:  48%|     | 4809/10000 [01:55<02:16, 38.09it/s]Training CobwebTree:  48%|     | 4813/10000 [01:55<02:22, 36.46it/s]Training CobwebTree:  48%|     | 4817/10000 [01:55<02:22, 36.45it/s]Training CobwebTree:  48%|     | 4821/10000 [01:55<02:22, 36.40it/s]Training CobwebTree:  48%|     | 4825/10000 [01:56<02:22, 36.37it/s]Training CobwebTree:  48%|     | 4830/10000 [01:56<02:14, 38.36it/s]Training CobwebTree:  48%|     | 4834/10000 [01:56<02:18, 37.17it/s]Training CobwebTree:  48%|     | 4838/10000 [01:56<02:21, 36.41it/s]Training CobwebTree:  48%|     | 4842/10000 [01:56<02:20, 36.82it/s]Training CobwebTree:  48%|     | 4846/10000 [01:56<02:17, 37.51it/s]Training CobwebTree:  48%|     | 4850/10000 [01:56<02:16, 37.69it/s]Training CobwebTree:  49%|     | 4854/10000 [01:56<02:14, 38.16it/s]Training CobwebTree:  49%|     | 4859/10000 [01:56<02:15, 38.00it/s]Training CobwebTree:  49%|     | 4863/10000 [01:57<02:14, 38.23it/s]Training CobwebTree:  49%|     | 4867/10000 [01:57<02:17, 37.27it/s]Training CobwebTree:  49%|     | 4871/10000 [01:57<02:15, 37.90it/s]Training CobwebTree:  49%|     | 4876/10000 [01:57<02:11, 38.94it/s]Training CobwebTree:  49%|     | 4880/10000 [01:57<02:19, 36.82it/s]Training CobwebTree:  49%|     | 4884/10000 [01:57<02:16, 37.35it/s]Training CobwebTree:  49%|     | 4888/10000 [01:57<02:14, 38.02it/s]Training CobwebTree:  49%|     | 4892/10000 [01:57<02:14, 38.01it/s]Training CobwebTree:  49%|     | 4896/10000 [01:57<02:22, 35.88it/s]Training CobwebTree:  49%|     | 4900/10000 [01:58<02:24, 35.33it/s]Training CobwebTree:  49%|     | 4904/10000 [01:58<02:22, 35.74it/s]Training CobwebTree:  49%|     | 4908/10000 [01:58<02:26, 34.87it/s]Training CobwebTree:  49%|     | 4912/10000 [01:58<02:20, 36.21it/s]Training CobwebTree:  49%|     | 4916/10000 [01:58<02:23, 35.43it/s]Training CobwebTree:  49%|     | 4920/10000 [01:58<02:25, 34.85it/s]Training CobwebTree:  49%|     | 4924/10000 [01:58<02:23, 35.40it/s]Training CobwebTree:  49%|     | 4928/10000 [01:58<02:18, 36.60it/s]Training CobwebTree:  49%|     | 4933/10000 [01:58<02:13, 37.98it/s]Training CobwebTree:  49%|     | 4937/10000 [01:59<02:15, 37.44it/s]Training CobwebTree:  49%|     | 4942/10000 [01:59<02:12, 38.03it/s]Training CobwebTree:  49%|     | 4946/10000 [01:59<02:13, 37.89it/s]Training CobwebTree:  50%|     | 4950/10000 [01:59<02:12, 38.18it/s]Training CobwebTree:  50%|     | 4954/10000 [01:59<02:15, 37.25it/s]Training CobwebTree:  50%|     | 4958/10000 [01:59<02:17, 36.58it/s]Training CobwebTree:  50%|     | 4962/10000 [01:59<02:15, 37.27it/s]Training CobwebTree:  50%|     | 4966/10000 [01:59<02:15, 37.02it/s]Training CobwebTree:  50%|     | 4971/10000 [01:59<02:07, 39.50it/s]Training CobwebTree:  50%|     | 4975/10000 [02:00<02:15, 37.16it/s]Training CobwebTree:  50%|     | 4979/10000 [02:00<02:18, 36.28it/s]Training CobwebTree:  50%|     | 4983/10000 [02:00<02:17, 36.43it/s]Training CobwebTree:  50%|     | 4987/10000 [02:00<02:17, 36.37it/s]Training CobwebTree:  50%|     | 4991/10000 [02:00<02:24, 34.71it/s]Training CobwebTree:  50%|     | 4995/10000 [02:00<02:21, 35.36it/s]Training CobwebTree:  50%|     | 5000/10000 [02:00<02:16, 36.53it/s]Training CobwebTree:  50%|     | 5004/10000 [02:00<02:14, 37.27it/s]Training CobwebTree:  50%|     | 5008/10000 [02:00<02:13, 37.39it/s]Training CobwebTree:  50%|     | 5013/10000 [02:01<02:09, 38.56it/s]Training CobwebTree:  50%|     | 5018/10000 [02:01<02:07, 38.92it/s]Training CobwebTree:  50%|     | 5023/10000 [02:01<02:03, 40.30it/s]Training CobwebTree:  50%|     | 5028/10000 [02:01<02:07, 39.14it/s]Training CobwebTree:  50%|     | 5032/10000 [02:01<02:08, 38.72it/s]Training CobwebTree:  50%|     | 5036/10000 [02:01<02:12, 37.59it/s]Training CobwebTree:  50%|     | 5041/10000 [02:01<02:07, 38.92it/s]Training CobwebTree:  50%|     | 5045/10000 [02:01<02:06, 39.14it/s]Training CobwebTree:  50%|     | 5049/10000 [02:02<02:12, 37.29it/s]Training CobwebTree:  51%|     | 5053/10000 [02:02<02:13, 37.14it/s]Training CobwebTree:  51%|     | 5057/10000 [02:02<02:17, 35.89it/s]Training CobwebTree:  51%|     | 5061/10000 [02:02<02:16, 36.26it/s]Training CobwebTree:  51%|     | 5065/10000 [02:02<02:17, 35.80it/s]Training CobwebTree:  51%|     | 5069/10000 [02:02<02:17, 35.95it/s]Training CobwebTree:  51%|     | 5073/10000 [02:02<02:21, 34.88it/s]Training CobwebTree:  51%|     | 5077/10000 [02:02<02:16, 36.01it/s]Training CobwebTree:  51%|     | 5081/10000 [02:02<02:17, 35.72it/s]Training CobwebTree:  51%|     | 5085/10000 [02:03<02:16, 35.88it/s]Training CobwebTree:  51%|     | 5090/10000 [02:03<02:09, 37.81it/s]Training CobwebTree:  51%|     | 5094/10000 [02:03<02:12, 36.90it/s]Training CobwebTree:  51%|     | 5099/10000 [02:03<02:10, 37.49it/s]Training CobwebTree:  51%|     | 5104/10000 [02:03<02:06, 38.63it/s]Training CobwebTree:  51%|     | 5108/10000 [02:03<02:06, 38.76it/s]Training CobwebTree:  51%|     | 5112/10000 [02:03<02:09, 37.88it/s]Training CobwebTree:  51%|     | 5116/10000 [02:03<02:12, 36.97it/s]Training CobwebTree:  51%|     | 5120/10000 [02:03<02:14, 36.28it/s]Training CobwebTree:  51%|     | 5124/10000 [02:04<02:11, 37.02it/s]Training CobwebTree:  51%|    | 5128/10000 [02:04<02:14, 36.15it/s]Training CobwebTree:  51%|    | 5132/10000 [02:04<02:14, 36.32it/s]Training CobwebTree:  51%|    | 5136/10000 [02:04<02:13, 36.32it/s]Training CobwebTree:  51%|    | 5141/10000 [02:04<02:07, 38.02it/s]Training CobwebTree:  51%|    | 5145/10000 [02:04<02:08, 37.80it/s]Training CobwebTree:  51%|    | 5149/10000 [02:04<02:14, 35.97it/s]Training CobwebTree:  52%|    | 5153/10000 [02:04<02:16, 35.51it/s]Training CobwebTree:  52%|    | 5157/10000 [02:04<02:12, 36.48it/s]Training CobwebTree:  52%|    | 5161/10000 [02:05<02:12, 36.55it/s]Training CobwebTree:  52%|    | 5166/10000 [02:05<02:09, 37.32it/s]Training CobwebTree:  52%|    | 5170/10000 [02:05<02:10, 36.90it/s]Training CobwebTree:  52%|    | 5174/10000 [02:05<02:08, 37.45it/s]Training CobwebTree:  52%|    | 5178/10000 [02:05<02:09, 37.15it/s]Training CobwebTree:  52%|    | 5182/10000 [02:05<02:12, 36.40it/s]Training CobwebTree:  52%|    | 5186/10000 [02:05<02:09, 37.16it/s]Training CobwebTree:  52%|    | 5191/10000 [02:05<02:04, 38.60it/s]Training CobwebTree:  52%|    | 5195/10000 [02:05<02:06, 37.91it/s]Training CobwebTree:  52%|    | 5199/10000 [02:06<02:06, 37.83it/s]Training CobwebTree:  52%|    | 5203/10000 [02:06<02:12, 36.33it/s]Training CobwebTree:  52%|    | 5207/10000 [02:06<02:11, 36.47it/s]Training CobwebTree:  52%|    | 5211/10000 [02:06<02:16, 35.21it/s]Training CobwebTree:  52%|    | 5215/10000 [02:06<02:12, 35.99it/s]Training CobwebTree:  52%|    | 5219/10000 [02:06<02:09, 36.97it/s]Training CobwebTree:  52%|    | 5223/10000 [02:06<02:07, 37.33it/s]Training CobwebTree:  52%|    | 5228/10000 [02:06<01:59, 39.86it/s]Training CobwebTree:  52%|    | 5232/10000 [02:06<02:00, 39.43it/s]Training CobwebTree:  52%|    | 5236/10000 [02:07<02:04, 38.11it/s]Training CobwebTree:  52%|    | 5240/10000 [02:07<02:09, 36.82it/s]Training CobwebTree:  52%|    | 5244/10000 [02:07<02:08, 37.06it/s]Training CobwebTree:  52%|    | 5248/10000 [02:07<02:08, 37.12it/s]Training CobwebTree:  53%|    | 5252/10000 [02:07<02:11, 36.05it/s]Training CobwebTree:  53%|    | 5256/10000 [02:07<02:12, 35.91it/s]Training CobwebTree:  53%|    | 5260/10000 [02:07<02:14, 35.25it/s]Training CobwebTree:  53%|    | 5264/10000 [02:07<02:15, 35.01it/s]Training CobwebTree:  53%|    | 5268/10000 [02:07<02:10, 36.22it/s]Training CobwebTree:  53%|    | 5272/10000 [02:08<02:15, 34.82it/s]Training CobwebTree:  53%|    | 5277/10000 [02:08<02:10, 36.30it/s]Training CobwebTree:  53%|    | 5281/10000 [02:08<02:08, 36.68it/s]Training CobwebTree:  53%|    | 5286/10000 [02:08<02:05, 37.69it/s]Training CobwebTree:  53%|    | 5291/10000 [02:08<01:59, 39.41it/s]Training CobwebTree:  53%|    | 5295/10000 [02:08<02:02, 38.44it/s]Training CobwebTree:  53%|    | 5300/10000 [02:08<01:59, 39.38it/s]Training CobwebTree:  53%|    | 5304/10000 [02:08<01:58, 39.54it/s]Training CobwebTree:  53%|    | 5308/10000 [02:09<02:00, 38.94it/s]Training CobwebTree:  53%|    | 5312/10000 [02:09<02:01, 38.58it/s]Training CobwebTree:  53%|    | 5316/10000 [02:09<02:03, 38.00it/s]Training CobwebTree:  53%|    | 5320/10000 [02:09<02:05, 37.31it/s]Training CobwebTree:  53%|    | 5325/10000 [02:09<01:59, 39.25it/s]Training CobwebTree:  53%|    | 5329/10000 [02:09<02:04, 37.40it/s]Training CobwebTree:  53%|    | 5334/10000 [02:09<01:59, 39.04it/s]Training CobwebTree:  53%|    | 5338/10000 [02:09<02:00, 38.68it/s]Training CobwebTree:  53%|    | 5342/10000 [02:09<02:05, 37.23it/s]Training CobwebTree:  53%|    | 5346/10000 [02:10<02:09, 35.96it/s]Training CobwebTree:  54%|    | 5350/10000 [02:10<02:07, 36.41it/s]Training CobwebTree:  54%|    | 5354/10000 [02:10<02:06, 36.78it/s]Training CobwebTree:  54%|    | 5358/10000 [02:10<02:05, 36.92it/s]Training CobwebTree:  54%|    | 5362/10000 [02:10<02:08, 36.01it/s]Training CobwebTree:  54%|    | 5366/10000 [02:10<02:09, 35.66it/s]Training CobwebTree:  54%|    | 5370/10000 [02:10<02:10, 35.45it/s]Training CobwebTree:  54%|    | 5374/10000 [02:10<02:06, 36.49it/s]Training CobwebTree:  54%|    | 5378/10000 [02:10<02:09, 35.71it/s]Training CobwebTree:  54%|    | 5382/10000 [02:11<02:05, 36.71it/s]Training CobwebTree:  54%|    | 5387/10000 [02:11<02:03, 37.23it/s]Training CobwebTree:  54%|    | 5391/10000 [02:11<02:08, 35.79it/s]Training CobwebTree:  54%|    | 5395/10000 [02:11<02:06, 36.44it/s]Training CobwebTree:  54%|    | 5399/10000 [02:11<02:08, 35.75it/s]Training CobwebTree:  54%|    | 5403/10000 [02:11<02:05, 36.55it/s]Training CobwebTree:  54%|    | 5408/10000 [02:11<02:03, 37.10it/s]Training CobwebTree:  54%|    | 5412/10000 [02:11<02:04, 36.85it/s]Training CobwebTree:  54%|    | 5416/10000 [02:11<02:06, 36.13it/s]Training CobwebTree:  54%|    | 5420/10000 [02:12<02:04, 36.71it/s]Training CobwebTree:  54%|    | 5424/10000 [02:12<02:14, 34.14it/s]Training CobwebTree:  54%|    | 5428/10000 [02:12<02:12, 34.45it/s]Training CobwebTree:  54%|    | 5432/10000 [02:12<02:17, 33.34it/s]Training CobwebTree:  54%|    | 5436/10000 [02:12<02:11, 34.68it/s]Training CobwebTree:  54%|    | 5441/10000 [02:12<02:01, 37.59it/s]Training CobwebTree:  54%|    | 5445/10000 [02:12<02:01, 37.62it/s]Training CobwebTree:  54%|    | 5449/10000 [02:12<02:06, 35.94it/s]Training CobwebTree:  55%|    | 5453/10000 [02:12<02:08, 35.36it/s]Training CobwebTree:  55%|    | 5457/10000 [02:13<02:08, 35.24it/s]Training CobwebTree:  55%|    | 5461/10000 [02:13<02:10, 34.91it/s]Training CobwebTree:  55%|    | 5465/10000 [02:13<02:10, 34.70it/s]Training CobwebTree:  55%|    | 5469/10000 [02:13<02:07, 35.47it/s]Training CobwebTree:  55%|    | 5473/10000 [02:13<02:07, 35.64it/s]Training CobwebTree:  55%|    | 5477/10000 [02:13<02:09, 35.04it/s]Training CobwebTree:  55%|    | 5481/10000 [02:13<02:14, 33.50it/s]Training CobwebTree:  55%|    | 5485/10000 [02:13<02:12, 34.04it/s]Training CobwebTree:  55%|    | 5490/10000 [02:14<02:07, 35.49it/s]Training CobwebTree:  55%|    | 5494/10000 [02:14<02:05, 36.00it/s]Training CobwebTree:  55%|    | 5498/10000 [02:14<02:03, 36.51it/s]Training CobwebTree:  55%|    | 5503/10000 [02:14<01:57, 38.15it/s]Training CobwebTree:  55%|    | 5507/10000 [02:14<02:01, 37.01it/s]Training CobwebTree:  55%|    | 5511/10000 [02:14<02:02, 36.76it/s]Training CobwebTree:  55%|    | 5515/10000 [02:14<02:00, 37.29it/s]Training CobwebTree:  55%|    | 5520/10000 [02:14<01:52, 39.88it/s]Training CobwebTree:  55%|    | 5524/10000 [02:14<01:54, 38.97it/s]Training CobwebTree:  55%|    | 5529/10000 [02:15<01:54, 39.21it/s]Training CobwebTree:  55%|    | 5533/10000 [02:15<02:00, 37.04it/s]Training CobwebTree:  55%|    | 5537/10000 [02:15<02:00, 37.16it/s]Training CobwebTree:  55%|    | 5541/10000 [02:15<02:01, 36.70it/s]Training CobwebTree:  55%|    | 5546/10000 [02:15<01:55, 38.50it/s]Training CobwebTree:  56%|    | 5550/10000 [02:15<01:54, 38.77it/s]Training CobwebTree:  56%|    | 5554/10000 [02:15<01:58, 37.67it/s]Training CobwebTree:  56%|    | 5559/10000 [02:15<01:54, 38.69it/s]Training CobwebTree:  56%|    | 5563/10000 [02:15<01:56, 38.23it/s]Training CobwebTree:  56%|    | 5567/10000 [02:16<02:04, 35.64it/s]Training CobwebTree:  56%|    | 5571/10000 [02:16<02:06, 34.95it/s]Training CobwebTree:  56%|    | 5576/10000 [02:16<01:59, 36.92it/s]Training CobwebTree:  56%|    | 5580/10000 [02:16<01:59, 36.91it/s]Training CobwebTree:  56%|    | 5584/10000 [02:16<01:59, 37.01it/s]Training CobwebTree:  56%|    | 5588/10000 [02:16<02:01, 36.34it/s]Training CobwebTree:  56%|    | 5592/10000 [02:16<02:05, 35.01it/s]Training CobwebTree:  56%|    | 5596/10000 [02:16<02:03, 35.55it/s]Training CobwebTree:  56%|    | 5600/10000 [02:17<02:07, 34.40it/s]Training CobwebTree:  56%|    | 5604/10000 [02:17<02:06, 34.62it/s]Training CobwebTree:  56%|    | 5608/10000 [02:17<02:13, 32.89it/s]Training CobwebTree:  56%|    | 5612/10000 [02:17<02:08, 34.21it/s]Training CobwebTree:  56%|    | 5616/10000 [02:17<02:06, 34.53it/s]Training CobwebTree:  56%|    | 5621/10000 [02:17<02:00, 36.40it/s]Training CobwebTree:  56%|    | 5625/10000 [02:17<01:58, 36.95it/s]Training CobwebTree:  56%|    | 5629/10000 [02:17<01:59, 36.52it/s]Training CobwebTree:  56%|    | 5633/10000 [02:17<01:59, 36.43it/s]Training CobwebTree:  56%|    | 5637/10000 [02:18<01:57, 37.11it/s]Training CobwebTree:  56%|    | 5641/10000 [02:18<01:59, 36.44it/s]Training CobwebTree:  56%|    | 5645/10000 [02:18<02:01, 35.74it/s]Training CobwebTree:  56%|    | 5650/10000 [02:18<01:54, 38.08it/s]Training CobwebTree:  57%|    | 5655/10000 [02:18<01:51, 38.84it/s]Training CobwebTree:  57%|    | 5659/10000 [02:18<01:51, 38.93it/s]Training CobwebTree:  57%|    | 5663/10000 [02:18<01:53, 38.11it/s]Training CobwebTree:  57%|    | 5667/10000 [02:18<01:56, 37.17it/s]Training CobwebTree:  57%|    | 5671/10000 [02:18<01:55, 37.43it/s]Training CobwebTree:  57%|    | 5675/10000 [02:19<02:03, 35.10it/s]Training CobwebTree:  57%|    | 5679/10000 [02:19<02:03, 34.90it/s]Training CobwebTree:  57%|    | 5683/10000 [02:19<02:02, 35.18it/s]Training CobwebTree:  57%|    | 5687/10000 [02:19<01:59, 36.07it/s]Training CobwebTree:  57%|    | 5691/10000 [02:19<02:01, 35.47it/s]Training CobwebTree:  57%|    | 5696/10000 [02:19<01:54, 37.51it/s]Training CobwebTree:  57%|    | 5700/10000 [02:19<01:56, 37.07it/s]Training CobwebTree:  57%|    | 5704/10000 [02:19<02:00, 35.76it/s]Training CobwebTree:  57%|    | 5708/10000 [02:19<02:00, 35.76it/s]Training CobwebTree:  57%|    | 5712/10000 [02:20<02:00, 35.57it/s]Training CobwebTree:  57%|    | 5716/10000 [02:20<01:58, 36.02it/s]Training CobwebTree:  57%|    | 5720/10000 [02:20<02:01, 35.15it/s]Training CobwebTree:  57%|    | 5724/10000 [02:20<02:02, 34.78it/s]Training CobwebTree:  57%|    | 5728/10000 [02:20<02:02, 34.80it/s]Training CobwebTree:  57%|    | 5733/10000 [02:20<01:57, 36.34it/s]Training CobwebTree:  57%|    | 5737/10000 [02:20<01:57, 36.32it/s]Training CobwebTree:  57%|    | 5741/10000 [02:20<01:57, 36.11it/s]Training CobwebTree:  57%|    | 5745/10000 [02:21<02:01, 35.09it/s]Training CobwebTree:  57%|    | 5749/10000 [02:21<02:02, 34.68it/s]Training CobwebTree:  58%|    | 5754/10000 [02:21<01:56, 36.35it/s]Training CobwebTree:  58%|    | 5758/10000 [02:21<01:54, 37.03it/s]Training CobwebTree:  58%|    | 5762/10000 [02:21<01:59, 35.42it/s]Training CobwebTree:  58%|    | 5767/10000 [02:21<01:55, 36.72it/s]Training CobwebTree:  58%|    | 5771/10000 [02:21<01:58, 35.78it/s]Training CobwebTree:  58%|    | 5775/10000 [02:21<02:00, 35.16it/s]Training CobwebTree:  58%|    | 5779/10000 [02:22<04:05, 17.21it/s]Training CobwebTree:  58%|    | 5783/10000 [02:22<03:27, 20.30it/s]Training CobwebTree:  58%|    | 5787/10000 [02:22<02:59, 23.44it/s]Training CobwebTree:  58%|    | 5791/10000 [02:22<02:39, 26.34it/s]Training CobwebTree:  58%|    | 5795/10000 [02:22<02:29, 28.07it/s]Training CobwebTree:  58%|    | 5799/10000 [02:22<02:20, 29.90it/s]Training CobwebTree:  58%|    | 5803/10000 [02:23<02:13, 31.46it/s]Training CobwebTree:  58%|    | 5807/10000 [02:23<02:08, 32.66it/s]Training CobwebTree:  58%|    | 5811/10000 [02:23<02:03, 33.85it/s]Training CobwebTree:  58%|    | 5815/10000 [02:23<02:01, 34.51it/s]Training CobwebTree:  58%|    | 5819/10000 [02:23<02:00, 34.61it/s]Training CobwebTree:  58%|    | 5823/10000 [02:23<02:02, 34.11it/s]Training CobwebTree:  58%|    | 5828/10000 [02:23<01:52, 37.10it/s]Training CobwebTree:  58%|    | 5832/10000 [02:23<01:54, 36.45it/s]Training CobwebTree:  58%|    | 5836/10000 [02:23<01:57, 35.38it/s]Training CobwebTree:  58%|    | 5840/10000 [02:24<01:59, 34.89it/s]Training CobwebTree:  58%|    | 5844/10000 [02:24<02:00, 34.61it/s]Training CobwebTree:  58%|    | 5848/10000 [02:24<02:03, 33.69it/s]Training CobwebTree:  59%|    | 5853/10000 [02:24<01:54, 36.27it/s]Training CobwebTree:  59%|    | 5857/10000 [02:24<01:54, 36.33it/s]Training CobwebTree:  59%|    | 5861/10000 [02:24<01:52, 36.64it/s]Training CobwebTree:  59%|    | 5865/10000 [02:24<01:57, 35.10it/s]Training CobwebTree:  59%|    | 5869/10000 [02:24<01:58, 34.79it/s]Training CobwebTree:  59%|    | 5873/10000 [02:25<02:01, 33.98it/s]Training CobwebTree:  59%|    | 5877/10000 [02:25<01:58, 34.80it/s]Training CobwebTree:  59%|    | 5881/10000 [02:25<01:56, 35.38it/s]Training CobwebTree:  59%|    | 5885/10000 [02:25<01:58, 34.60it/s]Training CobwebTree:  59%|    | 5889/10000 [02:25<01:54, 35.85it/s]Training CobwebTree:  59%|    | 5893/10000 [02:25<01:55, 35.69it/s]Training CobwebTree:  59%|    | 5897/10000 [02:25<01:53, 36.05it/s]Training CobwebTree:  59%|    | 5901/10000 [02:25<01:56, 35.15it/s]Training CobwebTree:  59%|    | 5905/10000 [02:25<01:56, 35.19it/s]Training CobwebTree:  59%|    | 5909/10000 [02:26<01:59, 34.18it/s]Training CobwebTree:  59%|    | 5913/10000 [02:26<01:58, 34.42it/s]Training CobwebTree:  59%|    | 5917/10000 [02:26<01:55, 35.40it/s]Training CobwebTree:  59%|    | 5921/10000 [02:26<01:53, 35.98it/s]Training CobwebTree:  59%|    | 5925/10000 [02:26<01:54, 35.58it/s]Training CobwebTree:  59%|    | 5929/10000 [02:26<01:51, 36.55it/s]Training CobwebTree:  59%|    | 5934/10000 [02:26<01:47, 37.83it/s]Training CobwebTree:  59%|    | 5938/10000 [02:26<01:48, 37.28it/s]Training CobwebTree:  59%|    | 5942/10000 [02:26<01:49, 37.05it/s]Training CobwebTree:  59%|    | 5946/10000 [02:27<01:51, 36.45it/s]Training CobwebTree:  60%|    | 5951/10000 [02:27<01:47, 37.77it/s]Training CobwebTree:  60%|    | 5955/10000 [02:27<01:48, 37.19it/s]Training CobwebTree:  60%|    | 5959/10000 [02:27<01:49, 36.87it/s]Training CobwebTree:  60%|    | 5964/10000 [02:27<01:46, 37.79it/s]Training CobwebTree:  60%|    | 5968/10000 [02:27<01:49, 36.81it/s]Training CobwebTree:  60%|    | 5972/10000 [02:27<01:51, 36.14it/s]Training CobwebTree:  60%|    | 5976/10000 [02:27<01:49, 36.85it/s]Training CobwebTree:  60%|    | 5980/10000 [02:28<01:51, 36.05it/s]Training CobwebTree:  60%|    | 5984/10000 [02:28<01:55, 34.64it/s]Training CobwebTree:  60%|    | 5988/10000 [02:28<01:52, 35.74it/s]Training CobwebTree:  60%|    | 5992/10000 [02:28<01:58, 33.88it/s]Training CobwebTree:  60%|    | 5996/10000 [02:28<01:57, 34.16it/s]Training CobwebTree:  60%|    | 6000/10000 [02:28<01:54, 35.02it/s]Training CobwebTree:  60%|    | 6004/10000 [02:28<01:54, 35.05it/s]Training CobwebTree:  60%|    | 6008/10000 [02:28<01:55, 34.64it/s]Training CobwebTree:  60%|    | 6013/10000 [02:28<01:50, 36.22it/s]Training CobwebTree:  60%|    | 6018/10000 [02:29<01:46, 37.47it/s]Training CobwebTree:  60%|    | 6022/10000 [02:29<01:49, 36.36it/s]Training CobwebTree:  60%|    | 6027/10000 [02:29<01:45, 37.77it/s]Training CobwebTree:  60%|    | 6031/10000 [02:29<01:45, 37.77it/s]Training CobwebTree:  60%|    | 6035/10000 [02:29<01:45, 37.68it/s]Training CobwebTree:  60%|    | 6039/10000 [02:29<01:44, 37.76it/s]Training CobwebTree:  60%|    | 6043/10000 [02:29<01:48, 36.47it/s]Training CobwebTree:  60%|    | 6048/10000 [02:29<01:43, 38.00it/s]Training CobwebTree:  61%|    | 6052/10000 [02:29<01:49, 36.06it/s]Training CobwebTree:  61%|    | 6056/10000 [02:30<01:48, 36.21it/s]Training CobwebTree:  61%|    | 6060/10000 [02:30<01:49, 35.84it/s]Training CobwebTree:  61%|    | 6064/10000 [02:30<01:53, 34.83it/s]Training CobwebTree:  61%|    | 6068/10000 [02:30<01:50, 35.60it/s]Training CobwebTree:  61%|    | 6073/10000 [02:30<01:43, 37.77it/s]Training CobwebTree:  61%|    | 6077/10000 [02:30<01:49, 35.75it/s]Training CobwebTree:  61%|    | 6081/10000 [02:30<01:48, 36.03it/s]Training CobwebTree:  61%|    | 6085/10000 [02:30<01:47, 36.31it/s]Training CobwebTree:  61%|    | 6089/10000 [02:31<01:56, 33.54it/s]Training CobwebTree:  61%|    | 6093/10000 [02:31<01:50, 35.21it/s]Training CobwebTree:  61%|    | 6097/10000 [02:31<01:50, 35.17it/s]Training CobwebTree:  61%|    | 6101/10000 [02:31<01:48, 35.88it/s]Training CobwebTree:  61%|    | 6106/10000 [02:31<01:45, 37.03it/s]Training CobwebTree:  61%|    | 6110/10000 [02:31<01:55, 33.57it/s]Training CobwebTree:  61%|    | 6114/10000 [02:31<01:52, 34.53it/s]Training CobwebTree:  61%|    | 6118/10000 [02:31<01:52, 34.43it/s]Training CobwebTree:  61%|    | 6122/10000 [02:31<01:50, 35.11it/s]Training CobwebTree:  61%|   | 6126/10000 [02:32<01:52, 34.42it/s]Training CobwebTree:  61%|   | 6131/10000 [02:32<01:44, 37.01it/s]Training CobwebTree:  61%|   | 6135/10000 [02:32<01:44, 36.99it/s]Training CobwebTree:  61%|   | 6139/10000 [02:32<01:44, 37.03it/s]Training CobwebTree:  61%|   | 6143/10000 [02:32<01:46, 36.37it/s]Training CobwebTree:  61%|   | 6147/10000 [02:32<01:46, 36.12it/s]Training CobwebTree:  62%|   | 6151/10000 [02:32<01:44, 36.95it/s]Training CobwebTree:  62%|   | 6155/10000 [02:32<01:44, 36.87it/s]Training CobwebTree:  62%|   | 6159/10000 [02:32<01:46, 36.13it/s]Training CobwebTree:  62%|   | 6163/10000 [02:33<01:46, 36.16it/s]Training CobwebTree:  62%|   | 6167/10000 [02:33<01:48, 35.42it/s]Training CobwebTree:  62%|   | 6171/10000 [02:33<01:46, 36.11it/s]Training CobwebTree:  62%|   | 6175/10000 [02:33<01:48, 35.20it/s]Training CobwebTree:  62%|   | 6179/10000 [02:33<01:47, 35.67it/s]Training CobwebTree:  62%|   | 6183/10000 [02:33<01:50, 34.52it/s]Training CobwebTree:  62%|   | 6188/10000 [02:33<01:45, 36.23it/s]Training CobwebTree:  62%|   | 6192/10000 [02:33<01:43, 36.93it/s]Training CobwebTree:  62%|   | 6196/10000 [02:34<01:42, 37.08it/s]Training CobwebTree:  62%|   | 6200/10000 [02:34<01:44, 36.47it/s]Training CobwebTree:  62%|   | 6204/10000 [02:34<01:43, 36.82it/s]Training CobwebTree:  62%|   | 6208/10000 [02:34<01:45, 35.99it/s]Training CobwebTree:  62%|   | 6212/10000 [02:34<01:48, 34.98it/s]Training CobwebTree:  62%|   | 6216/10000 [02:34<01:45, 36.02it/s]Training CobwebTree:  62%|   | 6220/10000 [02:34<01:48, 34.86it/s]Training CobwebTree:  62%|   | 6224/10000 [02:34<01:48, 34.78it/s]Training CobwebTree:  62%|   | 6228/10000 [02:34<01:49, 34.38it/s]Training CobwebTree:  62%|   | 6232/10000 [02:35<01:48, 34.65it/s]Training CobwebTree:  62%|   | 6236/10000 [02:35<01:49, 34.35it/s]Training CobwebTree:  62%|   | 6240/10000 [02:35<01:47, 34.98it/s]Training CobwebTree:  62%|   | 6244/10000 [02:35<01:47, 34.87it/s]Training CobwebTree:  62%|   | 6248/10000 [02:35<01:46, 35.08it/s]Training CobwebTree:  63%|   | 6252/10000 [02:35<01:44, 35.95it/s]Training CobwebTree:  63%|   | 6257/10000 [02:35<01:43, 36.02it/s]Training CobwebTree:  63%|   | 6261/10000 [02:35<01:46, 35.01it/s]Training CobwebTree:  63%|   | 6265/10000 [02:35<01:47, 34.65it/s]Training CobwebTree:  63%|   | 6269/10000 [02:36<01:47, 34.73it/s]Training CobwebTree:  63%|   | 6273/10000 [02:36<01:46, 35.02it/s]Training CobwebTree:  63%|   | 6277/10000 [02:36<01:43, 36.06it/s]Training CobwebTree:  63%|   | 6281/10000 [02:36<01:41, 36.74it/s]Training CobwebTree:  63%|   | 6285/10000 [02:36<01:44, 35.72it/s]Training CobwebTree:  63%|   | 6289/10000 [02:36<01:42, 36.28it/s]Training CobwebTree:  63%|   | 6293/10000 [02:36<01:40, 37.04it/s]Training CobwebTree:  63%|   | 6297/10000 [02:36<01:40, 36.86it/s]Training CobwebTree:  63%|   | 6301/10000 [02:36<01:40, 36.86it/s]Training CobwebTree:  63%|   | 6305/10000 [02:37<01:40, 36.94it/s]Training CobwebTree:  63%|   | 6309/10000 [02:37<01:40, 36.82it/s]Training CobwebTree:  63%|   | 6313/10000 [02:37<01:38, 37.49it/s]Training CobwebTree:  63%|   | 6318/10000 [02:37<01:35, 38.67it/s]Training CobwebTree:  63%|   | 6322/10000 [02:37<01:34, 38.80it/s]Training CobwebTree:  63%|   | 6326/10000 [02:37<01:36, 38.20it/s]Training CobwebTree:  63%|   | 6331/10000 [02:37<01:33, 39.19it/s]Training CobwebTree:  63%|   | 6335/10000 [02:37<01:38, 37.10it/s]Training CobwebTree:  63%|   | 6339/10000 [02:37<01:38, 37.24it/s]Training CobwebTree:  63%|   | 6343/10000 [02:38<01:40, 36.37it/s]Training CobwebTree:  63%|   | 6347/10000 [02:38<01:41, 36.03it/s]Training CobwebTree:  64%|   | 6351/10000 [02:38<01:43, 35.28it/s]Training CobwebTree:  64%|   | 6355/10000 [02:38<01:44, 34.73it/s]Training CobwebTree:  64%|   | 6359/10000 [02:38<01:42, 35.66it/s]Training CobwebTree:  64%|   | 6363/10000 [02:38<01:39, 36.42it/s]Training CobwebTree:  64%|   | 6367/10000 [02:38<01:42, 35.36it/s]Training CobwebTree:  64%|   | 6371/10000 [02:38<01:44, 34.65it/s]Training CobwebTree:  64%|   | 6375/10000 [02:39<01:46, 33.96it/s]Training CobwebTree:  64%|   | 6379/10000 [02:39<01:44, 34.63it/s]Training CobwebTree:  64%|   | 6383/10000 [02:39<01:44, 34.63it/s]Training CobwebTree:  64%|   | 6387/10000 [02:39<01:43, 34.93it/s]Training CobwebTree:  64%|   | 6391/10000 [02:39<01:41, 35.58it/s]Training CobwebTree:  64%|   | 6395/10000 [02:39<01:39, 36.32it/s]Training CobwebTree:  64%|   | 6399/10000 [02:39<01:37, 36.95it/s]Training CobwebTree:  64%|   | 6403/10000 [02:39<01:37, 36.93it/s]Training CobwebTree:  64%|   | 6407/10000 [02:39<01:38, 36.54it/s]Training CobwebTree:  64%|   | 6411/10000 [02:39<01:38, 36.39it/s]Training CobwebTree:  64%|   | 6415/10000 [02:40<01:40, 35.83it/s]Training CobwebTree:  64%|   | 6419/10000 [02:40<01:41, 35.22it/s]Training CobwebTree:  64%|   | 6423/10000 [02:40<01:42, 34.81it/s]Training CobwebTree:  64%|   | 6427/10000 [02:40<01:41, 35.37it/s]Training CobwebTree:  64%|   | 6431/10000 [02:40<01:42, 34.68it/s]Training CobwebTree:  64%|   | 6435/10000 [02:40<01:50, 32.35it/s]Training CobwebTree:  64%|   | 6439/10000 [02:40<01:48, 32.86it/s]Training CobwebTree:  64%|   | 6443/10000 [02:40<01:43, 34.23it/s]Training CobwebTree:  64%|   | 6447/10000 [02:41<01:40, 35.49it/s]Training CobwebTree:  65%|   | 6451/10000 [02:41<01:40, 35.30it/s]Training CobwebTree:  65%|   | 6455/10000 [02:41<01:46, 33.35it/s]Training CobwebTree:  65%|   | 6460/10000 [02:41<01:39, 35.51it/s]Training CobwebTree:  65%|   | 6464/10000 [02:41<01:41, 34.99it/s]Training CobwebTree:  65%|   | 6468/10000 [02:41<01:41, 34.84it/s]Training CobwebTree:  65%|   | 6472/10000 [02:41<01:41, 34.91it/s]Training CobwebTree:  65%|   | 6476/10000 [02:41<01:42, 34.45it/s]Training CobwebTree:  65%|   | 6480/10000 [02:41<01:41, 34.68it/s]Training CobwebTree:  65%|   | 6484/10000 [02:42<01:47, 32.61it/s]Training CobwebTree:  65%|   | 6488/10000 [02:42<01:43, 33.93it/s]Training CobwebTree:  65%|   | 6492/10000 [02:42<01:42, 34.35it/s]Training CobwebTree:  65%|   | 6496/10000 [02:42<01:49, 31.94it/s]Training CobwebTree:  65%|   | 6500/10000 [02:42<01:48, 32.29it/s]Training CobwebTree:  65%|   | 6504/10000 [02:42<01:45, 33.06it/s]Training CobwebTree:  65%|   | 6509/10000 [02:42<01:37, 35.93it/s]Training CobwebTree:  65%|   | 6513/10000 [02:42<01:36, 36.04it/s]Training CobwebTree:  65%|   | 6518/10000 [02:43<01:32, 37.75it/s]Training CobwebTree:  65%|   | 6522/10000 [02:43<01:31, 38.08it/s]Training CobwebTree:  65%|   | 6526/10000 [02:43<01:31, 38.07it/s]Training CobwebTree:  65%|   | 6530/10000 [02:43<01:32, 37.69it/s]Training CobwebTree:  65%|   | 6534/10000 [02:43<01:32, 37.38it/s]Training CobwebTree:  65%|   | 6538/10000 [02:43<01:31, 37.97it/s]Training CobwebTree:  65%|   | 6542/10000 [02:43<01:32, 37.49it/s]Training CobwebTree:  65%|   | 6546/10000 [02:43<01:37, 35.55it/s]Training CobwebTree:  66%|   | 6550/10000 [02:43<01:38, 34.97it/s]Training CobwebTree:  66%|   | 6554/10000 [02:44<01:38, 35.04it/s]Training CobwebTree:  66%|   | 6558/10000 [02:44<01:37, 35.29it/s]Training CobwebTree:  66%|   | 6562/10000 [02:44<01:40, 34.08it/s]Training CobwebTree:  66%|   | 6566/10000 [02:44<01:37, 35.40it/s]Training CobwebTree:  66%|   | 6570/10000 [02:44<01:39, 34.57it/s]Training CobwebTree:  66%|   | 6574/10000 [02:44<01:39, 34.42it/s]Training CobwebTree:  66%|   | 6578/10000 [02:44<01:42, 33.54it/s]Training CobwebTree:  66%|   | 6582/10000 [02:44<01:40, 34.13it/s]Training CobwebTree:  66%|   | 6586/10000 [02:45<01:39, 34.41it/s]Training CobwebTree:  66%|   | 6590/10000 [02:45<01:36, 35.34it/s]Training CobwebTree:  66%|   | 6595/10000 [02:45<01:28, 38.30it/s]Training CobwebTree:  66%|   | 6599/10000 [02:45<01:29, 37.94it/s]Training CobwebTree:  66%|   | 6603/10000 [02:45<01:30, 37.53it/s]Training CobwebTree:  66%|   | 6607/10000 [02:45<01:33, 36.15it/s]Training CobwebTree:  66%|   | 6611/10000 [02:45<01:31, 37.13it/s]Training CobwebTree:  66%|   | 6615/10000 [02:45<01:35, 35.55it/s]Training CobwebTree:  66%|   | 6620/10000 [02:45<01:31, 37.14it/s]Training CobwebTree:  66%|   | 6624/10000 [02:46<01:30, 37.26it/s]Training CobwebTree:  66%|   | 6628/10000 [02:46<01:28, 37.93it/s]Training CobwebTree:  66%|   | 6632/10000 [02:46<01:33, 36.08it/s]Training CobwebTree:  66%|   | 6636/10000 [02:46<01:34, 35.57it/s]Training CobwebTree:  66%|   | 6640/10000 [02:46<01:35, 35.30it/s]Training CobwebTree:  66%|   | 6644/10000 [02:46<01:35, 35.25it/s]Training CobwebTree:  66%|   | 6648/10000 [02:46<01:37, 34.25it/s]Training CobwebTree:  67%|   | 6652/10000 [02:46<01:34, 35.58it/s]Training CobwebTree:  67%|   | 6656/10000 [02:46<01:32, 36.01it/s]Training CobwebTree:  67%|   | 6660/10000 [02:47<01:32, 36.16it/s]Training CobwebTree:  67%|   | 6664/10000 [02:47<01:30, 36.95it/s]Training CobwebTree:  67%|   | 6669/10000 [02:47<01:26, 38.62it/s]Training CobwebTree:  67%|   | 6673/10000 [02:47<01:29, 37.27it/s]Training CobwebTree:  67%|   | 6677/10000 [02:47<01:30, 36.56it/s]Training CobwebTree:  67%|   | 6681/10000 [02:47<01:31, 36.08it/s]Training CobwebTree:  67%|   | 6685/10000 [02:47<01:32, 35.81it/s]Training CobwebTree:  67%|   | 6689/10000 [02:47<01:34, 34.95it/s]Training CobwebTree:  67%|   | 6693/10000 [02:47<01:33, 35.39it/s]Training CobwebTree:  67%|   | 6697/10000 [02:48<01:33, 35.23it/s]Training CobwebTree:  67%|   | 6701/10000 [02:48<01:40, 32.71it/s]Training CobwebTree:  67%|   | 6705/10000 [02:48<01:35, 34.56it/s]Training CobwebTree:  67%|   | 6709/10000 [02:48<01:35, 34.44it/s]Training CobwebTree:  67%|   | 6713/10000 [02:48<01:37, 33.74it/s]Training CobwebTree:  67%|   | 6717/10000 [02:48<01:38, 33.32it/s]Training CobwebTree:  67%|   | 6721/10000 [02:48<01:35, 34.16it/s]Training CobwebTree:  67%|   | 6725/10000 [02:48<01:35, 34.37it/s]Training CobwebTree:  67%|   | 6729/10000 [02:49<01:34, 34.46it/s]Training CobwebTree:  67%|   | 6733/10000 [02:49<01:32, 35.27it/s]Training CobwebTree:  67%|   | 6737/10000 [02:49<01:32, 35.12it/s]Training CobwebTree:  67%|   | 6741/10000 [02:49<01:32, 35.29it/s]Training CobwebTree:  67%|   | 6745/10000 [02:49<01:37, 33.49it/s]Training CobwebTree:  67%|   | 6749/10000 [02:49<01:35, 34.19it/s]Training CobwebTree:  68%|   | 6753/10000 [02:49<01:38, 33.07it/s]Training CobwebTree:  68%|   | 6757/10000 [02:49<01:35, 33.82it/s]Training CobwebTree:  68%|   | 6761/10000 [02:49<01:34, 34.28it/s]Training CobwebTree:  68%|   | 6766/10000 [02:50<01:28, 36.39it/s]Training CobwebTree:  68%|   | 6770/10000 [02:50<01:29, 35.91it/s]Training CobwebTree:  68%|   | 6774/10000 [02:50<01:33, 34.41it/s]Training CobwebTree:  68%|   | 6778/10000 [02:50<01:33, 34.60it/s]Training CobwebTree:  68%|   | 6782/10000 [02:50<01:31, 35.02it/s]Training CobwebTree:  68%|   | 6786/10000 [02:50<01:32, 34.74it/s]Training CobwebTree:  68%|   | 6790/10000 [02:50<01:30, 35.38it/s]Training CobwebTree:  68%|   | 6794/10000 [02:50<01:30, 35.26it/s]Training CobwebTree:  68%|   | 6798/10000 [02:50<01:27, 36.53it/s]Training CobwebTree:  68%|   | 6802/10000 [02:51<01:28, 35.94it/s]Training CobwebTree:  68%|   | 6806/10000 [02:51<01:31, 34.89it/s]Training CobwebTree:  68%|   | 6810/10000 [02:51<01:31, 34.94it/s]Training CobwebTree:  68%|   | 6814/10000 [02:51<01:31, 34.88it/s]Training CobwebTree:  68%|   | 6818/10000 [02:51<01:35, 33.40it/s]Training CobwebTree:  68%|   | 6822/10000 [02:51<01:31, 34.71it/s]Training CobwebTree:  68%|   | 6827/10000 [02:51<01:26, 36.49it/s]Training CobwebTree:  68%|   | 6831/10000 [02:51<01:33, 33.97it/s]Training CobwebTree:  68%|   | 6835/10000 [02:52<01:32, 34.30it/s]Training CobwebTree:  68%|   | 6839/10000 [02:52<01:32, 34.09it/s]Training CobwebTree:  68%|   | 6843/10000 [02:52<01:30, 35.03it/s]Training CobwebTree:  68%|   | 6847/10000 [02:52<01:31, 34.50it/s]Training CobwebTree:  69%|   | 6851/10000 [02:52<01:29, 35.33it/s]Training CobwebTree:  69%|   | 6855/10000 [02:52<01:32, 34.15it/s]Training CobwebTree:  69%|   | 6859/10000 [02:52<01:32, 34.08it/s]Training CobwebTree:  69%|   | 6863/10000 [02:52<01:30, 34.61it/s]Training CobwebTree:  69%|   | 6867/10000 [02:52<01:31, 34.23it/s]Training CobwebTree:  69%|   | 6871/10000 [02:53<01:29, 34.77it/s]Training CobwebTree:  69%|   | 6875/10000 [02:53<01:36, 32.45it/s]Training CobwebTree:  69%|   | 6879/10000 [02:53<01:35, 32.55it/s]Training CobwebTree:  69%|   | 6883/10000 [02:53<01:32, 33.56it/s]Training CobwebTree:  69%|   | 6887/10000 [02:53<01:35, 32.72it/s]Training CobwebTree:  69%|   | 6891/10000 [02:53<01:34, 33.03it/s]Training CobwebTree:  69%|   | 6895/10000 [02:53<01:34, 33.03it/s]Training CobwebTree:  69%|   | 6899/10000 [02:53<01:34, 32.65it/s]Training CobwebTree:  69%|   | 6903/10000 [02:54<01:30, 34.17it/s]Training CobwebTree:  69%|   | 6907/10000 [02:54<01:27, 35.23it/s]Training CobwebTree:  69%|   | 6911/10000 [02:54<01:32, 33.52it/s]Training CobwebTree:  69%|   | 6915/10000 [02:54<01:30, 34.00it/s]Training CobwebTree:  69%|   | 6919/10000 [02:54<01:32, 33.46it/s]Training CobwebTree:  69%|   | 6923/10000 [02:54<01:31, 33.71it/s]Training CobwebTree:  69%|   | 6927/10000 [02:54<01:33, 32.79it/s]Training CobwebTree:  69%|   | 6931/10000 [02:54<01:30, 33.75it/s]Training CobwebTree:  69%|   | 6935/10000 [02:55<01:27, 34.98it/s]Training CobwebTree:  69%|   | 6939/10000 [02:55<01:26, 35.55it/s]Training CobwebTree:  69%|   | 6943/10000 [02:55<01:27, 34.80it/s]Training CobwebTree:  69%|   | 6947/10000 [02:55<01:28, 34.35it/s]Training CobwebTree:  70%|   | 6951/10000 [02:55<01:27, 34.77it/s]Training CobwebTree:  70%|   | 6955/10000 [02:55<01:25, 35.54it/s]Training CobwebTree:  70%|   | 6959/10000 [02:55<01:23, 36.52it/s]Training CobwebTree:  70%|   | 6963/10000 [02:55<01:23, 36.22it/s]Training CobwebTree:  70%|   | 6967/10000 [02:55<01:22, 36.79it/s]Training CobwebTree:  70%|   | 6971/10000 [02:56<01:23, 36.10it/s]Training CobwebTree:  70%|   | 6975/10000 [02:56<01:22, 36.81it/s]Training CobwebTree:  70%|   | 6979/10000 [02:56<01:21, 36.99it/s]Training CobwebTree:  70%|   | 6983/10000 [02:56<01:21, 37.19it/s]Training CobwebTree:  70%|   | 6987/10000 [02:56<01:22, 36.51it/s]Training CobwebTree:  70%|   | 6992/10000 [02:56<01:19, 37.63it/s]Training CobwebTree:  70%|   | 6996/10000 [02:56<01:20, 37.47it/s]Training CobwebTree:  70%|   | 7000/10000 [02:56<01:24, 35.62it/s]Training CobwebTree:  70%|   | 7004/10000 [02:56<01:26, 34.65it/s]Training CobwebTree:  70%|   | 7008/10000 [02:57<01:27, 34.14it/s]Training CobwebTree:  70%|   | 7012/10000 [02:57<01:28, 33.87it/s]Training CobwebTree:  70%|   | 7016/10000 [02:57<01:26, 34.42it/s]Training CobwebTree:  70%|   | 7020/10000 [02:57<01:25, 35.01it/s]Training CobwebTree:  70%|   | 7024/10000 [02:57<01:25, 34.83it/s]Training CobwebTree:  70%|   | 7028/10000 [02:57<01:24, 35.00it/s]Training CobwebTree:  70%|   | 7032/10000 [02:57<01:25, 34.76it/s]Training CobwebTree:  70%|   | 7036/10000 [02:57<01:24, 35.10it/s]Training CobwebTree:  70%|   | 7041/10000 [02:57<01:20, 36.90it/s]Training CobwebTree:  70%|   | 7045/10000 [02:58<01:19, 37.18it/s]Training CobwebTree:  70%|   | 7049/10000 [02:58<01:19, 37.30it/s]Training CobwebTree:  71%|   | 7053/10000 [02:58<01:18, 37.45it/s]Training CobwebTree:  71%|   | 7058/10000 [02:58<01:14, 39.45it/s]Training CobwebTree:  71%|   | 7062/10000 [02:58<01:19, 36.83it/s]Training CobwebTree:  71%|   | 7066/10000 [02:58<01:23, 35.32it/s]Training CobwebTree:  71%|   | 7070/10000 [02:58<01:20, 36.26it/s]Training CobwebTree:  71%|   | 7074/10000 [02:58<01:20, 36.46it/s]Training CobwebTree:  71%|   | 7078/10000 [02:58<01:22, 35.27it/s]Training CobwebTree:  71%|   | 7082/10000 [02:59<01:21, 35.86it/s]Training CobwebTree:  71%|   | 7086/10000 [02:59<01:24, 34.35it/s]Training CobwebTree:  71%|   | 7090/10000 [02:59<01:24, 34.55it/s]Training CobwebTree:  71%|   | 7094/10000 [02:59<01:25, 34.07it/s]Training CobwebTree:  71%|   | 7098/10000 [02:59<01:24, 34.29it/s]Training CobwebTree:  71%|   | 7102/10000 [02:59<01:20, 35.82it/s]Training CobwebTree:  71%|   | 7106/10000 [02:59<01:18, 36.86it/s]Training CobwebTree:  71%|   | 7110/10000 [02:59<01:17, 37.15it/s]Training CobwebTree:  71%|   | 7114/10000 [02:59<01:16, 37.60it/s]Training CobwebTree:  71%|   | 7118/10000 [03:00<01:20, 35.72it/s]Training CobwebTree:  71%|   | 7122/10000 [03:00<01:22, 34.74it/s]Training CobwebTree:  71%|  | 7126/10000 [03:00<01:22, 34.78it/s]Training CobwebTree:  71%|  | 7130/10000 [03:00<01:21, 35.13it/s]Training CobwebTree:  71%|  | 7134/10000 [03:00<01:20, 35.58it/s]Training CobwebTree:  71%|  | 7138/10000 [03:00<01:21, 35.21it/s]Training CobwebTree:  71%|  | 7142/10000 [03:00<01:22, 34.46it/s]Training CobwebTree:  71%|  | 7146/10000 [03:00<01:20, 35.47it/s]Training CobwebTree:  72%|  | 7150/10000 [03:01<01:25, 33.41it/s]Training CobwebTree:  72%|  | 7154/10000 [03:01<01:27, 32.49it/s]Training CobwebTree:  72%|  | 7158/10000 [03:01<01:26, 32.69it/s]Training CobwebTree:  72%|  | 7162/10000 [03:01<01:25, 33.08it/s]Training CobwebTree:  72%|  | 7166/10000 [03:01<01:24, 33.44it/s]Training CobwebTree:  72%|  | 7170/10000 [03:01<01:27, 32.49it/s]Training CobwebTree:  72%|  | 7174/10000 [03:01<01:26, 32.65it/s]Training CobwebTree:  72%|  | 7179/10000 [03:01<01:20, 34.85it/s]Training CobwebTree:  72%|  | 7183/10000 [03:02<01:20, 34.94it/s]Training CobwebTree:  72%|  | 7187/10000 [03:02<01:19, 35.41it/s]Training CobwebTree:  72%|  | 7191/10000 [03:02<01:19, 35.35it/s]Training CobwebTree:  72%|  | 7195/10000 [03:02<01:21, 34.48it/s]Training CobwebTree:  72%|  | 7200/10000 [03:02<01:16, 36.73it/s]Training CobwebTree:  72%|  | 7204/10000 [03:02<01:17, 36.03it/s]Training CobwebTree:  72%|  | 7208/10000 [03:02<01:16, 36.33it/s]Training CobwebTree:  72%|  | 7212/10000 [03:02<01:22, 33.76it/s]Training CobwebTree:  72%|  | 7216/10000 [03:02<01:20, 34.74it/s]Training CobwebTree:  72%|  | 7220/10000 [03:03<01:17, 35.83it/s]Training CobwebTree:  72%|  | 7225/10000 [03:03<01:13, 37.61it/s]Training CobwebTree:  72%|  | 7229/10000 [03:03<01:14, 37.16it/s]Training CobwebTree:  72%|  | 7233/10000 [03:03<01:15, 36.88it/s]Training CobwebTree:  72%|  | 7237/10000 [03:03<01:14, 37.00it/s]Training CobwebTree:  72%|  | 7241/10000 [03:03<01:16, 35.99it/s]Training CobwebTree:  72%|  | 7245/10000 [03:03<01:16, 36.02it/s]Training CobwebTree:  72%|  | 7249/10000 [03:03<01:16, 35.97it/s]Training CobwebTree:  73%|  | 7253/10000 [03:03<01:21, 33.58it/s]Training CobwebTree:  73%|  | 7257/10000 [03:04<01:20, 34.20it/s]Training CobwebTree:  73%|  | 7261/10000 [03:04<01:21, 33.74it/s]Training CobwebTree:  73%|  | 7265/10000 [03:04<01:22, 33.02it/s]Training CobwebTree:  73%|  | 7269/10000 [03:04<01:20, 34.07it/s]Training CobwebTree:  73%|  | 7274/10000 [03:04<01:15, 36.27it/s]Training CobwebTree:  73%|  | 7278/10000 [03:04<01:14, 36.58it/s]Training CobwebTree:  73%|  | 7282/10000 [03:04<01:15, 35.96it/s]Training CobwebTree:  73%|  | 7286/10000 [03:04<01:13, 36.74it/s]Training CobwebTree:  73%|  | 7291/10000 [03:05<01:11, 38.07it/s]Training CobwebTree:  73%|  | 7295/10000 [03:05<01:12, 37.17it/s]Training CobwebTree:  73%|  | 7299/10000 [03:05<01:14, 36.46it/s]Training CobwebTree:  73%|  | 7303/10000 [03:05<01:13, 36.77it/s]Training CobwebTree:  73%|  | 7307/10000 [03:05<01:13, 36.64it/s]Training CobwebTree:  73%|  | 7311/10000 [03:05<01:16, 35.36it/s]Training CobwebTree:  73%|  | 7316/10000 [03:05<01:15, 35.67it/s]Training CobwebTree:  73%|  | 7320/10000 [03:05<01:14, 35.77it/s]Training CobwebTree:  73%|  | 7324/10000 [03:05<01:16, 34.88it/s]Training CobwebTree:  73%|  | 7328/10000 [03:06<01:14, 35.63it/s]Training CobwebTree:  73%|  | 7332/10000 [03:06<01:13, 36.26it/s]Training CobwebTree:  73%|  | 7336/10000 [03:06<01:12, 36.76it/s]Training CobwebTree:  73%|  | 7341/10000 [03:06<01:10, 37.75it/s]Training CobwebTree:  73%|  | 7345/10000 [03:06<01:10, 37.68it/s]Training CobwebTree:  73%|  | 7349/10000 [03:06<01:18, 33.95it/s]Training CobwebTree:  74%|  | 7353/10000 [03:06<01:17, 33.97it/s]Training CobwebTree:  74%|  | 7358/10000 [03:06<01:14, 35.65it/s]Training CobwebTree:  74%|  | 7362/10000 [03:07<01:12, 36.63it/s]Training CobwebTree:  74%|  | 7366/10000 [03:07<01:11, 37.00it/s]Training CobwebTree:  74%|  | 7370/10000 [03:07<01:11, 36.92it/s]Training CobwebTree:  74%|  | 7374/10000 [03:07<01:10, 37.21it/s]Training CobwebTree:  74%|  | 7378/10000 [03:07<01:13, 35.44it/s]Training CobwebTree:  74%|  | 7382/10000 [03:07<01:18, 33.37it/s]Training CobwebTree:  74%|  | 7386/10000 [03:07<01:16, 34.13it/s]Training CobwebTree:  74%|  | 7390/10000 [03:07<01:15, 34.69it/s]Training CobwebTree:  74%|  | 7394/10000 [03:07<01:12, 36.00it/s]Training CobwebTree:  74%|  | 7398/10000 [03:08<01:14, 35.10it/s]Training CobwebTree:  74%|  | 7402/10000 [03:08<01:12, 35.77it/s]Training CobwebTree:  74%|  | 7406/10000 [03:08<01:15, 34.38it/s]Training CobwebTree:  74%|  | 7410/10000 [03:08<01:17, 33.51it/s]Training CobwebTree:  74%|  | 7414/10000 [03:08<01:16, 33.73it/s]Training CobwebTree:  74%|  | 7418/10000 [03:08<01:16, 33.78it/s]Training CobwebTree:  74%|  | 7422/10000 [03:08<01:17, 33.36it/s]Training CobwebTree:  74%|  | 7426/10000 [03:08<01:13, 34.91it/s]Training CobwebTree:  74%|  | 7430/10000 [03:08<01:14, 34.46it/s]Training CobwebTree:  74%|  | 7434/10000 [03:09<01:12, 35.45it/s]Training CobwebTree:  74%|  | 7438/10000 [03:09<01:10, 36.27it/s]Training CobwebTree:  74%|  | 7443/10000 [03:09<01:10, 36.44it/s]Training CobwebTree:  74%|  | 7447/10000 [03:09<01:09, 36.76it/s]Training CobwebTree:  75%|  | 7451/10000 [03:09<01:09, 36.91it/s]Training CobwebTree:  75%|  | 7455/10000 [03:09<01:11, 35.50it/s]Training CobwebTree:  75%|  | 7459/10000 [03:09<01:14, 34.06it/s]Training CobwebTree:  75%|  | 7463/10000 [03:09<01:13, 34.39it/s]Training CobwebTree:  75%|  | 7467/10000 [03:10<01:12, 35.14it/s]Training CobwebTree:  75%|  | 7471/10000 [03:10<01:14, 33.96it/s]Training CobwebTree:  75%|  | 7475/10000 [03:10<01:13, 34.31it/s]Training CobwebTree:  75%|  | 7479/10000 [03:10<01:12, 34.76it/s]Training CobwebTree:  75%|  | 7483/10000 [03:10<01:13, 34.29it/s]Training CobwebTree:  75%|  | 7487/10000 [03:10<01:11, 34.93it/s]Training CobwebTree:  75%|  | 7492/10000 [03:10<01:07, 36.89it/s]Training CobwebTree:  75%|  | 7497/10000 [03:10<01:06, 37.67it/s]Training CobwebTree:  75%|  | 7501/10000 [03:10<01:10, 35.59it/s]Training CobwebTree:  75%|  | 7505/10000 [03:11<01:13, 33.81it/s]Training CobwebTree:  75%|  | 7509/10000 [03:11<01:11, 34.91it/s]Training CobwebTree:  75%|  | 7513/10000 [03:11<01:14, 33.49it/s]Training CobwebTree:  75%|  | 7517/10000 [03:11<01:14, 33.31it/s]Training CobwebTree:  75%|  | 7521/10000 [03:11<01:13, 33.52it/s]Training CobwebTree:  75%|  | 7525/10000 [03:11<01:16, 32.37it/s]Training CobwebTree:  75%|  | 7529/10000 [03:11<01:16, 32.39it/s]Training CobwebTree:  75%|  | 7533/10000 [03:11<01:13, 33.71it/s]Training CobwebTree:  75%|  | 7537/10000 [03:12<01:13, 33.55it/s]Training CobwebTree:  75%|  | 7541/10000 [03:12<01:11, 34.45it/s]Training CobwebTree:  75%|  | 7545/10000 [03:12<01:12, 33.85it/s]Training CobwebTree:  75%|  | 7549/10000 [03:12<01:11, 34.43it/s]Training CobwebTree:  76%|  | 7553/10000 [03:12<01:08, 35.70it/s]Training CobwebTree:  76%|  | 7557/10000 [03:12<01:07, 36.21it/s]Training CobwebTree:  76%|  | 7561/10000 [03:12<01:07, 36.05it/s]Training CobwebTree:  76%|  | 7565/10000 [03:12<01:07, 35.99it/s]Training CobwebTree:  76%|  | 7569/10000 [03:12<01:07, 35.88it/s]Training CobwebTree:  76%|  | 7573/10000 [03:13<01:08, 35.57it/s]Training CobwebTree:  76%|  | 7578/10000 [03:13<01:04, 37.38it/s]Training CobwebTree:  76%|  | 7582/10000 [03:13<01:04, 37.68it/s]Training CobwebTree:  76%|  | 7586/10000 [03:13<01:04, 37.39it/s]Training CobwebTree:  76%|  | 7590/10000 [03:13<01:06, 36.34it/s]Training CobwebTree:  76%|  | 7594/10000 [03:13<01:07, 35.52it/s]Training CobwebTree:  76%|  | 7599/10000 [03:13<01:04, 37.44it/s]Training CobwebTree:  76%|  | 7603/10000 [03:13<01:04, 36.94it/s]Training CobwebTree:  76%|  | 7607/10000 [03:13<01:03, 37.45it/s]Training CobwebTree:  76%|  | 7611/10000 [03:14<01:04, 36.78it/s]Training CobwebTree:  76%|  | 7615/10000 [03:14<01:05, 36.62it/s]Training CobwebTree:  76%|  | 7619/10000 [03:14<01:06, 35.76it/s]Training CobwebTree:  76%|  | 7623/10000 [03:14<01:08, 34.79it/s]Training CobwebTree:  76%|  | 7627/10000 [03:14<01:08, 34.86it/s]Training CobwebTree:  76%|  | 7631/10000 [03:14<01:08, 34.78it/s]Training CobwebTree:  76%|  | 7635/10000 [03:14<01:06, 35.49it/s]Training CobwebTree:  76%|  | 7639/10000 [03:14<01:08, 34.37it/s]Training CobwebTree:  76%|  | 7643/10000 [03:14<01:06, 35.58it/s]Training CobwebTree:  76%|  | 7647/10000 [03:15<01:05, 36.16it/s]Training CobwebTree:  77%|  | 7651/10000 [03:15<01:06, 35.11it/s]Training CobwebTree:  77%|  | 7655/10000 [03:15<01:05, 35.85it/s]Training CobwebTree:  77%|  | 7659/10000 [03:15<01:06, 35.44it/s]Training CobwebTree:  77%|  | 7663/10000 [03:15<01:05, 35.46it/s]Training CobwebTree:  77%|  | 7667/10000 [03:15<01:04, 35.98it/s]Training CobwebTree:  77%|  | 7671/10000 [03:15<01:07, 34.68it/s]Training CobwebTree:  77%|  | 7675/10000 [03:15<01:06, 35.00it/s]Training CobwebTree:  77%|  | 7679/10000 [03:16<01:08, 33.77it/s]Training CobwebTree:  77%|  | 7683/10000 [03:16<01:07, 34.43it/s]Training CobwebTree:  77%|  | 7687/10000 [03:16<01:05, 35.10it/s]Training CobwebTree:  77%|  | 7691/10000 [03:16<01:05, 35.33it/s]Training CobwebTree:  77%|  | 7695/10000 [03:16<01:06, 34.70it/s]Training CobwebTree:  77%|  | 7699/10000 [03:16<01:07, 33.85it/s]Training CobwebTree:  77%|  | 7703/10000 [03:16<01:05, 34.91it/s]Training CobwebTree:  77%|  | 7708/10000 [03:16<01:02, 36.38it/s]Training CobwebTree:  77%|  | 7712/10000 [03:16<01:02, 36.51it/s]Training CobwebTree:  77%|  | 7716/10000 [03:17<01:01, 37.28it/s]Training CobwebTree:  77%|  | 7720/10000 [03:17<01:01, 37.36it/s]Training CobwebTree:  77%|  | 7724/10000 [03:17<01:00, 37.64it/s]Training CobwebTree:  77%|  | 7728/10000 [03:17<01:00, 37.64it/s]Training CobwebTree:  77%|  | 7732/10000 [03:17<01:01, 36.77it/s]Training CobwebTree:  77%|  | 7736/10000 [03:17<01:01, 36.63it/s]Training CobwebTree:  77%|  | 7740/10000 [03:17<01:04, 34.91it/s]Training CobwebTree:  77%|  | 7744/10000 [03:17<01:04, 35.24it/s]Training CobwebTree:  77%|  | 7748/10000 [03:17<01:05, 34.46it/s]Training CobwebTree:  78%|  | 7752/10000 [03:18<01:06, 33.77it/s]Training CobwebTree:  78%|  | 7756/10000 [03:18<01:04, 34.62it/s]Training CobwebTree:  78%|  | 7760/10000 [03:18<01:03, 35.53it/s]Training CobwebTree:  78%|  | 7764/10000 [03:18<01:02, 36.04it/s]Training CobwebTree:  78%|  | 7768/10000 [03:18<01:03, 35.10it/s]Training CobwebTree:  78%|  | 7772/10000 [03:18<01:07, 32.87it/s]Training CobwebTree:  78%|  | 7776/10000 [03:18<01:07, 33.15it/s]Training CobwebTree:  78%|  | 7780/10000 [03:18<01:05, 33.74it/s]Training CobwebTree:  78%|  | 7784/10000 [03:19<01:05, 33.72it/s]Training CobwebTree:  78%|  | 7789/10000 [03:19<01:03, 34.97it/s]Training CobwebTree:  78%|  | 7793/10000 [03:19<01:03, 34.57it/s]Training CobwebTree:  78%|  | 7797/10000 [03:19<01:05, 33.72it/s]Training CobwebTree:  78%|  | 7801/10000 [03:19<01:04, 33.88it/s]Training CobwebTree:  78%|  | 7805/10000 [03:19<01:04, 33.77it/s]Training CobwebTree:  78%|  | 7809/10000 [03:19<01:05, 33.26it/s]Training CobwebTree:  78%|  | 7813/10000 [03:19<01:05, 33.30it/s]Training CobwebTree:  78%|  | 7817/10000 [03:19<01:05, 33.33it/s]Training CobwebTree:  78%|  | 7821/10000 [03:20<01:06, 32.79it/s]Training CobwebTree:  78%|  | 7825/10000 [03:20<01:08, 31.86it/s]Training CobwebTree:  78%|  | 7829/10000 [03:20<01:06, 32.84it/s]Training CobwebTree:  78%|  | 7833/10000 [03:20<01:04, 33.35it/s]Training CobwebTree:  78%|  | 7837/10000 [03:20<01:04, 33.73it/s]Training CobwebTree:  78%|  | 7841/10000 [03:20<01:06, 32.45it/s]Training CobwebTree:  78%|  | 7845/10000 [03:20<01:07, 32.16it/s]Training CobwebTree:  78%|  | 7849/10000 [03:20<01:06, 32.57it/s]Training CobwebTree:  79%|  | 7853/10000 [03:21<01:06, 32.33it/s]Training CobwebTree:  79%|  | 7857/10000 [03:21<01:05, 32.54it/s]Training CobwebTree:  79%|  | 7861/10000 [03:21<01:04, 33.09it/s]Training CobwebTree:  79%|  | 7865/10000 [03:21<01:02, 34.16it/s]Training CobwebTree:  79%|  | 7869/10000 [03:21<01:01, 34.72it/s]Training CobwebTree:  79%|  | 7873/10000 [03:21<01:02, 34.23it/s]Training CobwebTree:  79%|  | 7877/10000 [03:21<01:02, 34.22it/s]Training CobwebTree:  79%|  | 7881/10000 [03:21<01:02, 34.03it/s]Training CobwebTree:  79%|  | 7885/10000 [03:22<01:02, 33.89it/s]Training CobwebTree:  79%|  | 7889/10000 [03:22<01:02, 33.68it/s]Training CobwebTree:  79%|  | 7893/10000 [03:22<01:02, 33.70it/s]Training CobwebTree:  79%|  | 7897/10000 [03:22<00:59, 35.32it/s]Training CobwebTree:  79%|  | 7901/10000 [03:22<00:58, 35.78it/s]Training CobwebTree:  79%|  | 7905/10000 [03:22<00:58, 35.90it/s]Training CobwebTree:  79%|  | 7909/10000 [03:22<00:58, 35.91it/s]Training CobwebTree:  79%|  | 7913/10000 [03:22<00:57, 36.29it/s]Training CobwebTree:  79%|  | 7917/10000 [03:22<00:57, 36.35it/s]Training CobwebTree:  79%|  | 7921/10000 [03:23<01:00, 34.60it/s]Training CobwebTree:  79%|  | 7925/10000 [03:23<00:58, 35.47it/s]Training CobwebTree:  79%|  | 7929/10000 [03:23<00:58, 35.61it/s]Training CobwebTree:  79%|  | 7933/10000 [03:23<00:58, 35.59it/s]Training CobwebTree:  79%|  | 7937/10000 [03:23<00:59, 34.70it/s]Training CobwebTree:  79%|  | 7941/10000 [03:23<01:00, 34.28it/s]Training CobwebTree:  79%|  | 7945/10000 [03:23<00:59, 34.80it/s]Training CobwebTree:  79%|  | 7949/10000 [03:23<00:58, 35.05it/s]Training CobwebTree:  80%|  | 7953/10000 [03:23<00:58, 35.04it/s]Training CobwebTree:  80%|  | 7957/10000 [03:24<00:58, 34.90it/s]Training CobwebTree:  80%|  | 7961/10000 [03:24<00:59, 34.11it/s]Training CobwebTree:  80%|  | 7965/10000 [03:24<01:02, 32.69it/s]Training CobwebTree:  80%|  | 7969/10000 [03:24<00:59, 33.86it/s]Training CobwebTree:  80%|  | 7973/10000 [03:24<01:00, 33.45it/s]Training CobwebTree:  80%|  | 7977/10000 [03:24<00:58, 34.53it/s]Training CobwebTree:  80%|  | 7981/10000 [03:24<00:57, 34.97it/s]Training CobwebTree:  80%|  | 7985/10000 [03:24<00:58, 34.17it/s]Training CobwebTree:  80%|  | 7989/10000 [03:25<00:59, 33.95it/s]Training CobwebTree:  80%|  | 7993/10000 [03:25<00:58, 34.32it/s]Training CobwebTree:  80%|  | 7997/10000 [03:25<00:58, 34.06it/s]Training CobwebTree:  80%|  | 8002/10000 [03:25<00:56, 35.67it/s]Training CobwebTree:  80%|  | 8006/10000 [03:25<00:57, 34.68it/s]Training CobwebTree:  80%|  | 8010/10000 [03:25<00:55, 35.65it/s]Training CobwebTree:  80%|  | 8014/10000 [03:25<00:57, 34.25it/s]Training CobwebTree:  80%|  | 8018/10000 [03:25<00:58, 33.79it/s]Training CobwebTree:  80%|  | 8022/10000 [03:25<00:59, 33.39it/s]Training CobwebTree:  80%|  | 8026/10000 [03:26<00:59, 33.27it/s]Training CobwebTree:  80%|  | 8030/10000 [03:26<00:58, 33.55it/s]Training CobwebTree:  80%|  | 8034/10000 [03:26<00:58, 33.43it/s]Training CobwebTree:  80%|  | 8038/10000 [03:26<00:56, 34.68it/s]Training CobwebTree:  80%|  | 8042/10000 [03:26<00:57, 34.23it/s]Training CobwebTree:  80%|  | 8046/10000 [03:26<00:55, 35.11it/s]Training CobwebTree:  80%|  | 8050/10000 [03:26<00:54, 36.01it/s]Training CobwebTree:  81%|  | 8054/10000 [03:26<00:56, 34.70it/s]Training CobwebTree:  81%|  | 8059/10000 [03:27<00:53, 36.34it/s]Training CobwebTree:  81%|  | 8063/10000 [03:27<00:52, 36.62it/s]Training CobwebTree:  81%|  | 8067/10000 [03:27<00:56, 34.41it/s]Training CobwebTree:  81%|  | 8071/10000 [03:27<00:55, 34.89it/s]Training CobwebTree:  81%|  | 8075/10000 [03:27<00:55, 34.96it/s]Training CobwebTree:  81%|  | 8079/10000 [03:27<00:54, 35.16it/s]Training CobwebTree:  81%|  | 8083/10000 [03:27<00:54, 34.99it/s]Training CobwebTree:  81%|  | 8087/10000 [03:27<00:55, 34.64it/s]Training CobwebTree:  81%|  | 8091/10000 [03:27<00:55, 34.43it/s]Training CobwebTree:  81%|  | 8095/10000 [03:28<00:55, 34.54it/s]Training CobwebTree:  81%|  | 8099/10000 [03:28<00:54, 34.95it/s]Training CobwebTree:  81%|  | 8103/10000 [03:28<00:52, 35.99it/s]Training CobwebTree:  81%|  | 8107/10000 [03:28<00:51, 36.50it/s]Training CobwebTree:  81%|  | 8111/10000 [03:28<00:52, 35.90it/s]Training CobwebTree:  81%|  | 8115/10000 [03:28<00:52, 35.94it/s]Training CobwebTree:  81%|  | 8119/10000 [03:28<00:53, 34.87it/s]Training CobwebTree:  81%|  | 8123/10000 [03:28<00:52, 35.74it/s]Training CobwebTree:  81%| | 8127/10000 [03:28<00:53, 34.87it/s]Training CobwebTree:  81%| | 8131/10000 [03:29<00:53, 34.81it/s]Training CobwebTree:  81%| | 8136/10000 [03:29<00:51, 36.23it/s]Training CobwebTree:  81%| | 8140/10000 [03:29<00:54, 34.19it/s]Training CobwebTree:  81%| | 8144/10000 [03:29<00:55, 33.29it/s]Training CobwebTree:  81%| | 8148/10000 [03:29<00:54, 33.90it/s]Training CobwebTree:  82%| | 8152/10000 [03:29<00:52, 35.16it/s]Training CobwebTree:  82%| | 8156/10000 [03:29<00:52, 34.95it/s]Training CobwebTree:  82%| | 8160/10000 [03:29<00:54, 33.53it/s]Training CobwebTree:  82%| | 8164/10000 [03:30<00:53, 34.52it/s]Training CobwebTree:  82%| | 8168/10000 [03:30<00:51, 35.29it/s]Training CobwebTree:  82%| | 8172/10000 [03:30<00:53, 34.47it/s]Training CobwebTree:  82%| | 8176/10000 [03:30<00:52, 34.56it/s]Training CobwebTree:  82%| | 8180/10000 [03:30<00:52, 34.71it/s]Training CobwebTree:  82%| | 8184/10000 [03:30<00:53, 33.66it/s]Training CobwebTree:  82%| | 8188/10000 [03:30<00:53, 34.12it/s]Training CobwebTree:  82%| | 8192/10000 [03:30<00:52, 34.36it/s]Training CobwebTree:  82%| | 8196/10000 [03:30<00:50, 35.48it/s]Training CobwebTree:  82%| | 8200/10000 [03:31<00:53, 33.70it/s]Training CobwebTree:  82%| | 8204/10000 [03:31<00:51, 34.74it/s]Training CobwebTree:  82%| | 8208/10000 [03:31<00:51, 34.74it/s]Training CobwebTree:  82%| | 8212/10000 [03:31<00:52, 34.01it/s]Training CobwebTree:  82%| | 8217/10000 [03:31<00:49, 35.92it/s]Training CobwebTree:  82%| | 8221/10000 [03:31<00:50, 35.01it/s]Training CobwebTree:  82%| | 8225/10000 [03:31<00:49, 35.53it/s]Training CobwebTree:  82%| | 8229/10000 [03:31<00:52, 33.78it/s]Training CobwebTree:  82%| | 8233/10000 [03:32<00:51, 34.61it/s]Training CobwebTree:  82%| | 8237/10000 [03:32<00:50, 34.72it/s]Training CobwebTree:  82%| | 8241/10000 [03:32<00:51, 34.29it/s]Training CobwebTree:  82%| | 8245/10000 [03:32<00:50, 34.97it/s]Training CobwebTree:  82%| | 8249/10000 [03:32<00:50, 34.97it/s]Training CobwebTree:  83%| | 8253/10000 [03:32<00:49, 35.38it/s]Training CobwebTree:  83%| | 8257/10000 [03:32<00:51, 34.00it/s]Training CobwebTree:  83%| | 8261/10000 [03:32<00:50, 34.12it/s]Training CobwebTree:  83%| | 8265/10000 [03:32<00:53, 32.70it/s]Training CobwebTree:  83%| | 8269/10000 [03:33<00:50, 34.29it/s]Training CobwebTree:  83%| | 8273/10000 [03:33<00:49, 35.21it/s]Training CobwebTree:  83%| | 8277/10000 [03:33<00:48, 35.21it/s]Training CobwebTree:  83%| | 8281/10000 [03:33<00:50, 33.99it/s]Training CobwebTree:  83%| | 8285/10000 [03:33<00:50, 33.63it/s]Training CobwebTree:  83%| | 8289/10000 [03:33<00:50, 33.70it/s]Training CobwebTree:  83%| | 8293/10000 [03:33<00:51, 33.29it/s]Training CobwebTree:  83%| | 8297/10000 [03:33<00:49, 34.52it/s]Training CobwebTree:  83%| | 8301/10000 [03:34<00:49, 34.57it/s]Training CobwebTree:  83%| | 8305/10000 [03:34<00:50, 33.24it/s]Training CobwebTree:  83%| | 8309/10000 [03:34<00:52, 32.45it/s]Training CobwebTree:  83%| | 8313/10000 [03:34<00:54, 30.96it/s]Training CobwebTree:  83%| | 8317/10000 [03:34<00:52, 31.76it/s]Training CobwebTree:  83%| | 8321/10000 [03:34<00:51, 32.79it/s]Training CobwebTree:  83%| | 8325/10000 [03:34<00:51, 32.47it/s]Training CobwebTree:  83%| | 8329/10000 [03:34<00:51, 32.64it/s]Training CobwebTree:  83%| | 8333/10000 [03:35<00:48, 34.11it/s]Training CobwebTree:  83%| | 8337/10000 [03:35<00:47, 34.89it/s]Training CobwebTree:  83%| | 8341/10000 [03:35<00:47, 34.96it/s]Training CobwebTree:  83%| | 8345/10000 [03:35<00:48, 33.84it/s]Training CobwebTree:  83%| | 8349/10000 [03:35<00:50, 32.90it/s]Training CobwebTree:  84%| | 8353/10000 [03:35<00:50, 32.47it/s]Training CobwebTree:  84%| | 8357/10000 [03:35<00:50, 32.45it/s]Training CobwebTree:  84%| | 8361/10000 [03:35<00:51, 32.11it/s]Training CobwebTree:  84%| | 8365/10000 [03:35<00:49, 32.76it/s]Training CobwebTree:  84%| | 8369/10000 [03:36<00:51, 31.67it/s]Training CobwebTree:  84%| | 8373/10000 [03:36<00:52, 30.90it/s]Training CobwebTree:  84%| | 8377/10000 [03:36<00:50, 32.43it/s]Training CobwebTree:  84%| | 8381/10000 [03:36<00:48, 33.43it/s]Training CobwebTree:  84%| | 8385/10000 [03:36<00:48, 33.28it/s]Training CobwebTree:  84%| | 8389/10000 [03:36<00:47, 34.07it/s]Training CobwebTree:  84%| | 8393/10000 [03:36<00:47, 34.05it/s]Training CobwebTree:  84%| | 8397/10000 [03:36<00:47, 33.77it/s]Training CobwebTree:  84%| | 8401/10000 [03:37<00:48, 33.21it/s]Training CobwebTree:  84%| | 8405/10000 [03:37<00:47, 33.85it/s]Training CobwebTree:  84%| | 8409/10000 [03:37<00:47, 33.79it/s]Training CobwebTree:  84%| | 8413/10000 [03:37<00:46, 33.95it/s]Training CobwebTree:  84%| | 8417/10000 [03:37<00:47, 33.54it/s]Training CobwebTree:  84%| | 8421/10000 [03:37<00:45, 34.64it/s]Training CobwebTree:  84%| | 8425/10000 [03:37<00:45, 34.39it/s]Training CobwebTree:  84%| | 8430/10000 [03:37<00:43, 36.01it/s]Training CobwebTree:  84%| | 8435/10000 [03:38<00:42, 36.93it/s]Training CobwebTree:  84%| | 8439/10000 [03:38<00:41, 37.36it/s]Training CobwebTree:  84%| | 8443/10000 [03:38<00:41, 37.51it/s]Training CobwebTree:  84%| | 8447/10000 [03:38<00:42, 36.82it/s]Training CobwebTree:  85%| | 8452/10000 [03:38<00:41, 37.57it/s]Training CobwebTree:  85%| | 8456/10000 [03:38<00:41, 36.91it/s]Training CobwebTree:  85%| | 8460/10000 [03:38<00:44, 34.36it/s]Training CobwebTree:  85%| | 8464/10000 [03:38<00:44, 34.48it/s]Training CobwebTree:  85%| | 8468/10000 [03:38<00:44, 34.12it/s]Training CobwebTree:  85%| | 8472/10000 [03:39<00:46, 32.62it/s]Training CobwebTree:  85%| | 8476/10000 [03:39<00:45, 33.64it/s]Training CobwebTree:  85%| | 8480/10000 [03:39<00:45, 33.65it/s]Training CobwebTree:  85%| | 8484/10000 [03:39<00:43, 35.19it/s]Training CobwebTree:  85%| | 8488/10000 [03:39<00:42, 35.90it/s]Training CobwebTree:  85%| | 8492/10000 [03:39<00:43, 34.52it/s]Training CobwebTree:  85%| | 8496/10000 [03:39<00:42, 35.17it/s]Training CobwebTree:  85%| | 8500/10000 [03:39<00:42, 34.97it/s]Training CobwebTree:  85%| | 8505/10000 [03:40<00:40, 36.59it/s]Training CobwebTree:  85%| | 8509/10000 [03:40<00:41, 35.78it/s]Training CobwebTree:  85%| | 8513/10000 [03:40<00:41, 36.12it/s]Training CobwebTree:  85%| | 8517/10000 [03:40<00:42, 34.82it/s]Training CobwebTree:  85%| | 8521/10000 [03:40<00:41, 35.24it/s]Training CobwebTree:  85%| | 8525/10000 [03:40<00:41, 35.16it/s]Training CobwebTree:  85%| | 8530/10000 [03:40<00:39, 36.87it/s]Training CobwebTree:  85%| | 8534/10000 [03:40<00:42, 34.57it/s]Training CobwebTree:  85%| | 8538/10000 [03:40<00:42, 34.33it/s]Training CobwebTree:  85%| | 8542/10000 [03:41<00:44, 32.98it/s]Training CobwebTree:  85%| | 8546/10000 [03:41<00:43, 33.09it/s]Training CobwebTree:  86%| | 8550/10000 [03:41<00:44, 32.65it/s]Training CobwebTree:  86%| | 8554/10000 [03:41<00:46, 31.31it/s]Training CobwebTree:  86%| | 8558/10000 [03:41<00:44, 32.70it/s]Training CobwebTree:  86%| | 8562/10000 [03:41<00:42, 33.64it/s]Training CobwebTree:  86%| | 8566/10000 [03:41<00:43, 33.03it/s]Training CobwebTree:  86%| | 8570/10000 [03:41<00:42, 33.51it/s]Training CobwebTree:  86%| | 8574/10000 [03:42<00:42, 33.34it/s]Training CobwebTree:  86%| | 8578/10000 [03:42<00:41, 34.12it/s]Training CobwebTree:  86%| | 8582/10000 [03:42<00:40, 34.70it/s]Training CobwebTree:  86%| | 8586/10000 [03:42<00:39, 35.37it/s]Training CobwebTree:  86%| | 8590/10000 [03:42<00:41, 33.62it/s]Training CobwebTree:  86%| | 8594/10000 [03:42<00:42, 33.06it/s]Training CobwebTree:  86%| | 8598/10000 [03:42<00:41, 34.05it/s]Training CobwebTree:  86%| | 8602/10000 [03:42<00:40, 34.29it/s]Training CobwebTree:  86%| | 8606/10000 [03:42<00:40, 34.78it/s]Training CobwebTree:  86%| | 8610/10000 [03:43<00:40, 34.27it/s]Training CobwebTree:  86%| | 8614/10000 [03:43<00:41, 33.76it/s]Training CobwebTree:  86%| | 8618/10000 [03:43<00:41, 33.24it/s]Training CobwebTree:  86%| | 8622/10000 [03:43<00:41, 33.26it/s]Training CobwebTree:  86%| | 8627/10000 [03:43<00:38, 35.52it/s]Training CobwebTree:  86%| | 8631/10000 [03:43<00:40, 34.15it/s]Training CobwebTree:  86%| | 8635/10000 [03:43<00:39, 34.23it/s]Training CobwebTree:  86%| | 8639/10000 [03:43<00:40, 33.87it/s]Training CobwebTree:  86%| | 8643/10000 [03:44<00:38, 35.20it/s]Training CobwebTree:  86%| | 8647/10000 [03:44<00:39, 34.56it/s]Training CobwebTree:  87%| | 8651/10000 [03:44<00:38, 35.06it/s]Training CobwebTree:  87%| | 8655/10000 [03:44<00:38, 35.12it/s]Training CobwebTree:  87%| | 8659/10000 [03:44<00:36, 36.28it/s]Training CobwebTree:  87%| | 8663/10000 [03:44<00:37, 35.76it/s]Training CobwebTree:  87%| | 8667/10000 [03:44<00:36, 36.26it/s]Training CobwebTree:  87%| | 8671/10000 [03:44<00:36, 36.87it/s]Training CobwebTree:  87%| | 8675/10000 [03:44<00:36, 36.22it/s]Training CobwebTree:  87%| | 8679/10000 [03:45<00:35, 36.72it/s]Training CobwebTree:  87%| | 8683/10000 [03:45<00:34, 37.63it/s]Training CobwebTree:  87%| | 8687/10000 [03:45<00:36, 35.93it/s]Training CobwebTree:  87%| | 8692/10000 [03:45<00:34, 38.45it/s]Training CobwebTree:  87%| | 8696/10000 [03:45<00:34, 37.73it/s]Training CobwebTree:  87%| | 8701/10000 [03:45<00:32, 39.75it/s]Training CobwebTree:  87%| | 8705/10000 [03:45<00:33, 38.41it/s]Training CobwebTree:  87%| | 8709/10000 [03:45<00:34, 37.32it/s]Training CobwebTree:  87%| | 8713/10000 [03:45<00:33, 37.89it/s]Training CobwebTree:  87%| | 8717/10000 [03:46<00:33, 38.48it/s]Training CobwebTree:  87%| | 8721/10000 [03:46<00:35, 36.22it/s]Training CobwebTree:  87%| | 8725/10000 [03:46<00:34, 37.07it/s]Training CobwebTree:  87%| | 8729/10000 [03:46<00:35, 35.40it/s]Training CobwebTree:  87%| | 8733/10000 [03:46<00:38, 33.09it/s]Training CobwebTree:  87%| | 8737/10000 [03:46<00:38, 32.69it/s]Training CobwebTree:  87%| | 8741/10000 [03:46<00:37, 33.29it/s]Training CobwebTree:  87%| | 8745/10000 [03:46<00:38, 32.95it/s]Training CobwebTree:  87%| | 8749/10000 [03:47<00:37, 33.74it/s]Training CobwebTree:  88%| | 8753/10000 [03:47<00:35, 34.87it/s]Training CobwebTree:  88%| | 8757/10000 [03:47<00:35, 34.78it/s]Training CobwebTree:  88%| | 8761/10000 [03:47<00:35, 34.82it/s]Training CobwebTree:  88%| | 8765/10000 [03:47<00:35, 34.33it/s]Training CobwebTree:  88%| | 8769/10000 [03:47<00:37, 33.14it/s]Training CobwebTree:  88%| | 8773/10000 [03:47<00:35, 34.69it/s]Training CobwebTree:  88%| | 8778/10000 [03:47<00:32, 37.19it/s]Training CobwebTree:  88%| | 8783/10000 [03:47<00:32, 37.58it/s]Training CobwebTree:  88%| | 8787/10000 [03:48<00:31, 38.00it/s]Training CobwebTree:  88%| | 8791/10000 [03:48<00:33, 36.45it/s]Training CobwebTree:  88%| | 8795/10000 [03:48<00:34, 35.23it/s]Training CobwebTree:  88%| | 8799/10000 [03:48<00:34, 34.45it/s]Training CobwebTree:  88%| | 8803/10000 [03:48<00:34, 35.07it/s]Training CobwebTree:  88%| | 8807/10000 [03:48<00:33, 35.44it/s]Training CobwebTree:  88%| | 8811/10000 [03:48<00:32, 36.05it/s]Training CobwebTree:  88%| | 8816/10000 [03:48<00:31, 37.33it/s]Training CobwebTree:  88%| | 8820/10000 [03:48<00:32, 36.25it/s]Training CobwebTree:  88%| | 8824/10000 [03:49<00:32, 35.92it/s]Training CobwebTree:  88%| | 8828/10000 [03:49<00:32, 35.98it/s]Training CobwebTree:  88%| | 8833/10000 [03:49<00:32, 36.03it/s]Training CobwebTree:  88%| | 8838/10000 [03:49<00:30, 37.92it/s]Training CobwebTree:  88%| | 8842/10000 [03:49<00:30, 37.81it/s]Training CobwebTree:  88%| | 8846/10000 [03:49<00:32, 35.39it/s]Training CobwebTree:  88%| | 8850/10000 [03:49<00:32, 35.22it/s]Training CobwebTree:  89%| | 8854/10000 [03:49<00:31, 35.86it/s]Training CobwebTree:  89%| | 8858/10000 [03:50<00:32, 35.45it/s]Training CobwebTree:  89%| | 8862/10000 [03:50<00:33, 34.44it/s]Training CobwebTree:  89%| | 8866/10000 [03:50<00:33, 33.89it/s]Training CobwebTree:  89%| | 8870/10000 [03:50<00:32, 35.14it/s]Training CobwebTree:  89%| | 8874/10000 [03:50<00:32, 34.71it/s]Training CobwebTree:  89%| | 8878/10000 [03:50<00:31, 35.65it/s]Training CobwebTree:  89%| | 8882/10000 [03:50<00:32, 34.59it/s]Training CobwebTree:  89%| | 8887/10000 [03:50<00:30, 36.60it/s]Training CobwebTree:  89%| | 8891/10000 [03:50<00:32, 34.24it/s]Training CobwebTree:  89%| | 8895/10000 [03:51<00:31, 34.97it/s]Training CobwebTree:  89%| | 8899/10000 [03:51<00:31, 34.74it/s]Training CobwebTree:  89%| | 8903/10000 [03:51<00:31, 35.05it/s]Training CobwebTree:  89%| | 8907/10000 [03:51<00:33, 32.73it/s]Training CobwebTree:  89%| | 8911/10000 [03:51<00:33, 32.71it/s]Training CobwebTree:  89%| | 8915/10000 [03:51<00:32, 33.52it/s]Training CobwebTree:  89%| | 8920/10000 [03:51<00:30, 35.45it/s]Training CobwebTree:  89%| | 8924/10000 [03:51<00:31, 34.58it/s]Training CobwebTree:  89%| | 8928/10000 [03:52<00:30, 34.73it/s]Training CobwebTree:  89%| | 8932/10000 [03:52<00:31, 34.43it/s]Training CobwebTree:  89%| | 8936/10000 [03:52<00:32, 33.16it/s]Training CobwebTree:  89%| | 8940/10000 [03:52<00:33, 32.08it/s]Training CobwebTree:  89%| | 8944/10000 [03:52<00:31, 33.95it/s]Training CobwebTree:  89%| | 8948/10000 [03:52<00:29, 35.14it/s]Training CobwebTree:  90%| | 8952/10000 [03:52<00:29, 35.30it/s]Training CobwebTree:  90%| | 8956/10000 [03:52<00:30, 33.75it/s]Training CobwebTree:  90%| | 8960/10000 [03:53<00:31, 32.89it/s]Training CobwebTree:  90%| | 8964/10000 [03:53<00:30, 33.48it/s]Training CobwebTree:  90%| | 8968/10000 [03:53<00:30, 33.67it/s]Training CobwebTree:  90%| | 8972/10000 [03:53<00:30, 33.62it/s]Training CobwebTree:  90%| | 8976/10000 [03:53<00:30, 33.46it/s]Training CobwebTree:  90%| | 8980/10000 [03:53<00:30, 33.07it/s]Training CobwebTree:  90%| | 8984/10000 [03:53<00:30, 33.44it/s]Training CobwebTree:  90%| | 8988/10000 [03:53<00:29, 34.27it/s]Training CobwebTree:  90%| | 8992/10000 [03:53<00:29, 34.52it/s]Training CobwebTree:  90%| | 8996/10000 [03:54<00:29, 33.81it/s]Training CobwebTree:  90%| | 9000/10000 [03:54<00:30, 32.55it/s]Training CobwebTree:  90%| | 9004/10000 [03:54<00:29, 33.84it/s]Training CobwebTree:  90%| | 9008/10000 [03:54<00:28, 34.43it/s]Training CobwebTree:  90%| | 9012/10000 [03:54<00:28, 34.08it/s]Training CobwebTree:  90%| | 9016/10000 [03:54<00:29, 33.49it/s]Training CobwebTree:  90%| | 9020/10000 [03:54<00:29, 33.43it/s]Training CobwebTree:  90%| | 9024/10000 [03:54<00:28, 34.46it/s]Training CobwebTree:  90%| | 9028/10000 [03:55<00:27, 34.83it/s]Training CobwebTree:  90%| | 9032/10000 [03:55<00:27, 34.94it/s]Training CobwebTree:  90%| | 9036/10000 [03:55<00:27, 35.20it/s]Training CobwebTree:  90%| | 9040/10000 [03:55<00:28, 34.02it/s]Training CobwebTree:  90%| | 9044/10000 [03:55<00:28, 33.27it/s]Training CobwebTree:  90%| | 9048/10000 [03:55<00:28, 33.81it/s]Training CobwebTree:  91%| | 9052/10000 [03:55<00:29, 32.41it/s]Training CobwebTree:  91%| | 9056/10000 [03:55<00:29, 32.03it/s]Training CobwebTree:  91%| | 9060/10000 [03:55<00:28, 33.44it/s]Training CobwebTree:  91%| | 9064/10000 [03:56<00:27, 34.34it/s]Training CobwebTree:  91%| | 9068/10000 [03:56<00:27, 33.43it/s]Training CobwebTree:  91%| | 9072/10000 [03:56<00:27, 34.21it/s]Training CobwebTree:  91%| | 9076/10000 [03:56<00:26, 34.43it/s]Training CobwebTree:  91%| | 9080/10000 [03:56<00:27, 33.21it/s]Training CobwebTree:  91%| | 9084/10000 [03:56<00:26, 34.87it/s]Training CobwebTree:  91%| | 9088/10000 [03:56<00:26, 33.86it/s]Training CobwebTree:  91%| | 9092/10000 [03:56<00:26, 33.87it/s]Training CobwebTree:  91%| | 9096/10000 [03:57<00:27, 33.25it/s]Training CobwebTree:  91%| | 9100/10000 [03:57<00:26, 33.61it/s]Training CobwebTree:  91%| | 9104/10000 [03:57<00:25, 34.54it/s]Training CobwebTree:  91%| | 9108/10000 [03:57<00:25, 34.47it/s]Training CobwebTree:  91%| | 9112/10000 [03:57<00:25, 34.42it/s]Training CobwebTree:  91%| | 9116/10000 [03:57<00:26, 33.58it/s]Training CobwebTree:  91%| | 9120/10000 [03:57<00:26, 33.57it/s]Training CobwebTree:  91%| | 9124/10000 [03:57<00:25, 33.86it/s]Training CobwebTree:  91%|| 9129/10000 [03:57<00:24, 35.84it/s]Training CobwebTree:  91%|| 9133/10000 [03:58<00:24, 35.91it/s]Training CobwebTree:  91%|| 9137/10000 [03:58<00:24, 35.59it/s]Training CobwebTree:  91%|| 9141/10000 [03:58<00:24, 34.78it/s]Training CobwebTree:  91%|| 9145/10000 [03:58<00:24, 34.39it/s]Training CobwebTree:  91%|| 9149/10000 [03:58<00:25, 33.36it/s]Training CobwebTree:  92%|| 9153/10000 [03:58<00:25, 32.62it/s]Training CobwebTree:  92%|| 9157/10000 [03:58<00:26, 32.19it/s]Training CobwebTree:  92%|| 9161/10000 [03:58<00:25, 32.96it/s]Training CobwebTree:  92%|| 9165/10000 [03:59<00:25, 32.82it/s]Training CobwebTree:  92%|| 9169/10000 [03:59<00:24, 33.45it/s]Training CobwebTree:  92%|| 9173/10000 [03:59<00:24, 34.00it/s]Training CobwebTree:  92%|| 9177/10000 [03:59<00:25, 32.17it/s]Training CobwebTree:  92%|| 9181/10000 [03:59<00:24, 32.77it/s]Training CobwebTree:  92%|| 9185/10000 [03:59<00:24, 33.00it/s]Training CobwebTree:  92%|| 9189/10000 [03:59<00:24, 33.24it/s]Training CobwebTree:  92%|| 9193/10000 [03:59<00:23, 33.82it/s]Training CobwebTree:  92%|| 9197/10000 [04:00<00:23, 34.14it/s]Training CobwebTree:  92%|| 9201/10000 [04:00<00:23, 34.34it/s]Training CobwebTree:  92%|| 9205/10000 [04:00<00:22, 34.72it/s]Training CobwebTree:  92%|| 9209/10000 [04:00<00:22, 35.53it/s]Training CobwebTree:  92%|| 9213/10000 [04:00<00:22, 34.50it/s]Training CobwebTree:  92%|| 9217/10000 [04:00<00:23, 33.72it/s]Training CobwebTree:  92%|| 9221/10000 [04:00<00:22, 34.66it/s]Training CobwebTree:  92%|| 9225/10000 [04:00<00:21, 35.29it/s]Training CobwebTree:  92%|| 9229/10000 [04:00<00:21, 36.21it/s]Training CobwebTree:  92%|| 9233/10000 [04:01<00:22, 34.20it/s]Training CobwebTree:  92%|| 9237/10000 [04:01<00:21, 35.61it/s]Training CobwebTree:  92%|| 9241/10000 [04:01<00:22, 34.48it/s]Training CobwebTree:  92%|| 9245/10000 [04:01<00:21, 34.81it/s]Training CobwebTree:  92%|| 9249/10000 [04:01<00:21, 35.07it/s]Training CobwebTree:  93%|| 9253/10000 [04:01<00:21, 34.57it/s]Training CobwebTree:  93%|| 9257/10000 [04:01<00:21, 34.32it/s]Training CobwebTree:  93%|| 9261/10000 [04:01<00:22, 32.25it/s]Training CobwebTree:  93%|| 9265/10000 [04:02<00:22, 32.30it/s]Training CobwebTree:  93%|| 9269/10000 [04:02<00:21, 33.32it/s]Training CobwebTree:  93%|| 9273/10000 [04:02<00:22, 32.82it/s]Training CobwebTree:  93%|| 9277/10000 [04:02<00:21, 33.08it/s]Training CobwebTree:  93%|| 9281/10000 [04:02<00:22, 32.68it/s]Training CobwebTree:  93%|| 9285/10000 [04:02<00:22, 32.42it/s]Training CobwebTree:  93%|| 9289/10000 [04:02<00:22, 32.15it/s]Training CobwebTree:  93%|| 9293/10000 [04:02<00:21, 32.81it/s]Training CobwebTree:  93%|| 9297/10000 [04:03<00:22, 31.22it/s]Training CobwebTree:  93%|| 9301/10000 [04:03<00:21, 32.93it/s]Training CobwebTree:  93%|| 9306/10000 [04:03<00:19, 36.07it/s]Training CobwebTree:  93%|| 9310/10000 [04:03<00:19, 36.01it/s]Training CobwebTree:  93%|| 9314/10000 [04:03<00:19, 35.71it/s]Training CobwebTree:  93%|| 9318/10000 [04:03<00:20, 33.90it/s]Training CobwebTree:  93%|| 9322/10000 [04:03<00:19, 34.90it/s]Training CobwebTree:  93%|| 9326/10000 [04:03<00:19, 33.86it/s]Training CobwebTree:  93%|| 9330/10000 [04:03<00:19, 33.63it/s]Training CobwebTree:  93%|| 9334/10000 [04:04<00:19, 34.88it/s]Training CobwebTree:  93%|| 9338/10000 [04:04<00:19, 34.02it/s]Training CobwebTree:  93%|| 9342/10000 [04:04<00:19, 34.38it/s]Training CobwebTree:  93%|| 9346/10000 [04:04<00:19, 33.40it/s]Training CobwebTree:  94%|| 9351/10000 [04:04<00:18, 35.50it/s]Training CobwebTree:  94%|| 9355/10000 [04:04<00:18, 35.58it/s]Training CobwebTree:  94%|| 9359/10000 [04:04<00:18, 33.77it/s]Training CobwebTree:  94%|| 9363/10000 [04:04<00:18, 33.98it/s]Training CobwebTree:  94%|| 9367/10000 [04:05<00:19, 32.42it/s]Training CobwebTree:  94%|| 9371/10000 [04:05<00:19, 33.10it/s]Training CobwebTree:  94%|| 9375/10000 [04:05<00:18, 34.33it/s]Training CobwebTree:  94%|| 9379/10000 [04:05<00:18, 33.90it/s]Training CobwebTree:  94%|| 9383/10000 [04:05<00:18, 33.52it/s]Training CobwebTree:  94%|| 9387/10000 [04:05<00:18, 32.61it/s]Training CobwebTree:  94%|| 9391/10000 [04:05<00:19, 31.94it/s]Training CobwebTree:  94%|| 9395/10000 [04:05<00:18, 32.12it/s]Training CobwebTree:  94%|| 9399/10000 [04:05<00:17, 34.02it/s]Training CobwebTree:  94%|| 9403/10000 [04:06<00:17, 33.84it/s]Training CobwebTree:  94%|| 9407/10000 [04:06<00:16, 35.02it/s]Training CobwebTree:  94%|| 9411/10000 [04:06<00:16, 35.61it/s]Training CobwebTree:  94%|| 9415/10000 [04:06<00:15, 36.62it/s]Training CobwebTree:  94%|| 9419/10000 [04:06<00:16, 34.74it/s]Training CobwebTree:  94%|| 9423/10000 [04:06<00:16, 34.00it/s]Training CobwebTree:  94%|| 9427/10000 [04:06<00:16, 34.29it/s]Training CobwebTree:  94%|| 9431/10000 [04:06<00:16, 35.54it/s]Training CobwebTree:  94%|| 9435/10000 [04:07<00:16, 34.30it/s]Training CobwebTree:  94%|| 9439/10000 [04:07<00:16, 33.25it/s]Training CobwebTree:  94%|| 9443/10000 [04:07<00:16, 33.13it/s]Training CobwebTree:  94%|| 9447/10000 [04:07<00:16, 33.60it/s]Training CobwebTree:  95%|| 9451/10000 [04:07<00:16, 33.98it/s]Training CobwebTree:  95%|| 9455/10000 [04:07<00:16, 32.86it/s]Training CobwebTree:  95%|| 9459/10000 [04:07<00:16, 33.06it/s]Training CobwebTree:  95%|| 9463/10000 [04:07<00:16, 33.00it/s]Training CobwebTree:  95%|| 9467/10000 [04:08<00:16, 32.76it/s]Training CobwebTree:  95%|| 9471/10000 [04:08<00:16, 31.53it/s]Training CobwebTree:  95%|| 9475/10000 [04:08<00:16, 31.32it/s]Training CobwebTree:  95%|| 9479/10000 [04:08<00:15, 32.73it/s]Training CobwebTree:  95%|| 9483/10000 [04:08<00:15, 34.46it/s]Training CobwebTree:  95%|| 9487/10000 [04:08<00:15, 33.14it/s]Training CobwebTree:  95%|| 9491/10000 [04:08<00:15, 33.64it/s]Training CobwebTree:  95%|| 9495/10000 [04:08<00:14, 34.78it/s]Training CobwebTree:  95%|| 9500/10000 [04:08<00:13, 36.34it/s]Training CobwebTree:  95%|| 9504/10000 [04:09<00:13, 36.81it/s]Training CobwebTree:  95%|| 9508/10000 [04:09<00:13, 35.98it/s]Training CobwebTree:  95%|| 9512/10000 [04:09<00:13, 35.42it/s]Training CobwebTree:  95%|| 9516/10000 [04:09<00:14, 34.42it/s]Training CobwebTree:  95%|| 9520/10000 [04:09<00:13, 34.41it/s]Training CobwebTree:  95%|| 9524/10000 [04:09<00:14, 33.89it/s]Training CobwebTree:  95%|| 9528/10000 [04:09<00:13, 33.84it/s]Training CobwebTree:  95%|| 9532/10000 [04:09<00:14, 33.01it/s]Training CobwebTree:  95%|| 9536/10000 [04:10<00:14, 33.04it/s]Training CobwebTree:  95%|| 9540/10000 [04:10<00:13, 33.19it/s]Training CobwebTree:  95%|| 9544/10000 [04:10<00:14, 32.40it/s]Training CobwebTree:  95%|| 9548/10000 [04:10<00:13, 32.30it/s]Training CobwebTree:  96%|| 9552/10000 [04:10<00:13, 32.78it/s]Training CobwebTree:  96%|| 9556/10000 [04:10<00:13, 33.96it/s]Training CobwebTree:  96%|| 9560/10000 [04:10<00:12, 34.59it/s]Training CobwebTree:  96%|| 9564/10000 [04:10<00:12, 34.60it/s]Training CobwebTree:  96%|| 9568/10000 [04:10<00:12, 33.82it/s]Training CobwebTree:  96%|| 9572/10000 [04:11<00:12, 34.31it/s]Training CobwebTree:  96%|| 9576/10000 [04:11<00:12, 35.27it/s]Training CobwebTree:  96%|| 9580/10000 [04:11<00:12, 34.96it/s]Training CobwebTree:  96%|| 9584/10000 [04:11<00:11, 35.12it/s]Training CobwebTree:  96%|| 9588/10000 [04:11<00:12, 33.78it/s]Training CobwebTree:  96%|| 9592/10000 [04:11<00:11, 34.64it/s]Training CobwebTree:  96%|| 9596/10000 [04:11<00:11, 34.75it/s]Training CobwebTree:  96%|| 9601/10000 [04:11<00:10, 36.38it/s]Training CobwebTree:  96%|| 9605/10000 [04:12<00:11, 34.89it/s]Training CobwebTree:  96%|| 9609/10000 [04:12<00:11, 33.09it/s]Training CobwebTree:  96%|| 9614/10000 [04:12<00:10, 35.52it/s]Training CobwebTree:  96%|| 9618/10000 [04:12<00:10, 35.43it/s]Training CobwebTree:  96%|| 9622/10000 [04:12<00:10, 34.72it/s]Training CobwebTree:  96%|| 9626/10000 [04:12<00:10, 35.48it/s]Training CobwebTree:  96%|| 9630/10000 [04:12<00:10, 34.88it/s]Training CobwebTree:  96%|| 9634/10000 [04:12<00:10, 35.82it/s]Training CobwebTree:  96%|| 9638/10000 [04:12<00:10, 34.08it/s]Training CobwebTree:  96%|| 9642/10000 [04:13<00:10, 34.44it/s]Training CobwebTree:  96%|| 9646/10000 [04:13<00:10, 33.84it/s]Training CobwebTree:  96%|| 9650/10000 [04:13<00:10, 33.78it/s]Training CobwebTree:  97%|| 9654/10000 [04:13<00:10, 33.90it/s]Training CobwebTree:  97%|| 9658/10000 [04:13<00:10, 32.53it/s]Training CobwebTree:  97%|| 9662/10000 [04:13<00:10, 32.70it/s]Training CobwebTree:  97%|| 9666/10000 [04:13<00:09, 34.17it/s]Training CobwebTree:  97%|| 9670/10000 [04:13<00:09, 34.05it/s]Training CobwebTree:  97%|| 9674/10000 [04:14<00:09, 32.87it/s]Training CobwebTree:  97%|| 9678/10000 [04:14<00:09, 32.71it/s]Training CobwebTree:  97%|| 9682/10000 [04:14<00:09, 33.27it/s]Training CobwebTree:  97%|| 9686/10000 [04:14<00:09, 33.68it/s]Training CobwebTree:  97%|| 9690/10000 [04:14<00:09, 34.27it/s]Training CobwebTree:  97%|| 9694/10000 [04:14<00:09, 32.82it/s]Training CobwebTree:  97%|| 9698/10000 [04:14<00:08, 33.83it/s]Training CobwebTree:  97%|| 9702/10000 [04:14<00:08, 34.47it/s]Training CobwebTree:  97%|| 9706/10000 [04:15<00:08, 33.96it/s]Training CobwebTree:  97%|| 9710/10000 [04:15<00:08, 34.91it/s]Training CobwebTree:  97%|| 9714/10000 [04:15<00:08, 35.61it/s]Training CobwebTree:  97%|| 9718/10000 [04:15<00:07, 35.38it/s]Training CobwebTree:  97%|| 9722/10000 [04:15<00:08, 34.55it/s]Training CobwebTree:  97%|| 9727/10000 [04:15<00:07, 36.28it/s]Training CobwebTree:  97%|| 9731/10000 [04:15<00:07, 35.54it/s]Training CobwebTree:  97%|| 9735/10000 [04:15<00:07, 34.68it/s]Training CobwebTree:  97%|| 9739/10000 [04:15<00:07, 35.50it/s]Training CobwebTree:  97%|| 9743/10000 [04:16<00:07, 32.76it/s]Training CobwebTree:  97%|| 9747/10000 [04:16<00:07, 32.36it/s]Training CobwebTree:  98%|| 9751/10000 [04:16<00:08, 29.68it/s]Training CobwebTree:  98%|| 9755/10000 [04:16<00:07, 30.85it/s]Training CobwebTree:  98%|| 9759/10000 [04:16<00:07, 31.68it/s]Training CobwebTree:  98%|| 9763/10000 [04:16<00:07, 32.46it/s]Training CobwebTree:  98%|| 9767/10000 [04:16<00:07, 32.48it/s]Training CobwebTree:  98%|| 9771/10000 [04:16<00:06, 33.65it/s]Training CobwebTree:  98%|| 9775/10000 [04:17<00:06, 34.36it/s]Training CobwebTree:  98%|| 9779/10000 [04:17<00:06, 32.81it/s]Training CobwebTree:  98%|| 9783/10000 [04:17<00:06, 32.74it/s]Training CobwebTree:  98%|| 9787/10000 [04:17<00:06, 34.22it/s]Training CobwebTree:  98%|| 9791/10000 [04:17<00:06, 33.89it/s]Training CobwebTree:  98%|| 9795/10000 [04:17<00:05, 34.59it/s]Training CobwebTree:  98%|| 9799/10000 [04:17<00:05, 34.28it/s]Training CobwebTree:  98%|| 9803/10000 [04:17<00:05, 34.98it/s]Training CobwebTree:  98%|| 9808/10000 [04:18<00:05, 36.39it/s]Training CobwebTree:  98%|| 9812/10000 [04:18<00:05, 36.68it/s]Training CobwebTree:  98%|| 9816/10000 [04:18<00:05, 35.32it/s]Training CobwebTree:  98%|| 9820/10000 [04:18<00:05, 34.16it/s]Training CobwebTree:  98%|| 9824/10000 [04:18<00:05, 32.66it/s]Training CobwebTree:  98%|| 9828/10000 [04:18<00:05, 32.89it/s]Training CobwebTree:  98%|| 9832/10000 [04:18<00:05, 33.14it/s]Training CobwebTree:  98%|| 9836/10000 [04:18<00:04, 33.51it/s]Training CobwebTree:  98%|| 9840/10000 [04:18<00:04, 34.18it/s]Training CobwebTree:  98%|| 9844/10000 [04:19<00:04, 35.53it/s]Training CobwebTree:  98%|| 9848/10000 [04:19<00:04, 35.35it/s]Training CobwebTree:  99%|| 9852/10000 [04:19<00:04, 36.18it/s]Training CobwebTree:  99%|| 9856/10000 [04:19<00:04, 35.60it/s]Training CobwebTree:  99%|| 9860/10000 [04:19<00:03, 35.39it/s]Training CobwebTree:  99%|| 9864/10000 [04:19<00:03, 34.83it/s]Training CobwebTree:  99%|| 9868/10000 [04:19<00:03, 34.70it/s]Training CobwebTree:  99%|| 9872/10000 [04:19<00:03, 34.46it/s]Training CobwebTree:  99%|| 9876/10000 [04:19<00:03, 34.76it/s]Training CobwebTree:  99%|| 9880/10000 [04:20<00:03, 35.96it/s]Training CobwebTree:  99%|| 9884/10000 [04:20<00:03, 34.96it/s]Training CobwebTree:  99%|| 9888/10000 [04:20<00:03, 36.16it/s]Training CobwebTree:  99%|| 9892/10000 [04:20<00:02, 36.65it/s]Training CobwebTree:  99%|| 9896/10000 [04:20<00:02, 36.00it/s]Training CobwebTree:  99%|| 9900/10000 [04:20<00:02, 34.53it/s]Training CobwebTree:  99%|| 9904/10000 [04:20<00:02, 35.67it/s]Training CobwebTree:  99%|| 9908/10000 [04:20<00:02, 34.65it/s]Training CobwebTree:  99%|| 9912/10000 [04:20<00:02, 35.79it/s]Training CobwebTree:  99%|| 9916/10000 [04:21<00:02, 33.99it/s]Training CobwebTree:  99%|| 9920/10000 [04:21<00:02, 33.59it/s]Training CobwebTree:  99%|| 9924/10000 [04:21<00:02, 33.60it/s]Training CobwebTree:  99%|| 9928/10000 [04:21<00:02, 34.20it/s]Training CobwebTree:  99%|| 9932/10000 [04:21<00:01, 35.31it/s]Training CobwebTree:  99%|| 9936/10000 [04:21<00:01, 34.77it/s]Training CobwebTree:  99%|| 9940/10000 [04:21<00:01, 34.04it/s]Training CobwebTree:  99%|| 9944/10000 [04:21<00:01, 33.22it/s]Training CobwebTree:  99%|| 9948/10000 [04:22<00:01, 34.09it/s]Training CobwebTree: 100%|| 9953/10000 [04:22<00:01, 36.00it/s]Training CobwebTree: 100%|| 9957/10000 [04:22<00:01, 35.29it/s]Training CobwebTree: 100%|| 9961/10000 [04:22<00:01, 35.02it/s]Training CobwebTree: 100%|| 9965/10000 [04:22<00:01, 34.23it/s]Training CobwebTree: 100%|| 9969/10000 [04:22<00:00, 34.69it/s]Training CobwebTree: 100%|| 9973/10000 [04:22<00:00, 35.59it/s]Training CobwebTree: 100%|| 9977/10000 [04:22<00:00, 35.62it/s]Training CobwebTree: 100%|| 9981/10000 [04:22<00:00, 35.80it/s]Training CobwebTree: 100%|| 9985/10000 [04:23<00:00, 33.79it/s]Training CobwebTree: 100%|| 9989/10000 [04:23<00:00, 33.95it/s]Training CobwebTree: 100%|| 9993/10000 [04:23<00:00, 34.88it/s]Training CobwebTree: 100%|| 9997/10000 [04:23<00:00, 33.45it/s]Training CobwebTree: 100%|| 10000/10000 [04:23<00:00, 37.94it/s]
2025-12-23 23:49:17,062 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:49:25,916 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-23 23:49:25,929 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-23 23:49:25,937 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-23 23:49:25,956 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-23 23:49:26,104 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-23 23:49:26,120 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-23 23:49:26,384 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-23 23:49:27,595 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-23 23:49:27,767 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-23 23:49:28,016 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-23 23:49:28,097 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-23 23:49:28,192 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-23 23:49:28,707 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-23 23:49:28,849 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (-255004 virtual)
2025-12-23 23:49:28,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,873 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,880 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,881 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,884 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,885 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,887 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,890 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,892 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,894 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,895 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,918 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,922 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,926 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,926 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,926 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:28,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,930 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,933 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,933 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,934 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,934 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,938 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,938 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,938 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,942 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,942 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,946 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,946 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,946 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,950 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,950 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,950 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,952 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,954 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,954 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,957 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,958 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,958 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,962 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,962 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,962 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,962 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,966 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,966 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,966 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,970 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,970 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,970 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,972 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,974 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,974 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,974 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,977 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,978 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,978 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,978 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,981 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,982 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,982 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,982 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,982 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,986 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,986 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,986 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,986 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,990 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,992 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,994 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,994 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,996 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,997 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,998 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,998 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:28,998 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,001 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,002 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,002 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,002 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,006 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,006 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,008 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,010 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,010 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,013 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,014 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,018 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,018 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,022 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,028 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,033 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,038 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,042 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,054 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,206 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,223 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,262 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,279 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,318 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,374 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,390 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,516 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,538 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,658 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,686 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,746 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,769 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,882 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,908 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,930 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,954 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:29,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,002 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:29,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,030 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,050 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,064 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,074 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,130 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,133 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,134 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,198 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,230 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,230 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,262 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,298 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,321 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,330 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,333 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,349 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,390 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,394 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,433 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,437 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,482 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,522 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,441 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,582 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,642 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,686 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,730 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,765 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:30,842 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,897 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:30,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:31,065 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:31,243 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:31,263 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:31,562 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:31,564 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:31,678 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:31,694 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:36,712 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:49:36,891 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 200058 virtual documents
2025-12-23 23:49:37,354 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-23 23:49:47,083 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-23 23:49:47,086 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-23 23:49:47,087 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-23 23:49:47,088 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-23 23:49:47,090 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-23 23:49:47,091 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-23 23:49:47,092 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-23 23:49:47,094 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-23 23:49:47,095 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-23 23:49:47,097 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-23 23:49:47,098 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-23 23:49:47,100 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-23 23:49:47,101 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-23 23:49:47,103 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-23 23:49:47,104 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-23 23:49:47,105 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-23 23:49:47,106 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-23 23:49:47,107 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-23 23:49:47,108 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-23 23:49:47,110 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-23 23:49:47,111 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-23 23:49:47,113 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-23 23:49:47,114 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-23 23:49:47,116 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-23 23:49:47,117 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-23 23:49:47,118 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-23 23:49:47,120 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-23 23:49:47,121 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-23 23:49:47,122 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-23 23:49:47,123 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-23 23:49:47,124 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-23 23:49:47,126 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-23 23:49:47,127 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-23 23:49:47,128 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-23 23:49:47,129 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-23 23:49:47,131 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-23 23:49:47,132 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-23 23:49:47,133 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-23 23:49:47,134 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-23 23:49:47,135 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-23 23:49:47,137 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-23 23:49:47,138 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-23 23:49:47,139 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-23 23:49:47,140 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-23 23:49:47,141 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-23 23:49:47,142 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-23 23:49:47,144 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-23 23:49:47,147 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-23 23:49:47,149 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-23 23:49:47,151 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-23 23:49:47,153 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-23 23:49:47,155 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-23 23:49:47,157 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-23 23:49:47,159 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-23 23:49:47,161 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-23 23:49:47,163 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-23 23:49:47,165 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-23 23:49:47,167 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-23 23:49:47,169 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-23 23:49:47,171 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-23 23:49:47,173 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-23 23:49:47,175 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-23 23:49:47,177 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-23 23:49:47,178 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-23 23:49:47,180 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-23 23:49:47,182 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-23 23:49:47,195 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-23 23:49:47,197 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-23 23:49:47,199 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-23 23:49:47,201 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-23 23:49:47,203 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-23 23:49:47,205 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-23 23:49:47,207 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-23 23:49:47,209 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-23 23:49:47,211 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-23 23:49:47,227 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-23 23:49:47,229 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-23 23:49:47,231 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-23 23:49:47,233 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-23 23:49:47,235 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-23 23:49:47,236 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-23 23:49:47,239 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-23 23:49:47,251 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-23 23:49:47,253 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-23 23:49:47,255 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-23 23:49:47,257 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-23 23:49:47,259 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-23 23:49:47,261 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-23 23:49:47,284 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-23 23:49:47,297 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-23 23:49:47,305 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-23 23:49:47,313 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-23 23:49:47,315 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-23 23:49:47,316 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-23 23:49:47,318 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-23 23:49:47,319 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-23 23:49:47,321 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-23 23:49:47,410 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-23 23:49:47,412 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-23 23:49:47,414 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-23 23:49:47,427 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-23 23:49:47,435 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-23 23:49:47,447 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-23 23:49:47,460 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-23 23:49:47,475 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-23 23:49:47,503 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-23 23:49:47,515 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-23 23:49:47,555 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-23 23:49:47,580 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-23 23:49:47,651 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-23 23:49:47,671 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-23 23:49:47,723 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-23 23:49:47,739 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-23 23:49:47,779 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-23 23:49:47,819 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-23 23:49:47,871 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-23 23:49:47,903 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-23 23:49:48,135 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-23 23:49:48,137 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-23 23:49:48,140 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-23 23:49:48,281 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-23 23:49:48,403 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-23 23:49:48,447 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-23 23:49:48,547 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-23 23:49:48,587 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-23 23:49:48,716 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-23 23:49:48,777 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-23 23:49:48,791 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-23 23:49:48,929 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-23 23:49:48,943 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-23 23:49:49,076 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-23 23:49:49,169 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-23 23:49:49,249 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-23 23:49:49,263 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-23 23:49:49,397 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-23 23:49:49,411 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-23 23:49:49,423 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-23 23:49:49,581 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-23 23:49:49,584 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-23 23:49:49,586 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-23 23:49:49,749 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-23 23:49:49,752 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-23 23:49:49,754 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-23 23:49:49,897 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-23 23:49:49,900 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-23 23:49:49,902 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-23 23:49:50,020 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-23 23:49:50,022 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-23 23:49:50,024 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-23 23:49:50,169 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-23 23:49:50,172 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-23 23:49:50,173 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-23 23:49:50,272 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-23 23:49:50,274 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-23 23:49:50,277 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-23 23:49:50,353 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-23 23:49:50,365 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (744996 virtual)
2025-12-23 23:49:50,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,370 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,374 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,375 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,376 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,377 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,378 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,379 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,381 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,382 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,383 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,388 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,393 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,399 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,401 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,411 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,422 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,430 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,430 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,434 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,438 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,438 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,442 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,442 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,442 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,446 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,450 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,450 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,450 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,454 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,458 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,466 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,474 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,476 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,478 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,481 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,482 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,482 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,494 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,496 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,498 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,501 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,502 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,502 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,522 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,525 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,530 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,534 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,542 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,550 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,566 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,646 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,670 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,682 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,700 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,700 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,700 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,765 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,785 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,800 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,806 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,681 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,872 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,922 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,922 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,933 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:50,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:50,988 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,006 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,029 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,098 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,129 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,130 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,008 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,202 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,205 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,257 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,272 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,301 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,313 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,425 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,486 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,518 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,602 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,669 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,702 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,734 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,762 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,772 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,782 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,786 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,814 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,840 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,850 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,852 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,858 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,862 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,866 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,881 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,900 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,909 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,918 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:51,958 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:51,997 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,018 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,022 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,042 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,109 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,118 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,153 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,214 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,229 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,245 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,277 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,319 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,338 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,341 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,470 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,661 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,724 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,754 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:52,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:52,857 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:53,184 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:53,222 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,869 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,893 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:54,942 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:54,942 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,006 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,042 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:55,112 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-23 23:49:55,134 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-23 23:49:59,131 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-23 23:49:59,301 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 745134 virtual documents
2025-12-23 23:49:59,678 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.5846248365329049, 'coherence_npmi': 0.108415847313208, 'topic_diversity': 0.6007518796992481, 'inter_topic_similarity': 0.4657121002674103}
2025-12-23 23:49:59,678 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6739606907387982, 'coherence_npmi': 0.16879817334005368, 'topic_diversity': 0.518, 'inter_topic_similarity': 0.4890018701553345}
2025-12-23 23:49:59,678 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6423531154003569, 'coherence_npmi': 0.16249052594835905, 'topic_diversity': 0.5837209302325581, 'inter_topic_similarity': 0.4533065855503082}
