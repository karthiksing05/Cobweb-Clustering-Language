2025-12-22 11:52:43,426 INFO __main__: Starting benchmark for dataset=reuters
2025-12-22 11:52:50,941 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-22 11:52:51,450 INFO gensim.corpora.dictionary: adding document #10000 to Dictionary<29631 unique tokens: ['10', '15', '17', '1985', '30']...>
2025-12-22 11:52:51,491 INFO gensim.corpora.dictionary: built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)
2025-12-22 11:52:51,495 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)", 'datetime': '2025-12-22T11:52:51.491963', 'gensim': '4.4.0', 'python': '3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-187-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-22 11:52:53,017 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-22 11:52:53,017 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-22 11:52:56,051 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10788 docs
2025-12-22 11:55:02,898 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 11:55:04,822 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-22 11:55:04,834 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-22 11:55:04,840 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-22 11:55:04,859 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-22 11:55:04,947 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-22 11:55:04,993 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-22 11:55:05,217 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-22 11:55:06,510 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-22 11:55:06,661 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-22 11:55:07,040 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-22 11:55:07,073 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-22 11:55:07,253 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-22 11:55:07,909 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-22 11:55:08,325 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-22 11:55:08,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:08,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,834 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:08,856 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:08,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:08,896 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:08,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:08,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:08,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,012 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,043 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,049 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,052 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,123 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,136 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,175 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,235 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,394 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,402 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,443 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,509 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,531 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,544 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,559 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,595 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,615 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,648 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,649 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,782 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,766 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,835 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,974 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,976 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,977 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:09,993 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,006 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:09,974 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,038 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,056 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,075 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,390 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,447 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,459 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,536 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,597 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,676 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,726 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:10,981 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:10,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:11,128 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:11,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:11,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:11,197 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:12,559 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 11:55:12,722 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-22 11:55:13,715 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 11:55:15,899 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-22 11:55:15,902 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-22 11:55:15,903 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-22 11:55:15,904 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-22 11:55:15,905 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-22 11:55:15,906 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-22 11:55:15,907 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-22 11:55:15,909 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-22 11:55:15,912 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-22 11:55:15,914 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-22 11:55:15,916 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-22 11:55:15,919 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-22 11:55:15,921 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-22 11:55:15,922 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-22 11:55:15,924 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-22 11:55:15,926 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-22 11:55:15,927 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-22 11:55:15,929 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-22 11:55:15,931 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-22 11:55:15,933 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-22 11:55:15,935 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-22 11:55:15,937 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-22 11:55:15,940 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-22 11:55:15,956 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-22 11:55:15,958 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-22 11:55:15,960 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-22 11:55:15,962 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-22 11:55:15,964 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-22 11:55:15,966 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-22 11:55:15,968 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-22 11:55:15,970 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-22 11:55:15,984 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-22 11:55:15,986 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-22 11:55:15,988 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-22 11:55:15,990 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-22 11:55:15,991 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-22 11:55:15,995 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-22 11:55:15,997 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-22 11:55:16,001 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-22 11:55:16,016 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-22 11:55:16,018 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-22 11:55:16,020 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-22 11:55:16,022 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-22 11:55:16,024 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-22 11:55:16,026 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-22 11:55:16,028 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-22 11:55:16,041 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-22 11:55:16,043 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-22 11:55:16,045 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-22 11:55:16,047 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-22 11:55:16,050 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-22 11:55:16,051 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-22 11:55:16,064 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-22 11:55:16,066 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-22 11:55:16,173 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-22 11:55:16,175 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-22 11:55:16,223 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-22 11:55:16,236 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-22 11:55:16,242 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-22 11:55:16,321 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-22 11:55:16,358 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-22 11:55:16,372 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-22 11:55:16,388 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-22 11:55:16,499 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-22 11:55:16,513 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-22 11:55:16,591 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-22 11:55:16,593 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-22 11:55:16,595 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-22 11:55:16,694 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-22 11:55:16,696 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-22 11:55:16,698 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-22 11:55:16,700 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-22 11:55:16,712 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-22 11:55:16,804 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-22 11:55:16,807 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-22 11:55:16,808 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-22 11:55:16,832 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-22 11:55:16,872 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-22 11:55:16,896 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-22 11:55:16,940 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-22 11:55:16,964 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-22 11:55:17,008 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-22 11:55:17,028 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-22 11:55:17,060 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-22 11:55:17,088 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-22 11:55:17,124 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-22 11:55:17,156 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-22 11:55:17,193 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-22 11:55:17,277 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-22 11:55:17,279 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-22 11:55:17,281 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-22 11:55:17,393 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-22 11:55:17,395 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-22 11:55:17,397 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-22 11:55:17,414 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-22 11:55:17,449 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-22 11:55:17,536 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-22 11:55:17,538 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-22 11:55:17,541 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-22 11:55:17,652 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-22 11:55:17,655 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-22 11:55:17,656 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-22 11:55:17,672 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-22 11:55:17,781 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-22 11:55:17,859 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-22 11:55:17,861 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-22 11:55:17,863 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-22 11:55:17,874 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-22 11:55:17,973 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-22 11:55:17,981 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-22 11:55:17,986 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-22 11:55:18,059 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-22 11:55:18,105 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-22 11:55:18,112 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-22 11:55:18,118 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-22 11:55:18,191 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-22 11:55:18,242 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-22 11:55:18,245 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-22 11:55:18,247 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-22 11:55:18,325 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-22 11:55:18,329 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-22 11:55:18,402 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-22 11:55:18,405 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-22 11:55:18,407 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-22 11:55:18,505 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-22 11:55:18,507 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-22 11:55:18,510 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-22 11:55:18,594 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-22 11:55:18,596 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-22 11:55:18,599 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-22 11:55:18,605 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-22 11:55:18,624 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-22 11:55:18,661 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-22 11:55:18,764 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-22 11:55:18,767 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-22 11:55:18,769 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-22 11:55:18,796 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-22 11:55:18,872 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-22 11:55:18,875 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-22 11:55:18,877 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-22 11:55:18,913 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-22 11:55:19,008 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-22 11:55:19,010 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-22 11:55:19,012 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-22 11:55:19,096 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-22 11:55:19,098 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-22 11:55:19,100 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-22 11:55:19,180 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-22 11:55:19,183 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-22 11:55:19,186 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-22 11:55:19,289 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-22 11:55:19,291 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-22 11:55:19,379 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-22 11:55:19,381 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-22 11:55:19,443 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-22 11:55:19,446 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-22 11:55:19,519 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-22 11:55:19,521 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-22 11:55:19,524 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-22 11:55:19,611 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-22 11:55:19,614 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-22 11:55:19,616 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-22 11:55:19,668 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-22 11:55:19,671 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-22 11:55:19,759 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-22 11:55:19,769 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-22 11:55:19,853 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-22 11:55:19,864 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-22 11:55:19,872 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-22 11:55:19,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,917 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,918 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,947 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,959 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:19,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:19,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,006 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,045 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,111 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,130 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,106 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,515 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,519 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,552 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,553 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,585 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,623 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,705 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,712 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,729 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,763 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,786 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,792 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,840 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,846 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,866 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,883 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,916 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,949 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,949 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,962 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,969 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:20,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:20,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,028 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,085 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,148 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,176 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,193 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,276 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,403 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,459 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:21,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:55:21,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:55:23,333 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 11:55:23,454 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-22 11:55:23,950 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10788 docs
2025-12-22 11:57:05,808 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 11:57:08,076 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-22 11:57:08,087 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-22 11:57:08,091 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-22 11:57:08,110 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-22 11:57:08,214 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-22 11:57:08,241 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-22 11:57:08,518 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-22 11:57:09,777 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-22 11:57:09,971 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-22 11:57:10,262 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-22 11:57:10,369 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-22 11:57:10,498 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-22 11:57:11,147 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-22 11:57:11,346 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-22 11:57:11,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,523 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,524 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,525 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,531 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,556 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,567 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,579 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,587 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,675 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,860 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,921 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,922 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,956 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,978 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,980 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:11,980 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:11,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,025 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,043 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,114 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,181 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,201 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,239 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,274 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,285 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,373 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,449 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,497 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,510 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,677 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,690 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,736 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,868 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,920 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:12,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:12,966 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:13,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:13,110 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:13,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:13,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:13,232 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:13,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:13,255 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:13,290 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:13,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:15,231 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 11:57:15,331 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-22 11:57:15,806 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 11:57:18,252 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-22 11:57:18,254 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-22 11:57:18,255 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-22 11:57:18,257 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-22 11:57:18,259 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-22 11:57:18,261 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-22 11:57:18,263 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-22 11:57:18,265 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-22 11:57:18,267 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-22 11:57:18,269 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-22 11:57:18,271 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-22 11:57:18,274 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-22 11:57:18,276 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-22 11:57:18,278 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-22 11:57:18,279 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-22 11:57:18,281 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-22 11:57:18,283 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-22 11:57:18,285 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-22 11:57:18,287 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-22 11:57:18,289 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-22 11:57:18,291 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-22 11:57:18,292 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-22 11:57:18,295 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-22 11:57:18,297 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-22 11:57:18,299 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-22 11:57:18,302 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-22 11:57:18,304 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-22 11:57:18,305 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-22 11:57:18,308 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-22 11:57:18,310 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-22 11:57:18,311 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-22 11:57:18,313 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-22 11:57:18,315 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-22 11:57:18,317 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-22 11:57:18,318 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-22 11:57:18,320 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-22 11:57:18,357 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-22 11:57:18,366 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-22 11:57:18,392 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-22 11:57:18,408 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-22 11:57:18,439 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-22 11:57:18,470 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-22 11:57:18,488 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-22 11:57:18,519 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-22 11:57:18,533 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-22 11:57:18,538 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-22 11:57:18,618 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-22 11:57:18,663 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-22 11:57:18,676 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-22 11:57:18,682 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-22 11:57:18,761 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-22 11:57:18,803 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-22 11:57:18,817 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-22 11:57:18,883 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-22 11:57:18,886 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-22 11:57:18,969 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-22 11:57:19,015 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-22 11:57:19,017 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-22 11:57:19,070 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-22 11:57:19,085 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-22 11:57:19,163 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-22 11:57:19,176 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-22 11:57:19,182 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-22 11:57:19,291 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-22 11:57:19,293 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-22 11:57:19,296 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-22 11:57:19,379 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-22 11:57:19,381 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-22 11:57:19,383 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-22 11:57:19,471 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-22 11:57:19,473 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-22 11:57:19,475 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-22 11:57:19,559 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-22 11:57:19,561 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-22 11:57:19,563 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-22 11:57:19,642 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-22 11:57:19,644 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-22 11:57:19,646 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-22 11:57:19,725 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-22 11:57:19,727 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-22 11:57:19,729 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-22 11:57:19,797 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-22 11:57:19,800 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-22 11:57:19,802 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-22 11:57:19,865 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-22 11:57:19,867 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-22 11:57:19,870 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-22 11:57:19,947 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-22 11:57:19,977 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-22 11:57:20,029 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-22 11:57:20,071 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-22 11:57:20,085 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-22 11:57:20,153 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-22 11:57:20,156 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-22 11:57:20,158 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-22 11:57:20,215 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-22 11:57:20,228 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-22 11:57:20,295 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-22 11:57:20,309 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-22 11:57:20,399 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-22 11:57:20,401 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-22 11:57:20,403 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-22 11:57:20,455 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-22 11:57:20,469 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-22 11:57:20,555 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-22 11:57:20,557 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-22 11:57:20,559 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-22 11:57:20,622 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-22 11:57:20,637 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-22 11:57:20,703 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-22 11:57:20,717 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-22 11:57:20,778 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-22 11:57:20,780 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-22 11:57:20,782 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-22 11:57:20,855 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-22 11:57:20,857 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-22 11:57:20,859 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-22 11:57:20,943 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-22 11:57:20,946 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-22 11:57:20,948 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-22 11:57:21,015 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-22 11:57:21,017 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-22 11:57:21,032 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-22 11:57:21,085 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-22 11:57:21,122 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-22 11:57:21,163 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-22 11:57:21,166 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-22 11:57:21,168 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-22 11:57:21,225 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-22 11:57:21,257 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-22 11:57:21,294 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-22 11:57:21,309 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-22 11:57:21,357 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-22 11:57:21,359 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-22 11:57:21,409 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-22 11:57:21,412 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-22 11:57:21,465 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-22 11:57:21,511 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-22 11:57:21,525 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-22 11:57:21,530 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-22 11:57:21,533 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-22 11:57:21,615 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-22 11:57:21,618 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-22 11:57:21,632 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-22 11:57:21,719 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-22 11:57:21,724 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-22 11:57:21,736 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-22 11:57:21,738 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-22 11:57:21,740 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-22 11:57:21,841 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-22 11:57:21,844 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-22 11:57:21,846 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-22 11:57:21,879 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-22 11:57:21,893 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-22 11:57:21,935 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-22 11:57:21,938 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-22 11:57:21,975 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-22 11:57:21,978 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-22 11:57:21,981 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-22 11:57:22,039 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-22 11:57:22,042 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-22 11:57:22,044 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-22 11:57:22,107 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-22 11:57:22,120 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-22 11:57:22,126 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-22 11:57:22,185 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-22 11:57:22,188 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-22 11:57:22,190 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-22 11:57:22,237 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-22 11:57:22,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,248 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,249 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,249 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,250 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,250 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,259 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,267 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,271 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,275 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,295 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,303 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,311 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,372 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,433 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,463 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,497 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,605 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,550 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,656 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,666 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,688 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,696 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,844 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,857 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,867 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,898 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,909 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:22,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,971 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:22,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,016 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,098 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,103 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,278 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,312 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,363 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,391 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,478 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,498 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,513 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,527 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,543 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,641 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,710 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,734 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:23,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 11:57:23,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 11:57:25,409 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 11:57:25,496 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-22 11:57:25,880 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10788 docs
Training CobwebTree:   0%|          | 0/10788 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 20/10788 [00:00<00:56, 190.27it/s]Training CobwebTree:   0%|          | 40/10788 [00:00<01:32, 116.72it/s]Training CobwebTree:   1%|          | 54/10788 [00:00<01:48, 98.85it/s] Training CobwebTree:   1%|          | 65/10788 [00:00<01:50, 97.45it/s]Training CobwebTree:   1%|          | 76/10788 [00:00<02:02, 87.17it/s]Training CobwebTree:   1%|          | 86/10788 [00:00<02:19, 76.90it/s]Training CobwebTree:   1%|          | 94/10788 [00:01<02:23, 74.49it/s]Training CobwebTree:   1%|          | 102/10788 [00:01<02:29, 71.38it/s]Training CobwebTree:   1%|          | 110/10788 [00:01<02:32, 70.16it/s]Training CobwebTree:   1%|          | 118/10788 [00:01<02:31, 70.57it/s]Training CobwebTree:   1%|          | 126/10788 [00:01<02:36, 68.11it/s]Training CobwebTree:   1%|          | 134/10788 [00:01<02:34, 69.17it/s]Training CobwebTree:   1%|         | 141/10788 [00:01<02:38, 67.24it/s]Training CobwebTree:   1%|         | 148/10788 [00:01<02:44, 64.66it/s]Training CobwebTree:   1%|         | 155/10788 [00:02<02:55, 60.67it/s]Training CobwebTree:   2%|         | 162/10788 [00:02<02:55, 60.66it/s]Training CobwebTree:   2%|         | 169/10788 [00:02<02:58, 59.43it/s]Training CobwebTree:   2%|         | 175/10788 [00:02<03:07, 56.73it/s]Training CobwebTree:   2%|         | 182/10788 [00:02<03:01, 58.29it/s]Training CobwebTree:   2%|         | 189/10788 [00:02<02:58, 59.24it/s]Training CobwebTree:   2%|         | 195/10788 [00:02<03:00, 58.58it/s]Training CobwebTree:   2%|         | 201/10788 [00:02<03:04, 57.33it/s]Training CobwebTree:   2%|         | 207/10788 [00:02<03:10, 55.56it/s]Training CobwebTree:   2%|         | 213/10788 [00:03<03:10, 55.48it/s]Training CobwebTree:   2%|         | 219/10788 [00:03<03:11, 55.10it/s]Training CobwebTree:   2%|         | 225/10788 [00:03<03:14, 54.30it/s]Training CobwebTree:   2%|         | 231/10788 [00:03<03:24, 51.52it/s]Training CobwebTree:   2%|         | 237/10788 [00:03<03:26, 51.09it/s]Training CobwebTree:   2%|         | 243/10788 [00:03<03:29, 50.25it/s]Training CobwebTree:   2%|         | 249/10788 [00:03<03:35, 48.81it/s]Training CobwebTree:   2%|         | 254/10788 [00:03<03:35, 48.82it/s]Training CobwebTree:   2%|         | 259/10788 [00:03<03:41, 47.60it/s]Training CobwebTree:   2%|         | 264/10788 [00:04<03:39, 47.86it/s]Training CobwebTree:   2%|         | 269/10788 [00:04<03:40, 47.67it/s]Training CobwebTree:   3%|         | 275/10788 [00:04<03:26, 50.80it/s]Training CobwebTree:   3%|         | 281/10788 [00:04<03:25, 51.03it/s]Training CobwebTree:   3%|         | 287/10788 [00:04<03:22, 51.76it/s]Training CobwebTree:   3%|         | 294/10788 [00:04<03:09, 55.27it/s]Training CobwebTree:   3%|         | 300/10788 [00:04<03:22, 51.80it/s]Training CobwebTree:   3%|         | 306/10788 [00:04<03:16, 53.32it/s]Training CobwebTree:   3%|         | 312/10788 [00:05<03:19, 52.63it/s]Training CobwebTree:   3%|         | 318/10788 [00:05<03:17, 53.09it/s]Training CobwebTree:   3%|         | 324/10788 [00:05<03:21, 51.99it/s]Training CobwebTree:   3%|         | 330/10788 [00:05<03:29, 49.88it/s]Training CobwebTree:   3%|         | 336/10788 [00:05<03:32, 49.29it/s]Training CobwebTree:   3%|         | 341/10788 [00:05<03:34, 48.73it/s]Training CobwebTree:   3%|         | 346/10788 [00:05<03:39, 47.64it/s]Training CobwebTree:   3%|         | 351/10788 [00:05<03:38, 47.81it/s]Training CobwebTree:   3%|         | 357/10788 [00:05<03:30, 49.66it/s]Training CobwebTree:   3%|         | 362/10788 [00:06<03:37, 47.85it/s]Training CobwebTree:   3%|         | 368/10788 [00:06<03:25, 50.72it/s]Training CobwebTree:   3%|         | 374/10788 [00:06<03:27, 50.16it/s]Training CobwebTree:   4%|         | 380/10788 [00:06<03:28, 49.82it/s]Training CobwebTree:   4%|         | 385/10788 [00:06<03:32, 48.98it/s]Training CobwebTree:   4%|         | 392/10788 [00:06<03:20, 51.90it/s]Training CobwebTree:   4%|         | 398/10788 [00:06<03:29, 49.69it/s]Training CobwebTree:   4%|         | 403/10788 [00:06<03:40, 47.07it/s]Training CobwebTree:   4%|         | 408/10788 [00:06<03:49, 45.27it/s]Training CobwebTree:   4%|         | 413/10788 [00:07<03:50, 45.10it/s]Training CobwebTree:   4%|         | 419/10788 [00:07<03:33, 48.57it/s]Training CobwebTree:   4%|         | 424/10788 [00:07<03:34, 48.34it/s]Training CobwebTree:   4%|         | 429/10788 [00:07<03:35, 48.13it/s]Training CobwebTree:   4%|         | 434/10788 [00:07<03:35, 47.98it/s]Training CobwebTree:   4%|         | 439/10788 [00:07<03:47, 45.41it/s]Training CobwebTree:   4%|         | 444/10788 [00:07<03:43, 46.30it/s]Training CobwebTree:   4%|         | 449/10788 [00:07<04:08, 41.63it/s]Training CobwebTree:   4%|         | 456/10788 [00:08<03:44, 45.93it/s]Training CobwebTree:   4%|         | 462/10788 [00:08<03:34, 48.04it/s]Training CobwebTree:   4%|         | 467/10788 [00:08<03:42, 46.43it/s]Training CobwebTree:   4%|         | 472/10788 [00:08<03:41, 46.53it/s]Training CobwebTree:   4%|         | 477/10788 [00:08<03:46, 45.58it/s]Training CobwebTree:   4%|         | 482/10788 [00:08<03:46, 45.56it/s]Training CobwebTree:   5%|         | 487/10788 [00:08<03:41, 46.46it/s]Training CobwebTree:   5%|         | 492/10788 [00:08<03:40, 46.73it/s]Training CobwebTree:   5%|         | 497/10788 [00:08<03:58, 43.16it/s]Training CobwebTree:   5%|         | 502/10788 [00:09<03:49, 44.92it/s]Training CobwebTree:   5%|         | 507/10788 [00:09<03:45, 45.63it/s]Training CobwebTree:   5%|         | 513/10788 [00:09<03:36, 47.51it/s]Training CobwebTree:   5%|         | 518/10788 [00:09<03:39, 46.82it/s]Training CobwebTree:   5%|         | 523/10788 [00:09<03:49, 44.74it/s]Training CobwebTree:   5%|         | 528/10788 [00:09<03:50, 44.51it/s]Training CobwebTree:   5%|         | 533/10788 [00:09<03:47, 45.07it/s]Training CobwebTree:   5%|         | 538/10788 [00:09<03:42, 45.98it/s]Training CobwebTree:   5%|         | 543/10788 [00:09<03:38, 46.81it/s]Training CobwebTree:   5%|         | 548/10788 [00:10<03:35, 47.61it/s]Training CobwebTree:   5%|         | 553/10788 [00:10<03:45, 45.38it/s]Training CobwebTree:   5%|         | 558/10788 [00:10<03:54, 43.64it/s]Training CobwebTree:   5%|         | 563/10788 [00:10<04:04, 41.74it/s]Training CobwebTree:   5%|         | 568/10788 [00:10<04:25, 38.43it/s]Training CobwebTree:   5%|         | 572/10788 [00:10<04:38, 36.66it/s]Training CobwebTree:   5%|         | 576/10788 [00:10<04:48, 35.37it/s]Training CobwebTree:   5%|         | 581/10788 [00:10<04:26, 38.25it/s]Training CobwebTree:   5%|         | 586/10788 [00:11<04:18, 39.51it/s]Training CobwebTree:   5%|         | 591/10788 [00:11<04:06, 41.36it/s]Training CobwebTree:   6%|         | 597/10788 [00:11<03:47, 44.73it/s]Training CobwebTree:   6%|         | 602/10788 [00:11<03:47, 44.81it/s]Training CobwebTree:   6%|         | 607/10788 [00:11<03:55, 43.18it/s]Training CobwebTree:   6%|         | 612/10788 [00:11<03:59, 42.44it/s]Training CobwebTree:   6%|         | 617/10788 [00:11<04:02, 41.97it/s]Training CobwebTree:   6%|         | 623/10788 [00:11<03:43, 45.52it/s]Training CobwebTree:   6%|         | 629/10788 [00:11<03:26, 49.22it/s]Training CobwebTree:   6%|         | 634/10788 [00:12<03:27, 48.92it/s]Training CobwebTree:   6%|         | 639/10788 [00:12<03:31, 48.05it/s]Training CobwebTree:   6%|         | 645/10788 [00:12<03:30, 48.26it/s]Training CobwebTree:   6%|         | 650/10788 [00:12<03:31, 47.93it/s]Training CobwebTree:   6%|         | 656/10788 [00:12<03:25, 49.25it/s]Training CobwebTree:   6%|         | 661/10788 [00:12<03:25, 49.24it/s]Training CobwebTree:   6%|         | 666/10788 [00:12<03:26, 49.02it/s]Training CobwebTree:   6%|         | 672/10788 [00:12<03:31, 47.87it/s]Training CobwebTree:   6%|         | 677/10788 [00:12<03:40, 45.78it/s]Training CobwebTree:   6%|         | 683/10788 [00:13<03:33, 47.26it/s]Training CobwebTree:   6%|         | 689/10788 [00:13<03:27, 48.67it/s]Training CobwebTree:   6%|         | 694/10788 [00:13<03:28, 48.33it/s]Training CobwebTree:   6%|         | 699/10788 [00:13<03:36, 46.66it/s]Training CobwebTree:   7%|         | 704/10788 [00:13<03:36, 46.60it/s]Training CobwebTree:   7%|         | 709/10788 [00:13<03:40, 45.77it/s]Training CobwebTree:   7%|         | 714/10788 [00:13<03:37, 46.26it/s]Training CobwebTree:   7%|         | 719/10788 [00:13<03:35, 46.76it/s]Training CobwebTree:   7%|         | 725/10788 [00:13<03:27, 48.49it/s]Training CobwebTree:   7%|         | 730/10788 [00:14<03:26, 48.74it/s]Training CobwebTree:   7%|         | 735/10788 [00:14<03:33, 47.04it/s]Training CobwebTree:   7%|         | 740/10788 [00:14<03:43, 44.93it/s]Training CobwebTree:   7%|         | 745/10788 [00:14<03:50, 43.61it/s]Training CobwebTree:   7%|         | 751/10788 [00:14<03:37, 46.06it/s]Training CobwebTree:   7%|         | 756/10788 [00:14<03:39, 45.65it/s]Training CobwebTree:   7%|         | 761/10788 [00:14<03:45, 44.50it/s]Training CobwebTree:   7%|         | 766/10788 [00:14<03:41, 45.31it/s]Training CobwebTree:   7%|         | 771/10788 [00:14<03:47, 43.97it/s]Training CobwebTree:   7%|         | 776/10788 [00:15<03:46, 44.22it/s]Training CobwebTree:   7%|         | 781/10788 [00:15<03:45, 44.42it/s]Training CobwebTree:   7%|         | 786/10788 [00:15<03:50, 43.47it/s]Training CobwebTree:   7%|         | 791/10788 [00:15<03:54, 42.58it/s]Training CobwebTree:   7%|         | 796/10788 [00:15<03:44, 44.49it/s]Training CobwebTree:   7%|         | 801/10788 [00:15<03:42, 44.94it/s]Training CobwebTree:   7%|         | 807/10788 [00:15<03:33, 46.70it/s]Training CobwebTree:   8%|         | 812/10788 [00:15<03:49, 43.47it/s]Training CobwebTree:   8%|         | 817/10788 [00:16<03:51, 43.07it/s]Training CobwebTree:   8%|         | 822/10788 [00:16<03:46, 44.02it/s]Training CobwebTree:   8%|         | 828/10788 [00:16<03:29, 47.54it/s]Training CobwebTree:   8%|         | 833/10788 [00:16<03:27, 48.01it/s]Training CobwebTree:   8%|         | 838/10788 [00:16<03:29, 47.56it/s]Training CobwebTree:   8%|         | 843/10788 [00:16<03:28, 47.80it/s]Training CobwebTree:   8%|         | 848/10788 [00:16<03:32, 46.70it/s]Training CobwebTree:   8%|         | 853/10788 [00:16<03:54, 42.38it/s]Training CobwebTree:   8%|         | 858/10788 [00:16<03:58, 41.63it/s]Training CobwebTree:   8%|         | 863/10788 [00:17<03:54, 42.25it/s]Training CobwebTree:   8%|         | 868/10788 [00:17<04:00, 41.23it/s]Training CobwebTree:   8%|         | 873/10788 [00:17<04:02, 40.97it/s]Training CobwebTree:   8%|         | 878/10788 [00:17<04:06, 40.18it/s]Training CobwebTree:   8%|         | 883/10788 [00:17<03:58, 41.44it/s]Training CobwebTree:   8%|         | 888/10788 [00:17<03:47, 43.48it/s]Training CobwebTree:   8%|         | 893/10788 [00:17<03:42, 44.38it/s]Training CobwebTree:   8%|         | 898/10788 [00:17<03:44, 44.12it/s]Training CobwebTree:   8%|         | 903/10788 [00:17<03:47, 43.40it/s]Training CobwebTree:   8%|         | 908/10788 [00:18<03:43, 44.23it/s]Training CobwebTree:   8%|         | 913/10788 [00:18<03:41, 44.68it/s]Training CobwebTree:   9%|         | 918/10788 [00:18<03:45, 43.86it/s]Training CobwebTree:   9%|         | 923/10788 [00:18<03:40, 44.68it/s]Training CobwebTree:   9%|         | 928/10788 [00:18<03:37, 45.26it/s]Training CobwebTree:   9%|         | 933/10788 [00:18<03:39, 44.91it/s]Training CobwebTree:   9%|         | 938/10788 [00:18<03:33, 46.11it/s]Training CobwebTree:   9%|         | 944/10788 [00:18<03:21, 48.82it/s]Training CobwebTree:   9%|         | 949/10788 [00:18<03:22, 48.65it/s]Training CobwebTree:   9%|         | 954/10788 [00:19<03:35, 45.74it/s]Training CobwebTree:   9%|         | 959/10788 [00:19<03:34, 45.86it/s]Training CobwebTree:   9%|         | 964/10788 [00:19<03:43, 44.05it/s]Training CobwebTree:   9%|         | 969/10788 [00:19<03:41, 44.34it/s]Training CobwebTree:   9%|         | 974/10788 [00:19<03:46, 43.39it/s]Training CobwebTree:   9%|         | 979/10788 [00:19<03:39, 44.74it/s]Training CobwebTree:   9%|         | 984/10788 [00:19<03:46, 43.35it/s]Training CobwebTree:   9%|         | 990/10788 [00:19<03:35, 45.44it/s]Training CobwebTree:   9%|         | 995/10788 [00:20<03:38, 44.79it/s]Training CobwebTree:   9%|         | 1000/10788 [00:20<03:46, 43.23it/s]Training CobwebTree:   9%|         | 1005/10788 [00:20<03:49, 42.60it/s]Training CobwebTree:   9%|         | 1010/10788 [00:20<03:40, 44.37it/s]Training CobwebTree:   9%|         | 1015/10788 [00:20<03:40, 44.24it/s]Training CobwebTree:   9%|         | 1020/10788 [00:20<03:40, 44.34it/s]Training CobwebTree:  10%|         | 1025/10788 [00:20<03:34, 45.44it/s]Training CobwebTree:  10%|         | 1030/10788 [00:20<03:30, 46.35it/s]Training CobwebTree:  10%|         | 1035/10788 [00:20<03:34, 45.44it/s]Training CobwebTree:  10%|         | 1041/10788 [00:21<03:24, 47.59it/s]Training CobwebTree:  10%|         | 1046/10788 [00:21<03:38, 44.61it/s]Training CobwebTree:  10%|         | 1051/10788 [00:21<03:35, 45.17it/s]Training CobwebTree:  10%|         | 1056/10788 [00:21<03:31, 46.02it/s]Training CobwebTree:  10%|         | 1061/10788 [00:21<03:42, 43.68it/s]Training CobwebTree:  10%|         | 1066/10788 [00:21<03:49, 42.41it/s]Training CobwebTree:  10%|         | 1071/10788 [00:21<03:48, 42.48it/s]Training CobwebTree:  10%|         | 1076/10788 [00:21<03:46, 42.80it/s]Training CobwebTree:  10%|         | 1081/10788 [00:21<03:47, 42.67it/s]Training CobwebTree:  10%|         | 1086/10788 [00:22<03:58, 40.72it/s]Training CobwebTree:  10%|         | 1091/10788 [00:22<03:52, 41.71it/s]Training CobwebTree:  10%|         | 1096/10788 [00:22<03:47, 42.63it/s]Training CobwebTree:  10%|         | 1101/10788 [00:22<03:47, 42.65it/s]Training CobwebTree:  10%|         | 1106/10788 [00:22<03:39, 44.15it/s]Training CobwebTree:  10%|         | 1111/10788 [00:22<03:33, 45.35it/s]Training CobwebTree:  10%|         | 1116/10788 [00:22<03:33, 45.28it/s]Training CobwebTree:  10%|         | 1121/10788 [00:22<03:39, 44.06it/s]Training CobwebTree:  10%|         | 1126/10788 [00:22<03:33, 45.34it/s]Training CobwebTree:  10%|         | 1131/10788 [00:23<03:29, 46.10it/s]Training CobwebTree:  11%|         | 1136/10788 [00:23<03:42, 43.43it/s]Training CobwebTree:  11%|         | 1141/10788 [00:23<03:42, 43.33it/s]Training CobwebTree:  11%|         | 1146/10788 [00:23<03:41, 43.45it/s]Training CobwebTree:  11%|         | 1151/10788 [00:23<03:35, 44.79it/s]Training CobwebTree:  11%|         | 1157/10788 [00:23<03:26, 46.63it/s]Training CobwebTree:  11%|         | 1162/10788 [00:23<03:31, 45.61it/s]Training CobwebTree:  11%|         | 1167/10788 [00:23<03:29, 45.91it/s]Training CobwebTree:  11%|         | 1172/10788 [00:24<03:39, 43.71it/s]Training CobwebTree:  11%|         | 1177/10788 [00:24<03:39, 43.83it/s]Training CobwebTree:  11%|         | 1183/10788 [00:24<03:32, 45.22it/s]Training CobwebTree:  11%|         | 1188/10788 [00:24<03:51, 41.41it/s]Training CobwebTree:  11%|         | 1193/10788 [00:24<03:52, 41.34it/s]Training CobwebTree:  11%|         | 1198/10788 [00:24<03:44, 42.70it/s]Training CobwebTree:  11%|         | 1203/10788 [00:24<03:41, 43.30it/s]Training CobwebTree:  11%|         | 1208/10788 [00:24<03:45, 42.50it/s]Training CobwebTree:  11%|         | 1213/10788 [00:24<03:39, 43.54it/s]Training CobwebTree:  11%|        | 1218/10788 [00:25<03:35, 44.35it/s]Training CobwebTree:  11%|        | 1223/10788 [00:25<03:29, 45.62it/s]Training CobwebTree:  11%|        | 1228/10788 [00:25<03:34, 44.60it/s]Training CobwebTree:  11%|        | 1233/10788 [00:25<03:33, 44.81it/s]Training CobwebTree:  11%|        | 1238/10788 [00:25<03:36, 44.08it/s]Training CobwebTree:  12%|        | 1243/10788 [00:25<03:42, 42.87it/s]Training CobwebTree:  12%|        | 1248/10788 [00:25<03:59, 39.90it/s]Training CobwebTree:  12%|        | 1253/10788 [00:25<03:52, 40.97it/s]Training CobwebTree:  12%|        | 1258/10788 [00:26<03:50, 41.39it/s]Training CobwebTree:  12%|        | 1263/10788 [00:26<03:52, 40.89it/s]Training CobwebTree:  12%|        | 1268/10788 [00:26<03:54, 40.57it/s]Training CobwebTree:  12%|        | 1273/10788 [00:26<03:49, 41.51it/s]Training CobwebTree:  12%|        | 1278/10788 [00:26<03:49, 41.53it/s]Training CobwebTree:  12%|        | 1283/10788 [00:26<03:43, 42.62it/s]Training CobwebTree:  12%|        | 1288/10788 [00:26<03:43, 42.47it/s]Training CobwebTree:  12%|        | 1293/10788 [00:26<03:41, 42.88it/s]Training CobwebTree:  12%|        | 1298/10788 [00:26<03:33, 44.43it/s]Training CobwebTree:  12%|        | 1303/10788 [00:27<03:38, 43.40it/s]Training CobwebTree:  12%|        | 1308/10788 [00:27<03:39, 43.15it/s]Training CobwebTree:  12%|        | 1313/10788 [00:27<03:37, 43.65it/s]Training CobwebTree:  12%|        | 1318/10788 [00:27<03:40, 43.02it/s]Training CobwebTree:  12%|        | 1323/10788 [00:27<03:50, 41.01it/s]Training CobwebTree:  12%|        | 1328/10788 [00:27<03:50, 41.08it/s]Training CobwebTree:  12%|        | 1334/10788 [00:27<03:37, 43.37it/s]Training CobwebTree:  12%|        | 1339/10788 [00:27<03:36, 43.63it/s]Training CobwebTree:  12%|        | 1344/10788 [00:28<03:43, 42.20it/s]Training CobwebTree:  13%|        | 1349/10788 [00:28<03:43, 42.28it/s]Training CobwebTree:  13%|        | 1354/10788 [00:28<03:46, 41.60it/s]Training CobwebTree:  13%|        | 1359/10788 [00:28<03:39, 43.04it/s]Training CobwebTree:  13%|        | 1364/10788 [00:28<03:40, 42.70it/s]Training CobwebTree:  13%|        | 1369/10788 [00:28<03:49, 41.07it/s]Training CobwebTree:  13%|        | 1374/10788 [00:28<03:41, 42.47it/s]Training CobwebTree:  13%|        | 1379/10788 [00:28<03:43, 42.06it/s]Training CobwebTree:  13%|        | 1384/10788 [00:29<03:51, 40.59it/s]Training CobwebTree:  13%|        | 1389/10788 [00:29<03:59, 39.32it/s]Training CobwebTree:  13%|        | 1394/10788 [00:29<03:59, 39.28it/s]Training CobwebTree:  13%|        | 1398/10788 [00:29<03:58, 39.44it/s]Training CobwebTree:  13%|        | 1403/10788 [00:29<03:54, 39.99it/s]Training CobwebTree:  13%|        | 1408/10788 [00:29<03:57, 39.49it/s]Training CobwebTree:  13%|        | 1412/10788 [00:29<03:57, 39.53it/s]Training CobwebTree:  13%|        | 1417/10788 [00:29<03:50, 40.66it/s]Training CobwebTree:  13%|        | 1422/10788 [00:30<03:57, 39.44it/s]Training CobwebTree:  13%|        | 1427/10788 [00:30<03:46, 41.40it/s]Training CobwebTree:  13%|        | 1432/10788 [00:30<03:38, 42.83it/s]Training CobwebTree:  13%|        | 1437/10788 [00:30<03:40, 42.45it/s]Training CobwebTree:  13%|        | 1442/10788 [00:30<03:41, 42.22it/s]Training CobwebTree:  13%|        | 1447/10788 [00:30<03:42, 42.08it/s]Training CobwebTree:  13%|        | 1452/10788 [00:30<03:38, 42.78it/s]Training CobwebTree:  14%|        | 1457/10788 [00:30<03:41, 42.05it/s]Training CobwebTree:  14%|        | 1462/10788 [00:30<03:50, 40.50it/s]Training CobwebTree:  14%|        | 1467/10788 [00:31<03:47, 40.91it/s]Training CobwebTree:  14%|        | 1472/10788 [00:31<03:47, 41.00it/s]Training CobwebTree:  14%|        | 1477/10788 [00:31<03:45, 41.20it/s]Training CobwebTree:  14%|        | 1482/10788 [00:31<03:39, 42.47it/s]Training CobwebTree:  14%|        | 1487/10788 [00:31<03:40, 42.26it/s]Training CobwebTree:  14%|        | 1492/10788 [00:31<03:47, 40.91it/s]Training CobwebTree:  14%|        | 1497/10788 [00:31<03:55, 39.47it/s]Training CobwebTree:  14%|        | 1502/10788 [00:31<03:44, 41.29it/s]Training CobwebTree:  14%|        | 1507/10788 [00:32<03:45, 41.22it/s]Training CobwebTree:  14%|        | 1512/10788 [00:32<03:43, 41.43it/s]Training CobwebTree:  14%|        | 1517/10788 [00:32<03:45, 41.19it/s]Training CobwebTree:  14%|        | 1522/10788 [00:32<03:40, 42.01it/s]Training CobwebTree:  14%|        | 1527/10788 [00:32<03:52, 39.75it/s]Training CobwebTree:  14%|        | 1532/10788 [00:32<03:52, 39.75it/s]Training CobwebTree:  14%|        | 1536/10788 [00:32<03:54, 39.52it/s]Training CobwebTree:  14%|        | 1541/10788 [00:32<03:42, 41.50it/s]Training CobwebTree:  14%|        | 1546/10788 [00:32<03:45, 40.92it/s]Training CobwebTree:  14%|        | 1551/10788 [00:33<03:42, 41.44it/s]Training CobwebTree:  14%|        | 1556/10788 [00:33<03:34, 43.01it/s]Training CobwebTree:  14%|        | 1561/10788 [00:33<03:37, 42.50it/s]Training CobwebTree:  15%|        | 1566/10788 [00:33<03:48, 40.31it/s]Training CobwebTree:  15%|        | 1571/10788 [00:33<03:47, 40.47it/s]Training CobwebTree:  15%|        | 1576/10788 [00:33<03:48, 40.37it/s]Training CobwebTree:  15%|        | 1581/10788 [00:33<03:37, 42.31it/s]Training CobwebTree:  15%|        | 1586/10788 [00:33<03:45, 40.90it/s]Training CobwebTree:  15%|        | 1592/10788 [00:34<03:29, 43.83it/s]Training CobwebTree:  15%|        | 1597/10788 [00:34<03:24, 44.99it/s]Training CobwebTree:  15%|        | 1602/10788 [00:34<03:29, 43.91it/s]Training CobwebTree:  15%|        | 1607/10788 [00:34<03:40, 41.55it/s]Training CobwebTree:  15%|        | 1612/10788 [00:34<03:39, 41.84it/s]Training CobwebTree:  15%|        | 1617/10788 [00:34<03:33, 42.86it/s]Training CobwebTree:  15%|        | 1622/10788 [00:34<03:32, 43.20it/s]Training CobwebTree:  15%|        | 1627/10788 [00:34<03:38, 41.88it/s]Training CobwebTree:  15%|        | 1632/10788 [00:35<03:40, 41.47it/s]Training CobwebTree:  15%|        | 1637/10788 [00:35<03:37, 42.16it/s]Training CobwebTree:  15%|        | 1642/10788 [00:35<03:42, 41.06it/s]Training CobwebTree:  15%|        | 1648/10788 [00:35<03:26, 44.18it/s]Training CobwebTree:  15%|        | 1653/10788 [00:35<03:28, 43.78it/s]Training CobwebTree:  15%|        | 1658/10788 [00:35<03:30, 43.38it/s]Training CobwebTree:  15%|        | 1663/10788 [00:35<03:25, 44.39it/s]Training CobwebTree:  15%|        | 1668/10788 [00:35<03:25, 44.34it/s]Training CobwebTree:  16%|        | 1673/10788 [00:35<03:30, 43.32it/s]Training CobwebTree:  16%|        | 1678/10788 [00:36<03:34, 42.43it/s]Training CobwebTree:  16%|        | 1683/10788 [00:36<03:43, 40.82it/s]Training CobwebTree:  16%|        | 1689/10788 [00:36<03:30, 43.30it/s]Training CobwebTree:  16%|        | 1694/10788 [00:36<03:38, 41.55it/s]Training CobwebTree:  16%|        | 1699/10788 [00:36<03:36, 42.07it/s]Training CobwebTree:  16%|        | 1704/10788 [00:36<03:40, 41.23it/s]Training CobwebTree:  16%|        | 1709/10788 [00:36<03:37, 41.80it/s]Training CobwebTree:  16%|        | 1715/10788 [00:36<03:25, 44.19it/s]Training CobwebTree:  16%|        | 1721/10788 [00:37<03:13, 46.85it/s]Training CobwebTree:  16%|        | 1726/10788 [00:37<03:11, 47.24it/s]Training CobwebTree:  16%|        | 1731/10788 [00:37<03:22, 44.78it/s]Training CobwebTree:  16%|        | 1736/10788 [00:37<03:30, 43.00it/s]Training CobwebTree:  16%|        | 1741/10788 [00:37<03:35, 42.02it/s]Training CobwebTree:  16%|        | 1746/10788 [00:37<03:27, 43.67it/s]Training CobwebTree:  16%|        | 1751/10788 [00:37<03:39, 41.09it/s]Training CobwebTree:  16%|        | 1756/10788 [00:37<03:47, 39.78it/s]Training CobwebTree:  16%|        | 1761/10788 [00:38<03:40, 40.86it/s]Training CobwebTree:  16%|        | 1766/10788 [00:38<03:39, 41.16it/s]Training CobwebTree:  16%|        | 1771/10788 [00:38<03:41, 40.65it/s]Training CobwebTree:  16%|        | 1776/10788 [00:38<03:50, 39.03it/s]Training CobwebTree:  17%|        | 1781/10788 [00:38<03:47, 39.55it/s]Training CobwebTree:  17%|        | 1785/10788 [00:38<03:51, 38.87it/s]Training CobwebTree:  17%|        | 1789/10788 [00:38<03:59, 37.51it/s]Training CobwebTree:  17%|        | 1794/10788 [00:38<03:52, 38.66it/s]Training CobwebTree:  17%|        | 1798/10788 [00:39<03:55, 38.20it/s]Training CobwebTree:  17%|        | 1802/10788 [00:39<03:53, 38.53it/s]Training CobwebTree:  17%|        | 1806/10788 [00:39<03:51, 38.85it/s]Training CobwebTree:  17%|        | 1810/10788 [00:39<04:06, 36.44it/s]Training CobwebTree:  17%|        | 1815/10788 [00:39<03:57, 37.84it/s]Training CobwebTree:  17%|        | 1820/10788 [00:39<03:52, 38.59it/s]Training CobwebTree:  17%|        | 1824/10788 [00:39<03:56, 37.92it/s]Training CobwebTree:  17%|        | 1828/10788 [00:39<04:02, 36.97it/s]Training CobwebTree:  17%|        | 1832/10788 [00:39<04:03, 36.83it/s]Training CobwebTree:  17%|        | 1836/10788 [00:40<04:02, 36.95it/s]Training CobwebTree:  17%|        | 1841/10788 [00:40<03:53, 38.31it/s]Training CobwebTree:  17%|        | 1845/10788 [00:40<03:51, 38.71it/s]Training CobwebTree:  17%|        | 1850/10788 [00:40<03:35, 41.43it/s]Training CobwebTree:  17%|        | 1855/10788 [00:40<03:29, 42.57it/s]Training CobwebTree:  17%|        | 1860/10788 [00:40<03:36, 41.16it/s]Training CobwebTree:  17%|        | 1865/10788 [00:40<03:43, 39.89it/s]Training CobwebTree:  17%|        | 1870/10788 [00:40<03:35, 41.46it/s]Training CobwebTree:  17%|        | 1875/10788 [00:40<03:46, 39.40it/s]Training CobwebTree:  17%|        | 1880/10788 [00:41<03:36, 41.20it/s]Training CobwebTree:  17%|        | 1885/10788 [00:41<03:38, 40.78it/s]Training CobwebTree:  18%|        | 1890/10788 [00:41<03:40, 40.41it/s]Training CobwebTree:  18%|        | 1895/10788 [00:41<03:48, 38.94it/s]Training CobwebTree:  18%|        | 1899/10788 [00:41<03:54, 37.94it/s]Training CobwebTree:  18%|        | 1904/10788 [00:41<03:47, 39.01it/s]Training CobwebTree:  18%|        | 1909/10788 [00:41<03:42, 39.92it/s]Training CobwebTree:  18%|        | 1914/10788 [00:41<03:34, 41.36it/s]Training CobwebTree:  18%|        | 1919/10788 [00:42<03:38, 40.60it/s]Training CobwebTree:  18%|        | 1924/10788 [00:42<03:43, 39.58it/s]Training CobwebTree:  18%|        | 1928/10788 [00:42<03:43, 39.66it/s]Training CobwebTree:  18%|        | 1932/10788 [00:42<03:49, 38.58it/s]Training CobwebTree:  18%|        | 1936/10788 [00:42<03:52, 38.10it/s]Training CobwebTree:  18%|        | 1941/10788 [00:42<03:46, 39.12it/s]Training CobwebTree:  18%|        | 1945/10788 [00:42<03:46, 39.06it/s]Training CobwebTree:  18%|        | 1949/10788 [00:42<03:46, 39.09it/s]Training CobwebTree:  18%|        | 1953/10788 [00:42<03:54, 37.63it/s]Training CobwebTree:  18%|        | 1958/10788 [00:43<03:49, 38.41it/s]Training CobwebTree:  18%|        | 1962/10788 [00:43<03:53, 37.77it/s]Training CobwebTree:  18%|        | 1967/10788 [00:43<03:39, 40.16it/s]Training CobwebTree:  18%|        | 1972/10788 [00:43<03:36, 40.75it/s]Training CobwebTree:  18%|        | 1977/10788 [00:43<03:53, 37.69it/s]Training CobwebTree:  18%|        | 1982/10788 [00:43<03:47, 38.69it/s]Training CobwebTree:  18%|        | 1986/10788 [00:43<03:45, 38.96it/s]Training CobwebTree:  18%|        | 1991/10788 [00:43<03:40, 39.96it/s]Training CobwebTree:  19%|        | 1996/10788 [00:44<03:45, 39.07it/s]Training CobwebTree:  19%|        | 2001/10788 [00:44<03:41, 39.64it/s]Training CobwebTree:  19%|        | 2005/10788 [00:44<03:44, 39.15it/s]Training CobwebTree:  19%|        | 2009/10788 [00:44<03:45, 38.98it/s]Training CobwebTree:  19%|        | 2014/10788 [00:44<03:40, 39.77it/s]Training CobwebTree:  19%|        | 2019/10788 [00:44<03:38, 40.09it/s]Training CobwebTree:  19%|        | 2024/10788 [00:44<03:47, 38.55it/s]Training CobwebTree:  19%|        | 2028/10788 [00:44<03:46, 38.70it/s]Training CobwebTree:  19%|        | 2033/10788 [00:44<03:38, 40.07it/s]Training CobwebTree:  19%|        | 2038/10788 [00:45<03:37, 40.14it/s]Training CobwebTree:  19%|        | 2043/10788 [00:45<03:35, 40.54it/s]Training CobwebTree:  19%|        | 2048/10788 [00:45<03:29, 41.78it/s]Training CobwebTree:  19%|        | 2053/10788 [00:45<03:39, 39.87it/s]Training CobwebTree:  19%|        | 2058/10788 [00:45<03:37, 40.10it/s]Training CobwebTree:  19%|        | 2063/10788 [00:45<03:37, 40.20it/s]Training CobwebTree:  19%|        | 2068/10788 [00:45<03:31, 41.26it/s]Training CobwebTree:  19%|        | 2073/10788 [00:45<03:45, 38.70it/s]Training CobwebTree:  19%|        | 2077/10788 [00:46<03:52, 37.50it/s]Training CobwebTree:  19%|        | 2082/10788 [00:46<03:46, 38.48it/s]Training CobwebTree:  19%|        | 2087/10788 [00:46<03:41, 39.32it/s]Training CobwebTree:  19%|        | 2092/10788 [00:46<03:41, 39.22it/s]Training CobwebTree:  19%|        | 2096/10788 [00:46<03:47, 38.25it/s]Training CobwebTree:  19%|        | 2100/10788 [00:46<03:45, 38.58it/s]Training CobwebTree:  20%|        | 2104/10788 [00:46<03:53, 37.27it/s]Training CobwebTree:  20%|        | 2108/10788 [00:46<03:53, 37.15it/s]Training CobwebTree:  20%|        | 2112/10788 [00:47<03:55, 36.85it/s]Training CobwebTree:  20%|        | 2116/10788 [00:47<03:56, 36.65it/s]Training CobwebTree:  20%|        | 2120/10788 [00:47<03:55, 36.83it/s]Training CobwebTree:  20%|        | 2124/10788 [00:47<03:51, 37.46it/s]Training CobwebTree:  20%|        | 2128/10788 [00:47<03:48, 37.84it/s]Training CobwebTree:  20%|        | 2132/10788 [00:47<03:58, 36.29it/s]Training CobwebTree:  20%|        | 2136/10788 [00:47<04:08, 34.83it/s]Training CobwebTree:  20%|        | 2140/10788 [00:47<03:59, 36.08it/s]Training CobwebTree:  20%|        | 2144/10788 [00:47<04:09, 34.66it/s]Training CobwebTree:  20%|        | 2148/10788 [00:48<04:08, 34.74it/s]Training CobwebTree:  20%|        | 2153/10788 [00:48<03:45, 38.30it/s]Training CobwebTree:  20%|        | 2157/10788 [00:48<03:47, 38.01it/s]Training CobwebTree:  20%|        | 2161/10788 [00:48<03:44, 38.49it/s]Training CobwebTree:  20%|        | 2165/10788 [00:48<03:50, 37.49it/s]Training CobwebTree:  20%|        | 2169/10788 [00:48<03:49, 37.52it/s]Training CobwebTree:  20%|        | 2173/10788 [00:48<03:51, 37.16it/s]Training CobwebTree:  20%|        | 2177/10788 [00:48<04:00, 35.86it/s]Training CobwebTree:  20%|        | 2182/10788 [00:48<03:44, 38.40it/s]Training CobwebTree:  20%|        | 2187/10788 [00:49<03:39, 39.12it/s]Training CobwebTree:  20%|        | 2191/10788 [00:49<03:43, 38.50it/s]Training CobwebTree:  20%|        | 2196/10788 [00:49<03:34, 40.15it/s]Training CobwebTree:  20%|        | 2201/10788 [00:49<03:39, 39.09it/s]Training CobwebTree:  20%|        | 2205/10788 [00:49<03:41, 38.71it/s]Training CobwebTree:  20%|        | 2209/10788 [00:49<03:41, 38.76it/s]Training CobwebTree:  21%|        | 2213/10788 [00:49<03:48, 37.49it/s]Training CobwebTree:  21%|        | 2217/10788 [00:49<03:47, 37.67it/s]Training CobwebTree:  21%|        | 2221/10788 [00:49<03:45, 38.04it/s]Training CobwebTree:  21%|        | 2225/10788 [00:50<03:47, 37.64it/s]Training CobwebTree:  21%|        | 2230/10788 [00:50<03:39, 39.02it/s]Training CobwebTree:  21%|        | 2234/10788 [00:50<03:55, 36.27it/s]Training CobwebTree:  21%|        | 2238/10788 [00:50<03:49, 37.20it/s]Training CobwebTree:  21%|        | 2243/10788 [00:50<03:43, 38.16it/s]Training CobwebTree:  21%|        | 2248/10788 [00:50<03:35, 39.62it/s]Training CobwebTree:  21%|        | 2252/10788 [00:50<03:39, 38.91it/s]Training CobwebTree:  21%|        | 2256/10788 [00:50<03:44, 37.99it/s]Training CobwebTree:  21%|        | 2260/10788 [00:50<03:43, 38.10it/s]Training CobwebTree:  21%|        | 2265/10788 [00:51<03:32, 40.04it/s]Training CobwebTree:  21%|        | 2270/10788 [00:51<03:33, 39.91it/s]Training CobwebTree:  21%|        | 2274/10788 [00:51<03:36, 39.39it/s]Training CobwebTree:  21%|        | 2278/10788 [00:51<03:35, 39.45it/s]Training CobwebTree:  21%|        | 2282/10788 [00:51<03:46, 37.54it/s]Training CobwebTree:  21%|        | 2287/10788 [00:51<03:34, 39.66it/s]Training CobwebTree:  21%|        | 2291/10788 [00:51<03:35, 39.38it/s]Training CobwebTree:  21%|       | 2295/10788 [00:51<03:42, 38.25it/s]Training CobwebTree:  21%|       | 2299/10788 [00:51<03:41, 38.37it/s]Training CobwebTree:  21%|       | 2304/10788 [00:52<03:40, 38.55it/s]Training CobwebTree:  21%|       | 2309/10788 [00:52<03:36, 39.08it/s]Training CobwebTree:  21%|       | 2313/10788 [00:52<03:50, 36.74it/s]Training CobwebTree:  21%|       | 2317/10788 [00:52<03:47, 37.27it/s]Training CobwebTree:  22%|       | 2321/10788 [00:52<03:47, 37.27it/s]Training CobwebTree:  22%|       | 2325/10788 [00:52<03:46, 37.41it/s]Training CobwebTree:  22%|       | 2330/10788 [00:52<03:32, 39.78it/s]Training CobwebTree:  22%|       | 2334/10788 [00:52<03:39, 38.60it/s]Training CobwebTree:  22%|       | 2338/10788 [00:52<03:37, 38.78it/s]Training CobwebTree:  22%|       | 2342/10788 [00:53<03:50, 36.58it/s]Training CobwebTree:  22%|       | 2346/10788 [00:53<03:52, 36.34it/s]Training CobwebTree:  22%|       | 2350/10788 [00:53<03:48, 36.86it/s]Training CobwebTree:  22%|       | 2354/10788 [00:53<03:43, 37.65it/s]Training CobwebTree:  22%|       | 2358/10788 [00:53<03:47, 37.09it/s]Training CobwebTree:  22%|       | 2362/10788 [00:53<03:55, 35.71it/s]Training CobwebTree:  22%|       | 2366/10788 [00:53<03:55, 35.72it/s]Training CobwebTree:  22%|       | 2370/10788 [00:53<03:52, 36.26it/s]Training CobwebTree:  22%|       | 2374/10788 [00:53<03:55, 35.75it/s]Training CobwebTree:  22%|       | 2378/10788 [00:54<03:55, 35.71it/s]Training CobwebTree:  22%|       | 2382/10788 [00:54<03:55, 35.77it/s]Training CobwebTree:  22%|       | 2387/10788 [00:54<03:51, 36.23it/s]Training CobwebTree:  22%|       | 2391/10788 [00:54<03:51, 36.30it/s]Training CobwebTree:  22%|       | 2396/10788 [00:54<03:43, 37.62it/s]Training CobwebTree:  22%|       | 2400/10788 [00:54<03:41, 37.94it/s]Training CobwebTree:  22%|       | 2404/10788 [00:54<03:38, 38.29it/s]Training CobwebTree:  22%|       | 2408/10788 [00:54<03:41, 37.81it/s]Training CobwebTree:  22%|       | 2412/10788 [00:54<03:45, 37.10it/s]Training CobwebTree:  22%|       | 2416/10788 [00:55<03:52, 35.99it/s]Training CobwebTree:  22%|       | 2420/10788 [00:55<03:51, 36.18it/s]Training CobwebTree:  22%|       | 2424/10788 [00:55<04:01, 34.56it/s]Training CobwebTree:  23%|       | 2428/10788 [00:55<04:00, 34.70it/s]Training CobwebTree:  23%|       | 2432/10788 [00:55<03:53, 35.80it/s]Training CobwebTree:  23%|       | 2436/10788 [00:55<03:48, 36.47it/s]Training CobwebTree:  23%|       | 2440/10788 [00:55<03:50, 36.27it/s]Training CobwebTree:  23%|       | 2444/10788 [00:55<03:49, 36.42it/s]Training CobwebTree:  23%|       | 2448/10788 [00:55<03:46, 36.77it/s]Training CobwebTree:  23%|       | 2452/10788 [00:56<03:55, 35.32it/s]Training CobwebTree:  23%|       | 2457/10788 [00:56<03:45, 36.96it/s]Training CobwebTree:  23%|       | 2461/10788 [00:56<03:58, 34.95it/s]Training CobwebTree:  23%|       | 2466/10788 [00:56<03:43, 37.19it/s]Training CobwebTree:  23%|       | 2471/10788 [00:56<03:42, 37.44it/s]Training CobwebTree:  23%|       | 2475/10788 [00:56<03:38, 38.01it/s]Training CobwebTree:  23%|       | 2479/10788 [00:56<03:41, 37.53it/s]Training CobwebTree:  23%|       | 2484/10788 [00:56<03:37, 38.23it/s]Training CobwebTree:  23%|       | 2488/10788 [00:57<03:37, 38.10it/s]Training CobwebTree:  23%|       | 2493/10788 [00:57<03:35, 38.52it/s]Training CobwebTree:  23%|       | 2497/10788 [00:57<03:34, 38.66it/s]Training CobwebTree:  23%|       | 2501/10788 [00:57<03:44, 36.97it/s]Training CobwebTree:  23%|       | 2505/10788 [00:57<03:53, 35.40it/s]Training CobwebTree:  23%|       | 2509/10788 [00:57<03:53, 35.43it/s]Training CobwebTree:  23%|       | 2514/10788 [00:57<03:46, 36.47it/s]Training CobwebTree:  23%|       | 2518/10788 [00:57<03:44, 36.80it/s]Training CobwebTree:  23%|       | 2522/10788 [00:57<03:40, 37.53it/s]Training CobwebTree:  23%|       | 2526/10788 [00:58<03:40, 37.43it/s]Training CobwebTree:  23%|       | 2530/10788 [00:58<03:37, 37.88it/s]Training CobwebTree:  23%|       | 2534/10788 [00:58<03:46, 36.42it/s]Training CobwebTree:  24%|       | 2538/10788 [00:58<03:48, 36.07it/s]Training CobwebTree:  24%|       | 2542/10788 [00:58<03:54, 35.12it/s]Training CobwebTree:  24%|       | 2546/10788 [00:58<03:50, 35.73it/s]Training CobwebTree:  24%|       | 2551/10788 [00:58<03:37, 37.92it/s]Training CobwebTree:  24%|       | 2555/10788 [00:58<03:35, 38.16it/s]Training CobwebTree:  24%|       | 2559/10788 [00:58<03:41, 37.22it/s]Training CobwebTree:  24%|       | 2564/10788 [00:59<03:36, 37.97it/s]Training CobwebTree:  24%|       | 2568/10788 [00:59<03:42, 37.00it/s]Training CobwebTree:  24%|       | 2572/10788 [00:59<03:38, 37.54it/s]Training CobwebTree:  24%|       | 2576/10788 [00:59<03:44, 36.51it/s]Training CobwebTree:  24%|       | 2580/10788 [00:59<03:40, 37.17it/s]Training CobwebTree:  24%|       | 2584/10788 [00:59<03:48, 35.94it/s]Training CobwebTree:  24%|       | 2588/10788 [00:59<03:50, 35.53it/s]Training CobwebTree:  24%|       | 2592/10788 [00:59<03:50, 35.61it/s]Training CobwebTree:  24%|       | 2596/10788 [01:00<03:53, 35.13it/s]Training CobwebTree:  24%|       | 2600/10788 [01:00<04:01, 33.84it/s]Training CobwebTree:  24%|       | 2604/10788 [01:00<03:52, 35.18it/s]Training CobwebTree:  24%|       | 2608/10788 [01:00<03:48, 35.74it/s]Training CobwebTree:  24%|       | 2612/10788 [01:00<03:42, 36.70it/s]Training CobwebTree:  24%|       | 2616/10788 [01:00<03:42, 36.68it/s]Training CobwebTree:  24%|       | 2620/10788 [01:00<03:51, 35.35it/s]Training CobwebTree:  24%|       | 2625/10788 [01:00<03:41, 36.87it/s]Training CobwebTree:  24%|       | 2629/10788 [01:00<03:44, 36.34it/s]Training CobwebTree:  24%|       | 2633/10788 [01:01<03:42, 36.64it/s]Training CobwebTree:  24%|       | 2637/10788 [01:01<03:52, 35.09it/s]Training CobwebTree:  24%|       | 2641/10788 [01:01<03:55, 34.63it/s]Training CobwebTree:  25%|       | 2645/10788 [01:01<03:55, 34.56it/s]Training CobwebTree:  25%|       | 2649/10788 [01:01<03:49, 35.46it/s]Training CobwebTree:  25%|       | 2653/10788 [01:01<03:53, 34.86it/s]Training CobwebTree:  25%|       | 2657/10788 [01:01<03:48, 35.65it/s]Training CobwebTree:  25%|       | 2661/10788 [01:01<03:40, 36.80it/s]Training CobwebTree:  25%|       | 2666/10788 [01:01<03:34, 37.79it/s]Training CobwebTree:  25%|       | 2670/10788 [01:02<03:32, 38.19it/s]Training CobwebTree:  25%|       | 2674/10788 [01:02<03:37, 37.29it/s]Training CobwebTree:  25%|       | 2679/10788 [01:02<03:29, 38.68it/s]Training CobwebTree:  25%|       | 2683/10788 [01:02<03:32, 38.11it/s]Training CobwebTree:  25%|       | 2688/10788 [01:02<03:22, 40.08it/s]Training CobwebTree:  25%|       | 2693/10788 [01:02<03:30, 38.52it/s]Training CobwebTree:  25%|       | 2697/10788 [01:02<03:33, 37.94it/s]Training CobwebTree:  25%|       | 2701/10788 [01:02<03:34, 37.63it/s]Training CobwebTree:  25%|       | 2706/10788 [01:03<03:30, 38.34it/s]Training CobwebTree:  25%|       | 2710/10788 [01:03<03:33, 37.84it/s]Training CobwebTree:  25%|       | 2714/10788 [01:03<03:37, 37.06it/s]Training CobwebTree:  25%|       | 2718/10788 [01:03<03:38, 36.92it/s]Training CobwebTree:  25%|       | 2722/10788 [01:03<03:35, 37.51it/s]Training CobwebTree:  25%|       | 2726/10788 [01:03<03:35, 37.43it/s]Training CobwebTree:  25%|       | 2730/10788 [01:03<03:31, 38.13it/s]Training CobwebTree:  25%|       | 2734/10788 [01:03<03:32, 37.94it/s]Training CobwebTree:  25%|       | 2739/10788 [01:03<03:29, 38.50it/s]Training CobwebTree:  25%|       | 2744/10788 [01:04<03:24, 39.33it/s]Training CobwebTree:  25%|       | 2749/10788 [01:04<03:17, 40.75it/s]Training CobwebTree:  26%|       | 2754/10788 [01:04<03:24, 39.32it/s]Training CobwebTree:  26%|       | 2759/10788 [01:04<03:21, 39.85it/s]Training CobwebTree:  26%|       | 2763/10788 [01:04<03:22, 39.59it/s]Training CobwebTree:  26%|       | 2767/10788 [01:04<03:31, 37.92it/s]Training CobwebTree:  26%|       | 2772/10788 [01:04<03:26, 38.90it/s]Training CobwebTree:  26%|       | 2776/10788 [01:04<03:30, 38.07it/s]Training CobwebTree:  26%|       | 2780/10788 [01:04<03:29, 38.14it/s]Training CobwebTree:  26%|       | 2784/10788 [01:05<03:28, 38.42it/s]Training CobwebTree:  26%|       | 2788/10788 [01:05<03:26, 38.71it/s]Training CobwebTree:  26%|       | 2792/10788 [01:05<03:42, 35.96it/s]Training CobwebTree:  26%|       | 2796/10788 [01:05<03:39, 36.45it/s]Training CobwebTree:  26%|       | 2800/10788 [01:05<03:39, 36.37it/s]Training CobwebTree:  26%|       | 2804/10788 [01:05<03:36, 36.91it/s]Training CobwebTree:  26%|       | 2808/10788 [01:05<03:33, 37.34it/s]Training CobwebTree:  26%|       | 2812/10788 [01:05<03:43, 35.68it/s]Training CobwebTree:  26%|       | 2817/10788 [01:05<03:34, 37.08it/s]Training CobwebTree:  26%|       | 2821/10788 [01:06<03:36, 36.74it/s]Training CobwebTree:  26%|       | 2825/10788 [01:06<03:47, 34.94it/s]Training CobwebTree:  26%|       | 2830/10788 [01:06<03:38, 36.45it/s]Training CobwebTree:  26%|       | 2834/10788 [01:06<03:41, 35.94it/s]Training CobwebTree:  26%|       | 2838/10788 [01:06<03:53, 34.06it/s]Training CobwebTree:  26%|       | 2842/10788 [01:06<03:49, 34.63it/s]Training CobwebTree:  26%|       | 2846/10788 [01:06<03:54, 33.80it/s]Training CobwebTree:  26%|       | 2850/10788 [01:06<03:47, 34.94it/s]Training CobwebTree:  26%|       | 2854/10788 [01:07<03:43, 35.50it/s]Training CobwebTree:  26%|       | 2858/10788 [01:07<03:50, 34.38it/s]Training CobwebTree:  27%|       | 2862/10788 [01:07<03:40, 35.88it/s]Training CobwebTree:  27%|       | 2866/10788 [01:07<03:36, 36.62it/s]Training CobwebTree:  27%|       | 2870/10788 [01:07<03:32, 37.31it/s]Training CobwebTree:  27%|       | 2874/10788 [01:07<03:34, 36.92it/s]Training CobwebTree:  27%|       | 2878/10788 [01:07<03:30, 37.54it/s]Training CobwebTree:  27%|       | 2882/10788 [01:07<03:42, 35.47it/s]Training CobwebTree:  27%|       | 2886/10788 [01:07<03:45, 35.12it/s]Training CobwebTree:  27%|       | 2890/10788 [01:08<03:56, 33.46it/s]Training CobwebTree:  27%|       | 2894/10788 [01:08<03:46, 34.90it/s]Training CobwebTree:  27%|       | 2898/10788 [01:08<03:47, 34.63it/s]Training CobwebTree:  27%|       | 2902/10788 [01:08<03:38, 36.03it/s]Training CobwebTree:  27%|       | 2906/10788 [01:08<03:42, 35.46it/s]Training CobwebTree:  27%|       | 2910/10788 [01:08<03:44, 35.02it/s]Training CobwebTree:  27%|       | 2914/10788 [01:08<03:51, 34.07it/s]Training CobwebTree:  27%|       | 2918/10788 [01:08<03:49, 34.22it/s]Training CobwebTree:  27%|       | 2922/10788 [01:08<03:58, 32.93it/s]Training CobwebTree:  27%|       | 2926/10788 [01:09<03:55, 33.43it/s]Training CobwebTree:  27%|       | 2930/10788 [01:09<03:53, 33.60it/s]Training CobwebTree:  27%|       | 2934/10788 [01:09<03:50, 34.03it/s]Training CobwebTree:  27%|       | 2938/10788 [01:09<03:41, 35.45it/s]Training CobwebTree:  27%|       | 2942/10788 [01:09<03:49, 34.17it/s]Training CobwebTree:  27%|       | 2946/10788 [01:09<03:58, 32.89it/s]Training CobwebTree:  27%|       | 2950/10788 [01:09<03:48, 34.28it/s]Training CobwebTree:  27%|       | 2954/10788 [01:09<03:51, 33.82it/s]Training CobwebTree:  27%|       | 2958/10788 [01:09<03:45, 34.66it/s]Training CobwebTree:  27%|       | 2963/10788 [01:10<03:27, 37.79it/s]Training CobwebTree:  28%|       | 2967/10788 [01:10<03:25, 38.00it/s]Training CobwebTree:  28%|       | 2971/10788 [01:10<03:24, 38.23it/s]Training CobwebTree:  28%|       | 2975/10788 [01:10<03:25, 38.01it/s]Training CobwebTree:  28%|       | 2980/10788 [01:10<03:16, 39.65it/s]Training CobwebTree:  28%|       | 2984/10788 [01:10<03:17, 39.58it/s]Training CobwebTree:  28%|       | 2988/10788 [01:10<03:18, 39.39it/s]Training CobwebTree:  28%|       | 2993/10788 [01:10<03:13, 40.36it/s]Training CobwebTree:  28%|       | 2998/10788 [01:10<03:12, 40.53it/s]Training CobwebTree:  28%|       | 3003/10788 [01:11<03:27, 37.60it/s]Training CobwebTree:  28%|       | 3007/10788 [01:11<03:29, 37.11it/s]Training CobwebTree:  28%|       | 3011/10788 [01:11<03:28, 37.34it/s]Training CobwebTree:  28%|       | 3015/10788 [01:11<03:34, 36.17it/s]Training CobwebTree:  28%|       | 3019/10788 [01:11<03:29, 37.00it/s]Training CobwebTree:  28%|       | 3023/10788 [01:11<03:31, 36.76it/s]Training CobwebTree:  28%|       | 3027/10788 [01:11<03:31, 36.69it/s]Training CobwebTree:  28%|       | 3031/10788 [01:11<03:27, 37.37it/s]Training CobwebTree:  28%|       | 3035/10788 [01:12<03:29, 37.03it/s]Training CobwebTree:  28%|       | 3040/10788 [01:12<03:15, 39.66it/s]Training CobwebTree:  28%|       | 3044/10788 [01:12<03:19, 38.72it/s]Training CobwebTree:  28%|       | 3048/10788 [01:12<03:26, 37.57it/s]Training CobwebTree:  28%|       | 3052/10788 [01:12<03:24, 37.84it/s]Training CobwebTree:  28%|       | 3056/10788 [01:12<03:37, 35.59it/s]Training CobwebTree:  28%|       | 3060/10788 [01:12<03:34, 35.99it/s]Training CobwebTree:  28%|       | 3064/10788 [01:12<03:28, 37.06it/s]Training CobwebTree:  28%|       | 3069/10788 [01:12<03:19, 38.61it/s]Training CobwebTree:  28%|       | 3074/10788 [01:13<03:16, 39.36it/s]Training CobwebTree:  29%|       | 3078/10788 [01:13<03:19, 38.63it/s]Training CobwebTree:  29%|       | 3082/10788 [01:13<03:23, 37.86it/s]Training CobwebTree:  29%|       | 3087/10788 [01:13<03:15, 39.46it/s]Training CobwebTree:  29%|       | 3091/10788 [01:13<03:18, 38.68it/s]Training CobwebTree:  29%|       | 3096/10788 [01:13<03:13, 39.83it/s]Training CobwebTree:  29%|       | 3100/10788 [01:13<03:21, 38.23it/s]Training CobwebTree:  29%|       | 3104/10788 [01:13<03:28, 36.87it/s]Training CobwebTree:  29%|       | 3109/10788 [01:13<03:16, 39.04it/s]Training CobwebTree:  29%|       | 3114/10788 [01:14<03:15, 39.23it/s]Training CobwebTree:  29%|       | 3119/10788 [01:14<03:02, 41.98it/s]Training CobwebTree:  29%|       | 3124/10788 [01:14<03:03, 41.66it/s]Training CobwebTree:  29%|       | 3129/10788 [01:14<03:12, 39.89it/s]Training CobwebTree:  29%|       | 3134/10788 [01:14<03:10, 40.22it/s]Training CobwebTree:  29%|       | 3139/10788 [01:14<03:13, 39.52it/s]Training CobwebTree:  29%|       | 3143/10788 [01:14<03:15, 39.18it/s]Training CobwebTree:  29%|       | 3147/10788 [01:14<03:16, 38.94it/s]Training CobwebTree:  29%|       | 3152/10788 [01:15<03:15, 39.14it/s]Training CobwebTree:  29%|       | 3157/10788 [01:15<03:07, 40.80it/s]Training CobwebTree:  29%|       | 3162/10788 [01:15<03:20, 38.12it/s]Training CobwebTree:  29%|       | 3166/10788 [01:15<03:24, 37.30it/s]Training CobwebTree:  29%|       | 3170/10788 [01:15<03:21, 37.79it/s]Training CobwebTree:  29%|       | 3174/10788 [01:15<03:26, 36.80it/s]Training CobwebTree:  29%|       | 3178/10788 [01:15<03:24, 37.26it/s]Training CobwebTree:  29%|       | 3182/10788 [01:15<03:29, 36.30it/s]Training CobwebTree:  30%|       | 3186/10788 [01:15<03:24, 37.25it/s]Training CobwebTree:  30%|       | 3191/10788 [01:16<03:14, 39.04it/s]Training CobwebTree:  30%|       | 3196/10788 [01:16<03:17, 38.37it/s]Training CobwebTree:  30%|       | 3201/10788 [01:16<03:11, 39.65it/s]Training CobwebTree:  30%|       | 3205/10788 [01:16<03:12, 39.41it/s]Training CobwebTree:  30%|       | 3209/10788 [01:16<03:22, 37.40it/s]Training CobwebTree:  30%|       | 3213/10788 [01:16<03:25, 36.91it/s]Training CobwebTree:  30%|       | 3217/10788 [01:16<03:22, 37.43it/s]Training CobwebTree:  30%|       | 3221/10788 [01:16<03:30, 35.92it/s]Training CobwebTree:  30%|       | 3225/10788 [01:16<03:26, 36.56it/s]Training CobwebTree:  30%|       | 3229/10788 [01:17<03:30, 35.97it/s]Training CobwebTree:  30%|       | 3233/10788 [01:17<03:31, 35.67it/s]Training CobwebTree:  30%|       | 3237/10788 [01:17<03:33, 35.34it/s]Training CobwebTree:  30%|       | 3241/10788 [01:17<03:31, 35.64it/s]Training CobwebTree:  30%|       | 3245/10788 [01:17<03:37, 34.62it/s]Training CobwebTree:  30%|       | 3249/10788 [01:17<03:29, 35.95it/s]Training CobwebTree:  30%|       | 3253/10788 [01:17<03:24, 36.81it/s]Training CobwebTree:  30%|       | 3257/10788 [01:17<03:21, 37.43it/s]Training CobwebTree:  30%|       | 3261/10788 [01:17<03:20, 37.51it/s]Training CobwebTree:  30%|       | 3265/10788 [01:18<03:24, 36.74it/s]Training CobwebTree:  30%|       | 3270/10788 [01:18<03:15, 38.39it/s]Training CobwebTree:  30%|       | 3274/10788 [01:18<03:17, 38.05it/s]Training CobwebTree:  30%|       | 3278/10788 [01:18<03:16, 38.23it/s]Training CobwebTree:  30%|       | 3283/10788 [01:18<03:06, 40.35it/s]Training CobwebTree:  30%|       | 3288/10788 [01:18<03:04, 40.62it/s]Training CobwebTree:  31%|       | 3293/10788 [01:18<03:07, 40.03it/s]Training CobwebTree:  31%|       | 3298/10788 [01:18<03:14, 38.56it/s]Training CobwebTree:  31%|       | 3302/10788 [01:19<03:18, 37.73it/s]Training CobwebTree:  31%|       | 3306/10788 [01:19<03:20, 37.23it/s]Training CobwebTree:  31%|       | 3310/10788 [01:19<03:18, 37.66it/s]Training CobwebTree:  31%|       | 3315/10788 [01:19<03:10, 39.21it/s]Training CobwebTree:  31%|       | 3319/10788 [01:19<03:19, 37.52it/s]Training CobwebTree:  31%|       | 3324/10788 [01:19<03:11, 38.96it/s]Training CobwebTree:  31%|       | 3329/10788 [01:19<03:09, 39.39it/s]Training CobwebTree:  31%|       | 3333/10788 [01:19<03:13, 38.47it/s]Training CobwebTree:  31%|       | 3338/10788 [01:19<03:04, 40.35it/s]Training CobwebTree:  31%|       | 3343/10788 [01:20<03:13, 38.40it/s]Training CobwebTree:  31%|       | 3347/10788 [01:20<03:22, 36.79it/s]Training CobwebTree:  31%|       | 3352/10788 [01:20<03:13, 38.43it/s]Training CobwebTree:  31%|       | 3357/10788 [01:20<03:05, 40.09it/s]Training CobwebTree:  31%|       | 3362/10788 [01:20<03:02, 40.60it/s]Training CobwebTree:  31%|       | 3367/10788 [01:20<03:01, 40.78it/s]Training CobwebTree:  31%|      | 3372/10788 [01:20<03:08, 39.45it/s]Training CobwebTree:  31%|      | 3376/10788 [01:20<03:08, 39.40it/s]Training CobwebTree:  31%|      | 3380/10788 [01:21<03:15, 37.84it/s]Training CobwebTree:  31%|      | 3384/10788 [01:21<03:14, 38.08it/s]Training CobwebTree:  31%|      | 3389/10788 [01:21<03:10, 38.84it/s]Training CobwebTree:  31%|      | 3394/10788 [01:21<03:07, 39.34it/s]Training CobwebTree:  32%|      | 3399/10788 [01:21<03:05, 39.76it/s]Training CobwebTree:  32%|      | 3403/10788 [01:21<03:08, 39.27it/s]Training CobwebTree:  32%|      | 3408/10788 [01:21<03:04, 39.96it/s]Training CobwebTree:  32%|      | 3412/10788 [01:21<03:06, 39.46it/s]Training CobwebTree:  32%|      | 3417/10788 [01:21<03:01, 40.64it/s]Training CobwebTree:  32%|      | 3422/10788 [01:22<03:01, 40.60it/s]Training CobwebTree:  32%|      | 3427/10788 [01:22<03:00, 40.71it/s]Training CobwebTree:  32%|      | 3432/10788 [01:22<03:01, 40.51it/s]Training CobwebTree:  32%|      | 3437/10788 [01:22<03:01, 40.47it/s]Training CobwebTree:  32%|      | 3442/10788 [01:22<03:05, 39.52it/s]Training CobwebTree:  32%|      | 3446/10788 [01:22<03:08, 38.86it/s]Training CobwebTree:  32%|      | 3450/10788 [01:22<03:10, 38.54it/s]Training CobwebTree:  32%|      | 3454/10788 [01:22<03:16, 37.28it/s]Training CobwebTree:  32%|      | 3458/10788 [01:22<03:17, 37.09it/s]Training CobwebTree:  32%|      | 3463/10788 [01:23<03:13, 37.92it/s]Training CobwebTree:  32%|      | 3467/10788 [01:23<03:17, 37.07it/s]Training CobwebTree:  32%|      | 3471/10788 [01:23<03:23, 35.90it/s]Training CobwebTree:  32%|      | 3476/10788 [01:23<03:11, 38.26it/s]Training CobwebTree:  32%|      | 3481/10788 [01:23<03:06, 39.27it/s]Training CobwebTree:  32%|      | 3486/10788 [01:23<02:59, 40.64it/s]Training CobwebTree:  32%|      | 3491/10788 [01:23<02:57, 41.04it/s]Training CobwebTree:  32%|      | 3496/10788 [01:23<03:03, 39.84it/s]Training CobwebTree:  32%|      | 3501/10788 [01:24<02:59, 40.50it/s]Training CobwebTree:  32%|      | 3506/10788 [01:24<02:55, 41.48it/s]Training CobwebTree:  33%|      | 3511/10788 [01:24<02:56, 41.35it/s]Training CobwebTree:  33%|      | 3516/10788 [01:24<03:02, 39.84it/s]Training CobwebTree:  33%|      | 3521/10788 [01:24<03:11, 37.89it/s]Training CobwebTree:  33%|      | 3525/10788 [01:24<03:15, 37.06it/s]Training CobwebTree:  33%|      | 3529/10788 [01:24<03:16, 36.90it/s]Training CobwebTree:  33%|      | 3534/10788 [01:24<03:04, 39.33it/s]Training CobwebTree:  33%|      | 3538/10788 [01:25<03:07, 38.68it/s]Training CobwebTree:  33%|      | 3543/10788 [01:25<02:59, 40.27it/s]Training CobwebTree:  33%|      | 3548/10788 [01:25<03:00, 40.05it/s]Training CobwebTree:  33%|      | 3553/10788 [01:25<03:11, 37.86it/s]Training CobwebTree:  33%|      | 3557/10788 [01:25<03:20, 36.04it/s]Training CobwebTree:  33%|      | 3561/10788 [01:25<03:27, 34.87it/s]Training CobwebTree:  33%|      | 3565/10788 [01:25<03:28, 34.58it/s]Training CobwebTree:  33%|      | 3569/10788 [01:25<03:28, 34.56it/s]Training CobwebTree:  33%|      | 3573/10788 [01:26<03:25, 35.13it/s]Training CobwebTree:  33%|      | 3577/10788 [01:26<03:24, 35.21it/s]Training CobwebTree:  33%|      | 3582/10788 [01:26<03:14, 37.07it/s]Training CobwebTree:  33%|      | 3586/10788 [01:26<03:15, 36.92it/s]Training CobwebTree:  33%|      | 3590/10788 [01:26<03:19, 36.14it/s]Training CobwebTree:  33%|      | 3595/10788 [01:26<03:11, 37.47it/s]Training CobwebTree:  33%|      | 3599/10788 [01:26<03:10, 37.84it/s]Training CobwebTree:  33%|      | 3603/10788 [01:26<03:11, 37.45it/s]Training CobwebTree:  33%|      | 3607/10788 [01:26<03:20, 35.85it/s]Training CobwebTree:  33%|      | 3611/10788 [01:27<03:21, 35.62it/s]Training CobwebTree:  34%|      | 3615/10788 [01:27<03:16, 36.53it/s]Training CobwebTree:  34%|      | 3619/10788 [01:27<03:30, 34.12it/s]Training CobwebTree:  34%|      | 3623/10788 [01:27<03:27, 34.58it/s]Training CobwebTree:  34%|      | 3627/10788 [01:27<03:22, 35.44it/s]Training CobwebTree:  34%|      | 3631/10788 [01:27<03:21, 35.59it/s]Training CobwebTree:  34%|      | 3635/10788 [01:27<03:16, 36.45it/s]Training CobwebTree:  34%|      | 3639/10788 [01:27<03:20, 35.71it/s]Training CobwebTree:  34%|      | 3644/10788 [01:27<03:12, 37.13it/s]Training CobwebTree:  34%|      | 3649/10788 [01:28<03:02, 39.10it/s]Training CobwebTree:  34%|      | 3653/10788 [01:28<03:03, 38.85it/s]Training CobwebTree:  34%|      | 3657/10788 [01:28<03:02, 39.00it/s]Training CobwebTree:  34%|      | 3662/10788 [01:28<03:00, 39.49it/s]Training CobwebTree:  34%|      | 3667/10788 [01:28<02:58, 39.86it/s]Training CobwebTree:  34%|      | 3671/10788 [01:28<02:58, 39.80it/s]Training CobwebTree:  34%|      | 3676/10788 [01:28<02:54, 40.80it/s]Training CobwebTree:  34%|      | 3681/10788 [01:28<03:05, 38.32it/s]Training CobwebTree:  34%|      | 3685/10788 [01:29<03:13, 36.79it/s]Training CobwebTree:  34%|      | 3690/10788 [01:29<03:04, 38.48it/s]Training CobwebTree:  34%|      | 3695/10788 [01:29<03:05, 38.26it/s]Training CobwebTree:  34%|      | 3699/10788 [01:29<03:06, 38.00it/s]Training CobwebTree:  34%|      | 3703/10788 [01:29<03:09, 37.32it/s]Training CobwebTree:  34%|      | 3708/10788 [01:29<03:03, 38.54it/s]Training CobwebTree:  34%|      | 3713/10788 [01:29<02:58, 39.69it/s]Training CobwebTree:  34%|      | 3718/10788 [01:29<02:53, 40.83it/s]Training CobwebTree:  35%|      | 3723/10788 [01:29<02:52, 41.05it/s]Training CobwebTree:  35%|      | 3728/10788 [01:30<02:55, 40.17it/s]Training CobwebTree:  35%|      | 3733/10788 [01:30<03:02, 38.72it/s]Training CobwebTree:  35%|      | 3737/10788 [01:30<03:01, 38.85it/s]Training CobwebTree:  35%|      | 3741/10788 [01:30<03:04, 38.10it/s]Training CobwebTree:  35%|      | 3745/10788 [01:30<03:09, 37.08it/s]Training CobwebTree:  35%|      | 3749/10788 [01:30<03:07, 37.55it/s]Training CobwebTree:  35%|      | 3753/10788 [01:30<03:04, 38.04it/s]Training CobwebTree:  35%|      | 3758/10788 [01:30<02:55, 40.01it/s]Training CobwebTree:  35%|      | 3763/10788 [01:31<02:58, 39.46it/s]Training CobwebTree:  35%|      | 3768/10788 [01:31<02:54, 40.14it/s]Training CobwebTree:  35%|      | 3773/10788 [01:31<02:51, 40.86it/s]Training CobwebTree:  35%|      | 3778/10788 [01:31<02:54, 40.22it/s]Training CobwebTree:  35%|      | 3783/10788 [01:31<02:45, 42.22it/s]Training CobwebTree:  35%|      | 3788/10788 [01:31<02:49, 41.22it/s]Training CobwebTree:  35%|      | 3793/10788 [01:31<02:54, 40.14it/s]Training CobwebTree:  35%|      | 3798/10788 [01:31<02:55, 39.77it/s]Training CobwebTree:  35%|      | 3802/10788 [01:31<03:05, 37.61it/s]Training CobwebTree:  35%|      | 3806/10788 [01:32<03:05, 37.72it/s]Training CobwebTree:  35%|      | 3810/10788 [01:32<03:04, 37.85it/s]Training CobwebTree:  35%|      | 3814/10788 [01:32<03:07, 37.20it/s]Training CobwebTree:  35%|      | 3818/10788 [01:32<03:08, 37.00it/s]Training CobwebTree:  35%|      | 3822/10788 [01:32<03:06, 37.34it/s]Training CobwebTree:  35%|      | 3826/10788 [01:32<03:09, 36.74it/s]Training CobwebTree:  36%|      | 3830/10788 [01:32<03:13, 35.94it/s]Training CobwebTree:  36%|      | 3834/10788 [01:32<03:10, 36.50it/s]Training CobwebTree:  36%|      | 3838/10788 [01:32<03:07, 37.06it/s]Training CobwebTree:  36%|      | 3843/10788 [01:33<03:02, 38.12it/s]Training CobwebTree:  36%|      | 3847/10788 [01:33<03:00, 38.50it/s]Training CobwebTree:  36%|      | 3851/10788 [01:33<03:05, 37.40it/s]Training CobwebTree:  36%|      | 3855/10788 [01:33<03:05, 37.45it/s]Training CobwebTree:  36%|      | 3859/10788 [01:33<03:05, 37.27it/s]Training CobwebTree:  36%|      | 3863/10788 [01:33<03:14, 35.65it/s]Training CobwebTree:  36%|      | 3868/10788 [01:33<03:00, 38.29it/s]Training CobwebTree:  36%|      | 3872/10788 [01:33<03:05, 37.35it/s]Training CobwebTree:  36%|      | 3876/10788 [01:33<03:08, 36.75it/s]Training CobwebTree:  36%|      | 3880/10788 [01:34<03:10, 36.25it/s]Training CobwebTree:  36%|      | 3884/10788 [01:34<03:08, 36.56it/s]Training CobwebTree:  36%|      | 3889/10788 [01:34<03:02, 37.78it/s]Training CobwebTree:  36%|      | 3894/10788 [01:34<03:02, 37.88it/s]Training CobwebTree:  36%|      | 3898/10788 [01:34<02:59, 38.43it/s]Training CobwebTree:  36%|      | 3902/10788 [01:34<03:01, 38.00it/s]Training CobwebTree:  36%|      | 3907/10788 [01:34<02:53, 39.55it/s]Training CobwebTree:  36%|      | 3912/10788 [01:34<02:55, 39.27it/s]Training CobwebTree:  36%|      | 3916/10788 [01:35<03:00, 38.13it/s]Training CobwebTree:  36%|      | 3920/10788 [01:35<02:59, 38.31it/s]Training CobwebTree:  36%|      | 3925/10788 [01:35<02:54, 39.43it/s]Training CobwebTree:  36%|      | 3929/10788 [01:35<03:01, 37.85it/s]Training CobwebTree:  36%|      | 3933/10788 [01:35<03:09, 36.27it/s]Training CobwebTree:  37%|      | 3938/10788 [01:35<02:55, 38.95it/s]Training CobwebTree:  37%|      | 3943/10788 [01:35<02:47, 40.89it/s]Training CobwebTree:  37%|      | 3948/10788 [01:35<02:51, 39.83it/s]Training CobwebTree:  37%|      | 3953/10788 [01:35<02:43, 41.73it/s]Training CobwebTree:  37%|      | 3958/10788 [01:36<02:53, 39.30it/s]Training CobwebTree:  37%|      | 3962/10788 [01:36<02:57, 38.41it/s]Training CobwebTree:  37%|      | 3966/10788 [01:36<02:59, 38.02it/s]Training CobwebTree:  37%|      | 3971/10788 [01:36<02:55, 38.75it/s]Training CobwebTree:  37%|      | 3975/10788 [01:36<02:55, 38.78it/s]Training CobwebTree:  37%|      | 3979/10788 [01:36<02:57, 38.38it/s]Training CobwebTree:  37%|      | 3983/10788 [01:36<02:59, 37.93it/s]Training CobwebTree:  37%|      | 3987/10788 [01:36<03:00, 37.70it/s]Training CobwebTree:  37%|      | 3991/10788 [01:36<02:57, 38.21it/s]Training CobwebTree:  37%|      | 3995/10788 [01:37<02:56, 38.41it/s]Training CobwebTree:  37%|      | 3999/10788 [01:37<02:59, 37.73it/s]Training CobwebTree:  37%|      | 4004/10788 [01:37<02:58, 38.07it/s]Training CobwebTree:  37%|      | 4009/10788 [01:37<02:54, 38.77it/s]Training CobwebTree:  37%|      | 4013/10788 [01:37<03:04, 36.65it/s]Training CobwebTree:  37%|      | 4017/10788 [01:37<03:08, 35.91it/s]Training CobwebTree:  37%|      | 4021/10788 [01:37<03:02, 36.98it/s]Training CobwebTree:  37%|      | 4025/10788 [01:37<02:59, 37.72it/s]Training CobwebTree:  37%|      | 4029/10788 [01:38<03:10, 35.50it/s]Training CobwebTree:  37%|      | 4033/10788 [01:38<03:06, 36.20it/s]Training CobwebTree:  37%|      | 4037/10788 [01:38<03:04, 36.49it/s]Training CobwebTree:  37%|      | 4042/10788 [01:38<02:49, 39.85it/s]Training CobwebTree:  38%|      | 4047/10788 [01:38<02:53, 38.81it/s]Training CobwebTree:  38%|      | 4051/10788 [01:38<02:58, 37.79it/s]Training CobwebTree:  38%|      | 4055/10788 [01:38<03:01, 37.19it/s]Training CobwebTree:  38%|      | 4059/10788 [01:38<02:59, 37.53it/s]Training CobwebTree:  38%|      | 4063/10788 [01:38<03:00, 37.28it/s]Training CobwebTree:  38%|      | 4068/10788 [01:39<02:56, 38.17it/s]Training CobwebTree:  38%|      | 4072/10788 [01:39<02:57, 37.79it/s]Training CobwebTree:  38%|      | 4077/10788 [01:39<02:52, 38.84it/s]Training CobwebTree:  38%|      | 4081/10788 [01:39<02:54, 38.35it/s]Training CobwebTree:  38%|      | 4085/10788 [01:39<02:58, 37.47it/s]Training CobwebTree:  38%|      | 4090/10788 [01:39<02:56, 37.90it/s]Training CobwebTree:  38%|      | 4095/10788 [01:39<02:50, 39.17it/s]Training CobwebTree:  38%|      | 4100/10788 [01:39<02:43, 41.01it/s]Training CobwebTree:  38%|      | 4105/10788 [01:39<02:51, 38.90it/s]Training CobwebTree:  38%|      | 4110/10788 [01:40<02:48, 39.75it/s]Training CobwebTree:  38%|      | 4115/10788 [01:40<02:47, 39.87it/s]Training CobwebTree:  38%|      | 4120/10788 [01:40<02:52, 38.57it/s]Training CobwebTree:  38%|      | 4124/10788 [01:40<02:54, 38.16it/s]Training CobwebTree:  38%|      | 4129/10788 [01:40<02:51, 38.79it/s]Training CobwebTree:  38%|      | 4133/10788 [01:40<02:54, 38.23it/s]Training CobwebTree:  38%|      | 4137/10788 [01:40<02:57, 37.52it/s]Training CobwebTree:  38%|      | 4142/10788 [01:40<02:56, 37.68it/s]Training CobwebTree:  38%|      | 4147/10788 [01:41<02:51, 38.77it/s]Training CobwebTree:  38%|      | 4152/10788 [01:41<02:50, 38.92it/s]Training CobwebTree:  39%|      | 4157/10788 [01:41<02:44, 40.26it/s]Training CobwebTree:  39%|      | 4162/10788 [01:41<02:45, 39.95it/s]Training CobwebTree:  39%|      | 4167/10788 [01:41<02:43, 40.50it/s]Training CobwebTree:  39%|      | 4172/10788 [01:41<02:39, 41.44it/s]Training CobwebTree:  39%|      | 4177/10788 [01:41<02:48, 39.32it/s]Training CobwebTree:  39%|      | 4182/10788 [01:41<02:40, 41.26it/s]Training CobwebTree:  39%|      | 4187/10788 [01:42<02:55, 37.65it/s]Training CobwebTree:  39%|      | 4192/10788 [01:42<02:49, 38.81it/s]Training CobwebTree:  39%|      | 4196/10788 [01:42<02:51, 38.49it/s]Training CobwebTree:  39%|      | 4201/10788 [01:42<02:43, 40.41it/s]Training CobwebTree:  39%|      | 4206/10788 [01:42<02:44, 39.93it/s]Training CobwebTree:  39%|      | 4211/10788 [01:42<02:47, 39.37it/s]Training CobwebTree:  39%|      | 4215/10788 [01:42<02:53, 37.89it/s]Training CobwebTree:  39%|      | 4220/10788 [01:42<02:46, 39.53it/s]Training CobwebTree:  39%|      | 4224/10788 [01:43<02:51, 38.36it/s]Training CobwebTree:  39%|      | 4228/10788 [01:43<02:49, 38.78it/s]Training CobwebTree:  39%|      | 4233/10788 [01:43<02:51, 38.33it/s]Training CobwebTree:  39%|      | 4238/10788 [01:43<02:47, 39.00it/s]Training CobwebTree:  39%|      | 4242/10788 [01:43<02:51, 38.23it/s]Training CobwebTree:  39%|      | 4246/10788 [01:43<02:52, 37.98it/s]Training CobwebTree:  39%|      | 4250/10788 [01:43<02:53, 37.76it/s]Training CobwebTree:  39%|      | 4254/10788 [01:43<02:56, 37.09it/s]Training CobwebTree:  39%|      | 4258/10788 [01:43<02:54, 37.36it/s]Training CobwebTree:  40%|      | 4262/10788 [01:44<02:52, 37.91it/s]Training CobwebTree:  40%|      | 4266/10788 [01:44<02:50, 38.30it/s]Training CobwebTree:  40%|      | 4270/10788 [01:44<02:56, 37.01it/s]Training CobwebTree:  40%|      | 4274/10788 [01:44<02:54, 37.37it/s]Training CobwebTree:  40%|      | 4278/10788 [01:44<02:51, 38.05it/s]Training CobwebTree:  40%|      | 4283/10788 [01:44<02:42, 40.12it/s]Training CobwebTree:  40%|      | 4288/10788 [01:44<02:53, 37.37it/s]Training CobwebTree:  40%|      | 4292/10788 [01:44<02:53, 37.49it/s]Training CobwebTree:  40%|      | 4296/10788 [01:44<02:57, 36.63it/s]Training CobwebTree:  40%|      | 4300/10788 [01:45<02:54, 37.17it/s]Training CobwebTree:  40%|      | 4304/10788 [01:45<03:01, 35.81it/s]Training CobwebTree:  40%|      | 4309/10788 [01:45<02:53, 37.35it/s]Training CobwebTree:  40%|      | 4314/10788 [01:45<02:44, 39.33it/s]Training CobwebTree:  40%|      | 4318/10788 [01:45<02:47, 38.60it/s]Training CobwebTree:  40%|      | 4322/10788 [01:45<02:48, 38.27it/s]Training CobwebTree:  40%|      | 4326/10788 [01:45<02:47, 38.48it/s]Training CobwebTree:  40%|      | 4330/10788 [01:45<02:50, 37.91it/s]Training CobwebTree:  40%|      | 4334/10788 [01:45<02:48, 38.38it/s]Training CobwebTree:  40%|      | 4339/10788 [01:46<02:47, 38.50it/s]Training CobwebTree:  40%|      | 4343/10788 [01:46<02:48, 38.36it/s]Training CobwebTree:  40%|      | 4347/10788 [01:46<02:47, 38.56it/s]Training CobwebTree:  40%|      | 4351/10788 [01:46<02:47, 38.40it/s]Training CobwebTree:  40%|      | 4355/10788 [01:46<02:48, 38.19it/s]Training CobwebTree:  40%|      | 4359/10788 [01:46<02:46, 38.70it/s]Training CobwebTree:  40%|      | 4364/10788 [01:46<02:38, 40.45it/s]Training CobwebTree:  40%|      | 4369/10788 [01:46<02:44, 39.09it/s]Training CobwebTree:  41%|      | 4374/10788 [01:46<02:36, 40.98it/s]Training CobwebTree:  41%|      | 4379/10788 [01:47<02:39, 40.26it/s]Training CobwebTree:  41%|      | 4384/10788 [01:47<02:44, 38.93it/s]Training CobwebTree:  41%|      | 4388/10788 [01:47<02:52, 37.07it/s]Training CobwebTree:  41%|      | 4392/10788 [01:47<02:53, 36.90it/s]Training CobwebTree:  41%|      | 4397/10788 [01:47<02:46, 38.36it/s]Training CobwebTree:  41%|      | 4402/10788 [01:47<02:41, 39.44it/s]Training CobwebTree:  41%|      | 4406/10788 [01:47<02:51, 37.23it/s]Training CobwebTree:  41%|      | 4410/10788 [01:47<02:50, 37.48it/s]Training CobwebTree:  41%|      | 4414/10788 [01:47<02:48, 37.82it/s]Training CobwebTree:  41%|      | 4418/10788 [01:48<02:55, 36.31it/s]Training CobwebTree:  41%|      | 4422/10788 [01:48<02:58, 35.70it/s]Training CobwebTree:  41%|      | 4426/10788 [01:48<02:59, 35.36it/s]Training CobwebTree:  41%|      | 4430/10788 [01:48<02:57, 35.75it/s]Training CobwebTree:  41%|      | 4435/10788 [01:48<02:50, 37.30it/s]Training CobwebTree:  41%|      | 4440/10788 [01:48<02:43, 38.71it/s]Training CobwebTree:  41%|      | 4444/10788 [01:48<02:47, 37.86it/s]Training CobwebTree:  41%|      | 4448/10788 [01:48<02:49, 37.48it/s]Training CobwebTree:  41%|     | 4452/10788 [01:49<03:00, 35.07it/s]Training CobwebTree:  41%|     | 4456/10788 [01:49<03:12, 32.81it/s]Training CobwebTree:  41%|     | 4460/10788 [01:49<03:11, 33.08it/s]Training CobwebTree:  41%|     | 4465/10788 [01:49<02:58, 35.36it/s]Training CobwebTree:  41%|     | 4469/10788 [01:49<02:53, 36.38it/s]Training CobwebTree:  41%|     | 4474/10788 [01:49<02:44, 38.33it/s]Training CobwebTree:  42%|     | 4478/10788 [01:49<02:45, 38.11it/s]Training CobwebTree:  42%|     | 4482/10788 [01:49<02:46, 37.94it/s]Training CobwebTree:  42%|     | 4486/10788 [01:49<02:51, 36.77it/s]Training CobwebTree:  42%|     | 4491/10788 [01:50<02:43, 38.57it/s]Training CobwebTree:  42%|     | 4495/10788 [01:50<02:44, 38.22it/s]Training CobwebTree:  42%|     | 4500/10788 [01:50<02:42, 38.71it/s]Training CobwebTree:  42%|     | 4505/10788 [01:50<02:37, 39.92it/s]Training CobwebTree:  42%|     | 4509/10788 [01:50<02:38, 39.58it/s]Training CobwebTree:  42%|     | 4513/10788 [01:50<02:45, 37.97it/s]Training CobwebTree:  42%|     | 4517/10788 [01:50<02:43, 38.38it/s]Training CobwebTree:  42%|     | 4521/10788 [01:50<02:50, 36.86it/s]Training CobwebTree:  42%|     | 4525/10788 [01:51<02:56, 35.41it/s]Training CobwebTree:  42%|     | 4529/10788 [01:51<02:53, 36.05it/s]Training CobwebTree:  42%|     | 4533/10788 [01:51<02:52, 36.31it/s]Training CobwebTree:  42%|     | 4537/10788 [01:51<02:47, 37.26it/s]Training CobwebTree:  42%|     | 4541/10788 [01:51<02:53, 36.06it/s]Training CobwebTree:  42%|     | 4545/10788 [01:51<03:00, 34.57it/s]Training CobwebTree:  42%|     | 4549/10788 [01:51<03:00, 34.65it/s]Training CobwebTree:  42%|     | 4553/10788 [01:51<02:53, 35.91it/s]Training CobwebTree:  42%|     | 4558/10788 [01:51<02:48, 37.05it/s]Training CobwebTree:  42%|     | 4563/10788 [01:52<02:44, 37.80it/s]Training CobwebTree:  42%|     | 4567/10788 [01:52<02:51, 36.37it/s]Training CobwebTree:  42%|     | 4571/10788 [01:52<02:48, 36.96it/s]Training CobwebTree:  42%|     | 4576/10788 [01:52<02:40, 38.68it/s]Training CobwebTree:  42%|     | 4580/10788 [01:52<02:42, 38.19it/s]Training CobwebTree:  42%|     | 4584/10788 [01:52<02:41, 38.41it/s]Training CobwebTree:  43%|     | 4589/10788 [01:52<02:33, 40.35it/s]Training CobwebTree:  43%|     | 4594/10788 [01:52<02:32, 40.58it/s]Training CobwebTree:  43%|     | 4599/10788 [01:52<02:33, 40.37it/s]Training CobwebTree:  43%|     | 4604/10788 [01:53<02:40, 38.48it/s]Training CobwebTree:  43%|     | 4608/10788 [01:53<02:42, 38.03it/s]Training CobwebTree:  43%|     | 4612/10788 [01:53<02:43, 37.77it/s]Training CobwebTree:  43%|     | 4616/10788 [01:53<02:43, 37.80it/s]Training CobwebTree:  43%|     | 4621/10788 [01:53<02:39, 38.75it/s]Training CobwebTree:  43%|     | 4625/10788 [01:53<02:38, 38.77it/s]Training CobwebTree:  43%|     | 4629/10788 [01:53<02:43, 37.66it/s]Training CobwebTree:  43%|     | 4633/10788 [01:53<02:47, 36.77it/s]Training CobwebTree:  43%|     | 4637/10788 [01:53<02:50, 36.12it/s]Training CobwebTree:  43%|     | 4641/10788 [01:54<02:54, 35.29it/s]Training CobwebTree:  43%|     | 4645/10788 [01:54<02:51, 35.75it/s]Training CobwebTree:  43%|     | 4649/10788 [01:54<02:47, 36.74it/s]Training CobwebTree:  43%|     | 4654/10788 [01:54<02:42, 37.67it/s]Training CobwebTree:  43%|     | 4659/10788 [01:54<02:32, 40.11it/s]Training CobwebTree:  43%|     | 4664/10788 [01:54<02:38, 38.70it/s]Training CobwebTree:  43%|     | 4668/10788 [01:54<02:37, 38.98it/s]Training CobwebTree:  43%|     | 4672/10788 [01:54<02:36, 39.12it/s]Training CobwebTree:  43%|     | 4676/10788 [01:54<02:35, 39.25it/s]Training CobwebTree:  43%|     | 4680/10788 [01:55<02:39, 38.26it/s]Training CobwebTree:  43%|     | 4685/10788 [01:55<02:35, 39.32it/s]Training CobwebTree:  43%|     | 4689/10788 [01:55<02:38, 38.59it/s]Training CobwebTree:  44%|     | 4693/10788 [01:55<02:42, 37.50it/s]Training CobwebTree:  44%|     | 4697/10788 [01:55<02:50, 35.81it/s]Training CobwebTree:  44%|     | 4701/10788 [01:55<02:51, 35.51it/s]Training CobwebTree:  44%|     | 4705/10788 [01:55<02:48, 36.14it/s]Training CobwebTree:  44%|     | 4710/10788 [01:55<02:40, 37.85it/s]Training CobwebTree:  44%|     | 4714/10788 [01:56<02:41, 37.58it/s]Training CobwebTree:  44%|     | 4718/10788 [01:56<02:43, 37.02it/s]Training CobwebTree:  44%|     | 4722/10788 [01:56<02:43, 37.01it/s]Training CobwebTree:  44%|     | 4727/10788 [01:56<02:38, 38.14it/s]Training CobwebTree:  44%|     | 4731/10788 [01:56<02:39, 38.07it/s]Training CobwebTree:  44%|     | 4735/10788 [01:56<02:39, 37.89it/s]Training CobwebTree:  44%|     | 4739/10788 [01:56<02:44, 36.86it/s]Training CobwebTree:  44%|     | 4743/10788 [01:56<02:40, 37.67it/s]Training CobwebTree:  44%|     | 4747/10788 [01:56<02:39, 37.77it/s]Training CobwebTree:  44%|     | 4751/10788 [01:57<02:42, 37.04it/s]Training CobwebTree:  44%|     | 4755/10788 [01:57<02:42, 37.22it/s]Training CobwebTree:  44%|     | 4759/10788 [01:57<02:47, 36.01it/s]Training CobwebTree:  44%|     | 4763/10788 [01:57<02:45, 36.39it/s]Training CobwebTree:  44%|     | 4767/10788 [01:57<02:52, 34.98it/s]Training CobwebTree:  44%|     | 4771/10788 [01:57<02:53, 34.77it/s]Training CobwebTree:  44%|     | 4775/10788 [01:57<02:49, 35.44it/s]Training CobwebTree:  44%|     | 4779/10788 [01:57<02:45, 36.27it/s]Training CobwebTree:  44%|     | 4783/10788 [01:57<02:48, 35.74it/s]Training CobwebTree:  44%|     | 4787/10788 [01:58<02:48, 35.62it/s]Training CobwebTree:  44%|     | 4791/10788 [01:58<02:43, 36.77it/s]Training CobwebTree:  44%|     | 4796/10788 [01:58<02:37, 38.06it/s]Training CobwebTree:  44%|     | 4800/10788 [01:58<02:41, 37.12it/s]Training CobwebTree:  45%|     | 4804/10788 [01:58<02:41, 37.01it/s]Training CobwebTree:  45%|     | 4808/10788 [01:58<02:41, 36.99it/s]Training CobwebTree:  45%|     | 4812/10788 [01:58<02:39, 37.40it/s]Training CobwebTree:  45%|     | 4816/10788 [01:58<02:38, 37.66it/s]Training CobwebTree:  45%|     | 4820/10788 [01:58<02:37, 37.84it/s]Training CobwebTree:  45%|     | 4824/10788 [01:59<02:41, 36.86it/s]Training CobwebTree:  45%|     | 4828/10788 [01:59<02:39, 37.28it/s]Training CobwebTree:  45%|     | 4832/10788 [01:59<02:36, 38.02it/s]Training CobwebTree:  45%|     | 4836/10788 [01:59<02:36, 38.07it/s]Training CobwebTree:  45%|     | 4841/10788 [01:59<02:33, 38.72it/s]Training CobwebTree:  45%|     | 4845/10788 [01:59<02:35, 38.19it/s]Training CobwebTree:  45%|     | 4849/10788 [01:59<02:36, 38.02it/s]Training CobwebTree:  45%|     | 4853/10788 [01:59<02:40, 37.06it/s]Training CobwebTree:  45%|     | 4858/10788 [01:59<02:28, 39.84it/s]Training CobwebTree:  45%|     | 4863/10788 [02:00<02:27, 40.08it/s]Training CobwebTree:  45%|     | 4868/10788 [02:00<02:31, 39.16it/s]Training CobwebTree:  45%|     | 4872/10788 [02:00<02:34, 38.22it/s]Training CobwebTree:  45%|     | 4876/10788 [02:00<02:33, 38.54it/s]Training CobwebTree:  45%|     | 4880/10788 [02:00<02:40, 36.77it/s]Training CobwebTree:  45%|     | 4885/10788 [02:00<02:33, 38.51it/s]Training CobwebTree:  45%|     | 4889/10788 [02:00<02:35, 37.84it/s]Training CobwebTree:  45%|     | 4893/10788 [02:00<02:36, 37.65it/s]Training CobwebTree:  45%|     | 4897/10788 [02:00<02:40, 36.69it/s]Training CobwebTree:  45%|     | 4901/10788 [02:01<02:41, 36.48it/s]Training CobwebTree:  45%|     | 4905/10788 [02:01<02:44, 35.73it/s]Training CobwebTree:  46%|     | 4909/10788 [02:01<02:40, 36.53it/s]Training CobwebTree:  46%|     | 4913/10788 [02:01<02:39, 36.94it/s]Training CobwebTree:  46%|     | 4917/10788 [02:01<02:45, 35.48it/s]Training CobwebTree:  46%|     | 4921/10788 [02:01<02:41, 36.24it/s]Training CobwebTree:  46%|     | 4925/10788 [02:01<02:44, 35.66it/s]Training CobwebTree:  46%|     | 4929/10788 [02:01<02:41, 36.19it/s]Training CobwebTree:  46%|     | 4933/10788 [02:01<02:37, 37.16it/s]Training CobwebTree:  46%|     | 4937/10788 [02:02<02:37, 37.17it/s]Training CobwebTree:  46%|     | 4942/10788 [02:02<02:33, 38.10it/s]Training CobwebTree:  46%|     | 4946/10788 [02:02<02:38, 36.85it/s]Training CobwebTree:  46%|     | 4951/10788 [02:02<02:32, 38.25it/s]Training CobwebTree:  46%|     | 4955/10788 [02:02<02:33, 37.91it/s]Training CobwebTree:  46%|     | 4960/10788 [02:02<02:25, 40.07it/s]Training CobwebTree:  46%|     | 4965/10788 [02:02<02:24, 40.35it/s]Training CobwebTree:  46%|     | 4970/10788 [02:02<02:20, 41.49it/s]Training CobwebTree:  46%|     | 4975/10788 [02:02<02:26, 39.81it/s]Training CobwebTree:  46%|     | 4980/10788 [02:03<02:31, 38.34it/s]Training CobwebTree:  46%|     | 4984/10788 [02:03<02:32, 38.05it/s]Training CobwebTree:  46%|     | 4988/10788 [02:03<02:36, 36.96it/s]Training CobwebTree:  46%|     | 4992/10788 [02:03<02:40, 36.15it/s]Training CobwebTree:  46%|     | 4996/10788 [02:03<02:37, 36.68it/s]Training CobwebTree:  46%|     | 5001/10788 [02:03<02:35, 37.15it/s]Training CobwebTree:  46%|     | 5006/10788 [02:03<02:31, 38.05it/s]Training CobwebTree:  46%|     | 5010/10788 [02:03<02:31, 38.04it/s]Training CobwebTree:  46%|     | 5015/10788 [02:04<02:25, 39.64it/s]Training CobwebTree:  47%|     | 5019/10788 [02:04<02:28, 38.87it/s]Training CobwebTree:  47%|     | 5024/10788 [02:04<02:25, 39.71it/s]Training CobwebTree:  47%|     | 5028/10788 [02:04<02:25, 39.59it/s]Training CobwebTree:  47%|     | 5032/10788 [02:04<02:30, 38.25it/s]Training CobwebTree:  47%|     | 5036/10788 [02:04<02:34, 37.23it/s]Training CobwebTree:  47%|     | 5041/10788 [02:04<02:21, 40.51it/s]Training CobwebTree:  47%|     | 5046/10788 [02:04<02:21, 40.62it/s]Training CobwebTree:  47%|     | 5051/10788 [02:04<02:26, 39.06it/s]Training CobwebTree:  47%|     | 5055/10788 [02:05<02:33, 37.23it/s]Training CobwebTree:  47%|     | 5059/10788 [02:05<02:31, 37.88it/s]Training CobwebTree:  47%|     | 5063/10788 [02:05<02:32, 37.51it/s]Training CobwebTree:  47%|     | 5067/10788 [02:05<02:34, 36.93it/s]Training CobwebTree:  47%|     | 5072/10788 [02:05<02:31, 37.85it/s]Training CobwebTree:  47%|     | 5076/10788 [02:05<02:31, 37.62it/s]Training CobwebTree:  47%|     | 5080/10788 [02:05<02:30, 37.97it/s]Training CobwebTree:  47%|     | 5084/10788 [02:05<02:33, 37.18it/s]Training CobwebTree:  47%|     | 5089/10788 [02:05<02:28, 38.40it/s]Training CobwebTree:  47%|     | 5094/10788 [02:06<02:24, 39.39it/s]Training CobwebTree:  47%|     | 5098/10788 [02:06<02:28, 38.20it/s]Training CobwebTree:  47%|     | 5102/10788 [02:06<02:28, 38.31it/s]Training CobwebTree:  47%|     | 5107/10788 [02:06<02:22, 39.83it/s]Training CobwebTree:  47%|     | 5111/10788 [02:06<02:29, 38.04it/s]Training CobwebTree:  47%|     | 5115/10788 [02:06<02:32, 37.13it/s]Training CobwebTree:  47%|     | 5119/10788 [02:06<02:30, 37.73it/s]Training CobwebTree:  47%|     | 5123/10788 [02:06<02:34, 36.76it/s]Training CobwebTree:  48%|     | 5127/10788 [02:06<02:34, 36.71it/s]Training CobwebTree:  48%|     | 5131/10788 [02:07<02:34, 36.63it/s]Training CobwebTree:  48%|     | 5135/10788 [02:07<02:33, 36.92it/s]Training CobwebTree:  48%|     | 5140/10788 [02:07<02:26, 38.47it/s]Training CobwebTree:  48%|     | 5144/10788 [02:07<02:26, 38.61it/s]Training CobwebTree:  48%|     | 5148/10788 [02:07<02:37, 35.84it/s]Training CobwebTree:  48%|     | 5152/10788 [02:07<02:39, 35.33it/s]Training CobwebTree:  48%|     | 5156/10788 [02:07<02:39, 35.27it/s]Training CobwebTree:  48%|     | 5160/10788 [02:07<02:38, 35.53it/s]Training CobwebTree:  48%|     | 5164/10788 [02:08<02:33, 36.52it/s]Training CobwebTree:  48%|     | 5168/10788 [02:08<02:42, 34.59it/s]Training CobwebTree:  48%|     | 5172/10788 [02:08<02:51, 32.75it/s]Training CobwebTree:  48%|     | 5176/10788 [02:08<02:44, 34.05it/s]Training CobwebTree:  48%|     | 5180/10788 [02:08<02:51, 32.75it/s]Training CobwebTree:  48%|     | 5184/10788 [02:08<02:47, 33.37it/s]Training CobwebTree:  48%|     | 5189/10788 [02:08<02:37, 35.57it/s]Training CobwebTree:  48%|     | 5193/10788 [02:08<02:36, 35.70it/s]Training CobwebTree:  48%|     | 5198/10788 [02:08<02:30, 37.05it/s]Training CobwebTree:  48%|     | 5202/10788 [02:09<02:29, 37.40it/s]Training CobwebTree:  48%|     | 5206/10788 [02:09<02:27, 37.89it/s]Training CobwebTree:  48%|     | 5211/10788 [02:09<02:24, 38.63it/s]Training CobwebTree:  48%|     | 5216/10788 [02:09<02:19, 39.89it/s]Training CobwebTree:  48%|     | 5220/10788 [02:09<02:25, 38.27it/s]Training CobwebTree:  48%|     | 5224/10788 [02:09<02:28, 37.52it/s]Training CobwebTree:  48%|     | 5228/10788 [02:09<02:29, 37.12it/s]Training CobwebTree:  48%|     | 5232/10788 [02:09<02:32, 36.47it/s]Training CobwebTree:  49%|     | 5236/10788 [02:10<02:36, 35.51it/s]Training CobwebTree:  49%|     | 5240/10788 [02:10<02:42, 34.12it/s]Training CobwebTree:  49%|     | 5244/10788 [02:10<02:43, 33.84it/s]Training CobwebTree:  49%|     | 5248/10788 [02:10<02:42, 34.01it/s]Training CobwebTree:  49%|     | 5252/10788 [02:10<02:41, 34.35it/s]Training CobwebTree:  49%|     | 5256/10788 [02:10<02:37, 35.02it/s]Training CobwebTree:  49%|     | 5260/10788 [02:10<02:36, 35.29it/s]Training CobwebTree:  49%|     | 5264/10788 [02:10<02:41, 34.18it/s]Training CobwebTree:  49%|     | 5269/10788 [02:10<02:34, 35.75it/s]Training CobwebTree:  49%|     | 5273/10788 [02:11<02:36, 35.14it/s]Training CobwebTree:  49%|     | 5278/10788 [02:11<02:22, 38.78it/s]Training CobwebTree:  49%|     | 5282/10788 [02:11<02:26, 37.69it/s]Training CobwebTree:  49%|     | 5286/10788 [02:11<02:29, 36.87it/s]Training CobwebTree:  49%|     | 5291/10788 [02:11<02:24, 38.09it/s]Training CobwebTree:  49%|     | 5295/10788 [02:11<02:25, 37.77it/s]Training CobwebTree:  49%|     | 5300/10788 [02:11<02:20, 38.95it/s]Training CobwebTree:  49%|     | 5304/10788 [02:11<02:26, 37.52it/s]Training CobwebTree:  49%|     | 5308/10788 [02:11<02:31, 36.17it/s]Training CobwebTree:  49%|     | 5313/10788 [02:12<02:26, 37.33it/s]Training CobwebTree:  49%|     | 5318/10788 [02:12<02:19, 39.23it/s]Training CobwebTree:  49%|     | 5322/10788 [02:12<02:29, 36.67it/s]Training CobwebTree:  49%|     | 5327/10788 [02:12<02:22, 38.35it/s]Training CobwebTree:  49%|     | 5332/10788 [02:12<02:13, 40.76it/s]Training CobwebTree:  49%|     | 5337/10788 [02:12<02:13, 40.92it/s]Training CobwebTree:  50%|     | 5342/10788 [02:12<02:19, 39.04it/s]Training CobwebTree:  50%|     | 5346/10788 [02:12<02:24, 37.54it/s]Training CobwebTree:  50%|     | 5350/10788 [02:13<02:28, 36.66it/s]Training CobwebTree:  50%|     | 5354/10788 [02:13<02:26, 36.97it/s]Training CobwebTree:  50%|     | 5358/10788 [02:13<02:28, 36.66it/s]Training CobwebTree:  50%|     | 5362/10788 [02:13<02:28, 36.48it/s]Training CobwebTree:  50%|     | 5366/10788 [02:13<02:25, 37.29it/s]Training CobwebTree:  50%|     | 5370/10788 [02:13<02:27, 36.79it/s]Training CobwebTree:  50%|     | 5375/10788 [02:13<02:25, 37.12it/s]Training CobwebTree:  50%|     | 5379/10788 [02:13<02:29, 36.23it/s]Training CobwebTree:  50%|     | 5383/10788 [02:13<02:25, 37.09it/s]Training CobwebTree:  50%|     | 5388/10788 [02:14<02:22, 37.97it/s]Training CobwebTree:  50%|     | 5392/10788 [02:14<02:25, 37.08it/s]Training CobwebTree:  50%|     | 5396/10788 [02:14<02:26, 36.81it/s]Training CobwebTree:  50%|     | 5400/10788 [02:14<02:29, 36.09it/s]Training CobwebTree:  50%|     | 5405/10788 [02:14<02:22, 37.79it/s]Training CobwebTree:  50%|     | 5410/10788 [02:14<02:18, 38.80it/s]Training CobwebTree:  50%|     | 5414/10788 [02:14<02:19, 38.58it/s]Training CobwebTree:  50%|     | 5418/10788 [02:14<02:20, 38.18it/s]Training CobwebTree:  50%|     | 5422/10788 [02:15<02:29, 35.96it/s]Training CobwebTree:  50%|     | 5426/10788 [02:15<02:31, 35.29it/s]Training CobwebTree:  50%|     | 5430/10788 [02:15<02:29, 35.82it/s]Training CobwebTree:  50%|     | 5434/10788 [02:15<02:30, 35.66it/s]Training CobwebTree:  50%|     | 5438/10788 [02:15<02:29, 35.75it/s]Training CobwebTree:  50%|     | 5442/10788 [02:15<02:29, 35.80it/s]Training CobwebTree:  50%|     | 5446/10788 [02:15<02:29, 35.80it/s]Training CobwebTree:  51%|     | 5450/10788 [02:15<02:29, 35.70it/s]Training CobwebTree:  51%|     | 5454/10788 [02:15<02:27, 36.07it/s]Training CobwebTree:  51%|     | 5458/10788 [02:16<02:24, 36.82it/s]Training CobwebTree:  51%|     | 5462/10788 [02:16<02:23, 37.06it/s]Training CobwebTree:  51%|     | 5466/10788 [02:16<02:24, 36.92it/s]Training CobwebTree:  51%|     | 5470/10788 [02:16<02:30, 35.33it/s]Training CobwebTree:  51%|     | 5474/10788 [02:16<02:32, 34.78it/s]Training CobwebTree:  51%|     | 5478/10788 [02:16<02:31, 35.03it/s]Training CobwebTree:  51%|     | 5482/10788 [02:16<02:28, 35.78it/s]Training CobwebTree:  51%|     | 5486/10788 [02:16<02:23, 36.93it/s]Training CobwebTree:  51%|     | 5490/10788 [02:16<02:24, 36.72it/s]Training CobwebTree:  51%|     | 5495/10788 [02:17<02:20, 37.60it/s]Training CobwebTree:  51%|     | 5499/10788 [02:17<02:21, 37.37it/s]Training CobwebTree:  51%|     | 5503/10788 [02:17<02:21, 37.39it/s]Training CobwebTree:  51%|     | 5508/10788 [02:17<02:15, 39.00it/s]Training CobwebTree:  51%|     | 5512/10788 [02:17<02:17, 38.40it/s]Training CobwebTree:  51%|     | 5516/10788 [02:17<02:19, 37.72it/s]Training CobwebTree:  51%|     | 5520/10788 [02:17<02:17, 38.33it/s]Training CobwebTree:  51%|     | 5524/10788 [02:17<02:24, 36.52it/s]Training CobwebTree:  51%|     | 5528/10788 [02:17<02:20, 37.32it/s]Training CobwebTree:  51%|    | 5532/10788 [02:18<02:19, 37.71it/s]Training CobwebTree:  51%|    | 5536/10788 [02:18<02:18, 37.92it/s]Training CobwebTree:  51%|    | 5540/10788 [02:18<02:26, 35.87it/s]Training CobwebTree:  51%|    | 5544/10788 [02:18<02:24, 36.27it/s]Training CobwebTree:  51%|    | 5548/10788 [02:18<02:26, 35.65it/s]Training CobwebTree:  51%|    | 5553/10788 [02:18<02:20, 37.30it/s]Training CobwebTree:  52%|    | 5557/10788 [02:18<02:21, 37.05it/s]Training CobwebTree:  52%|    | 5562/10788 [02:18<02:17, 37.89it/s]Training CobwebTree:  52%|    | 5566/10788 [02:18<02:22, 36.66it/s]Training CobwebTree:  52%|    | 5570/10788 [02:19<02:23, 36.29it/s]Training CobwebTree:  52%|    | 5575/10788 [02:19<02:20, 37.14it/s]Training CobwebTree:  52%|    | 5579/10788 [02:19<02:20, 37.13it/s]Training CobwebTree:  52%|    | 5583/10788 [02:19<02:25, 35.89it/s]Training CobwebTree:  52%|    | 5587/10788 [02:19<02:26, 35.49it/s]Training CobwebTree:  52%|    | 5591/10788 [02:19<02:22, 36.49it/s]Training CobwebTree:  52%|    | 5595/10788 [02:19<02:22, 36.39it/s]Training CobwebTree:  52%|    | 5599/10788 [02:19<02:21, 36.66it/s]Training CobwebTree:  52%|    | 5603/10788 [02:19<02:22, 36.44it/s]Training CobwebTree:  52%|    | 5607/10788 [02:20<02:31, 34.20it/s]Training CobwebTree:  52%|    | 5611/10788 [02:20<02:31, 34.18it/s]Training CobwebTree:  52%|    | 5615/10788 [02:20<02:32, 33.98it/s]Training CobwebTree:  52%|    | 5619/10788 [02:20<02:30, 34.32it/s]Training CobwebTree:  52%|    | 5623/10788 [02:20<02:24, 35.65it/s]Training CobwebTree:  52%|    | 5628/10788 [02:20<02:19, 37.00it/s]Training CobwebTree:  52%|    | 5632/10788 [02:20<02:23, 35.99it/s]Training CobwebTree:  52%|    | 5636/10788 [02:20<02:27, 34.93it/s]Training CobwebTree:  52%|    | 5640/10788 [02:21<02:23, 35.95it/s]Training CobwebTree:  52%|    | 5644/10788 [02:21<02:22, 35.97it/s]Training CobwebTree:  52%|    | 5649/10788 [02:21<02:17, 37.48it/s]Training CobwebTree:  52%|    | 5654/10788 [02:21<02:14, 38.18it/s]Training CobwebTree:  52%|    | 5659/10788 [02:21<02:09, 39.46it/s]Training CobwebTree:  52%|    | 5663/10788 [02:21<02:11, 39.06it/s]Training CobwebTree:  53%|    | 5667/10788 [02:21<02:12, 38.79it/s]Training CobwebTree:  53%|    | 5672/10788 [02:21<02:10, 39.18it/s]Training CobwebTree:  53%|    | 5676/10788 [02:21<02:16, 37.44it/s]Training CobwebTree:  53%|    | 5680/10788 [02:22<02:15, 37.60it/s]Training CobwebTree:  53%|    | 5684/10788 [02:22<02:15, 37.59it/s]Training CobwebTree:  53%|    | 5688/10788 [02:22<02:17, 37.11it/s]Training CobwebTree:  53%|    | 5692/10788 [02:22<02:16, 37.27it/s]Training CobwebTree:  53%|    | 5697/10788 [02:22<02:11, 38.80it/s]Training CobwebTree:  53%|    | 5701/10788 [02:22<02:13, 38.07it/s]Training CobwebTree:  53%|    | 5705/10788 [02:22<02:20, 36.26it/s]Training CobwebTree:  53%|    | 5709/10788 [02:22<02:18, 36.74it/s]Training CobwebTree:  53%|    | 5713/10788 [02:22<02:20, 36.24it/s]Training CobwebTree:  53%|    | 5717/10788 [02:23<02:19, 36.27it/s]Training CobwebTree:  53%|    | 5721/10788 [02:23<02:23, 35.37it/s]Training CobwebTree:  53%|    | 5725/10788 [02:23<02:19, 36.38it/s]Training CobwebTree:  53%|    | 5729/10788 [02:23<02:23, 35.30it/s]Training CobwebTree:  53%|    | 5733/10788 [02:23<02:19, 36.16it/s]Training CobwebTree:  53%|    | 5737/10788 [02:23<02:25, 34.79it/s]Training CobwebTree:  53%|    | 5742/10788 [02:23<02:19, 36.16it/s]Training CobwebTree:  53%|    | 5746/10788 [02:23<02:21, 35.71it/s]Training CobwebTree:  53%|    | 5750/10788 [02:23<02:19, 36.03it/s]Training CobwebTree:  53%|    | 5754/10788 [02:24<02:20, 35.92it/s]Training CobwebTree:  53%|    | 5758/10788 [02:24<02:18, 36.44it/s]Training CobwebTree:  53%|    | 5762/10788 [02:24<02:15, 36.98it/s]Training CobwebTree:  53%|    | 5766/10788 [02:24<02:14, 37.29it/s]Training CobwebTree:  53%|    | 5770/10788 [02:24<02:14, 37.24it/s]Training CobwebTree:  54%|    | 5774/10788 [02:24<02:15, 37.13it/s]Training CobwebTree:  54%|    | 5778/10788 [02:24<02:14, 37.22it/s]Training CobwebTree:  54%|    | 5782/10788 [02:24<02:16, 36.58it/s]Training CobwebTree:  54%|    | 5786/10788 [02:24<02:14, 37.12it/s]Training CobwebTree:  54%|    | 5790/10788 [02:25<02:15, 36.82it/s]Training CobwebTree:  54%|    | 5794/10788 [02:25<02:17, 36.23it/s]Training CobwebTree:  54%|    | 5798/10788 [02:25<02:22, 34.92it/s]Training CobwebTree:  54%|    | 5802/10788 [02:25<02:21, 35.33it/s]Training CobwebTree:  54%|    | 5806/10788 [02:25<02:21, 35.11it/s]Training CobwebTree:  54%|    | 5810/10788 [02:25<02:21, 35.29it/s]Training CobwebTree:  54%|    | 5814/10788 [02:25<02:16, 36.42it/s]Training CobwebTree:  54%|    | 5818/10788 [02:25<02:19, 35.52it/s]Training CobwebTree:  54%|    | 5822/10788 [02:25<02:15, 36.56it/s]Training CobwebTree:  54%|    | 5827/10788 [02:26<02:13, 37.23it/s]Training CobwebTree:  54%|    | 5831/10788 [02:26<02:18, 35.77it/s]Training CobwebTree:  54%|    | 5835/10788 [02:26<02:15, 36.63it/s]Training CobwebTree:  54%|    | 5839/10788 [02:26<02:17, 36.03it/s]Training CobwebTree:  54%|    | 5843/10788 [02:26<02:18, 35.58it/s]Training CobwebTree:  54%|    | 5848/10788 [02:26<02:15, 36.40it/s]Training CobwebTree:  54%|    | 5852/10788 [02:26<02:14, 36.62it/s]Training CobwebTree:  54%|    | 5857/10788 [02:26<02:13, 36.99it/s]Training CobwebTree:  54%|    | 5862/10788 [02:27<02:09, 38.11it/s]Training CobwebTree:  54%|    | 5866/10788 [02:27<02:16, 36.13it/s]Training CobwebTree:  54%|    | 5871/10788 [02:27<02:10, 37.66it/s]Training CobwebTree:  54%|    | 5875/10788 [02:27<02:16, 36.07it/s]Training CobwebTree:  54%|    | 5879/10788 [02:27<02:21, 34.72it/s]Training CobwebTree:  55%|    | 5883/10788 [02:27<02:17, 35.61it/s]Training CobwebTree:  55%|    | 5887/10788 [02:27<02:21, 34.73it/s]Training CobwebTree:  55%|    | 5891/10788 [02:27<02:20, 34.84it/s]Training CobwebTree:  55%|    | 5895/10788 [02:28<02:19, 35.03it/s]Training CobwebTree:  55%|    | 5899/10788 [02:28<02:18, 35.36it/s]Training CobwebTree:  55%|    | 5903/10788 [02:28<02:22, 34.37it/s]Training CobwebTree:  55%|    | 5907/10788 [02:28<02:24, 33.86it/s]Training CobwebTree:  55%|    | 5911/10788 [02:28<02:22, 34.20it/s]Training CobwebTree:  55%|    | 5915/10788 [02:28<02:20, 34.71it/s]Training CobwebTree:  55%|    | 5919/10788 [02:28<02:16, 35.61it/s]Training CobwebTree:  55%|    | 5923/10788 [02:28<02:20, 34.72it/s]Training CobwebTree:  55%|    | 5927/10788 [02:28<02:23, 33.98it/s]Training CobwebTree:  55%|    | 5931/10788 [02:29<02:17, 35.34it/s]Training CobwebTree:  55%|    | 5935/10788 [02:29<02:17, 35.26it/s]Training CobwebTree:  55%|    | 5939/10788 [02:29<02:16, 35.51it/s]Training CobwebTree:  55%|    | 5943/10788 [02:29<02:13, 36.32it/s]Training CobwebTree:  55%|    | 5947/10788 [02:29<02:12, 36.45it/s]Training CobwebTree:  55%|    | 5951/10788 [02:29<02:15, 35.63it/s]Training CobwebTree:  55%|    | 5956/10788 [02:29<02:10, 37.05it/s]Training CobwebTree:  55%|    | 5960/10788 [02:29<02:10, 37.13it/s]Training CobwebTree:  55%|    | 5964/10788 [02:29<02:14, 35.95it/s]Training CobwebTree:  55%|    | 5968/10788 [02:30<02:16, 35.20it/s]Training CobwebTree:  55%|    | 5972/10788 [02:30<02:14, 35.89it/s]Training CobwebTree:  55%|    | 5976/10788 [02:30<02:11, 36.72it/s]Training CobwebTree:  55%|    | 5980/10788 [02:30<02:13, 36.07it/s]Training CobwebTree:  55%|    | 5984/10788 [02:30<02:16, 35.11it/s]Training CobwebTree:  56%|    | 5988/10788 [02:30<02:14, 35.63it/s]Training CobwebTree:  56%|    | 5992/10788 [02:30<02:16, 35.22it/s]Training CobwebTree:  56%|    | 5996/10788 [02:30<02:12, 36.11it/s]Training CobwebTree:  56%|    | 6000/10788 [02:30<02:14, 35.47it/s]Training CobwebTree:  56%|    | 6005/10788 [02:31<02:10, 36.73it/s]Training CobwebTree:  56%|    | 6009/10788 [02:31<02:08, 37.32it/s]Training CobwebTree:  56%|    | 6013/10788 [02:31<02:11, 36.21it/s]Training CobwebTree:  56%|    | 6018/10788 [02:31<02:03, 38.71it/s]Training CobwebTree:  56%|    | 6022/10788 [02:31<02:10, 36.50it/s]Training CobwebTree:  56%|    | 6026/10788 [02:31<02:13, 35.57it/s]Training CobwebTree:  56%|    | 6031/10788 [02:31<02:07, 37.37it/s]Training CobwebTree:  56%|    | 6035/10788 [02:31<02:12, 35.88it/s]Training CobwebTree:  56%|    | 6039/10788 [02:32<02:15, 35.17it/s]Training CobwebTree:  56%|    | 6043/10788 [02:32<02:14, 35.38it/s]Training CobwebTree:  56%|    | 6047/10788 [02:32<02:10, 36.25it/s]Training CobwebTree:  56%|    | 6051/10788 [02:32<02:12, 35.63it/s]Training CobwebTree:  56%|    | 6055/10788 [02:32<02:10, 36.37it/s]Training CobwebTree:  56%|    | 6059/10788 [02:32<02:09, 36.49it/s]Training CobwebTree:  56%|    | 6063/10788 [02:32<02:08, 36.76it/s]Training CobwebTree:  56%|    | 6067/10788 [02:32<02:12, 35.50it/s]Training CobwebTree:  56%|    | 6072/10788 [02:32<02:02, 38.46it/s]Training CobwebTree:  56%|    | 6076/10788 [02:33<02:08, 36.56it/s]Training CobwebTree:  56%|    | 6080/10788 [02:33<02:09, 36.24it/s]Training CobwebTree:  56%|    | 6084/10788 [02:33<02:10, 36.02it/s]Training CobwebTree:  56%|    | 6088/10788 [02:33<02:16, 34.34it/s]Training CobwebTree:  56%|    | 6092/10788 [02:33<02:11, 35.72it/s]Training CobwebTree:  57%|    | 6096/10788 [02:33<02:12, 35.29it/s]Training CobwebTree:  57%|    | 6100/10788 [02:33<02:09, 36.29it/s]Training CobwebTree:  57%|    | 6104/10788 [02:33<02:09, 36.24it/s]Training CobwebTree:  57%|    | 6108/10788 [02:33<02:13, 34.97it/s]Training CobwebTree:  57%|    | 6112/10788 [02:34<02:16, 34.19it/s]Training CobwebTree:  57%|    | 6116/10788 [02:34<02:11, 35.49it/s]Training CobwebTree:  57%|    | 6120/10788 [02:34<02:11, 35.53it/s]Training CobwebTree:  57%|    | 6124/10788 [02:34<02:07, 36.44it/s]Training CobwebTree:  57%|    | 6128/10788 [02:34<02:08, 36.21it/s]Training CobwebTree:  57%|    | 6132/10788 [02:34<02:09, 36.05it/s]Training CobwebTree:  57%|    | 6136/10788 [02:34<02:10, 35.65it/s]Training CobwebTree:  57%|    | 6140/10788 [02:34<02:12, 35.06it/s]Training CobwebTree:  57%|    | 6144/10788 [02:34<02:15, 34.34it/s]Training CobwebTree:  57%|    | 6148/10788 [02:35<02:21, 32.83it/s]Training CobwebTree:  57%|    | 6152/10788 [02:35<02:16, 33.93it/s]Training CobwebTree:  57%|    | 6156/10788 [02:35<02:17, 33.63it/s]Training CobwebTree:  57%|    | 6160/10788 [02:35<02:15, 34.12it/s]Training CobwebTree:  57%|    | 6164/10788 [02:35<02:09, 35.66it/s]Training CobwebTree:  57%|    | 6169/10788 [02:35<02:02, 37.79it/s]Training CobwebTree:  57%|    | 6173/10788 [02:35<02:05, 36.70it/s]Training CobwebTree:  57%|    | 6177/10788 [02:35<02:04, 36.94it/s]Training CobwebTree:  57%|    | 6181/10788 [02:35<02:03, 37.31it/s]Training CobwebTree:  57%|    | 6185/10788 [02:36<02:03, 37.30it/s]Training CobwebTree:  57%|    | 6189/10788 [02:36<02:02, 37.58it/s]Training CobwebTree:  57%|    | 6194/10788 [02:36<01:59, 38.33it/s]Training CobwebTree:  57%|    | 6198/10788 [02:36<02:01, 37.92it/s]Training CobwebTree:  57%|    | 6202/10788 [02:36<02:02, 37.44it/s]Training CobwebTree:  58%|    | 6206/10788 [02:36<02:00, 37.97it/s]Training CobwebTree:  58%|    | 6210/10788 [02:36<02:04, 36.75it/s]Training CobwebTree:  58%|    | 6214/10788 [02:36<02:06, 36.22it/s]Training CobwebTree:  58%|    | 6218/10788 [02:36<02:03, 36.96it/s]Training CobwebTree:  58%|    | 6222/10788 [02:37<02:08, 35.49it/s]Training CobwebTree:  58%|    | 6226/10788 [02:37<02:08, 35.57it/s]Training CobwebTree:  58%|    | 6230/10788 [02:37<02:04, 36.59it/s]Training CobwebTree:  58%|    | 6234/10788 [02:37<02:09, 35.19it/s]Training CobwebTree:  58%|    | 6238/10788 [02:37<02:11, 34.60it/s]Training CobwebTree:  58%|    | 6242/10788 [02:37<02:10, 34.76it/s]Training CobwebTree:  58%|    | 6247/10788 [02:37<02:01, 37.34it/s]Training CobwebTree:  58%|    | 6251/10788 [02:37<02:04, 36.31it/s]Training CobwebTree:  58%|    | 6255/10788 [02:38<02:08, 35.24it/s]Training CobwebTree:  58%|    | 6259/10788 [02:38<02:17, 32.86it/s]Training CobwebTree:  58%|    | 6263/10788 [02:38<02:12, 34.22it/s]Training CobwebTree:  58%|    | 6267/10788 [02:38<02:10, 34.75it/s]Training CobwebTree:  58%|    | 6271/10788 [02:38<02:06, 35.65it/s]Training CobwebTree:  58%|    | 6276/10788 [02:38<01:57, 38.42it/s]Training CobwebTree:  58%|    | 6281/10788 [02:38<01:52, 40.12it/s]Training CobwebTree:  58%|    | 6286/10788 [02:38<01:54, 39.36it/s]Training CobwebTree:  58%|    | 6290/10788 [02:38<01:56, 38.74it/s]Training CobwebTree:  58%|    | 6294/10788 [02:39<02:01, 37.01it/s]Training CobwebTree:  58%|    | 6298/10788 [02:39<02:07, 35.15it/s]Training CobwebTree:  58%|    | 6302/10788 [02:39<02:07, 35.28it/s]Training CobwebTree:  58%|    | 6306/10788 [02:39<02:09, 34.71it/s]Training CobwebTree:  59%|    | 6311/10788 [02:39<02:04, 35.85it/s]Training CobwebTree:  59%|    | 6315/10788 [02:39<02:03, 36.14it/s]Training CobwebTree:  59%|    | 6320/10788 [02:39<01:54, 39.08it/s]Training CobwebTree:  59%|    | 6324/10788 [02:39<01:55, 38.71it/s]Training CobwebTree:  59%|    | 6328/10788 [02:39<01:54, 38.84it/s]Training CobwebTree:  59%|    | 6332/10788 [02:40<01:57, 37.86it/s]Training CobwebTree:  59%|    | 6336/10788 [02:40<01:59, 37.32it/s]Training CobwebTree:  59%|    | 6340/10788 [02:40<02:00, 36.83it/s]Training CobwebTree:  59%|    | 6344/10788 [02:40<01:59, 37.17it/s]Training CobwebTree:  59%|    | 6348/10788 [02:40<01:59, 37.02it/s]Training CobwebTree:  59%|    | 6352/10788 [02:40<01:57, 37.61it/s]Training CobwebTree:  59%|    | 6356/10788 [02:40<02:01, 36.50it/s]Training CobwebTree:  59%|    | 6360/10788 [02:40<02:02, 36.23it/s]Training CobwebTree:  59%|    | 6364/10788 [02:40<02:01, 36.42it/s]Training CobwebTree:  59%|    | 6368/10788 [02:41<02:05, 35.17it/s]Training CobwebTree:  59%|    | 6372/10788 [02:41<02:09, 34.06it/s]Training CobwebTree:  59%|    | 6376/10788 [02:41<02:10, 33.76it/s]Training CobwebTree:  59%|    | 6380/10788 [02:41<02:08, 34.33it/s]Training CobwebTree:  59%|    | 6384/10788 [02:41<02:04, 35.35it/s]Training CobwebTree:  59%|    | 6388/10788 [02:41<02:06, 34.87it/s]Training CobwebTree:  59%|    | 6392/10788 [02:41<02:03, 35.72it/s]Training CobwebTree:  59%|    | 6397/10788 [02:41<01:56, 37.60it/s]Training CobwebTree:  59%|    | 6401/10788 [02:42<02:01, 36.05it/s]Training CobwebTree:  59%|    | 6405/10788 [02:42<02:02, 35.92it/s]Training CobwebTree:  59%|    | 6409/10788 [02:42<02:06, 34.48it/s]Training CobwebTree:  59%|    | 6413/10788 [02:42<02:07, 34.38it/s]Training CobwebTree:  59%|    | 6417/10788 [02:42<02:07, 34.37it/s]Training CobwebTree:  60%|    | 6421/10788 [02:42<02:06, 34.43it/s]Training CobwebTree:  60%|    | 6425/10788 [02:42<02:07, 34.23it/s]Training CobwebTree:  60%|    | 6429/10788 [02:42<02:08, 34.05it/s]Training CobwebTree:  60%|    | 6433/10788 [02:42<02:05, 34.74it/s]Training CobwebTree:  60%|    | 6437/10788 [02:43<02:08, 33.97it/s]Training CobwebTree:  60%|    | 6441/10788 [02:43<02:04, 34.90it/s]Training CobwebTree:  60%|    | 6445/10788 [02:43<02:04, 35.02it/s]Training CobwebTree:  60%|    | 6449/10788 [02:43<02:05, 34.68it/s]Training CobwebTree:  60%|    | 6453/10788 [02:43<02:10, 33.25it/s]Training CobwebTree:  60%|    | 6457/10788 [02:43<02:05, 34.54it/s]Training CobwebTree:  60%|    | 6461/10788 [02:43<02:01, 35.52it/s]Training CobwebTree:  60%|    | 6465/10788 [02:43<02:01, 35.63it/s]Training CobwebTree:  60%|    | 6469/10788 [02:43<01:58, 36.38it/s]Training CobwebTree:  60%|    | 6473/10788 [02:44<02:00, 35.82it/s]Training CobwebTree:  60%|    | 6477/10788 [02:44<02:00, 35.82it/s]Training CobwebTree:  60%|    | 6481/10788 [02:44<02:03, 34.97it/s]Training CobwebTree:  60%|    | 6485/10788 [02:44<02:03, 34.75it/s]Training CobwebTree:  60%|    | 6489/10788 [02:44<02:06, 34.09it/s]Training CobwebTree:  60%|    | 6493/10788 [02:44<02:01, 35.22it/s]Training CobwebTree:  60%|    | 6497/10788 [02:44<02:11, 32.64it/s]Training CobwebTree:  60%|    | 6501/10788 [02:44<02:13, 32.16it/s]Training CobwebTree:  60%|    | 6505/10788 [02:45<02:13, 32.15it/s]Training CobwebTree:  60%|    | 6509/10788 [02:45<02:05, 34.11it/s]Training CobwebTree:  60%|    | 6513/10788 [02:45<02:06, 33.80it/s]Training CobwebTree:  60%|    | 6517/10788 [02:45<02:02, 34.90it/s]Training CobwebTree:  60%|    | 6521/10788 [02:45<02:00, 35.52it/s]Training CobwebTree:  60%|    | 6525/10788 [02:45<02:02, 34.69it/s]Training CobwebTree:  61%|    | 6529/10788 [02:45<02:00, 35.28it/s]Training CobwebTree:  61%|    | 6533/10788 [02:45<02:01, 34.95it/s]Training CobwebTree:  61%|    | 6537/10788 [02:45<01:58, 35.87it/s]Training CobwebTree:  61%|    | 6541/10788 [02:46<01:58, 35.90it/s]Training CobwebTree:  61%|    | 6545/10788 [02:46<01:57, 36.05it/s]Training CobwebTree:  61%|    | 6549/10788 [02:46<01:58, 35.91it/s]Training CobwebTree:  61%|    | 6553/10788 [02:46<02:01, 34.97it/s]Training CobwebTree:  61%|    | 6557/10788 [02:46<01:56, 36.28it/s]Training CobwebTree:  61%|    | 6561/10788 [02:46<01:59, 35.40it/s]Training CobwebTree:  61%|    | 6565/10788 [02:46<01:55, 36.64it/s]Training CobwebTree:  61%|    | 6569/10788 [02:46<01:55, 36.47it/s]Training CobwebTree:  61%|    | 6573/10788 [02:46<01:58, 35.68it/s]Training CobwebTree:  61%|    | 6577/10788 [02:47<02:03, 34.08it/s]Training CobwebTree:  61%|    | 6581/10788 [02:47<02:02, 34.22it/s]Training CobwebTree:  61%|    | 6585/10788 [02:47<02:02, 34.28it/s]Training CobwebTree:  61%|    | 6589/10788 [02:47<02:01, 34.51it/s]Training CobwebTree:  61%|    | 6593/10788 [02:47<02:01, 34.41it/s]Training CobwebTree:  61%|    | 6598/10788 [02:47<01:56, 35.92it/s]Training CobwebTree:  61%|    | 6602/10788 [02:47<01:55, 36.10it/s]Training CobwebTree:  61%|    | 6606/10788 [02:47<01:55, 36.06it/s]Training CobwebTree:  61%|   | 6611/10788 [02:48<01:50, 37.70it/s]Training CobwebTree:  61%|   | 6615/10788 [02:48<01:52, 37.12it/s]Training CobwebTree:  61%|   | 6619/10788 [02:48<01:54, 36.48it/s]Training CobwebTree:  61%|   | 6623/10788 [02:48<01:52, 37.06it/s]Training CobwebTree:  61%|   | 6627/10788 [02:48<01:50, 37.49it/s]Training CobwebTree:  61%|   | 6631/10788 [02:48<01:51, 37.39it/s]Training CobwebTree:  62%|   | 6635/10788 [02:48<01:52, 36.79it/s]Training CobwebTree:  62%|   | 6639/10788 [02:48<01:59, 34.82it/s]Training CobwebTree:  62%|   | 6643/10788 [02:48<01:56, 35.71it/s]Training CobwebTree:  62%|   | 6647/10788 [02:49<01:53, 36.37it/s]Training CobwebTree:  62%|   | 6651/10788 [02:49<01:55, 35.76it/s]Training CobwebTree:  62%|   | 6655/10788 [02:49<01:54, 36.10it/s]Training CobwebTree:  62%|   | 6659/10788 [02:49<01:59, 34.67it/s]Training CobwebTree:  62%|   | 6663/10788 [02:49<02:00, 34.31it/s]Training CobwebTree:  62%|   | 6667/10788 [02:49<02:02, 33.66it/s]Training CobwebTree:  62%|   | 6671/10788 [02:49<02:01, 33.99it/s]Training CobwebTree:  62%|   | 6675/10788 [02:49<02:00, 34.12it/s]Training CobwebTree:  62%|   | 6679/10788 [02:49<02:05, 32.73it/s]Training CobwebTree:  62%|   | 6684/10788 [02:50<01:56, 35.15it/s]Training CobwebTree:  62%|   | 6688/10788 [02:50<02:01, 33.79it/s]Training CobwebTree:  62%|   | 6692/10788 [02:50<01:57, 34.86it/s]Training CobwebTree:  62%|   | 6696/10788 [02:50<01:55, 35.45it/s]Training CobwebTree:  62%|   | 6700/10788 [02:50<02:05, 32.58it/s]Training CobwebTree:  62%|   | 6704/10788 [02:50<02:00, 34.01it/s]Training CobwebTree:  62%|   | 6708/10788 [02:50<01:58, 34.41it/s]Training CobwebTree:  62%|   | 6712/10788 [02:50<01:59, 34.18it/s]Training CobwebTree:  62%|   | 6716/10788 [02:51<02:02, 33.35it/s]Training CobwebTree:  62%|   | 6720/10788 [02:51<01:59, 33.93it/s]Training CobwebTree:  62%|   | 6724/10788 [02:51<01:58, 34.34it/s]Training CobwebTree:  62%|   | 6728/10788 [02:51<02:01, 33.37it/s]Training CobwebTree:  62%|   | 6732/10788 [02:51<02:03, 32.96it/s]Training CobwebTree:  62%|   | 6736/10788 [02:51<02:01, 33.41it/s]Training CobwebTree:  62%|   | 6740/10788 [02:51<01:57, 34.38it/s]Training CobwebTree:  63%|   | 6744/10788 [02:51<01:56, 34.79it/s]Training CobwebTree:  63%|   | 6748/10788 [02:51<01:57, 34.45it/s]Training CobwebTree:  63%|   | 6752/10788 [02:52<01:55, 34.88it/s]Training CobwebTree:  63%|   | 6756/10788 [02:52<01:56, 34.60it/s]Training CobwebTree:  63%|   | 6760/10788 [02:52<01:55, 35.03it/s]Training CobwebTree:  63%|   | 6765/10788 [02:52<01:51, 36.20it/s]Training CobwebTree:  63%|   | 6769/10788 [02:52<01:51, 36.03it/s]Training CobwebTree:  63%|   | 6773/10788 [02:52<01:48, 36.94it/s]Training CobwebTree:  63%|   | 6777/10788 [02:52<01:47, 37.32it/s]Training CobwebTree:  63%|   | 6781/10788 [02:52<01:48, 36.87it/s]Training CobwebTree:  63%|   | 6785/10788 [02:53<01:48, 36.95it/s]Training CobwebTree:  63%|   | 6789/10788 [02:53<01:49, 36.58it/s]Training CobwebTree:  63%|   | 6793/10788 [02:53<01:53, 35.27it/s]Training CobwebTree:  63%|   | 6797/10788 [02:53<01:51, 35.83it/s]Training CobwebTree:  63%|   | 6801/10788 [02:53<01:49, 36.31it/s]Training CobwebTree:  63%|   | 6805/10788 [02:53<01:55, 34.58it/s]Training CobwebTree:  63%|   | 6809/10788 [02:53<01:55, 34.49it/s]Training CobwebTree:  63%|   | 6813/10788 [02:53<01:51, 35.62it/s]Training CobwebTree:  63%|   | 6817/10788 [02:53<01:59, 33.19it/s]Training CobwebTree:  63%|   | 6822/10788 [02:54<01:52, 35.28it/s]Training CobwebTree:  63%|   | 6826/10788 [02:54<01:50, 35.89it/s]Training CobwebTree:  63%|   | 6830/10788 [02:54<01:50, 35.97it/s]Training CobwebTree:  63%|   | 6834/10788 [02:54<01:51, 35.31it/s]Training CobwebTree:  63%|   | 6838/10788 [02:54<01:52, 35.16it/s]Training CobwebTree:  63%|   | 6842/10788 [02:54<01:54, 34.56it/s]Training CobwebTree:  63%|   | 6846/10788 [02:54<01:55, 34.13it/s]Training CobwebTree:  63%|   | 6850/10788 [02:54<01:59, 32.84it/s]Training CobwebTree:  64%|   | 6855/10788 [02:55<01:54, 34.42it/s]Training CobwebTree:  64%|   | 6859/10788 [02:55<01:59, 32.83it/s]Training CobwebTree:  64%|   | 6863/10788 [02:55<01:57, 33.36it/s]Training CobwebTree:  64%|   | 6867/10788 [02:55<01:59, 32.69it/s]Training CobwebTree:  64%|   | 6871/10788 [02:55<01:58, 33.02it/s]Training CobwebTree:  64%|   | 6875/10788 [02:55<02:04, 31.51it/s]Training CobwebTree:  64%|   | 6879/10788 [02:55<01:59, 32.65it/s]Training CobwebTree:  64%|   | 6883/10788 [02:55<01:57, 33.17it/s]Training CobwebTree:  64%|   | 6887/10788 [02:56<02:00, 32.44it/s]Training CobwebTree:  64%|   | 6891/10788 [02:56<01:56, 33.42it/s]Training CobwebTree:  64%|   | 6895/10788 [02:56<01:54, 33.90it/s]Training CobwebTree:  64%|   | 6899/10788 [02:56<01:53, 34.15it/s]Training CobwebTree:  64%|   | 6903/10788 [02:56<01:50, 35.14it/s]Training CobwebTree:  64%|   | 6907/10788 [02:56<01:49, 35.48it/s]Training CobwebTree:  64%|   | 6911/10788 [02:56<01:51, 34.65it/s]Training CobwebTree:  64%|   | 6915/10788 [02:56<01:51, 34.65it/s]Training CobwebTree:  64%|   | 6919/10788 [02:56<01:54, 33.66it/s]Training CobwebTree:  64%|   | 6923/10788 [02:57<01:52, 34.46it/s]Training CobwebTree:  64%|   | 6927/10788 [02:57<01:53, 33.90it/s]Training CobwebTree:  64%|   | 6931/10788 [02:57<01:54, 33.62it/s]Training CobwebTree:  64%|   | 6935/10788 [02:57<01:49, 35.17it/s]Training CobwebTree:  64%|   | 6939/10788 [02:57<01:46, 36.00it/s]Training CobwebTree:  64%|   | 6943/10788 [02:57<01:49, 35.27it/s]Training CobwebTree:  64%|   | 6947/10788 [02:57<01:51, 34.50it/s]Training CobwebTree:  64%|   | 6951/10788 [02:57<01:54, 33.54it/s]Training CobwebTree:  64%|   | 6955/10788 [02:57<01:49, 35.05it/s]Training CobwebTree:  65%|   | 6959/10788 [02:58<01:47, 35.78it/s]Training CobwebTree:  65%|   | 6963/10788 [02:58<01:46, 36.04it/s]Training CobwebTree:  65%|   | 6967/10788 [02:58<01:50, 34.69it/s]Training CobwebTree:  65%|   | 6971/10788 [02:58<01:49, 34.89it/s]Training CobwebTree:  65%|   | 6975/10788 [02:58<01:45, 35.99it/s]Training CobwebTree:  65%|   | 6979/10788 [02:58<01:47, 35.39it/s]Training CobwebTree:  65%|   | 6983/10788 [02:58<01:50, 34.29it/s]Training CobwebTree:  65%|   | 6987/10788 [02:58<01:47, 35.28it/s]Training CobwebTree:  65%|   | 6991/10788 [02:58<01:45, 36.11it/s]Training CobwebTree:  65%|   | 6995/10788 [02:59<01:47, 35.44it/s]Training CobwebTree:  65%|   | 6999/10788 [02:59<01:52, 33.77it/s]Training CobwebTree:  65%|   | 7003/10788 [02:59<01:49, 34.50it/s]Training CobwebTree:  65%|   | 7007/10788 [02:59<01:50, 34.37it/s]Training CobwebTree:  65%|   | 7012/10788 [02:59<01:44, 36.20it/s]Training CobwebTree:  65%|   | 7016/10788 [02:59<01:46, 35.54it/s]Training CobwebTree:  65%|   | 7020/10788 [02:59<01:44, 36.10it/s]Training CobwebTree:  65%|   | 7024/10788 [02:59<01:44, 36.09it/s]Training CobwebTree:  65%|   | 7028/10788 [03:00<01:44, 36.07it/s]Training CobwebTree:  65%|   | 7032/10788 [03:00<01:47, 34.91it/s]Training CobwebTree:  65%|   | 7036/10788 [03:00<01:51, 33.79it/s]Training CobwebTree:  65%|   | 7041/10788 [03:00<01:41, 36.84it/s]Training CobwebTree:  65%|   | 7045/10788 [03:00<01:41, 36.88it/s]Training CobwebTree:  65%|   | 7049/10788 [03:00<01:44, 35.85it/s]Training CobwebTree:  65%|   | 7053/10788 [03:00<01:42, 36.45it/s]Training CobwebTree:  65%|   | 7057/10788 [03:00<01:40, 37.22it/s]Training CobwebTree:  65%|   | 7061/10788 [03:00<01:47, 34.80it/s]Training CobwebTree:  65%|   | 7065/10788 [03:01<01:44, 35.47it/s]Training CobwebTree:  66%|   | 7070/10788 [03:01<01:41, 36.77it/s]Training CobwebTree:  66%|   | 7074/10788 [03:01<01:44, 35.65it/s]Training CobwebTree:  66%|   | 7078/10788 [03:01<01:46, 34.91it/s]Training CobwebTree:  66%|   | 7082/10788 [03:01<01:48, 34.28it/s]Training CobwebTree:  66%|   | 7086/10788 [03:01<01:48, 33.99it/s]Training CobwebTree:  66%|   | 7090/10788 [03:01<01:47, 34.25it/s]Training CobwebTree:  66%|   | 7094/10788 [03:01<01:50, 33.38it/s]Training CobwebTree:  66%|   | 7098/10788 [03:02<01:50, 33.26it/s]Training CobwebTree:  66%|   | 7102/10788 [03:02<01:52, 32.82it/s]Training CobwebTree:  66%|   | 7106/10788 [03:02<01:55, 31.89it/s]Training CobwebTree:  66%|   | 7110/10788 [03:02<01:52, 32.64it/s]Training CobwebTree:  66%|   | 7114/10788 [03:02<01:50, 33.37it/s]Training CobwebTree:  66%|   | 7118/10788 [03:02<01:47, 34.14it/s]Training CobwebTree:  66%|   | 7123/10788 [03:02<01:41, 36.24it/s]Training CobwebTree:  66%|   | 7127/10788 [03:02<01:40, 36.36it/s]Training CobwebTree:  66%|   | 7131/10788 [03:02<01:40, 36.29it/s]Training CobwebTree:  66%|   | 7135/10788 [03:03<01:38, 37.18it/s]Training CobwebTree:  66%|   | 7139/10788 [03:03<01:41, 36.10it/s]Training CobwebTree:  66%|   | 7143/10788 [03:03<01:42, 35.64it/s]Training CobwebTree:  66%|   | 7147/10788 [03:03<01:45, 34.49it/s]Training CobwebTree:  66%|   | 7151/10788 [03:03<01:45, 34.60it/s]Training CobwebTree:  66%|   | 7155/10788 [03:03<01:43, 35.14it/s]Training CobwebTree:  66%|   | 7159/10788 [03:03<01:41, 35.74it/s]Training CobwebTree:  66%|   | 7163/10788 [03:03<01:38, 36.80it/s]Training CobwebTree:  66%|   | 7167/10788 [03:04<01:44, 34.51it/s]Training CobwebTree:  66%|   | 7171/10788 [03:04<01:44, 34.73it/s]Training CobwebTree:  67%|   | 7175/10788 [03:04<01:47, 33.64it/s]Training CobwebTree:  67%|   | 7179/10788 [03:04<01:44, 34.56it/s]Training CobwebTree:  67%|   | 7183/10788 [03:04<01:44, 34.52it/s]Training CobwebTree:  67%|   | 7188/10788 [03:04<01:36, 37.14it/s]Training CobwebTree:  67%|   | 7192/10788 [03:04<01:36, 37.22it/s]Training CobwebTree:  67%|   | 7197/10788 [03:04<01:31, 39.04it/s]Training CobwebTree:  67%|   | 7201/10788 [03:04<01:36, 37.26it/s]Training CobwebTree:  67%|   | 7205/10788 [03:05<01:40, 35.70it/s]Training CobwebTree:  67%|   | 7209/10788 [03:05<01:44, 34.11it/s]Training CobwebTree:  67%|   | 7213/10788 [03:05<01:47, 33.34it/s]Training CobwebTree:  67%|   | 7217/10788 [03:05<01:44, 34.13it/s]Training CobwebTree:  67%|   | 7221/10788 [03:05<01:42, 34.93it/s]Training CobwebTree:  67%|   | 7225/10788 [03:05<01:38, 36.11it/s]Training CobwebTree:  67%|   | 7229/10788 [03:05<01:36, 36.75it/s]Training CobwebTree:  67%|   | 7233/10788 [03:05<01:40, 35.24it/s]Training CobwebTree:  67%|   | 7237/10788 [03:05<01:38, 36.05it/s]Training CobwebTree:  67%|   | 7241/10788 [03:06<01:36, 36.92it/s]Training CobwebTree:  67%|   | 7245/10788 [03:06<01:33, 37.71it/s]Training CobwebTree:  67%|   | 7249/10788 [03:06<01:37, 36.38it/s]Training CobwebTree:  67%|   | 7253/10788 [03:06<01:38, 35.89it/s]Training CobwebTree:  67%|   | 7257/10788 [03:06<01:40, 35.20it/s]Training CobwebTree:  67%|   | 7261/10788 [03:06<01:38, 35.68it/s]Training CobwebTree:  67%|   | 7265/10788 [03:06<01:37, 35.99it/s]Training CobwebTree:  67%|   | 7269/10788 [03:06<01:37, 36.11it/s]Training CobwebTree:  67%|   | 7274/10788 [03:06<01:32, 37.81it/s]Training CobwebTree:  67%|   | 7278/10788 [03:07<01:33, 37.53it/s]Training CobwebTree:  68%|   | 7282/10788 [03:07<01:34, 37.10it/s]Training CobwebTree:  68%|   | 7287/10788 [03:07<01:30, 38.68it/s]Training CobwebTree:  68%|   | 7292/10788 [03:07<01:29, 39.14it/s]Training CobwebTree:  68%|   | 7296/10788 [03:07<01:33, 37.24it/s]Training CobwebTree:  68%|   | 7300/10788 [03:07<01:35, 36.46it/s]Training CobwebTree:  68%|   | 7304/10788 [03:07<01:34, 36.99it/s]Training CobwebTree:  68%|   | 7308/10788 [03:07<01:33, 37.21it/s]Training CobwebTree:  68%|   | 7312/10788 [03:08<01:38, 35.43it/s]Training CobwebTree:  68%|   | 7316/10788 [03:08<01:38, 35.08it/s]Training CobwebTree:  68%|   | 7320/10788 [03:08<01:37, 35.53it/s]Training CobwebTree:  68%|   | 7324/10788 [03:08<01:40, 34.32it/s]Training CobwebTree:  68%|   | 7328/10788 [03:08<01:37, 35.61it/s]Training CobwebTree:  68%|   | 7332/10788 [03:08<01:38, 35.17it/s]Training CobwebTree:  68%|   | 7336/10788 [03:08<01:39, 34.87it/s]Training CobwebTree:  68%|   | 7340/10788 [03:08<01:36, 35.79it/s]Training CobwebTree:  68%|   | 7344/10788 [03:08<01:41, 33.97it/s]Training CobwebTree:  68%|   | 7348/10788 [03:09<01:43, 33.25it/s]Training CobwebTree:  68%|   | 7352/10788 [03:09<01:38, 34.86it/s]Training CobwebTree:  68%|   | 7356/10788 [03:09<01:34, 36.22it/s]Training CobwebTree:  68%|   | 7360/10788 [03:09<01:33, 36.79it/s]Training CobwebTree:  68%|   | 7365/10788 [03:09<01:31, 37.58it/s]Training CobwebTree:  68%|   | 7369/10788 [03:09<01:30, 37.85it/s]Training CobwebTree:  68%|   | 7373/10788 [03:10<02:57, 19.26it/s]Training CobwebTree:  68%|   | 7377/10788 [03:10<02:35, 21.95it/s]Training CobwebTree:  68%|   | 7381/10788 [03:10<02:20, 24.22it/s]Training CobwebTree:  68%|   | 7385/10788 [03:10<02:07, 26.65it/s]Training CobwebTree:  68%|   | 7389/10788 [03:10<02:02, 27.75it/s]Training CobwebTree:  69%|   | 7393/10788 [03:10<01:53, 29.99it/s]Training CobwebTree:  69%|   | 7397/10788 [03:10<01:51, 30.55it/s]Training CobwebTree:  69%|   | 7401/10788 [03:10<01:45, 32.11it/s]Training CobwebTree:  69%|   | 7405/10788 [03:11<01:45, 31.98it/s]Training CobwebTree:  69%|   | 7409/10788 [03:11<01:46, 31.80it/s]Training CobwebTree:  69%|   | 7413/10788 [03:11<01:43, 32.65it/s]Training CobwebTree:  69%|   | 7417/10788 [03:11<01:44, 32.30it/s]Training CobwebTree:  69%|   | 7421/10788 [03:11<01:46, 31.64it/s]Training CobwebTree:  69%|   | 7425/10788 [03:11<01:43, 32.46it/s]Training CobwebTree:  69%|   | 7429/10788 [03:11<01:38, 34.23it/s]Training CobwebTree:  69%|   | 7433/10788 [03:11<01:37, 34.41it/s]Training CobwebTree:  69%|   | 7437/10788 [03:11<01:40, 33.20it/s]Training CobwebTree:  69%|   | 7441/10788 [03:12<01:37, 34.41it/s]Training CobwebTree:  69%|   | 7445/10788 [03:12<01:34, 35.45it/s]Training CobwebTree:  69%|   | 7449/10788 [03:12<01:35, 35.08it/s]Training CobwebTree:  69%|   | 7453/10788 [03:12<01:34, 35.20it/s]Training CobwebTree:  69%|   | 7457/10788 [03:12<01:35, 34.89it/s]Training CobwebTree:  69%|   | 7461/10788 [03:12<01:37, 34.05it/s]Training CobwebTree:  69%|   | 7465/10788 [03:12<01:37, 34.02it/s]Training CobwebTree:  69%|   | 7469/10788 [03:12<01:41, 32.57it/s]Training CobwebTree:  69%|   | 7473/10788 [03:13<01:38, 33.54it/s]Training CobwebTree:  69%|   | 7477/10788 [03:13<01:36, 34.24it/s]Training CobwebTree:  69%|   | 7481/10788 [03:13<01:33, 35.44it/s]Training CobwebTree:  69%|   | 7485/10788 [03:13<01:42, 32.32it/s]Training CobwebTree:  69%|   | 7489/10788 [03:13<01:37, 33.86it/s]Training CobwebTree:  69%|   | 7493/10788 [03:13<01:36, 34.01it/s]Training CobwebTree:  69%|   | 7497/10788 [03:13<01:35, 34.58it/s]Training CobwebTree:  70%|   | 7501/10788 [03:13<01:35, 34.28it/s]Training CobwebTree:  70%|   | 7505/10788 [03:13<01:36, 33.98it/s]Training CobwebTree:  70%|   | 7509/10788 [03:14<01:33, 34.99it/s]Training CobwebTree:  70%|   | 7513/10788 [03:14<01:33, 35.21it/s]Training CobwebTree:  70%|   | 7517/10788 [03:14<01:36, 34.02it/s]Training CobwebTree:  70%|   | 7521/10788 [03:14<01:37, 33.65it/s]Training CobwebTree:  70%|   | 7525/10788 [03:14<01:37, 33.48it/s]Training CobwebTree:  70%|   | 7529/10788 [03:14<01:36, 33.79it/s]Training CobwebTree:  70%|   | 7533/10788 [03:14<01:34, 34.34it/s]Training CobwebTree:  70%|   | 7537/10788 [03:14<01:32, 35.28it/s]Training CobwebTree:  70%|   | 7541/10788 [03:14<01:33, 34.72it/s]Training CobwebTree:  70%|   | 7545/10788 [03:15<01:37, 33.42it/s]Training CobwebTree:  70%|   | 7549/10788 [03:15<01:37, 33.20it/s]Training CobwebTree:  70%|   | 7553/10788 [03:15<01:34, 34.33it/s]Training CobwebTree:  70%|   | 7557/10788 [03:15<01:34, 34.29it/s]Training CobwebTree:  70%|   | 7561/10788 [03:15<01:34, 34.12it/s]Training CobwebTree:  70%|   | 7565/10788 [03:15<01:32, 34.94it/s]Training CobwebTree:  70%|   | 7569/10788 [03:15<01:33, 34.54it/s]Training CobwebTree:  70%|   | 7573/10788 [03:15<01:35, 33.71it/s]Training CobwebTree:  70%|   | 7578/10788 [03:16<01:29, 35.72it/s]Training CobwebTree:  70%|   | 7582/10788 [03:16<01:27, 36.45it/s]Training CobwebTree:  70%|   | 7586/10788 [03:16<01:28, 36.34it/s]Training CobwebTree:  70%|   | 7590/10788 [03:16<01:27, 36.50it/s]Training CobwebTree:  70%|   | 7594/10788 [03:16<01:29, 35.63it/s]Training CobwebTree:  70%|   | 7598/10788 [03:16<01:29, 35.84it/s]Training CobwebTree:  70%|   | 7602/10788 [03:16<01:31, 34.78it/s]Training CobwebTree:  71%|   | 7606/10788 [03:16<01:31, 34.59it/s]Training CobwebTree:  71%|   | 7610/10788 [03:16<01:29, 35.51it/s]Training CobwebTree:  71%|   | 7614/10788 [03:17<01:29, 35.48it/s]Training CobwebTree:  71%|   | 7618/10788 [03:17<01:33, 34.06it/s]Training CobwebTree:  71%|   | 7622/10788 [03:17<01:34, 33.46it/s]Training CobwebTree:  71%|   | 7626/10788 [03:17<01:32, 34.24it/s]Training CobwebTree:  71%|   | 7630/10788 [03:17<01:29, 35.40it/s]Training CobwebTree:  71%|   | 7634/10788 [03:17<01:28, 35.54it/s]Training CobwebTree:  71%|   | 7638/10788 [03:17<01:30, 34.94it/s]Training CobwebTree:  71%|   | 7642/10788 [03:17<01:27, 35.97it/s]Training CobwebTree:  71%|   | 7646/10788 [03:18<01:29, 35.06it/s]Training CobwebTree:  71%|   | 7650/10788 [03:18<01:29, 34.94it/s]Training CobwebTree:  71%|   | 7654/10788 [03:18<01:29, 35.03it/s]Training CobwebTree:  71%|   | 7658/10788 [03:18<01:26, 36.12it/s]Training CobwebTree:  71%|   | 7662/10788 [03:18<01:25, 36.57it/s]Training CobwebTree:  71%|   | 7666/10788 [03:18<01:24, 36.94it/s]Training CobwebTree:  71%|   | 7670/10788 [03:18<01:28, 35.36it/s]Training CobwebTree:  71%|   | 7674/10788 [03:18<01:28, 34.99it/s]Training CobwebTree:  71%|   | 7678/10788 [03:18<01:28, 35.14it/s]Training CobwebTree:  71%|   | 7682/10788 [03:19<01:27, 35.50it/s]Training CobwebTree:  71%|   | 7686/10788 [03:19<01:27, 35.38it/s]Training CobwebTree:  71%|  | 7690/10788 [03:19<01:25, 36.21it/s]Training CobwebTree:  71%|  | 7694/10788 [03:19<01:24, 36.64it/s]Training CobwebTree:  71%|  | 7698/10788 [03:19<01:24, 36.69it/s]Training CobwebTree:  71%|  | 7702/10788 [03:19<01:24, 36.32it/s]Training CobwebTree:  71%|  | 7706/10788 [03:19<01:24, 36.38it/s]Training CobwebTree:  71%|  | 7710/10788 [03:19<01:25, 35.98it/s]Training CobwebTree:  72%|  | 7714/10788 [03:19<01:24, 36.20it/s]Training CobwebTree:  72%|  | 7719/10788 [03:20<01:21, 37.60it/s]Training CobwebTree:  72%|  | 7723/10788 [03:20<01:22, 37.26it/s]Training CobwebTree:  72%|  | 7727/10788 [03:20<01:22, 37.29it/s]Training CobwebTree:  72%|  | 7731/10788 [03:20<01:20, 37.95it/s]Training CobwebTree:  72%|  | 7735/10788 [03:20<01:24, 35.92it/s]Training CobwebTree:  72%|  | 7739/10788 [03:20<01:23, 36.32it/s]Training CobwebTree:  72%|  | 7744/10788 [03:20<01:20, 37.72it/s]Training CobwebTree:  72%|  | 7748/10788 [03:20<01:21, 37.09it/s]Training CobwebTree:  72%|  | 7752/10788 [03:20<01:25, 35.71it/s]Training CobwebTree:  72%|  | 7756/10788 [03:21<01:29, 33.88it/s]Training CobwebTree:  72%|  | 7760/10788 [03:21<01:25, 35.44it/s]Training CobwebTree:  72%|  | 7764/10788 [03:21<01:26, 35.03it/s]Training CobwebTree:  72%|  | 7768/10788 [03:21<01:28, 33.94it/s]Training CobwebTree:  72%|  | 7772/10788 [03:21<01:31, 33.03it/s]Training CobwebTree:  72%|  | 7776/10788 [03:21<01:31, 32.79it/s]Training CobwebTree:  72%|  | 7780/10788 [03:21<01:26, 34.60it/s]Training CobwebTree:  72%|  | 7784/10788 [03:21<01:28, 33.85it/s]Training CobwebTree:  72%|  | 7788/10788 [03:21<01:26, 34.63it/s]Training CobwebTree:  72%|  | 7792/10788 [03:22<01:24, 35.34it/s]Training CobwebTree:  72%|  | 7796/10788 [03:22<01:24, 35.25it/s]Training CobwebTree:  72%|  | 7800/10788 [03:22<01:26, 34.37it/s]Training CobwebTree:  72%|  | 7804/10788 [03:22<01:27, 34.05it/s]Training CobwebTree:  72%|  | 7808/10788 [03:22<01:30, 33.10it/s]Training CobwebTree:  72%|  | 7812/10788 [03:22<01:26, 34.24it/s]Training CobwebTree:  72%|  | 7817/10788 [03:22<01:25, 34.91it/s]Training CobwebTree:  72%|  | 7821/10788 [03:22<01:25, 34.56it/s]Training CobwebTree:  73%|  | 7825/10788 [03:23<01:27, 33.84it/s]Training CobwebTree:  73%|  | 7829/10788 [03:23<01:29, 33.24it/s]Training CobwebTree:  73%|  | 7833/10788 [03:23<01:30, 32.81it/s]Training CobwebTree:  73%|  | 7837/10788 [03:23<01:30, 32.57it/s]Training CobwebTree:  73%|  | 7841/10788 [03:23<01:33, 31.67it/s]Training CobwebTree:  73%|  | 7845/10788 [03:23<01:34, 31.11it/s]Training CobwebTree:  73%|  | 7849/10788 [03:23<01:31, 32.16it/s]Training CobwebTree:  73%|  | 7853/10788 [03:23<01:29, 32.82it/s]Training CobwebTree:  73%|  | 7857/10788 [03:24<01:28, 33.16it/s]Training CobwebTree:  73%|  | 7861/10788 [03:24<01:24, 34.53it/s]Training CobwebTree:  73%|  | 7865/10788 [03:24<01:27, 33.59it/s]Training CobwebTree:  73%|  | 7869/10788 [03:24<01:29, 32.53it/s]Training CobwebTree:  73%|  | 7873/10788 [03:24<01:31, 32.02it/s]Training CobwebTree:  73%|  | 7877/10788 [03:24<01:27, 33.42it/s]Training CobwebTree:  73%|  | 7881/10788 [03:24<01:26, 33.58it/s]Training CobwebTree:  73%|  | 7885/10788 [03:24<01:22, 35.15it/s]Training CobwebTree:  73%|  | 7889/10788 [03:25<01:26, 33.67it/s]Training CobwebTree:  73%|  | 7893/10788 [03:25<01:25, 34.03it/s]Training CobwebTree:  73%|  | 7897/10788 [03:25<01:24, 34.37it/s]Training CobwebTree:  73%|  | 7901/10788 [03:25<01:23, 34.61it/s]Training CobwebTree:  73%|  | 7905/10788 [03:25<01:24, 34.30it/s]Training CobwebTree:  73%|  | 7909/10788 [03:25<01:26, 33.14it/s]Training CobwebTree:  73%|  | 7913/10788 [03:25<01:24, 34.03it/s]Training CobwebTree:  73%|  | 7917/10788 [03:25<01:23, 34.31it/s]Training CobwebTree:  73%|  | 7921/10788 [03:25<01:21, 34.99it/s]Training CobwebTree:  73%|  | 7925/10788 [03:26<01:21, 34.95it/s]Training CobwebTree:  73%|  | 7929/10788 [03:26<01:20, 35.58it/s]Training CobwebTree:  74%|  | 7933/10788 [03:26<01:21, 34.86it/s]Training CobwebTree:  74%|  | 7937/10788 [03:26<01:19, 35.92it/s]Training CobwebTree:  74%|  | 7941/10788 [03:26<01:20, 35.26it/s]Training CobwebTree:  74%|  | 7945/10788 [03:26<01:18, 36.39it/s]Training CobwebTree:  74%|  | 7949/10788 [03:26<01:20, 35.15it/s]Training CobwebTree:  74%|  | 7953/10788 [03:26<01:24, 33.54it/s]Training CobwebTree:  74%|  | 7958/10788 [03:26<01:19, 35.81it/s]Training CobwebTree:  74%|  | 7962/10788 [03:27<01:18, 35.92it/s]Training CobwebTree:  74%|  | 7966/10788 [03:27<01:20, 35.16it/s]Training CobwebTree:  74%|  | 7970/10788 [03:27<01:20, 35.08it/s]Training CobwebTree:  74%|  | 7974/10788 [03:27<01:19, 35.52it/s]Training CobwebTree:  74%|  | 7978/10788 [03:27<01:19, 35.38it/s]Training CobwebTree:  74%|  | 7982/10788 [03:27<01:23, 33.74it/s]Training CobwebTree:  74%|  | 7986/10788 [03:27<01:22, 33.99it/s]Training CobwebTree:  74%|  | 7990/10788 [03:27<01:23, 33.67it/s]Training CobwebTree:  74%|  | 7994/10788 [03:28<01:26, 32.41it/s]Training CobwebTree:  74%|  | 7998/10788 [03:28<01:27, 31.81it/s]Training CobwebTree:  74%|  | 8002/10788 [03:28<01:24, 33.13it/s]Training CobwebTree:  74%|  | 8006/10788 [03:28<01:27, 31.96it/s]Training CobwebTree:  74%|  | 8010/10788 [03:28<01:22, 33.53it/s]Training CobwebTree:  74%|  | 8014/10788 [03:28<01:19, 35.10it/s]Training CobwebTree:  74%|  | 8018/10788 [03:28<01:22, 33.71it/s]Training CobwebTree:  74%|  | 8022/10788 [03:28<01:19, 34.73it/s]Training CobwebTree:  74%|  | 8026/10788 [03:28<01:18, 35.32it/s]Training CobwebTree:  74%|  | 8030/10788 [03:29<01:18, 35.23it/s]Training CobwebTree:  74%|  | 8034/10788 [03:29<01:19, 34.65it/s]Training CobwebTree:  75%|  | 8038/10788 [03:29<01:19, 34.75it/s]Training CobwebTree:  75%|  | 8042/10788 [03:29<01:18, 34.91it/s]Training CobwebTree:  75%|  | 8046/10788 [03:29<01:16, 36.06it/s]Training CobwebTree:  75%|  | 8050/10788 [03:29<01:14, 36.72it/s]Training CobwebTree:  75%|  | 8054/10788 [03:29<01:15, 36.26it/s]Training CobwebTree:  75%|  | 8058/10788 [03:29<01:16, 35.87it/s]Training CobwebTree:  75%|  | 8063/10788 [03:29<01:13, 37.14it/s]Training CobwebTree:  75%|  | 8067/10788 [03:30<01:14, 36.71it/s]Training CobwebTree:  75%|  | 8071/10788 [03:30<01:17, 35.16it/s]Training CobwebTree:  75%|  | 8075/10788 [03:30<01:18, 34.56it/s]Training CobwebTree:  75%|  | 8079/10788 [03:30<01:19, 34.20it/s]Training CobwebTree:  75%|  | 8083/10788 [03:30<01:19, 34.01it/s]Training CobwebTree:  75%|  | 8087/10788 [03:30<01:18, 34.56it/s]Training CobwebTree:  75%|  | 8091/10788 [03:30<01:18, 34.19it/s]Training CobwebTree:  75%|  | 8095/10788 [03:30<01:21, 32.93it/s]Training CobwebTree:  75%|  | 8099/10788 [03:31<01:22, 32.73it/s]Training CobwebTree:  75%|  | 8103/10788 [03:31<01:20, 33.47it/s]Training CobwebTree:  75%|  | 8107/10788 [03:31<01:18, 34.28it/s]Training CobwebTree:  75%|  | 8111/10788 [03:31<01:15, 35.31it/s]Training CobwebTree:  75%|  | 8115/10788 [03:31<01:18, 34.11it/s]Training CobwebTree:  75%|  | 8119/10788 [03:31<01:16, 35.03it/s]Training CobwebTree:  75%|  | 8123/10788 [03:31<01:15, 35.33it/s]Training CobwebTree:  75%|  | 8127/10788 [03:31<01:19, 33.68it/s]Training CobwebTree:  75%|  | 8131/10788 [03:32<01:17, 34.18it/s]Training CobwebTree:  75%|  | 8135/10788 [03:32<01:17, 34.16it/s]Training CobwebTree:  75%|  | 8139/10788 [03:32<01:15, 35.29it/s]Training CobwebTree:  75%|  | 8143/10788 [03:32<01:20, 32.77it/s]Training CobwebTree:  76%|  | 8147/10788 [03:32<01:16, 34.60it/s]Training CobwebTree:  76%|  | 8151/10788 [03:32<01:15, 34.74it/s]Training CobwebTree:  76%|  | 8155/10788 [03:32<01:18, 33.62it/s]Training CobwebTree:  76%|  | 8159/10788 [03:32<01:18, 33.28it/s]Training CobwebTree:  76%|  | 8163/10788 [03:32<01:21, 32.35it/s]Training CobwebTree:  76%|  | 8167/10788 [03:33<01:20, 32.68it/s]Training CobwebTree:  76%|  | 8171/10788 [03:33<01:17, 33.64it/s]Training CobwebTree:  76%|  | 8175/10788 [03:33<01:17, 33.72it/s]Training CobwebTree:  76%|  | 8179/10788 [03:33<01:17, 33.57it/s]Training CobwebTree:  76%|  | 8183/10788 [03:33<01:16, 34.08it/s]Training CobwebTree:  76%|  | 8187/10788 [03:33<01:19, 32.67it/s]Training CobwebTree:  76%|  | 8191/10788 [03:33<01:17, 33.40it/s]Training CobwebTree:  76%|  | 8195/10788 [03:33<01:17, 33.27it/s]Training CobwebTree:  76%|  | 8199/10788 [03:34<01:22, 31.55it/s]Training CobwebTree:  76%|  | 8203/10788 [03:34<01:18, 32.81it/s]Training CobwebTree:  76%|  | 8207/10788 [03:34<01:17, 33.48it/s]Training CobwebTree:  76%|  | 8211/10788 [03:34<01:13, 35.17it/s]Training CobwebTree:  76%|  | 8215/10788 [03:34<01:12, 35.42it/s]Training CobwebTree:  76%|  | 8219/10788 [03:34<01:13, 34.95it/s]Training CobwebTree:  76%|  | 8223/10788 [03:34<01:15, 33.95it/s]Training CobwebTree:  76%|  | 8227/10788 [03:34<01:16, 33.37it/s]Training CobwebTree:  76%|  | 8231/10788 [03:34<01:12, 35.07it/s]Training CobwebTree:  76%|  | 8235/10788 [03:35<01:14, 34.36it/s]Training CobwebTree:  76%|  | 8239/10788 [03:35<01:12, 35.05it/s]Training CobwebTree:  76%|  | 8243/10788 [03:35<01:13, 34.58it/s]Training CobwebTree:  76%|  | 8247/10788 [03:35<01:14, 33.94it/s]Training CobwebTree:  76%|  | 8251/10788 [03:35<01:18, 32.44it/s]Training CobwebTree:  77%|  | 8255/10788 [03:35<01:17, 32.71it/s]Training CobwebTree:  77%|  | 8259/10788 [03:35<01:16, 32.93it/s]Training CobwebTree:  77%|  | 8263/10788 [03:35<01:14, 33.76it/s]Training CobwebTree:  77%|  | 8267/10788 [03:36<01:17, 32.42it/s]Training CobwebTree:  77%|  | 8271/10788 [03:36<01:16, 32.75it/s]Training CobwebTree:  77%|  | 8275/10788 [03:36<01:14, 33.87it/s]Training CobwebTree:  77%|  | 8279/10788 [03:36<01:12, 34.57it/s]Training CobwebTree:  77%|  | 8283/10788 [03:36<01:12, 34.32it/s]Training CobwebTree:  77%|  | 8287/10788 [03:36<01:15, 33.16it/s]Training CobwebTree:  77%|  | 8291/10788 [03:36<01:19, 31.57it/s]Training CobwebTree:  77%|  | 8295/10788 [03:36<01:15, 33.15it/s]Training CobwebTree:  77%|  | 8299/10788 [03:37<01:16, 32.72it/s]Training CobwebTree:  77%|  | 8303/10788 [03:37<01:12, 34.19it/s]Training CobwebTree:  77%|  | 8307/10788 [03:37<01:13, 33.83it/s]Training CobwebTree:  77%|  | 8311/10788 [03:37<01:15, 32.75it/s]Training CobwebTree:  77%|  | 8315/10788 [03:37<01:13, 33.81it/s]Training CobwebTree:  77%|  | 8319/10788 [03:37<01:11, 34.34it/s]Training CobwebTree:  77%|  | 8323/10788 [03:37<01:10, 34.74it/s]Training CobwebTree:  77%|  | 8327/10788 [03:37<01:10, 34.87it/s]Training CobwebTree:  77%|  | 8331/10788 [03:37<01:15, 32.47it/s]Training CobwebTree:  77%|  | 8335/10788 [03:38<01:15, 32.56it/s]Training CobwebTree:  77%|  | 8339/10788 [03:38<01:15, 32.50it/s]Training CobwebTree:  77%|  | 8343/10788 [03:38<01:11, 34.36it/s]Training CobwebTree:  77%|  | 8347/10788 [03:38<01:13, 33.02it/s]Training CobwebTree:  77%|  | 8351/10788 [03:38<01:16, 31.82it/s]Training CobwebTree:  77%|  | 8355/10788 [03:38<01:17, 31.30it/s]Training CobwebTree:  77%|  | 8359/10788 [03:38<01:15, 32.21it/s]Training CobwebTree:  78%|  | 8363/10788 [03:38<01:11, 33.99it/s]Training CobwebTree:  78%|  | 8367/10788 [03:39<01:12, 33.25it/s]Training CobwebTree:  78%|  | 8371/10788 [03:39<01:10, 34.46it/s]Training CobwebTree:  78%|  | 8375/10788 [03:39<01:09, 34.65it/s]Training CobwebTree:  78%|  | 8379/10788 [03:39<01:06, 36.03it/s]Training CobwebTree:  78%|  | 8383/10788 [03:39<01:08, 35.23it/s]Training CobwebTree:  78%|  | 8387/10788 [03:39<01:08, 34.94it/s]Training CobwebTree:  78%|  | 8391/10788 [03:39<01:06, 35.78it/s]Training CobwebTree:  78%|  | 8395/10788 [03:39<01:07, 35.41it/s]Training CobwebTree:  78%|  | 8399/10788 [03:39<01:05, 36.48it/s]Training CobwebTree:  78%|  | 8403/10788 [03:40<01:08, 35.01it/s]Training CobwebTree:  78%|  | 8407/10788 [03:40<01:10, 33.92it/s]Training CobwebTree:  78%|  | 8411/10788 [03:40<01:08, 34.58it/s]Training CobwebTree:  78%|  | 8415/10788 [03:40<01:08, 34.52it/s]Training CobwebTree:  78%|  | 8419/10788 [03:40<01:08, 34.66it/s]Training CobwebTree:  78%|  | 8423/10788 [03:40<01:07, 35.10it/s]Training CobwebTree:  78%|  | 8427/10788 [03:40<01:06, 35.47it/s]Training CobwebTree:  78%|  | 8431/10788 [03:40<01:07, 35.07it/s]Training CobwebTree:  78%|  | 8435/10788 [03:40<01:06, 35.13it/s]Training CobwebTree:  78%|  | 8440/10788 [03:41<01:03, 36.90it/s]Training CobwebTree:  78%|  | 8445/10788 [03:41<01:01, 38.25it/s]Training CobwebTree:  78%|  | 8449/10788 [03:41<01:03, 36.85it/s]Training CobwebTree:  78%|  | 8453/10788 [03:41<01:04, 36.23it/s]Training CobwebTree:  78%|  | 8457/10788 [03:41<01:06, 35.31it/s]Training CobwebTree:  78%|  | 8461/10788 [03:41<01:06, 35.15it/s]Training CobwebTree:  78%|  | 8465/10788 [03:41<01:07, 34.64it/s]Training CobwebTree:  79%|  | 8469/10788 [03:41<01:10, 32.95it/s]Training CobwebTree:  79%|  | 8473/10788 [03:42<01:09, 33.22it/s]Training CobwebTree:  79%|  | 8477/10788 [03:42<01:07, 34.01it/s]Training CobwebTree:  79%|  | 8481/10788 [03:42<01:08, 33.63it/s]Training CobwebTree:  79%|  | 8485/10788 [03:42<01:06, 34.61it/s]Training CobwebTree:  79%|  | 8489/10788 [03:42<01:08, 33.76it/s]Training CobwebTree:  79%|  | 8493/10788 [03:42<01:06, 34.32it/s]Training CobwebTree:  79%|  | 8498/10788 [03:42<01:02, 36.43it/s]Training CobwebTree:  79%|  | 8502/10788 [03:42<01:03, 36.08it/s]Training CobwebTree:  79%|  | 8506/10788 [03:42<01:03, 35.87it/s]Training CobwebTree:  79%|  | 8510/10788 [03:43<01:05, 34.71it/s]Training CobwebTree:  79%|  | 8514/10788 [03:43<01:07, 33.70it/s]Training CobwebTree:  79%|  | 8518/10788 [03:43<01:07, 33.48it/s]Training CobwebTree:  79%|  | 8522/10788 [03:43<01:08, 32.98it/s]Training CobwebTree:  79%|  | 8526/10788 [03:43<01:05, 34.44it/s]Training CobwebTree:  79%|  | 8530/10788 [03:43<01:06, 33.94it/s]Training CobwebTree:  79%|  | 8534/10788 [03:43<01:06, 33.94it/s]Training CobwebTree:  79%|  | 8538/10788 [03:43<01:04, 34.88it/s]Training CobwebTree:  79%|  | 8542/10788 [03:44<01:05, 34.16it/s]Training CobwebTree:  79%|  | 8546/10788 [03:44<01:03, 35.30it/s]Training CobwebTree:  79%|  | 8550/10788 [03:44<01:04, 34.53it/s]Training CobwebTree:  79%|  | 8555/10788 [03:44<01:01, 36.13it/s]Training CobwebTree:  79%|  | 8559/10788 [03:44<01:01, 36.05it/s]Training CobwebTree:  79%|  | 8563/10788 [03:44<01:02, 35.42it/s]Training CobwebTree:  79%|  | 8567/10788 [03:44<01:02, 35.37it/s]Training CobwebTree:  79%|  | 8571/10788 [03:44<01:04, 34.31it/s]Training CobwebTree:  79%|  | 8575/10788 [03:45<01:05, 33.71it/s]Training CobwebTree:  80%|  | 8579/10788 [03:45<01:03, 34.97it/s]Training CobwebTree:  80%|  | 8583/10788 [03:45<01:01, 35.92it/s]Training CobwebTree:  80%|  | 8587/10788 [03:45<01:02, 34.98it/s]Training CobwebTree:  80%|  | 8591/10788 [03:45<01:07, 32.79it/s]Training CobwebTree:  80%|  | 8595/10788 [03:45<01:06, 32.94it/s]Training CobwebTree:  80%|  | 8599/10788 [03:45<01:03, 34.42it/s]Training CobwebTree:  80%|  | 8603/10788 [03:45<01:02, 35.10it/s]Training CobwebTree:  80%|  | 8607/10788 [03:45<01:01, 35.68it/s]Training CobwebTree:  80%|  | 8611/10788 [03:46<01:03, 34.33it/s]Training CobwebTree:  80%|  | 8615/10788 [03:46<01:04, 33.57it/s]Training CobwebTree:  80%|  | 8619/10788 [03:46<01:02, 34.55it/s]Training CobwebTree:  80%|  | 8623/10788 [03:46<01:02, 34.85it/s]Training CobwebTree:  80%|  | 8627/10788 [03:46<01:01, 34.98it/s]Training CobwebTree:  80%|  | 8631/10788 [03:46<01:03, 33.72it/s]Training CobwebTree:  80%|  | 8635/10788 [03:46<01:04, 33.30it/s]Training CobwebTree:  80%|  | 8639/10788 [03:46<01:05, 33.01it/s]Training CobwebTree:  80%|  | 8643/10788 [03:46<01:02, 34.14it/s]Training CobwebTree:  80%|  | 8647/10788 [03:47<01:06, 32.32it/s]Training CobwebTree:  80%|  | 8651/10788 [03:47<01:05, 32.44it/s]Training CobwebTree:  80%|  | 8655/10788 [03:47<01:03, 33.46it/s]Training CobwebTree:  80%|  | 8659/10788 [03:47<01:03, 33.63it/s]Training CobwebTree:  80%|  | 8663/10788 [03:47<01:02, 33.86it/s]Training CobwebTree:  80%|  | 8667/10788 [03:47<01:01, 34.58it/s]Training CobwebTree:  80%|  | 8671/10788 [03:47<00:59, 35.31it/s]Training CobwebTree:  80%|  | 8675/10788 [03:47<00:58, 35.92it/s]Training CobwebTree:  80%|  | 8679/10788 [03:48<00:58, 36.35it/s]Training CobwebTree:  80%|  | 8683/10788 [03:48<00:57, 36.37it/s]Training CobwebTree:  81%|  | 8687/10788 [03:48<00:59, 35.49it/s]Training CobwebTree:  81%|  | 8691/10788 [03:48<00:58, 35.63it/s]Training CobwebTree:  81%|  | 8695/10788 [03:48<01:00, 34.84it/s]Training CobwebTree:  81%|  | 8699/10788 [03:48<01:00, 34.77it/s]Training CobwebTree:  81%|  | 8704/10788 [03:48<00:58, 35.68it/s]Training CobwebTree:  81%|  | 8708/10788 [03:48<00:57, 36.01it/s]Training CobwebTree:  81%|  | 8713/10788 [03:48<00:56, 36.43it/s]Training CobwebTree:  81%|  | 8717/10788 [03:49<00:56, 36.58it/s]Training CobwebTree:  81%|  | 8721/10788 [03:49<00:59, 34.74it/s]Training CobwebTree:  81%|  | 8725/10788 [03:49<00:58, 35.11it/s]Training CobwebTree:  81%|  | 8729/10788 [03:49<01:00, 34.03it/s]Training CobwebTree:  81%|  | 8733/10788 [03:49<01:02, 32.85it/s]Training CobwebTree:  81%|  | 8737/10788 [03:49<01:02, 33.01it/s]Training CobwebTree:  81%|  | 8741/10788 [03:49<01:02, 32.74it/s]Training CobwebTree:  81%|  | 8745/10788 [03:49<01:01, 33.13it/s]Training CobwebTree:  81%|  | 8749/10788 [03:50<01:02, 32.75it/s]Training CobwebTree:  81%|  | 8753/10788 [03:50<01:03, 32.24it/s]Training CobwebTree:  81%|  | 8757/10788 [03:50<01:03, 32.17it/s]Training CobwebTree:  81%|  | 8761/10788 [03:50<01:02, 32.19it/s]Training CobwebTree:  81%|  | 8765/10788 [03:50<01:03, 31.95it/s]Training CobwebTree:  81%| | 8769/10788 [03:50<01:01, 32.79it/s]Training CobwebTree:  81%| | 8773/10788 [03:50<01:00, 33.30it/s]Training CobwebTree:  81%| | 8777/10788 [03:50<00:58, 34.15it/s]Training CobwebTree:  81%| | 8782/10788 [03:51<00:55, 35.86it/s]Training CobwebTree:  81%| | 8786/10788 [03:51<00:54, 36.81it/s]Training CobwebTree:  81%| | 8790/10788 [03:51<00:56, 35.53it/s]Training CobwebTree:  82%| | 8794/10788 [03:51<00:59, 33.29it/s]Training CobwebTree:  82%| | 8798/10788 [03:51<00:57, 34.72it/s]Training CobwebTree:  82%| | 8802/10788 [03:51<00:56, 34.98it/s]Training CobwebTree:  82%| | 8806/10788 [03:51<00:56, 35.25it/s]Training CobwebTree:  82%| | 8810/10788 [03:51<00:56, 34.96it/s]Training CobwebTree:  82%| | 8814/10788 [03:51<00:55, 35.63it/s]Training CobwebTree:  82%| | 8818/10788 [03:52<00:54, 35.87it/s]Training CobwebTree:  82%| | 8822/10788 [03:52<00:55, 35.22it/s]Training CobwebTree:  82%| | 8826/10788 [03:52<00:57, 34.30it/s]Training CobwebTree:  82%| | 8830/10788 [03:52<00:54, 35.71it/s]Training CobwebTree:  82%| | 8834/10788 [03:52<00:53, 36.34it/s]Training CobwebTree:  82%| | 8838/10788 [03:52<00:55, 35.27it/s]Training CobwebTree:  82%| | 8842/10788 [03:52<00:54, 35.95it/s]Training CobwebTree:  82%| | 8846/10788 [03:52<00:55, 34.96it/s]Training CobwebTree:  82%| | 8851/10788 [03:52<00:52, 36.69it/s]Training CobwebTree:  82%| | 8855/10788 [03:53<00:55, 34.66it/s]Training CobwebTree:  82%| | 8859/10788 [03:53<00:54, 35.68it/s]Training CobwebTree:  82%| | 8863/10788 [03:53<00:55, 34.47it/s]Training CobwebTree:  82%| | 8868/10788 [03:53<00:53, 36.08it/s]Training CobwebTree:  82%| | 8872/10788 [03:53<00:53, 36.11it/s]Training CobwebTree:  82%| | 8876/10788 [03:53<00:53, 35.52it/s]Training CobwebTree:  82%| | 8880/10788 [03:53<00:54, 35.30it/s]Training CobwebTree:  82%| | 8885/10788 [03:53<00:50, 37.61it/s]Training CobwebTree:  82%| | 8889/10788 [03:54<00:50, 37.48it/s]Training CobwebTree:  82%| | 8893/10788 [03:54<00:52, 36.00it/s]Training CobwebTree:  82%| | 8898/10788 [03:54<00:50, 37.33it/s]Training CobwebTree:  83%| | 8902/10788 [03:54<00:51, 36.32it/s]Training CobwebTree:  83%| | 8906/10788 [03:54<00:53, 34.95it/s]Training CobwebTree:  83%| | 8910/10788 [03:54<00:55, 33.85it/s]Training CobwebTree:  83%| | 8914/10788 [03:54<00:57, 32.71it/s]Training CobwebTree:  83%| | 8918/10788 [03:54<00:57, 32.25it/s]Training CobwebTree:  83%| | 8922/10788 [03:55<00:55, 33.87it/s]Training CobwebTree:  83%| | 8926/10788 [03:55<00:54, 34.10it/s]Training CobwebTree:  83%| | 8930/10788 [03:55<00:52, 35.25it/s]Training CobwebTree:  83%| | 8934/10788 [03:55<00:52, 35.09it/s]Training CobwebTree:  83%| | 8938/10788 [03:55<00:53, 34.64it/s]Training CobwebTree:  83%| | 8942/10788 [03:55<00:54, 34.00it/s]Training CobwebTree:  83%| | 8947/10788 [03:55<00:50, 36.72it/s]Training CobwebTree:  83%| | 8951/10788 [03:55<00:52, 34.75it/s]Training CobwebTree:  83%| | 8955/10788 [03:55<00:55, 33.15it/s]Training CobwebTree:  83%| | 8959/10788 [03:56<00:55, 32.87it/s]Training CobwebTree:  83%| | 8963/10788 [03:56<00:54, 33.58it/s]Training CobwebTree:  83%| | 8967/10788 [03:56<00:52, 34.84it/s]Training CobwebTree:  83%| | 8971/10788 [03:56<00:53, 34.13it/s]Training CobwebTree:  83%| | 8975/10788 [03:56<00:52, 34.41it/s]Training CobwebTree:  83%| | 8979/10788 [03:56<00:53, 33.61it/s]Training CobwebTree:  83%| | 8983/10788 [03:56<00:53, 33.59it/s]Training CobwebTree:  83%| | 8987/10788 [03:56<00:55, 32.58it/s]Training CobwebTree:  83%| | 8991/10788 [03:57<00:54, 33.10it/s]Training CobwebTree:  83%| | 8995/10788 [03:57<00:52, 34.29it/s]Training CobwebTree:  83%| | 8999/10788 [03:57<00:53, 33.73it/s]Training CobwebTree:  83%| | 9003/10788 [03:57<00:52, 34.07it/s]Training CobwebTree:  83%| | 9007/10788 [03:57<00:54, 32.78it/s]Training CobwebTree:  84%| | 9011/10788 [03:57<00:54, 32.53it/s]Training CobwebTree:  84%| | 9015/10788 [03:57<00:52, 33.93it/s]Training CobwebTree:  84%| | 9019/10788 [03:57<00:54, 32.69it/s]Training CobwebTree:  84%| | 9023/10788 [03:58<00:52, 33.61it/s]Training CobwebTree:  84%| | 9027/10788 [03:58<00:53, 32.96it/s]Training CobwebTree:  84%| | 9031/10788 [03:58<00:52, 33.78it/s]Training CobwebTree:  84%| | 9035/10788 [03:58<00:51, 33.81it/s]Training CobwebTree:  84%| | 9039/10788 [03:58<00:51, 34.25it/s]Training CobwebTree:  84%| | 9043/10788 [03:58<00:50, 34.32it/s]Training CobwebTree:  84%| | 9047/10788 [03:58<00:50, 34.36it/s]Training CobwebTree:  84%| | 9051/10788 [03:58<00:52, 33.28it/s]Training CobwebTree:  84%| | 9055/10788 [03:58<00:52, 32.85it/s]Training CobwebTree:  84%| | 9059/10788 [03:59<00:52, 32.83it/s]Training CobwebTree:  84%| | 9063/10788 [03:59<00:52, 32.62it/s]Training CobwebTree:  84%| | 9067/10788 [03:59<00:50, 34.06it/s]Training CobwebTree:  84%| | 9071/10788 [03:59<00:49, 34.70it/s]Training CobwebTree:  84%| | 9075/10788 [03:59<00:48, 35.54it/s]Training CobwebTree:  84%| | 9079/10788 [03:59<00:49, 34.58it/s]Training CobwebTree:  84%| | 9083/10788 [03:59<00:52, 32.65it/s]Training CobwebTree:  84%| | 9087/10788 [03:59<00:52, 32.67it/s]Training CobwebTree:  84%| | 9091/10788 [04:00<00:50, 33.34it/s]Training CobwebTree:  84%| | 9095/10788 [04:00<00:50, 33.67it/s]Training CobwebTree:  84%| | 9099/10788 [04:00<00:52, 32.28it/s]Training CobwebTree:  84%| | 9103/10788 [04:00<00:51, 32.62it/s]Training CobwebTree:  84%| | 9107/10788 [04:00<00:51, 32.55it/s]Training CobwebTree:  84%| | 9111/10788 [04:00<00:50, 32.90it/s]Training CobwebTree:  84%| | 9115/10788 [04:00<00:49, 33.51it/s]Training CobwebTree:  85%| | 9119/10788 [04:00<00:50, 33.27it/s]Training CobwebTree:  85%| | 9123/10788 [04:00<00:48, 34.42it/s]Training CobwebTree:  85%| | 9127/10788 [04:01<00:46, 35.52it/s]Training CobwebTree:  85%| | 9131/10788 [04:01<00:46, 35.56it/s]Training CobwebTree:  85%| | 9135/10788 [04:01<00:45, 36.06it/s]Training CobwebTree:  85%| | 9139/10788 [04:01<00:47, 34.97it/s]Training CobwebTree:  85%| | 9143/10788 [04:01<00:46, 35.19it/s]Training CobwebTree:  85%| | 9147/10788 [04:01<00:45, 36.21it/s]Training CobwebTree:  85%| | 9151/10788 [04:01<00:46, 35.49it/s]Training CobwebTree:  85%| | 9155/10788 [04:01<00:48, 33.87it/s]Training CobwebTree:  85%| | 9159/10788 [04:02<00:47, 34.25it/s]Training CobwebTree:  85%| | 9163/10788 [04:02<00:47, 34.46it/s]Training CobwebTree:  85%| | 9167/10788 [04:02<00:47, 33.86it/s]Training CobwebTree:  85%| | 9171/10788 [04:02<00:48, 33.47it/s]Training CobwebTree:  85%| | 9175/10788 [04:02<00:49, 32.57it/s]Training CobwebTree:  85%| | 9179/10788 [04:02<00:51, 31.55it/s]Training CobwebTree:  85%| | 9183/10788 [04:02<00:49, 32.40it/s]Training CobwebTree:  85%| | 9187/10788 [04:02<00:48, 33.31it/s]Training CobwebTree:  85%| | 9191/10788 [04:02<00:47, 33.60it/s]Training CobwebTree:  85%| | 9196/10788 [04:03<00:44, 36.17it/s]Training CobwebTree:  85%| | 9200/10788 [04:03<00:43, 36.72it/s]Training CobwebTree:  85%| | 9204/10788 [04:03<00:43, 36.38it/s]Training CobwebTree:  85%| | 9208/10788 [04:03<00:48, 32.54it/s]Training CobwebTree:  85%| | 9212/10788 [04:03<00:46, 33.89it/s]Training CobwebTree:  85%| | 9216/10788 [04:03<00:47, 32.89it/s]Training CobwebTree:  85%| | 9220/10788 [04:03<00:46, 34.03it/s]Training CobwebTree:  86%| | 9224/10788 [04:03<00:45, 34.20it/s]Training CobwebTree:  86%| | 9228/10788 [04:04<00:45, 34.23it/s]Training CobwebTree:  86%| | 9232/10788 [04:04<00:47, 33.09it/s]Training CobwebTree:  86%| | 9237/10788 [04:04<00:43, 35.51it/s]Training CobwebTree:  86%| | 9241/10788 [04:04<00:43, 35.44it/s]Training CobwebTree:  86%| | 9245/10788 [04:04<00:43, 35.09it/s]Training CobwebTree:  86%| | 9249/10788 [04:04<00:43, 35.70it/s]Training CobwebTree:  86%| | 9253/10788 [04:04<00:42, 35.71it/s]Training CobwebTree:  86%| | 9257/10788 [04:04<00:43, 35.25it/s]Training CobwebTree:  86%| | 9261/10788 [04:04<00:43, 34.94it/s]Training CobwebTree:  86%| | 9265/10788 [04:05<00:44, 34.09it/s]Training CobwebTree:  86%| | 9269/10788 [04:05<00:46, 32.52it/s]Training CobwebTree:  86%| | 9273/10788 [04:05<00:46, 32.59it/s]Training CobwebTree:  86%| | 9277/10788 [04:05<00:44, 33.71it/s]Training CobwebTree:  86%| | 9281/10788 [04:05<00:44, 33.63it/s]Training CobwebTree:  86%| | 9285/10788 [04:05<00:46, 32.64it/s]Training CobwebTree:  86%| | 9289/10788 [04:05<00:47, 31.89it/s]Training CobwebTree:  86%| | 9293/10788 [04:05<00:47, 31.74it/s]Training CobwebTree:  86%| | 9297/10788 [04:06<00:48, 30.86it/s]Training CobwebTree:  86%| | 9301/10788 [04:06<00:47, 31.61it/s]Training CobwebTree:  86%| | 9305/10788 [04:06<00:46, 32.16it/s]Training CobwebTree:  86%| | 9310/10788 [04:06<00:43, 34.27it/s]Training CobwebTree:  86%| | 9314/10788 [04:06<00:41, 35.67it/s]Training CobwebTree:  86%| | 9318/10788 [04:06<00:42, 34.38it/s]Training CobwebTree:  86%| | 9322/10788 [04:06<00:43, 34.04it/s]Training CobwebTree:  86%| | 9326/10788 [04:06<00:42, 34.17it/s]Training CobwebTree:  86%| | 9330/10788 [04:07<00:44, 33.08it/s]Training CobwebTree:  87%| | 9334/10788 [04:07<00:42, 33.95it/s]Training CobwebTree:  87%| | 9338/10788 [04:07<00:41, 34.82it/s]Training CobwebTree:  87%| | 9342/10788 [04:07<00:41, 34.92it/s]Training CobwebTree:  87%| | 9346/10788 [04:07<00:41, 34.45it/s]Training CobwebTree:  87%| | 9350/10788 [04:07<00:41, 34.65it/s]Training CobwebTree:  87%| | 9354/10788 [04:07<00:41, 34.60it/s]Training CobwebTree:  87%| | 9358/10788 [04:07<00:40, 35.22it/s]Training CobwebTree:  87%| | 9362/10788 [04:07<00:41, 34.36it/s]Training CobwebTree:  87%| | 9366/10788 [04:08<00:41, 34.59it/s]Training CobwebTree:  87%| | 9370/10788 [04:08<00:40, 34.79it/s]Training CobwebTree:  87%| | 9374/10788 [04:08<00:40, 35.00it/s]Training CobwebTree:  87%| | 9378/10788 [04:08<00:39, 35.94it/s]Training CobwebTree:  87%| | 9382/10788 [04:08<00:39, 35.45it/s]Training CobwebTree:  87%| | 9386/10788 [04:08<00:40, 34.85it/s]Training CobwebTree:  87%| | 9390/10788 [04:08<00:41, 33.30it/s]Training CobwebTree:  87%| | 9394/10788 [04:08<00:42, 32.64it/s]Training CobwebTree:  87%| | 9398/10788 [04:09<00:41, 33.24it/s]Training CobwebTree:  87%| | 9402/10788 [04:09<00:40, 34.63it/s]Training CobwebTree:  87%| | 9406/10788 [04:09<00:39, 34.89it/s]Training CobwebTree:  87%| | 9411/10788 [04:09<00:37, 36.39it/s]Training CobwebTree:  87%| | 9415/10788 [04:09<00:38, 35.79it/s]Training CobwebTree:  87%| | 9419/10788 [04:09<00:39, 34.92it/s]Training CobwebTree:  87%| | 9423/10788 [04:09<00:41, 32.93it/s]Training CobwebTree:  87%| | 9427/10788 [04:09<00:41, 32.56it/s]Training CobwebTree:  87%| | 9431/10788 [04:10<00:40, 33.56it/s]Training CobwebTree:  87%| | 9435/10788 [04:10<00:38, 34.72it/s]Training CobwebTree:  87%| | 9439/10788 [04:10<00:38, 35.35it/s]Training CobwebTree:  88%| | 9443/10788 [04:10<00:38, 34.76it/s]Training CobwebTree:  88%| | 9447/10788 [04:10<00:39, 33.88it/s]Training CobwebTree:  88%| | 9451/10788 [04:10<00:38, 34.46it/s]Training CobwebTree:  88%| | 9455/10788 [04:10<00:39, 33.88it/s]Training CobwebTree:  88%| | 9459/10788 [04:10<00:39, 33.84it/s]Training CobwebTree:  88%| | 9463/10788 [04:10<00:39, 33.53it/s]Training CobwebTree:  88%| | 9467/10788 [04:11<00:37, 34.96it/s]Training CobwebTree:  88%| | 9471/10788 [04:11<00:38, 33.86it/s]Training CobwebTree:  88%| | 9475/10788 [04:11<00:37, 35.13it/s]Training CobwebTree:  88%| | 9479/10788 [04:11<00:36, 35.84it/s]Training CobwebTree:  88%| | 9483/10788 [04:11<00:35, 36.48it/s]Training CobwebTree:  88%| | 9487/10788 [04:11<00:37, 34.46it/s]Training CobwebTree:  88%| | 9491/10788 [04:11<00:37, 34.81it/s]Training CobwebTree:  88%| | 9495/10788 [04:11<00:37, 34.94it/s]Training CobwebTree:  88%| | 9499/10788 [04:11<00:35, 35.95it/s]Training CobwebTree:  88%| | 9503/10788 [04:12<00:36, 35.24it/s]Training CobwebTree:  88%| | 9507/10788 [04:12<00:35, 36.21it/s]Training CobwebTree:  88%| | 9511/10788 [04:12<00:35, 35.56it/s]Training CobwebTree:  88%| | 9515/10788 [04:12<00:36, 34.48it/s]Training CobwebTree:  88%| | 9519/10788 [04:12<00:37, 33.53it/s]Training CobwebTree:  88%| | 9523/10788 [04:12<00:36, 34.50it/s]Training CobwebTree:  88%| | 9527/10788 [04:12<00:36, 34.40it/s]Training CobwebTree:  88%| | 9531/10788 [04:12<00:36, 34.13it/s]Training CobwebTree:  88%| | 9535/10788 [04:12<00:36, 34.23it/s]Training CobwebTree:  88%| | 9539/10788 [04:13<00:37, 33.13it/s]Training CobwebTree:  88%| | 9543/10788 [04:13<00:36, 34.03it/s]Training CobwebTree:  88%| | 9547/10788 [04:13<00:39, 31.80it/s]Training CobwebTree:  89%| | 9551/10788 [04:13<00:40, 30.88it/s]Training CobwebTree:  89%| | 9555/10788 [04:13<00:38, 32.41it/s]Training CobwebTree:  89%| | 9559/10788 [04:13<00:36, 33.41it/s]Training CobwebTree:  89%| | 9563/10788 [04:13<00:36, 33.20it/s]Training CobwebTree:  89%| | 9567/10788 [04:13<00:35, 34.11it/s]Training CobwebTree:  89%| | 9571/10788 [04:14<00:34, 35.15it/s]Training CobwebTree:  89%| | 9575/10788 [04:14<00:35, 34.19it/s]Training CobwebTree:  89%| | 9579/10788 [04:14<00:35, 34.19it/s]Training CobwebTree:  89%| | 9584/10788 [04:14<00:34, 35.36it/s]Training CobwebTree:  89%| | 9588/10788 [04:14<00:34, 35.18it/s]Training CobwebTree:  89%| | 9592/10788 [04:14<00:35, 34.11it/s]Training CobwebTree:  89%| | 9597/10788 [04:14<00:33, 35.49it/s]Training CobwebTree:  89%| | 9601/10788 [04:14<00:32, 36.35it/s]Training CobwebTree:  89%| | 9605/10788 [04:15<00:33, 35.13it/s]Training CobwebTree:  89%| | 9609/10788 [04:15<00:34, 34.07it/s]Training CobwebTree:  89%| | 9614/10788 [04:15<00:31, 37.04it/s]Training CobwebTree:  89%| | 9618/10788 [04:15<00:32, 35.60it/s]Training CobwebTree:  89%| | 9622/10788 [04:15<00:34, 34.25it/s]Training CobwebTree:  89%| | 9626/10788 [04:15<00:32, 35.25it/s]Training CobwebTree:  89%| | 9630/10788 [04:15<00:33, 34.73it/s]Training CobwebTree:  89%| | 9634/10788 [04:15<00:34, 33.87it/s]Training CobwebTree:  89%| | 9638/10788 [04:16<00:33, 34.01it/s]Training CobwebTree:  89%| | 9642/10788 [04:16<00:35, 32.66it/s]Training CobwebTree:  89%| | 9646/10788 [04:16<00:33, 34.09it/s]Training CobwebTree:  89%| | 9650/10788 [04:16<00:32, 34.56it/s]Training CobwebTree:  89%| | 9654/10788 [04:16<00:32, 34.69it/s]Training CobwebTree:  90%| | 9658/10788 [04:16<00:33, 33.39it/s]Training CobwebTree:  90%| | 9662/10788 [04:16<00:34, 33.00it/s]Training CobwebTree:  90%| | 9666/10788 [04:16<00:33, 33.90it/s]Training CobwebTree:  90%| | 9670/10788 [04:16<00:34, 32.87it/s]Training CobwebTree:  90%| | 9674/10788 [04:17<00:34, 32.33it/s]Training CobwebTree:  90%| | 9678/10788 [04:17<00:34, 31.73it/s]Training CobwebTree:  90%| | 9682/10788 [04:17<00:33, 33.01it/s]Training CobwebTree:  90%| | 9686/10788 [04:17<00:33, 33.06it/s]Training CobwebTree:  90%| | 9690/10788 [04:17<00:32, 34.30it/s]Training CobwebTree:  90%| | 9694/10788 [04:17<00:32, 33.71it/s]Training CobwebTree:  90%| | 9698/10788 [04:17<00:32, 34.06it/s]Training CobwebTree:  90%| | 9702/10788 [04:17<00:31, 33.94it/s]Training CobwebTree:  90%| | 9706/10788 [04:18<00:31, 34.63it/s]Training CobwebTree:  90%| | 9710/10788 [04:18<00:31, 34.45it/s]Training CobwebTree:  90%| | 9715/10788 [04:18<00:30, 35.46it/s]Training CobwebTree:  90%| | 9719/10788 [04:18<00:29, 35.70it/s]Training CobwebTree:  90%| | 9723/10788 [04:18<00:30, 34.87it/s]Training CobwebTree:  90%| | 9727/10788 [04:18<00:30, 34.56it/s]Training CobwebTree:  90%| | 9731/10788 [04:18<00:32, 32.54it/s]Training CobwebTree:  90%| | 9735/10788 [04:18<00:31, 33.70it/s]Training CobwebTree:  90%| | 9739/10788 [04:18<00:31, 33.57it/s]Training CobwebTree:  90%| | 9743/10788 [04:19<00:32, 32.33it/s]Training CobwebTree:  90%| | 9747/10788 [04:19<00:31, 32.62it/s]Training CobwebTree:  90%| | 9751/10788 [04:19<00:32, 31.59it/s]Training CobwebTree:  90%| | 9755/10788 [04:19<00:31, 32.32it/s]Training CobwebTree:  90%| | 9759/10788 [04:19<00:31, 32.69it/s]Training CobwebTree:  90%| | 9763/10788 [04:19<00:31, 32.85it/s]Training CobwebTree:  91%| | 9767/10788 [04:19<00:31, 31.96it/s]Training CobwebTree:  91%| | 9771/10788 [04:19<00:31, 32.32it/s]Training CobwebTree:  91%| | 9775/10788 [04:20<00:31, 32.06it/s]Training CobwebTree:  91%| | 9779/10788 [04:20<00:31, 31.95it/s]Training CobwebTree:  91%| | 9783/10788 [04:20<00:30, 32.77it/s]Training CobwebTree:  91%| | 9787/10788 [04:20<00:31, 32.02it/s]Training CobwebTree:  91%| | 9791/10788 [04:20<00:30, 33.13it/s]Training CobwebTree:  91%| | 9795/10788 [04:20<00:30, 33.07it/s]Training CobwebTree:  91%| | 9799/10788 [04:20<00:31, 31.66it/s]Training CobwebTree:  91%| | 9803/10788 [04:20<00:30, 32.29it/s]Training CobwebTree:  91%| | 9807/10788 [04:21<00:29, 33.29it/s]Training CobwebTree:  91%| | 9811/10788 [04:21<00:30, 32.46it/s]Training CobwebTree:  91%| | 9815/10788 [04:21<00:29, 32.79it/s]Training CobwebTree:  91%| | 9819/10788 [04:21<00:30, 31.36it/s]Training CobwebTree:  91%| | 9823/10788 [04:21<00:30, 31.63it/s]Training CobwebTree:  91%| | 9827/10788 [04:21<00:29, 32.81it/s]Training CobwebTree:  91%| | 9831/10788 [04:21<00:28, 33.92it/s]Training CobwebTree:  91%| | 9835/10788 [04:21<00:29, 32.56it/s]Training CobwebTree:  91%| | 9839/10788 [04:22<00:28, 33.12it/s]Training CobwebTree:  91%| | 9843/10788 [04:22<00:28, 33.18it/s]Training CobwebTree:  91%|| 9847/10788 [04:22<00:28, 32.60it/s]Training CobwebTree:  91%|| 9851/10788 [04:22<00:28, 33.38it/s]Training CobwebTree:  91%|| 9855/10788 [04:22<00:27, 34.00it/s]Training CobwebTree:  91%|| 9859/10788 [04:22<00:27, 33.47it/s]Training CobwebTree:  91%|| 9863/10788 [04:22<00:26, 34.34it/s]Training CobwebTree:  91%|| 9867/10788 [04:22<00:26, 34.76it/s]Training CobwebTree:  91%|| 9871/10788 [04:23<00:26, 34.98it/s]Training CobwebTree:  92%|| 9875/10788 [04:23<00:26, 34.89it/s]Training CobwebTree:  92%|| 9879/10788 [04:23<00:25, 35.14it/s]Training CobwebTree:  92%|| 9883/10788 [04:23<00:26, 33.70it/s]Training CobwebTree:  92%|| 9887/10788 [04:23<00:26, 33.89it/s]Training CobwebTree:  92%|| 9891/10788 [04:23<00:25, 35.12it/s]Training CobwebTree:  92%|| 9895/10788 [04:23<00:26, 34.27it/s]Training CobwebTree:  92%|| 9899/10788 [04:23<00:25, 34.63it/s]Training CobwebTree:  92%|| 9903/10788 [04:23<00:25, 34.94it/s]Training CobwebTree:  92%|| 9907/10788 [04:24<00:27, 32.43it/s]Training CobwebTree:  92%|| 9911/10788 [04:24<00:27, 31.61it/s]Training CobwebTree:  92%|| 9915/10788 [04:24<00:26, 32.47it/s]Training CobwebTree:  92%|| 9919/10788 [04:24<00:26, 33.39it/s]Training CobwebTree:  92%|| 9923/10788 [04:24<00:25, 33.33it/s]Training CobwebTree:  92%|| 9927/10788 [04:24<00:26, 32.14it/s]Training CobwebTree:  92%|| 9931/10788 [04:24<00:26, 32.34it/s]Training CobwebTree:  92%|| 9935/10788 [04:24<00:25, 33.48it/s]Training CobwebTree:  92%|| 9939/10788 [04:25<00:24, 34.43it/s]Training CobwebTree:  92%|| 9943/10788 [04:25<00:25, 33.24it/s]Training CobwebTree:  92%|| 9947/10788 [04:25<00:26, 31.65it/s]Training CobwebTree:  92%|| 9951/10788 [04:25<00:24, 33.49it/s]Training CobwebTree:  92%|| 9955/10788 [04:25<00:23, 34.71it/s]Training CobwebTree:  92%|| 9959/10788 [04:25<00:23, 34.83it/s]Training CobwebTree:  92%|| 9963/10788 [04:25<00:25, 32.26it/s]Training CobwebTree:  92%|| 9967/10788 [04:25<00:24, 32.90it/s]Training CobwebTree:  92%|| 9971/10788 [04:26<00:23, 34.20it/s]Training CobwebTree:  92%|| 9975/10788 [04:26<00:22, 35.55it/s]Training CobwebTree:  93%|| 9979/10788 [04:26<00:22, 35.33it/s]Training CobwebTree:  93%|| 9983/10788 [04:26<00:22, 35.02it/s]Training CobwebTree:  93%|| 9987/10788 [04:26<00:23, 34.13it/s]Training CobwebTree:  93%|| 9991/10788 [04:26<00:23, 33.48it/s]Training CobwebTree:  93%|| 9995/10788 [04:26<00:23, 33.92it/s]Training CobwebTree:  93%|| 9999/10788 [04:26<00:23, 33.82it/s]Training CobwebTree:  93%|| 10003/10788 [04:26<00:22, 34.83it/s]Training CobwebTree:  93%|| 10007/10788 [04:27<00:21, 35.86it/s]Training CobwebTree:  93%|| 10011/10788 [04:27<00:22, 34.37it/s]Training CobwebTree:  93%|| 10015/10788 [04:27<00:22, 34.68it/s]Training CobwebTree:  93%|| 10019/10788 [04:27<00:21, 35.36it/s]Training CobwebTree:  93%|| 10023/10788 [04:27<00:21, 35.55it/s]Training CobwebTree:  93%|| 10027/10788 [04:27<00:21, 35.37it/s]Training CobwebTree:  93%|| 10031/10788 [04:27<00:21, 35.92it/s]Training CobwebTree:  93%|| 10035/10788 [04:27<00:21, 35.80it/s]Training CobwebTree:  93%|| 10039/10788 [04:27<00:21, 35.22it/s]Training CobwebTree:  93%|| 10043/10788 [04:28<00:20, 36.39it/s]Training CobwebTree:  93%|| 10047/10788 [04:28<00:21, 34.98it/s]Training CobwebTree:  93%|| 10051/10788 [04:28<00:21, 33.94it/s]Training CobwebTree:  93%|| 10055/10788 [04:28<00:20, 35.23it/s]Training CobwebTree:  93%|| 10059/10788 [04:28<00:20, 35.11it/s]Training CobwebTree:  93%|| 10063/10788 [04:28<00:21, 33.86it/s]Training CobwebTree:  93%|| 10067/10788 [04:28<00:21, 32.90it/s]Training CobwebTree:  93%|| 10071/10788 [04:28<00:21, 33.18it/s]Training CobwebTree:  93%|| 10076/10788 [04:29<00:20, 35.26it/s]Training CobwebTree:  93%|| 10080/10788 [04:29<00:21, 32.77it/s]Training CobwebTree:  93%|| 10084/10788 [04:29<00:21, 33.22it/s]Training CobwebTree:  94%|| 10088/10788 [04:29<00:20, 33.83it/s]Training CobwebTree:  94%|| 10092/10788 [04:29<00:20, 34.64it/s]Training CobwebTree:  94%|| 10096/10788 [04:29<00:20, 33.47it/s]Training CobwebTree:  94%|| 10100/10788 [04:29<00:20, 33.22it/s]Training CobwebTree:  94%|| 10104/10788 [04:29<00:20, 33.42it/s]Training CobwebTree:  94%|| 10108/10788 [04:29<00:19, 34.26it/s]Training CobwebTree:  94%|| 10112/10788 [04:30<00:20, 33.43it/s]Training CobwebTree:  94%|| 10116/10788 [04:30<00:20, 33.07it/s]Training CobwebTree:  94%|| 10120/10788 [04:30<00:20, 33.00it/s]Training CobwebTree:  94%|| 10124/10788 [04:30<00:21, 31.53it/s]Training CobwebTree:  94%|| 10128/10788 [04:30<00:21, 30.50it/s]Training CobwebTree:  94%|| 10132/10788 [04:30<00:21, 30.73it/s]Training CobwebTree:  94%|| 10136/10788 [04:30<00:20, 31.64it/s]Training CobwebTree:  94%|| 10140/10788 [04:30<00:19, 32.51it/s]Training CobwebTree:  94%|| 10144/10788 [04:31<00:20, 31.82it/s]Training CobwebTree:  94%|| 10148/10788 [04:31<00:19, 32.78it/s]Training CobwebTree:  94%|| 10152/10788 [04:31<00:18, 34.52it/s]Training CobwebTree:  94%|| 10156/10788 [04:31<00:19, 32.88it/s]Training CobwebTree:  94%|| 10160/10788 [04:31<00:18, 33.15it/s]Training CobwebTree:  94%|| 10164/10788 [04:31<00:18, 33.70it/s]Training CobwebTree:  94%|| 10168/10788 [04:31<00:18, 33.19it/s]Training CobwebTree:  94%|| 10172/10788 [04:31<00:18, 33.94it/s]Training CobwebTree:  94%|| 10176/10788 [04:32<00:18, 33.82it/s]Training CobwebTree:  94%|| 10180/10788 [04:32<00:18, 33.09it/s]Training CobwebTree:  94%|| 10184/10788 [04:32<00:18, 32.65it/s]Training CobwebTree:  94%|| 10188/10788 [04:32<00:18, 32.75it/s]Training CobwebTree:  94%|| 10192/10788 [04:32<00:18, 32.89it/s]Training CobwebTree:  95%|| 10196/10788 [04:32<00:17, 33.07it/s]Training CobwebTree:  95%|| 10200/10788 [04:32<00:18, 32.28it/s]Training CobwebTree:  95%|| 10204/10788 [04:32<00:18, 31.98it/s]Training CobwebTree:  95%|| 10208/10788 [04:33<00:17, 32.31it/s]Training CobwebTree:  95%|| 10212/10788 [04:33<00:17, 32.33it/s]Training CobwebTree:  95%|| 10216/10788 [04:33<00:16, 33.91it/s]Training CobwebTree:  95%|| 10220/10788 [04:33<00:17, 32.51it/s]Training CobwebTree:  95%|| 10224/10788 [04:33<00:17, 32.35it/s]Training CobwebTree:  95%|| 10228/10788 [04:33<00:17, 32.74it/s]Training CobwebTree:  95%|| 10232/10788 [04:33<00:17, 32.67it/s]Training CobwebTree:  95%|| 10236/10788 [04:33<00:17, 32.14it/s]Training CobwebTree:  95%|| 10240/10788 [04:34<00:16, 32.24it/s]Training CobwebTree:  95%|| 10244/10788 [04:34<00:16, 32.96it/s]Training CobwebTree:  95%|| 10248/10788 [04:34<00:15, 33.84it/s]Training CobwebTree:  95%|| 10252/10788 [04:34<00:15, 33.97it/s]Training CobwebTree:  95%|| 10256/10788 [04:34<00:15, 34.96it/s]Training CobwebTree:  95%|| 10261/10788 [04:34<00:14, 37.00it/s]Training CobwebTree:  95%|| 10265/10788 [04:34<00:14, 35.34it/s]Training CobwebTree:  95%|| 10269/10788 [04:34<00:15, 33.79it/s]Training CobwebTree:  95%|| 10273/10788 [04:34<00:15, 33.04it/s]Training CobwebTree:  95%|| 10277/10788 [04:35<00:14, 34.47it/s]Training CobwebTree:  95%|| 10281/10788 [04:35<00:15, 32.74it/s]Training CobwebTree:  95%|| 10285/10788 [04:35<00:15, 32.83it/s]Training CobwebTree:  95%|| 10289/10788 [04:35<00:14, 33.41it/s]Training CobwebTree:  95%|| 10293/10788 [04:35<00:15, 32.35it/s]Training CobwebTree:  95%|| 10297/10788 [04:35<00:14, 33.69it/s]Training CobwebTree:  95%|| 10301/10788 [04:35<00:14, 34.21it/s]Training CobwebTree:  96%|| 10305/10788 [04:35<00:14, 33.46it/s]Training CobwebTree:  96%|| 10309/10788 [04:36<00:13, 34.61it/s]Training CobwebTree:  96%|| 10313/10788 [04:36<00:14, 33.81it/s]Training CobwebTree:  96%|| 10317/10788 [04:36<00:13, 34.81it/s]Training CobwebTree:  96%|| 10321/10788 [04:36<00:13, 34.77it/s]Training CobwebTree:  96%|| 10325/10788 [04:36<00:13, 33.56it/s]Training CobwebTree:  96%|| 10329/10788 [04:36<00:13, 33.78it/s]Training CobwebTree:  96%|| 10333/10788 [04:36<00:13, 34.68it/s]Training CobwebTree:  96%|| 10337/10788 [04:36<00:13, 33.57it/s]Training CobwebTree:  96%|| 10341/10788 [04:36<00:13, 33.60it/s]Training CobwebTree:  96%|| 10345/10788 [04:37<00:13, 33.92it/s]Training CobwebTree:  96%|| 10349/10788 [04:37<00:12, 34.11it/s]Training CobwebTree:  96%|| 10353/10788 [04:37<00:12, 34.90it/s]Training CobwebTree:  96%|| 10357/10788 [04:37<00:11, 35.98it/s]Training CobwebTree:  96%|| 10361/10788 [04:37<00:11, 35.82it/s]Training CobwebTree:  96%|| 10365/10788 [04:37<00:12, 34.74it/s]Training CobwebTree:  96%|| 10369/10788 [04:37<00:11, 35.78it/s]Training CobwebTree:  96%|| 10373/10788 [04:37<00:11, 35.41it/s]Training CobwebTree:  96%|| 10377/10788 [04:38<00:11, 34.68it/s]Training CobwebTree:  96%|| 10381/10788 [04:38<00:11, 34.41it/s]Training CobwebTree:  96%|| 10385/10788 [04:38<00:11, 35.12it/s]Training CobwebTree:  96%|| 10390/10788 [04:38<00:11, 35.39it/s]Training CobwebTree:  96%|| 10394/10788 [04:38<00:11, 33.92it/s]Training CobwebTree:  96%|| 10398/10788 [04:38<00:11, 33.92it/s]Training CobwebTree:  96%|| 10402/10788 [04:38<00:11, 32.43it/s]Training CobwebTree:  96%|| 10406/10788 [04:38<00:11, 32.52it/s]Training CobwebTree:  96%|| 10410/10788 [04:38<00:11, 33.55it/s]Training CobwebTree:  97%|| 10414/10788 [04:39<00:11, 32.87it/s]Training CobwebTree:  97%|| 10418/10788 [04:39<00:11, 33.52it/s]Training CobwebTree:  97%|| 10422/10788 [04:39<00:10, 34.66it/s]Training CobwebTree:  97%|| 10426/10788 [04:39<00:10, 33.99it/s]Training CobwebTree:  97%|| 10430/10788 [04:39<00:10, 34.05it/s]Training CobwebTree:  97%|| 10434/10788 [04:39<00:10, 33.46it/s]Training CobwebTree:  97%|| 10438/10788 [04:39<00:10, 33.74it/s]Training CobwebTree:  97%|| 10442/10788 [04:39<00:09, 34.73it/s]Training CobwebTree:  97%|| 10446/10788 [04:40<00:09, 34.62it/s]Training CobwebTree:  97%|| 10450/10788 [04:40<00:09, 34.11it/s]Training CobwebTree:  97%|| 10454/10788 [04:40<00:10, 33.05it/s]Training CobwebTree:  97%|| 10458/10788 [04:40<00:09, 34.46it/s]Training CobwebTree:  97%|| 10462/10788 [04:40<00:09, 34.66it/s]Training CobwebTree:  97%|| 10466/10788 [04:40<00:09, 35.17it/s]Training CobwebTree:  97%|| 10470/10788 [04:40<00:09, 34.93it/s]Training CobwebTree:  97%|| 10474/10788 [04:40<00:08, 35.24it/s]Training CobwebTree:  97%|| 10478/10788 [04:40<00:08, 36.17it/s]Training CobwebTree:  97%|| 10482/10788 [04:41<00:09, 33.95it/s]Training CobwebTree:  97%|| 10486/10788 [04:41<00:09, 33.31it/s]Training CobwebTree:  97%|| 10490/10788 [04:41<00:08, 33.75it/s]Training CobwebTree:  97%|| 10494/10788 [04:41<00:08, 33.42it/s]Training CobwebTree:  97%|| 10498/10788 [04:41<00:09, 31.67it/s]Training CobwebTree:  97%|| 10502/10788 [04:41<00:08, 33.16it/s]Training CobwebTree:  97%|| 10506/10788 [04:41<00:08, 34.62it/s]Training CobwebTree:  97%|| 10510/10788 [04:41<00:08, 33.20it/s]Training CobwebTree:  97%|| 10514/10788 [04:42<00:08, 32.90it/s]Training CobwebTree:  97%|| 10518/10788 [04:42<00:08, 32.35it/s]Training CobwebTree:  98%|| 10522/10788 [04:42<00:07, 34.03it/s]Training CobwebTree:  98%|| 10526/10788 [04:42<00:07, 35.34it/s]Training CobwebTree:  98%|| 10531/10788 [04:42<00:07, 36.63it/s]Training CobwebTree:  98%|| 10535/10788 [04:42<00:06, 36.73it/s]Training CobwebTree:  98%|| 10539/10788 [04:42<00:06, 36.33it/s]Training CobwebTree:  98%|| 10543/10788 [04:42<00:06, 35.39it/s]Training CobwebTree:  98%|| 10548/10788 [04:42<00:06, 37.24it/s]Training CobwebTree:  98%|| 10552/10788 [04:43<00:06, 34.80it/s]Training CobwebTree:  98%|| 10556/10788 [04:43<00:06, 35.33it/s]Training CobwebTree:  98%|| 10560/10788 [04:43<00:06, 33.75it/s]Training CobwebTree:  98%|| 10564/10788 [04:43<00:06, 32.21it/s]Training CobwebTree:  98%|| 10568/10788 [04:43<00:06, 33.32it/s]Training CobwebTree:  98%|| 10572/10788 [04:43<00:06, 33.33it/s]Training CobwebTree:  98%|| 10576/10788 [04:43<00:06, 33.84it/s]Training CobwebTree:  98%|| 10580/10788 [04:43<00:06, 33.95it/s]Training CobwebTree:  98%|| 10584/10788 [04:44<00:05, 34.84it/s]Training CobwebTree:  98%|| 10588/10788 [04:44<00:05, 34.56it/s]Training CobwebTree:  98%|| 10592/10788 [04:44<00:05, 34.45it/s]Training CobwebTree:  98%|| 10596/10788 [04:44<00:05, 33.69it/s]Training CobwebTree:  98%|| 10600/10788 [04:44<00:05, 34.23it/s]Training CobwebTree:  98%|| 10604/10788 [04:44<00:05, 35.35it/s]Training CobwebTree:  98%|| 10608/10788 [04:44<00:05, 35.94it/s]Training CobwebTree:  98%|| 10612/10788 [04:44<00:04, 36.07it/s]Training CobwebTree:  98%|| 10616/10788 [04:44<00:04, 34.58it/s]Training CobwebTree:  98%|| 10620/10788 [04:45<00:05, 33.31it/s]Training CobwebTree:  98%|| 10624/10788 [04:45<00:04, 33.71it/s]Training CobwebTree:  99%|| 10628/10788 [04:45<00:04, 33.45it/s]Training CobwebTree:  99%|| 10632/10788 [04:45<00:04, 34.09it/s]Training CobwebTree:  99%|| 10636/10788 [04:45<00:04, 32.98it/s]Training CobwebTree:  99%|| 10640/10788 [04:45<00:04, 34.06it/s]Training CobwebTree:  99%|| 10644/10788 [04:45<00:04, 33.00it/s]Training CobwebTree:  99%|| 10648/10788 [04:45<00:04, 33.06it/s]Training CobwebTree:  99%|| 10652/10788 [04:46<00:04, 33.28it/s]Training CobwebTree:  99%|| 10656/10788 [04:46<00:04, 32.77it/s]Training CobwebTree:  99%|| 10660/10788 [04:46<00:03, 33.42it/s]Training CobwebTree:  99%|| 10664/10788 [04:46<00:03, 34.61it/s]Training CobwebTree:  99%|| 10668/10788 [04:46<00:03, 35.41it/s]Training CobwebTree:  99%|| 10672/10788 [04:46<00:03, 36.04it/s]Training CobwebTree:  99%|| 10676/10788 [04:46<00:03, 36.78it/s]Training CobwebTree:  99%|| 10681/10788 [04:46<00:02, 36.95it/s]Training CobwebTree:  99%|| 10685/10788 [04:46<00:02, 35.83it/s]Training CobwebTree:  99%|| 10689/10788 [04:47<00:02, 35.26it/s]Training CobwebTree:  99%|| 10693/10788 [04:47<00:02, 36.46it/s]Training CobwebTree:  99%|| 10697/10788 [04:47<00:02, 34.42it/s]Training CobwebTree:  99%|| 10701/10788 [04:47<00:02, 33.33it/s]Training CobwebTree:  99%|| 10705/10788 [04:47<00:02, 32.69it/s]Training CobwebTree:  99%|| 10709/10788 [04:47<00:02, 34.12it/s]Training CobwebTree:  99%|| 10713/10788 [04:47<00:02, 34.13it/s]Training CobwebTree:  99%|| 10717/10788 [04:47<00:02, 34.66it/s]Training CobwebTree:  99%|| 10721/10788 [04:48<00:01, 35.42it/s]Training CobwebTree:  99%|| 10725/10788 [04:48<00:01, 36.38it/s]Training CobwebTree:  99%|| 10729/10788 [04:48<00:01, 36.84it/s]Training CobwebTree:  99%|| 10733/10788 [04:48<00:01, 35.12it/s]Training CobwebTree: 100%|| 10737/10788 [04:48<00:01, 33.38it/s]Training CobwebTree: 100%|| 10741/10788 [04:48<00:01, 32.97it/s]Training CobwebTree: 100%|| 10745/10788 [04:48<00:01, 34.13it/s]Training CobwebTree: 100%|| 10749/10788 [04:48<00:01, 34.22it/s]Training CobwebTree: 100%|| 10753/10788 [04:48<00:01, 34.65it/s]Training CobwebTree: 100%|| 10757/10788 [04:49<00:00, 33.18it/s]Training CobwebTree: 100%|| 10761/10788 [04:49<00:00, 33.10it/s]Training CobwebTree: 100%|| 10765/10788 [04:49<00:00, 33.81it/s]Training CobwebTree: 100%|| 10769/10788 [04:49<00:00, 34.42it/s]Training CobwebTree: 100%|| 10773/10788 [04:49<00:00, 35.02it/s]Training CobwebTree: 100%|| 10777/10788 [04:49<00:00, 35.82it/s]Training CobwebTree: 100%|| 10781/10788 [04:49<00:00, 35.11it/s]Training CobwebTree: 100%|| 10785/10788 [04:49<00:00, 34.55it/s]Training CobwebTree: 100%|| 10788/10788 [04:49<00:00, 37.20it/s]
2025-12-22 12:03:58,974 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:04:01,266 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-22 12:04:01,281 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-22 12:04:01,288 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-22 12:04:01,310 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-22 12:04:01,447 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-22 12:04:01,619 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-22 12:04:01,770 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-22 12:04:03,112 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-22 12:04:03,242 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-22 12:04:03,526 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-22 12:04:03,569 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-22 12:04:03,848 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-22 12:04:04,564 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-22 12:04:04,837 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-22 12:04:05,899 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:05,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:05,960 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:05,985 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:05,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,045 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,159 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,210 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,227 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,275 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,303 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,305 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,357 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,369 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,415 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,431 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,501 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,516 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,726 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,777 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,826 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,845 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,877 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,918 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:06,968 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,975 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:06,995 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,020 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,022 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,024 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,198 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,207 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,271 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,289 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,309 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,270 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,489 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,507 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,660 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:07,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:07,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,056 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,097 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,125 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,313 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,480 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,555 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,583 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,663 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:08,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:08,749 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:10,460 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:04:10,665 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-22 12:04:11,920 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=63, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-22 12:04:14,454 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-22 12:04:14,457 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-22 12:04:14,458 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-22 12:04:14,460 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-22 12:04:14,463 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-22 12:04:14,465 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-22 12:04:14,467 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-22 12:04:14,469 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-22 12:04:14,472 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-22 12:04:14,475 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-22 12:04:14,477 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-22 12:04:14,480 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-22 12:04:14,482 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-22 12:04:14,484 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-22 12:04:14,486 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-22 12:04:14,487 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-22 12:04:14,490 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-22 12:04:14,492 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-22 12:04:14,494 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-22 12:04:14,496 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-22 12:04:14,498 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-22 12:04:14,501 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-22 12:04:14,504 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-22 12:04:14,507 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-22 12:04:14,509 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-22 12:04:14,512 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-22 12:04:14,514 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-22 12:04:14,516 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-22 12:04:14,518 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-22 12:04:14,521 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-22 12:04:14,523 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-22 12:04:14,525 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-22 12:04:14,527 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-22 12:04:14,544 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-22 12:04:14,561 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-22 12:04:14,565 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-22 12:04:14,567 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-22 12:04:14,569 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-22 12:04:14,571 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-22 12:04:14,573 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-22 12:04:14,575 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-22 12:04:14,576 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-22 12:04:14,581 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-22 12:04:14,589 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-22 12:04:14,591 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-22 12:04:14,592 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-22 12:04:14,601 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-22 12:04:14,603 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-22 12:04:14,605 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-22 12:04:14,609 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-22 12:04:14,614 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-22 12:04:14,655 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-22 12:04:14,657 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-22 12:04:14,659 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-22 12:04:14,669 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-22 12:04:14,733 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-22 12:04:14,809 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-22 12:04:14,825 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-22 12:04:14,978 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-22 12:04:14,981 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-22 12:04:14,984 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-22 12:04:15,069 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-22 12:04:15,115 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-22 12:04:15,117 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-22 12:04:15,179 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-22 12:04:15,182 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-22 12:04:15,255 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-22 12:04:15,257 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-22 12:04:15,259 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-22 12:04:15,358 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-22 12:04:15,360 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-22 12:04:15,363 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-22 12:04:15,368 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-22 12:04:15,388 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-22 12:04:15,421 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-22 12:04:15,521 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-22 12:04:15,523 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-22 12:04:15,525 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-22 12:04:15,561 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-22 12:04:15,593 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-22 12:04:15,636 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-22 12:04:15,664 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-22 12:04:15,704 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-22 12:04:15,733 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-22 12:04:15,773 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-22 12:04:15,801 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-22 12:04:15,841 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-22 12:04:15,873 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-22 12:04:15,913 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-22 12:04:15,968 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-22 12:04:15,997 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-22 12:04:16,101 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-22 12:04:16,103 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-22 12:04:16,105 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-22 12:04:16,220 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-22 12:04:16,223 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-22 12:04:16,232 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-22 12:04:16,297 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-22 12:04:16,299 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-22 12:04:16,321 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-22 12:04:16,429 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-22 12:04:16,434 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-22 12:04:16,523 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-22 12:04:16,526 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-22 12:04:16,605 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-22 12:04:16,607 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-22 12:04:16,614 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-22 12:04:16,713 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-22 12:04:16,716 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-22 12:04:16,786 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-22 12:04:16,797 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-22 12:04:16,859 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-22 12:04:16,861 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-22 12:04:16,863 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-22 12:04:16,967 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-22 12:04:16,969 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-22 12:04:16,971 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-22 12:04:17,059 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-22 12:04:17,061 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-22 12:04:17,135 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-22 12:04:17,138 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-22 12:04:17,206 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-22 12:04:17,209 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-22 12:04:17,278 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-22 12:04:17,280 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-22 12:04:17,283 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-22 12:04:17,391 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-22 12:04:17,393 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-22 12:04:17,396 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-22 12:04:17,489 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-22 12:04:17,492 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-22 12:04:17,498 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-22 12:04:17,597 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-22 12:04:17,600 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-22 12:04:17,603 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-22 12:04:17,699 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-22 12:04:17,701 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-22 12:04:17,704 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-22 12:04:17,799 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-22 12:04:17,802 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-22 12:04:17,870 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-22 12:04:17,873 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-22 12:04:17,875 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-22 12:04:17,880 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-22 12:04:17,965 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-22 12:04:17,967 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-22 12:04:18,070 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-22 12:04:18,073 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-22 12:04:18,078 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-22 12:04:18,173 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-22 12:04:18,176 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-22 12:04:18,178 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-22 12:04:18,266 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-22 12:04:18,269 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-22 12:04:18,327 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-22 12:04:18,334 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-22 12:04:18,423 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-22 12:04:18,433 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-22 12:04:18,480 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-22 12:04:18,482 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-22 12:04:18,571 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-22 12:04:18,573 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-22 12:04:18,635 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-22 12:04:18,637 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-22 12:04:18,640 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-22 12:04:18,713 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-22 12:04:18,717 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-22 12:04:18,787 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-22 12:04:18,800 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-22 12:04:18,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,857 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,871 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,838 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:18,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:18,968 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,121 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,450 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,488 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,493 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,548 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,572 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,603 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,639 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,586 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,699 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,709 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,723 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,752 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,847 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,891 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,927 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,878 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,948 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,972 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,984 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:19,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:19,999 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,011 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,012 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,130 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,153 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,130 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,207 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,246 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,326 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,389 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,416 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,435 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,584 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:20,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-22 12:04:20,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-22 12:04:22,500 INFO gensim.topic_coherence.text_analysis: 63 accumulators retrieved from output queue
2025-12-22 12:04:22,668 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-22 12:04:23,340 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.6695714704689303, 'coherence_npmi': 0.1878727948204663, 'topic_diversity': 0.5925373134328358, 'inter_topic_similarity': 0.4867570996284485}
2025-12-22 12:04:23,340 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6528779850361134, 'coherence_npmi': 0.16328155292801924, 'topic_diversity': 0.54, 'inter_topic_similarity': 0.5179042816162109}
2025-12-22 12:04:23,340 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6575963236533569, 'coherence_npmi': 0.16692945822449223, 'topic_diversity': 0.4419047619047619, 'inter_topic_similarity': 0.4301261901855469}
