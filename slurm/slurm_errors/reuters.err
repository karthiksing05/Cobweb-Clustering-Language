2025-12-21 10:26:56,539 INFO __main__: Starting benchmark for dataset=reuters
2025-12-21 10:26:59,170 INFO gensim.corpora.dictionary: adding document #0 to Dictionary<0 unique tokens: []>
2025-12-21 10:26:59,609 INFO gensim.corpora.dictionary: adding document #10000 to Dictionary<29631 unique tokens: ['10', '15', '17', '1985', '30']...>
2025-12-21 10:26:59,645 INFO gensim.corpora.dictionary: built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)
2025-12-21 10:26:59,654 INFO gensim.utils: Dictionary lifecycle event {'msg': "built Dictionary<30627 unique tokens: ['10', '15', '17', '1985', '30']...> from 10788 documents (total 902308 corpus positions)", 'datetime': '2025-12-21T10:26:59.645300', 'gensim': '4.4.0', 'python': '3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]', 'platform': 'Linux-5.4.0-200-generic-x86_64-with-glibc2.31', 'event': 'created'}
2025-12-21 10:27:00,159 INFO sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda:0
2025-12-21 10:27:00,159 INFO sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: all-roberta-large-v1
2025-12-21 10:27:02,265 INFO src.utils.bertopic_utils: Fitting BERTopic model HDBSCAN on 10788 docs
2025-12-21 10:28:57,416 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 10:29:00,959 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-21 10:29:00,970 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-21 10:29:00,974 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-21 10:29:00,991 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-21 10:29:01,069 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-21 10:29:01,217 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-21 10:29:01,346 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-21 10:29:03,061 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-21 10:29:03,302 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-21 10:29:03,648 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-21 10:29:03,654 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-21 10:29:04,025 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-21 10:29:04,913 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-21 10:29:05,254 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-21 10:29:05,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,563 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,564 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,567 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,568 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,569 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,570 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,575 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,576 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,577 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,578 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,579 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,580 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,581 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,583 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,599 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,600 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,635 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,639 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,701 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,761 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:05,870 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:05,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,014 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,023 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,111 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,216 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,237 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,251 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,343 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,370 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,342 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,499 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,539 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,593 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,611 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,631 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,679 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,698 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,762 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,771 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,864 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:06,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:06,959 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,137 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,247 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,386 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,392 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,414 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,449 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,550 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,554 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,606 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,700 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,783 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,860 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,862 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,889 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,897 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,913 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,950 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:07,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:07,846 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,047 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,048 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,079 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,092 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,097 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,131 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,122 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,283 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,306 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,322 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,417 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,435 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,477 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,494 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,538 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,551 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,571 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,619 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,662 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,697 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,786 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,791 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,853 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:08,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:08,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,113 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,319 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,329 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,343 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,367 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,371 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,375 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,412 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,427 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,456 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,475 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,474 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,575 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,665 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:09,849 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:09,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:10,032 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:10,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:10,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:10,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:10,986 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:11,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:11,083 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:11,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:11,125 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:11,154 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:11,242 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:11,243 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:11,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:11,257 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:11,258 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:11,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:11,291 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:11,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:13,658 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 10:29:14,621 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-21 10:29:15,601 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 10:29:20,042 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-21 10:29:20,045 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-21 10:29:20,046 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-21 10:29:20,047 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-21 10:29:20,048 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-21 10:29:20,049 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-21 10:29:20,051 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-21 10:29:20,051 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-21 10:29:20,053 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-21 10:29:20,054 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-21 10:29:20,055 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-21 10:29:20,057 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-21 10:29:20,058 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-21 10:29:20,059 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-21 10:29:20,060 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-21 10:29:20,061 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-21 10:29:20,062 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-21 10:29:20,063 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-21 10:29:20,064 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-21 10:29:20,065 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-21 10:29:20,066 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-21 10:29:20,067 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-21 10:29:20,069 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-21 10:29:20,070 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-21 10:29:20,071 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-21 10:29:20,072 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-21 10:29:20,073 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-21 10:29:20,074 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-21 10:29:20,075 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-21 10:29:20,076 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-21 10:29:20,077 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-21 10:29:20,078 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-21 10:29:20,079 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-21 10:29:20,080 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-21 10:29:20,081 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-21 10:29:20,082 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-21 10:29:20,083 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-21 10:29:20,084 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-21 10:29:20,085 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-21 10:29:20,086 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-21 10:29:20,087 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-21 10:29:20,088 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-21 10:29:20,089 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-21 10:29:20,090 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-21 10:29:20,091 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-21 10:29:20,092 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-21 10:29:20,093 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-21 10:29:20,094 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-21 10:29:20,095 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-21 10:29:20,096 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-21 10:29:20,097 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-21 10:29:20,098 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-21 10:29:20,100 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-21 10:29:20,101 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-21 10:29:20,102 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-21 10:29:20,103 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-21 10:29:20,104 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-21 10:29:20,105 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-21 10:29:20,106 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-21 10:29:20,108 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-21 10:29:20,109 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-21 10:29:20,110 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-21 10:29:20,111 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-21 10:29:20,112 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-21 10:29:20,113 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-21 10:29:20,114 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-21 10:29:20,115 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-21 10:29:20,116 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-21 10:29:20,117 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-21 10:29:20,118 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-21 10:29:20,119 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-21 10:29:20,120 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-21 10:29:20,121 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-21 10:29:20,122 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-21 10:29:20,123 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-21 10:29:20,124 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-21 10:29:20,125 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-21 10:29:20,126 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-21 10:29:20,129 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-21 10:29:20,131 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-21 10:29:20,133 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-21 10:29:20,134 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-21 10:29:20,136 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-21 10:29:20,137 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-21 10:29:20,139 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-21 10:29:20,141 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-21 10:29:20,142 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-21 10:29:20,144 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-21 10:29:20,146 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-21 10:29:20,148 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-21 10:29:20,150 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-21 10:29:20,152 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-21 10:29:20,154 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-21 10:29:20,156 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-21 10:29:20,157 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-21 10:29:20,159 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-21 10:29:20,161 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-21 10:29:20,162 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-21 10:29:20,165 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-21 10:29:20,166 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-21 10:29:20,252 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-21 10:29:20,254 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-21 10:29:20,256 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-21 10:29:20,289 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-21 10:29:20,308 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-21 10:29:20,344 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-21 10:29:20,468 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-21 10:29:20,470 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-21 10:29:20,508 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-21 10:29:20,510 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-21 10:29:20,512 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-21 10:29:20,712 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-21 10:29:20,714 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-21 10:29:20,716 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-21 10:29:20,728 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-21 10:29:20,736 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-21 10:29:20,788 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-21 10:29:20,796 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-21 10:29:20,852 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-21 10:29:20,949 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-21 10:29:20,951 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-21 10:29:20,960 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-21 10:29:21,032 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-21 10:29:21,044 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-21 10:29:21,112 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-21 10:29:21,136 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-21 10:29:21,281 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-21 10:29:21,283 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-21 10:29:21,296 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-21 10:29:21,497 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-21 10:29:21,499 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-21 10:29:21,508 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-21 10:29:21,549 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-21 10:29:21,844 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-21 10:29:21,847 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-21 10:29:21,848 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-21 10:29:21,884 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-21 10:29:22,133 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-21 10:29:22,136 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-21 10:29:22,138 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-21 10:29:22,333 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-21 10:29:22,335 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-21 10:29:22,337 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-21 10:29:22,502 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-21 10:29:22,504 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-21 10:29:22,506 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-21 10:29:22,655 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-21 10:29:22,658 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-21 10:29:22,660 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-21 10:29:22,854 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-21 10:29:22,857 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-21 10:29:22,859 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-21 10:29:23,002 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-21 10:29:23,009 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-21 10:29:23,134 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-21 10:29:23,214 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-21 10:29:23,217 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-21 10:29:23,220 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-21 10:29:23,386 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-21 10:29:23,397 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-21 10:29:23,525 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-21 10:29:23,532 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-21 10:29:23,634 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-21 10:29:23,648 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-21 10:29:23,654 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-21 10:29:23,809 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-21 10:29:23,812 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-21 10:29:23,818 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-21 10:29:23,934 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-21 10:29:23,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,936 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,938 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,938 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,939 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,940 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,942 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,943 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,944 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,944 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,944 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,944 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,945 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,951 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,951 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,952 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,953 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,955 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,958 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,965 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,967 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,979 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:23,983 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,007 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,011 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,019 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,023 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,027 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,031 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,039 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,047 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,051 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,063 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,068 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,068 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,093 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,094 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,149 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,174 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,195 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,196 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,222 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,236 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,246 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,251 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,252 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,252 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,142 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,279 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,335 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,337 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,347 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,363 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,395 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,397 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,398 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,404 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,425 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,426 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,437 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,438 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,464 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,472 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,490 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,495 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,499 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,503 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,535 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,539 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,549 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,591 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,592 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,594 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,618 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,722 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,702 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,847 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,850 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,855 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,954 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,954 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,971 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,974 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,975 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:24,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,988 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,005 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:24,886 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,070 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,196 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,214 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,259 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,264 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,304 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,331 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,342 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,355 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,456 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,558 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,588 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,653 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,671 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,688 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,692 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,792 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,843 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,937 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:25,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:25,977 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,035 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,043 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,174 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,197 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,203 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,294 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,307 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,399 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,502 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,446 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,563 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,595 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,607 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,707 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,760 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:26,833 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:26,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:27,007 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:27,002 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:27,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:27,038 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:27,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:27,141 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:29:27,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:27,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:29:30,752 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 10:29:31,047 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-21 10:29:31,735 INFO src.utils.bertopic_utils: Fitting BERTopic model KMeans on 10788 docs
2025-12-21 10:31:07,330 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 10:31:11,611 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-21 10:31:11,622 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-21 10:31:11,627 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-21 10:31:11,645 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-21 10:31:11,965 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-21 10:31:12,033 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-21 10:31:12,196 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-21 10:31:13,166 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-21 10:31:13,320 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-21 10:31:13,560 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-21 10:31:13,614 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-21 10:31:13,708 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-21 10:31:14,176 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-21 10:31:14,376 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-21 10:31:14,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,607 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,608 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,610 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,611 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,612 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,613 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,614 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,615 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,616 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,617 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,618 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,619 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,620 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,621 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,622 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,624 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,625 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,626 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,627 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,628 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,630 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,631 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,632 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,634 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,635 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,636 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,645 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,646 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,647 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,651 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,655 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,675 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,687 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,689 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,691 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,694 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,695 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,699 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,703 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,709 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,709 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,711 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,714 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,723 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,729 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,731 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,742 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,743 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,747 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,755 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,759 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,767 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,775 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,778 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,784 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,829 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:14,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:14,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,015 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,067 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,082 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,099 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,110 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,146 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,164 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,166 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,180 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,219 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,231 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,278 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,280 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,288 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,293 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,299 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,319 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,382 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,384 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,310 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,451 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,471 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,354 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,547 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,558 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,570 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,578 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,603 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,629 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,633 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,636 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,650 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,683 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,735 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,590 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,751 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,763 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,793 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:15,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:15,987 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,019 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,021 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,071 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,353 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,409 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,461 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,462 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,533 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,559 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,643 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:16,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:16,998 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:17,015 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:17,059 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:17,081 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:17,082 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:17,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:17,177 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:17,223 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:17,348 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:17,368 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:17,370 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:17,380 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:17,407 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:17,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:20,889 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 10:31:21,049 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-21 10:31:21,556 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 10:31:26,161 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-21 10:31:26,164 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-21 10:31:26,165 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-21 10:31:26,166 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-21 10:31:26,167 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-21 10:31:26,168 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-21 10:31:26,169 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-21 10:31:26,170 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-21 10:31:26,172 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-21 10:31:26,173 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-21 10:31:26,174 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-21 10:31:26,176 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-21 10:31:26,177 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-21 10:31:26,178 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-21 10:31:26,178 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-21 10:31:26,179 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-21 10:31:26,180 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-21 10:31:26,181 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-21 10:31:26,182 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-21 10:31:26,184 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-21 10:31:26,185 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-21 10:31:26,185 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-21 10:31:26,187 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-21 10:31:26,188 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-21 10:31:26,189 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-21 10:31:26,190 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-21 10:31:26,191 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-21 10:31:26,192 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-21 10:31:26,193 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-21 10:31:26,194 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-21 10:31:26,195 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-21 10:31:26,196 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-21 10:31:26,197 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-21 10:31:26,198 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-21 10:31:26,199 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-21 10:31:26,200 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-21 10:31:26,201 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-21 10:31:26,202 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-21 10:31:26,203 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-21 10:31:26,204 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-21 10:31:26,205 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-21 10:31:26,206 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-21 10:31:26,207 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-21 10:31:26,208 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-21 10:31:26,209 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-21 10:31:26,210 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-21 10:31:26,211 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-21 10:31:26,212 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-21 10:31:26,213 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-21 10:31:26,215 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-21 10:31:26,217 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-21 10:31:26,219 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-21 10:31:26,221 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-21 10:31:26,222 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-21 10:31:26,225 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-21 10:31:26,226 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-21 10:31:26,228 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-21 10:31:26,230 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-21 10:31:26,232 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-21 10:31:26,234 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-21 10:31:26,235 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-21 10:31:26,237 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-21 10:31:26,238 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-21 10:31:26,240 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-21 10:31:26,241 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-21 10:31:26,243 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-21 10:31:26,245 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-21 10:31:26,246 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-21 10:31:26,248 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-21 10:31:26,249 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-21 10:31:26,251 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-21 10:31:26,253 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-21 10:31:26,254 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-21 10:31:26,256 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-21 10:31:26,258 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-21 10:31:26,259 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-21 10:31:26,261 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-21 10:31:26,452 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-21 10:31:26,454 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-21 10:31:26,456 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-21 10:31:26,464 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-21 10:31:26,465 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-21 10:31:26,487 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-21 10:31:26,489 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-21 10:31:26,496 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-21 10:31:26,516 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-21 10:31:26,524 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-21 10:31:26,532 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-21 10:31:26,548 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-21 10:31:26,560 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-21 10:31:26,572 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-21 10:31:26,616 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-21 10:31:26,618 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-21 10:31:26,620 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-21 10:31:26,621 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-21 10:31:26,768 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-21 10:31:26,770 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-21 10:31:26,772 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-21 10:31:26,893 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-21 10:31:26,895 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-21 10:31:26,900 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-21 10:31:26,924 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-21 10:31:26,952 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-21 10:31:27,073 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-21 10:31:27,075 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-21 10:31:27,076 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-21 10:31:27,288 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-21 10:31:27,290 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-21 10:31:27,292 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-21 10:31:27,352 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-21 10:31:27,548 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-21 10:31:27,550 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-21 10:31:27,551 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-21 10:31:27,576 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-21 10:31:27,624 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-21 10:31:27,680 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-21 10:31:27,870 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-21 10:31:27,884 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-21 10:31:27,900 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-21 10:31:28,050 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-21 10:31:28,053 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-21 10:31:28,058 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-21 10:31:28,195 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-21 10:31:28,197 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-21 10:31:28,199 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-21 10:31:28,302 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-21 10:31:28,305 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-21 10:31:28,307 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-21 10:31:28,418 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-21 10:31:28,421 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-21 10:31:28,423 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-21 10:31:28,537 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-21 10:31:28,598 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-21 10:31:28,601 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-21 10:31:28,603 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-21 10:31:28,690 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-21 10:31:28,704 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-21 10:31:28,766 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-21 10:31:28,780 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-21 10:31:28,857 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-21 10:31:28,860 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-21 10:31:28,862 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-21 10:31:28,934 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-21 10:31:28,948 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-21 10:31:28,964 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-21 10:31:29,066 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-21 10:31:29,080 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-21 10:31:29,082 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-21 10:31:29,174 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-21 10:31:29,189 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-21 10:31:29,264 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-21 10:31:29,276 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-21 10:31:29,322 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-21 10:31:29,336 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-21 10:31:29,342 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-21 10:31:29,438 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-21 10:31:29,440 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-21 10:31:29,443 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-21 10:31:29,490 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-21 10:31:29,504 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-21 10:31:29,510 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-21 10:31:29,567 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-21 10:31:29,569 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-21 10:31:29,571 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-21 10:31:29,613 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-21 10:31:29,645 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-21 10:31:29,648 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-21 10:31:29,710 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-21 10:31:29,724 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-21 10:31:29,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,731 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,732 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,733 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,734 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,735 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,736 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,737 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,738 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,739 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,740 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,741 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,742 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,743 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,744 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,745 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,746 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,747 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,748 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,749 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,750 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,751 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,752 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,752 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,752 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,752 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,753 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,754 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,755 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,755 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,756 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,757 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,759 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,763 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,764 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,765 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,771 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,775 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,776 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,779 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,783 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,787 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,791 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,795 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,799 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,801 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,803 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,806 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,811 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,815 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,819 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,821 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,823 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,826 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,827 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,842 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,845 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,850 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,850 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,857 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,862 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,865 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,870 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,885 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,886 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,913 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,913 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:29,982 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:29,991 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,017 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,079 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,138 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,194 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,225 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,250 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,261 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,273 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,275 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,334 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,339 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,364 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,378 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,383 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,467 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,543 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,604 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,623 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,647 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,656 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,719 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,727 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,730 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,730 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,826 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,875 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,879 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,888 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:30,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,983 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:30,987 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,157 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,205 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,215 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,267 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,318 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,334 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,350 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,366 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,385 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,439 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,471 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,487 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,494 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,601 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,668 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,737 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,730 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,850 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,872 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,876 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,905 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,941 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:31,963 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:31,979 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:32,100 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:31:32,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:31:35,177 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 10:31:35,305 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-21 10:31:35,741 INFO src.utils.bertopic_utils: Fitting BERTopic model BERTopicCobwebWrapper on 10788 docs
Training CobwebTree:   0%|          | 0/10788 [00:00<?, ?it/s]Training CobwebTree:   0%|          | 18/10788 [00:00<01:00, 177.69it/s]Training CobwebTree:   0%|          | 36/10788 [00:00<01:23, 128.97it/s]Training CobwebTree:   0%|          | 50/10788 [00:00<01:42, 104.78it/s]Training CobwebTree:   1%|          | 62/10788 [00:00<01:48, 99.30it/s] Training CobwebTree:   1%|          | 73/10788 [00:00<01:54, 93.73it/s]Training CobwebTree:   1%|          | 83/10788 [00:00<02:04, 85.74it/s]Training CobwebTree:   1%|          | 92/10788 [00:00<02:14, 79.59it/s]Training CobwebTree:   1%|          | 101/10788 [00:01<02:18, 77.42it/s]Training CobwebTree:   1%|          | 109/10788 [00:01<02:21, 75.31it/s]Training CobwebTree:   1%|          | 117/10788 [00:01<02:25, 73.18it/s]Training CobwebTree:   1%|          | 125/10788 [00:01<02:41, 65.94it/s]Training CobwebTree:   1%|          | 132/10788 [00:01<02:40, 66.34it/s]Training CobwebTree:   1%|         | 139/10788 [00:01<02:45, 64.21it/s]Training CobwebTree:   1%|         | 146/10788 [00:01<02:51, 62.08it/s]Training CobwebTree:   1%|         | 153/10788 [00:01<02:57, 60.03it/s]Training CobwebTree:   1%|         | 160/10788 [00:02<03:04, 57.65it/s]Training CobwebTree:   2%|         | 166/10788 [00:02<03:15, 54.37it/s]Training CobwebTree:   2%|         | 173/10788 [00:02<03:13, 54.97it/s]Training CobwebTree:   2%|         | 179/10788 [00:02<03:16, 53.86it/s]Training CobwebTree:   2%|         | 185/10788 [00:02<03:13, 54.69it/s]Training CobwebTree:   2%|         | 191/10788 [00:02<03:15, 54.26it/s]Training CobwebTree:   2%|         | 197/10788 [00:02<03:17, 53.56it/s]Training CobwebTree:   2%|         | 203/10788 [00:02<03:18, 53.41it/s]Training CobwebTree:   2%|         | 209/10788 [00:03<03:19, 53.01it/s]Training CobwebTree:   2%|         | 216/10788 [00:03<03:09, 55.82it/s]Training CobwebTree:   2%|         | 222/10788 [00:03<03:25, 51.33it/s]Training CobwebTree:   2%|         | 228/10788 [00:03<03:19, 53.03it/s]Training CobwebTree:   2%|         | 234/10788 [00:03<03:17, 53.34it/s]Training CobwebTree:   2%|         | 240/10788 [00:03<03:25, 51.37it/s]Training CobwebTree:   2%|         | 246/10788 [00:03<03:16, 53.56it/s]Training CobwebTree:   2%|         | 252/10788 [00:03<03:23, 51.67it/s]Training CobwebTree:   2%|         | 258/10788 [00:03<03:29, 50.37it/s]Training CobwebTree:   2%|         | 264/10788 [00:04<03:37, 48.49it/s]Training CobwebTree:   2%|         | 269/10788 [00:04<03:38, 48.20it/s]Training CobwebTree:   3%|         | 275/10788 [00:04<03:29, 50.28it/s]Training CobwebTree:   3%|         | 281/10788 [00:04<03:28, 50.34it/s]Training CobwebTree:   3%|         | 287/10788 [00:04<03:26, 50.79it/s]Training CobwebTree:   3%|         | 293/10788 [00:04<03:19, 52.67it/s]Training CobwebTree:   3%|         | 299/10788 [00:04<03:39, 47.87it/s]Training CobwebTree:   3%|         | 304/10788 [00:04<03:39, 47.84it/s]Training CobwebTree:   3%|         | 310/10788 [00:05<03:31, 49.46it/s]Training CobwebTree:   3%|         | 316/10788 [00:05<03:37, 48.25it/s]Training CobwebTree:   3%|         | 322/10788 [00:05<03:24, 51.06it/s]Training CobwebTree:   3%|         | 328/10788 [00:05<03:27, 50.30it/s]Training CobwebTree:   3%|         | 334/10788 [00:05<03:33, 48.89it/s]Training CobwebTree:   3%|         | 340/10788 [00:05<03:31, 49.38it/s]Training CobwebTree:   3%|         | 345/10788 [00:05<03:48, 45.69it/s]Training CobwebTree:   3%|         | 350/10788 [00:05<03:48, 45.61it/s]Training CobwebTree:   3%|         | 355/10788 [00:05<03:53, 44.65it/s]Training CobwebTree:   3%|         | 360/10788 [00:06<03:53, 44.68it/s]Training CobwebTree:   3%|         | 366/10788 [00:06<03:37, 47.86it/s]Training CobwebTree:   3%|         | 373/10788 [00:06<03:18, 52.52it/s]Training CobwebTree:   4%|         | 379/10788 [00:06<03:13, 53.84it/s]Training CobwebTree:   4%|         | 385/10788 [00:06<03:19, 52.25it/s]Training CobwebTree:   4%|         | 391/10788 [00:06<03:20, 51.80it/s]Training CobwebTree:   4%|         | 397/10788 [00:06<03:24, 50.89it/s]Training CobwebTree:   4%|         | 403/10788 [00:06<03:35, 48.09it/s]Training CobwebTree:   4%|         | 408/10788 [00:07<03:38, 47.40it/s]Training CobwebTree:   4%|         | 413/10788 [00:07<03:36, 47.88it/s]Training CobwebTree:   4%|         | 419/10788 [00:07<03:32, 48.76it/s]Training CobwebTree:   4%|         | 424/10788 [00:07<03:40, 46.92it/s]Training CobwebTree:   4%|         | 429/10788 [00:07<03:39, 47.26it/s]Training CobwebTree:   4%|         | 435/10788 [00:07<03:36, 47.72it/s]Training CobwebTree:   4%|         | 440/10788 [00:07<03:43, 46.35it/s]Training CobwebTree:   4%|         | 446/10788 [00:07<03:35, 47.90it/s]Training CobwebTree:   4%|         | 451/10788 [00:07<03:37, 47.52it/s]Training CobwebTree:   4%|         | 457/10788 [00:08<03:30, 49.17it/s]Training CobwebTree:   4%|         | 462/10788 [00:08<03:37, 47.48it/s]Training CobwebTree:   4%|         | 467/10788 [00:08<03:47, 45.41it/s]Training CobwebTree:   4%|         | 472/10788 [00:08<03:43, 46.21it/s]Training CobwebTree:   4%|         | 477/10788 [00:08<03:41, 46.46it/s]Training CobwebTree:   4%|         | 482/10788 [00:08<03:46, 45.45it/s]Training CobwebTree:   5%|         | 487/10788 [00:08<03:47, 45.21it/s]Training CobwebTree:   5%|         | 492/10788 [00:08<03:51, 44.50it/s]Training CobwebTree:   5%|         | 497/10788 [00:08<03:56, 43.46it/s]Training CobwebTree:   5%|         | 502/10788 [00:09<03:55, 43.74it/s]Training CobwebTree:   5%|         | 507/10788 [00:09<04:00, 42.82it/s]Training CobwebTree:   5%|         | 512/10788 [00:09<03:55, 43.59it/s]Training CobwebTree:   5%|         | 517/10788 [00:09<03:53, 44.02it/s]Training CobwebTree:   5%|         | 522/10788 [00:09<03:54, 43.79it/s]Training CobwebTree:   5%|         | 527/10788 [00:09<04:03, 42.12it/s]Training CobwebTree:   5%|         | 532/10788 [00:09<04:00, 42.71it/s]Training CobwebTree:   5%|         | 537/10788 [00:09<04:04, 41.93it/s]Training CobwebTree:   5%|         | 542/10788 [00:10<03:58, 42.91it/s]Training CobwebTree:   5%|         | 547/10788 [00:10<03:56, 43.34it/s]Training CobwebTree:   5%|         | 552/10788 [00:10<03:54, 43.74it/s]Training CobwebTree:   5%|         | 557/10788 [00:10<03:50, 44.33it/s]Training CobwebTree:   5%|         | 562/10788 [00:10<03:52, 43.98it/s]Training CobwebTree:   5%|         | 567/10788 [00:10<04:04, 41.89it/s]Training CobwebTree:   5%|         | 572/10788 [00:10<04:27, 38.17it/s]Training CobwebTree:   5%|         | 576/10788 [00:10<04:34, 37.19it/s]Training CobwebTree:   5%|         | 581/10788 [00:11<04:29, 37.89it/s]Training CobwebTree:   5%|         | 586/10788 [00:11<04:25, 38.45it/s]Training CobwebTree:   5%|         | 591/10788 [00:11<04:20, 39.21it/s]Training CobwebTree:   6%|         | 596/10788 [00:11<04:10, 40.66it/s]Training CobwebTree:   6%|         | 601/10788 [00:11<04:12, 40.34it/s]Training CobwebTree:   6%|         | 606/10788 [00:11<04:16, 39.75it/s]Training CobwebTree:   6%|         | 610/10788 [00:11<04:18, 39.30it/s]Training CobwebTree:   6%|         | 614/10788 [00:11<04:26, 38.18it/s]Training CobwebTree:   6%|         | 618/10788 [00:11<04:27, 38.03it/s]Training CobwebTree:   6%|         | 624/10788 [00:12<04:00, 42.30it/s]Training CobwebTree:   6%|         | 630/10788 [00:12<03:45, 45.04it/s]Training CobwebTree:   6%|         | 636/10788 [00:12<03:36, 46.99it/s]Training CobwebTree:   6%|         | 641/10788 [00:12<03:38, 46.45it/s]Training CobwebTree:   6%|         | 646/10788 [00:12<03:35, 47.17it/s]Training CobwebTree:   6%|         | 651/10788 [00:12<03:37, 46.69it/s]Training CobwebTree:   6%|         | 657/10788 [00:12<03:32, 47.75it/s]Training CobwebTree:   6%|         | 662/10788 [00:12<03:35, 46.99it/s]Training CobwebTree:   6%|         | 668/10788 [00:12<03:29, 48.34it/s]Training CobwebTree:   6%|         | 673/10788 [00:13<03:39, 46.16it/s]Training CobwebTree:   6%|         | 678/10788 [00:13<03:42, 45.34it/s]Training CobwebTree:   6%|         | 683/10788 [00:13<03:39, 45.99it/s]Training CobwebTree:   6%|         | 688/10788 [00:13<03:37, 46.36it/s]Training CobwebTree:   6%|         | 693/10788 [00:13<03:53, 43.19it/s]Training CobwebTree:   6%|         | 698/10788 [00:13<03:54, 42.99it/s]Training CobwebTree:   7%|         | 703/10788 [00:13<03:54, 42.99it/s]Training CobwebTree:   7%|         | 708/10788 [00:13<03:58, 42.29it/s]Training CobwebTree:   7%|         | 713/10788 [00:14<03:56, 42.57it/s]Training CobwebTree:   7%|         | 718/10788 [00:14<03:53, 43.08it/s]Training CobwebTree:   7%|         | 723/10788 [00:14<03:57, 42.33it/s]Training CobwebTree:   7%|         | 729/10788 [00:14<03:45, 44.51it/s]Training CobwebTree:   7%|         | 734/10788 [00:14<03:56, 42.45it/s]Training CobwebTree:   7%|         | 739/10788 [00:14<03:53, 43.11it/s]Training CobwebTree:   7%|         | 744/10788 [00:14<03:55, 42.63it/s]Training CobwebTree:   7%|         | 749/10788 [00:14<03:50, 43.56it/s]Training CobwebTree:   7%|         | 754/10788 [00:14<03:54, 42.83it/s]Training CobwebTree:   7%|         | 759/10788 [00:15<04:06, 40.73it/s]Training CobwebTree:   7%|         | 764/10788 [00:15<03:52, 43.03it/s]Training CobwebTree:   7%|         | 769/10788 [00:15<03:50, 43.52it/s]Training CobwebTree:   7%|         | 774/10788 [00:15<03:56, 42.38it/s]Training CobwebTree:   7%|         | 779/10788 [00:15<03:51, 43.22it/s]Training CobwebTree:   7%|         | 784/10788 [00:15<03:51, 43.30it/s]Training CobwebTree:   7%|         | 789/10788 [00:15<03:51, 43.18it/s]Training CobwebTree:   7%|         | 794/10788 [00:15<03:50, 43.37it/s]Training CobwebTree:   7%|         | 799/10788 [00:16<03:56, 42.16it/s]Training CobwebTree:   7%|         | 804/10788 [00:16<03:49, 43.50it/s]Training CobwebTree:   7%|         | 809/10788 [00:16<03:47, 43.81it/s]Training CobwebTree:   8%|         | 814/10788 [00:16<03:58, 41.79it/s]Training CobwebTree:   8%|         | 819/10788 [00:16<04:02, 41.17it/s]Training CobwebTree:   8%|         | 824/10788 [00:16<03:54, 42.55it/s]Training CobwebTree:   8%|         | 829/10788 [00:16<03:46, 43.88it/s]Training CobwebTree:   8%|         | 834/10788 [00:16<03:55, 42.19it/s]Training CobwebTree:   8%|         | 840/10788 [00:16<03:39, 45.39it/s]Training CobwebTree:   8%|         | 845/10788 [00:17<03:42, 44.59it/s]Training CobwebTree:   8%|         | 850/10788 [00:17<03:47, 43.69it/s]Training CobwebTree:   8%|         | 855/10788 [00:17<03:54, 42.42it/s]Training CobwebTree:   8%|         | 860/10788 [00:17<03:51, 42.90it/s]Training CobwebTree:   8%|         | 865/10788 [00:17<03:53, 42.47it/s]Training CobwebTree:   8%|         | 870/10788 [00:17<03:53, 42.49it/s]Training CobwebTree:   8%|         | 875/10788 [00:17<04:00, 41.15it/s]Training CobwebTree:   8%|         | 880/10788 [00:17<04:02, 40.89it/s]Training CobwebTree:   8%|         | 885/10788 [00:18<04:03, 40.59it/s]Training CobwebTree:   8%|         | 891/10788 [00:18<03:47, 43.41it/s]Training CobwebTree:   8%|         | 896/10788 [00:18<03:48, 43.24it/s]Training CobwebTree:   8%|         | 901/10788 [00:18<03:49, 43.03it/s]Training CobwebTree:   8%|         | 906/10788 [00:18<03:51, 42.70it/s]Training CobwebTree:   8%|         | 911/10788 [00:18<04:00, 41.02it/s]Training CobwebTree:   8%|         | 916/10788 [00:18<03:59, 41.29it/s]Training CobwebTree:   9%|         | 921/10788 [00:18<03:55, 41.82it/s]Training CobwebTree:   9%|         | 926/10788 [00:19<03:56, 41.69it/s]Training CobwebTree:   9%|         | 931/10788 [00:19<03:53, 42.15it/s]Training CobwebTree:   9%|         | 936/10788 [00:19<04:06, 40.01it/s]Training CobwebTree:   9%|         | 942/10788 [00:19<03:48, 43.01it/s]Training CobwebTree:   9%|         | 947/10788 [00:19<03:51, 42.42it/s]Training CobwebTree:   9%|         | 952/10788 [00:19<04:00, 40.89it/s]Training CobwebTree:   9%|         | 957/10788 [00:19<03:59, 41.07it/s]Training CobwebTree:   9%|         | 962/10788 [00:19<04:03, 40.36it/s]Training CobwebTree:   9%|         | 967/10788 [00:20<03:59, 40.94it/s]Training CobwebTree:   9%|         | 972/10788 [00:20<03:59, 41.04it/s]Training CobwebTree:   9%|         | 977/10788 [00:20<03:59, 40.90it/s]Training CobwebTree:   9%|         | 982/10788 [00:20<04:02, 40.49it/s]Training CobwebTree:   9%|         | 987/10788 [00:20<04:00, 40.80it/s]Training CobwebTree:   9%|         | 992/10788 [00:20<03:55, 41.54it/s]Training CobwebTree:   9%|         | 997/10788 [00:20<04:09, 39.25it/s]Training CobwebTree:   9%|         | 1001/10788 [00:20<04:15, 38.33it/s]Training CobwebTree:   9%|         | 1005/10788 [00:20<04:16, 38.16it/s]Training CobwebTree:   9%|         | 1009/10788 [00:21<04:20, 37.53it/s]Training CobwebTree:   9%|         | 1014/10788 [00:21<04:02, 40.28it/s]Training CobwebTree:   9%|         | 1019/10788 [00:21<04:05, 39.79it/s]Training CobwebTree:   9%|         | 1024/10788 [00:21<03:50, 42.32it/s]Training CobwebTree:  10%|         | 1029/10788 [00:21<03:54, 41.57it/s]Training CobwebTree:  10%|         | 1034/10788 [00:21<04:06, 39.50it/s]Training CobwebTree:  10%|         | 1039/10788 [00:21<03:54, 41.58it/s]Training CobwebTree:  10%|         | 1044/10788 [00:21<03:51, 42.07it/s]Training CobwebTree:  10%|         | 1049/10788 [00:22<03:58, 40.81it/s]Training CobwebTree:  10%|         | 1054/10788 [00:22<04:02, 40.17it/s]Training CobwebTree:  10%|         | 1059/10788 [00:22<03:59, 40.61it/s]Training CobwebTree:  10%|         | 1064/10788 [00:22<04:01, 40.33it/s]Training CobwebTree:  10%|         | 1069/10788 [00:22<04:12, 38.54it/s]Training CobwebTree:  10%|         | 1073/10788 [00:22<04:18, 37.56it/s]Training CobwebTree:  10%|         | 1077/10788 [00:22<04:27, 36.25it/s]Training CobwebTree:  10%|         | 1081/10788 [00:22<04:21, 37.18it/s]Training CobwebTree:  10%|         | 1085/10788 [00:23<04:20, 37.27it/s]Training CobwebTree:  10%|         | 1090/10788 [00:23<04:06, 39.27it/s]Training CobwebTree:  10%|         | 1095/10788 [00:23<04:00, 40.27it/s]Training CobwebTree:  10%|         | 1100/10788 [00:23<03:59, 40.42it/s]Training CobwebTree:  10%|         | 1105/10788 [00:23<03:56, 40.87it/s]Training CobwebTree:  10%|         | 1110/10788 [00:23<03:49, 42.25it/s]Training CobwebTree:  10%|         | 1115/10788 [00:23<03:54, 41.21it/s]Training CobwebTree:  10%|         | 1120/10788 [00:23<03:56, 40.81it/s]Training CobwebTree:  10%|         | 1125/10788 [00:23<03:58, 40.49it/s]Training CobwebTree:  10%|         | 1130/10788 [00:24<04:02, 39.80it/s]Training CobwebTree:  11%|         | 1135/10788 [00:24<04:01, 39.97it/s]Training CobwebTree:  11%|         | 1140/10788 [00:24<04:14, 37.96it/s]Training CobwebTree:  11%|         | 1144/10788 [00:24<04:11, 38.41it/s]Training CobwebTree:  11%|         | 1149/10788 [00:24<04:04, 39.35it/s]Training CobwebTree:  11%|         | 1153/10788 [00:24<04:05, 39.23it/s]Training CobwebTree:  11%|         | 1158/10788 [00:24<04:00, 40.11it/s]Training CobwebTree:  11%|         | 1163/10788 [00:24<03:55, 40.86it/s]Training CobwebTree:  11%|         | 1168/10788 [00:25<03:52, 41.30it/s]Training CobwebTree:  11%|         | 1173/10788 [00:25<03:49, 41.85it/s]Training CobwebTree:  11%|         | 1178/10788 [00:25<03:40, 43.61it/s]Training CobwebTree:  11%|         | 1183/10788 [00:25<03:44, 42.84it/s]Training CobwebTree:  11%|         | 1188/10788 [00:25<03:51, 41.47it/s]Training CobwebTree:  11%|         | 1193/10788 [00:25<04:11, 38.10it/s]Training CobwebTree:  11%|         | 1197/10788 [00:25<04:08, 38.56it/s]Training CobwebTree:  11%|         | 1201/10788 [00:25<04:07, 38.70it/s]Training CobwebTree:  11%|         | 1206/10788 [00:25<03:58, 40.22it/s]Training CobwebTree:  11%|         | 1211/10788 [00:26<03:57, 40.33it/s]Training CobwebTree:  11%|        | 1216/10788 [00:26<03:51, 41.40it/s]Training CobwebTree:  11%|        | 1221/10788 [00:26<03:49, 41.70it/s]Training CobwebTree:  11%|        | 1226/10788 [00:26<03:44, 42.59it/s]Training CobwebTree:  11%|        | 1231/10788 [00:26<03:43, 42.79it/s]Training CobwebTree:  11%|        | 1236/10788 [00:26<03:41, 43.05it/s]Training CobwebTree:  12%|        | 1241/10788 [00:26<03:48, 41.73it/s]Training CobwebTree:  12%|        | 1246/10788 [00:26<03:45, 42.22it/s]Training CobwebTree:  12%|        | 1251/10788 [00:27<03:51, 41.28it/s]Training CobwebTree:  12%|        | 1256/10788 [00:27<03:57, 40.18it/s]Training CobwebTree:  12%|        | 1261/10788 [00:27<03:59, 39.75it/s]Training CobwebTree:  12%|        | 1265/10788 [00:27<04:11, 37.81it/s]Training CobwebTree:  12%|        | 1270/10788 [00:27<04:00, 39.63it/s]Training CobwebTree:  12%|        | 1275/10788 [00:27<03:58, 39.83it/s]Training CobwebTree:  12%|        | 1280/10788 [00:27<03:58, 39.90it/s]Training CobwebTree:  12%|        | 1285/10788 [00:27<03:54, 40.58it/s]Training CobwebTree:  12%|        | 1290/10788 [00:28<03:49, 41.40it/s]Training CobwebTree:  12%|        | 1295/10788 [00:28<03:53, 40.73it/s]Training CobwebTree:  12%|        | 1300/10788 [00:28<03:55, 40.22it/s]Training CobwebTree:  12%|        | 1305/10788 [00:28<03:54, 40.49it/s]Training CobwebTree:  12%|        | 1310/10788 [00:28<03:50, 41.12it/s]Training CobwebTree:  12%|        | 1315/10788 [00:28<03:56, 40.02it/s]Training CobwebTree:  12%|        | 1320/10788 [00:28<03:56, 40.11it/s]Training CobwebTree:  12%|        | 1325/10788 [00:28<03:59, 39.44it/s]Training CobwebTree:  12%|        | 1329/10788 [00:29<04:00, 39.28it/s]Training CobwebTree:  12%|        | 1334/10788 [00:29<03:50, 41.00it/s]Training CobwebTree:  12%|        | 1339/10788 [00:29<03:58, 39.65it/s]Training CobwebTree:  12%|        | 1343/10788 [00:29<04:00, 39.24it/s]Training CobwebTree:  12%|        | 1348/10788 [00:29<03:56, 39.93it/s]Training CobwebTree:  13%|        | 1352/10788 [00:29<03:57, 39.68it/s]Training CobwebTree:  13%|        | 1357/10788 [00:29<03:56, 39.86it/s]Training CobwebTree:  13%|        | 1362/10788 [00:29<03:48, 41.32it/s]Training CobwebTree:  13%|        | 1367/10788 [00:29<04:05, 38.34it/s]Training CobwebTree:  13%|        | 1372/10788 [00:30<04:06, 38.24it/s]Training CobwebTree:  13%|        | 1377/10788 [00:30<03:53, 40.22it/s]Training CobwebTree:  13%|        | 1382/10788 [00:30<03:59, 39.24it/s]Training CobwebTree:  13%|        | 1386/10788 [00:30<04:08, 37.90it/s]Training CobwebTree:  13%|        | 1390/10788 [00:30<04:06, 38.19it/s]Training CobwebTree:  13%|        | 1394/10788 [00:30<04:15, 36.83it/s]Training CobwebTree:  13%|        | 1399/10788 [00:30<04:12, 37.14it/s]Training CobwebTree:  13%|        | 1404/10788 [00:30<04:03, 38.54it/s]Training CobwebTree:  13%|        | 1408/10788 [00:31<04:07, 37.93it/s]Training CobwebTree:  13%|        | 1413/10788 [00:31<03:57, 39.48it/s]Training CobwebTree:  13%|        | 1418/10788 [00:31<03:49, 40.80it/s]Training CobwebTree:  13%|        | 1423/10788 [00:31<03:51, 40.52it/s]Training CobwebTree:  13%|        | 1428/10788 [00:31<03:37, 42.96it/s]Training CobwebTree:  13%|        | 1433/10788 [00:31<03:44, 41.63it/s]Training CobwebTree:  13%|        | 1438/10788 [00:31<03:37, 43.08it/s]Training CobwebTree:  13%|        | 1443/10788 [00:31<03:38, 42.74it/s]Training CobwebTree:  13%|        | 1448/10788 [00:31<03:38, 42.80it/s]Training CobwebTree:  13%|        | 1453/10788 [00:32<03:33, 43.71it/s]Training CobwebTree:  14%|        | 1458/10788 [00:32<03:41, 42.18it/s]Training CobwebTree:  14%|        | 1463/10788 [00:32<03:53, 39.88it/s]Training CobwebTree:  14%|        | 1468/10788 [00:32<03:47, 40.89it/s]Training CobwebTree:  14%|        | 1473/10788 [00:32<03:45, 41.32it/s]Training CobwebTree:  14%|        | 1478/10788 [00:32<03:49, 40.51it/s]Training CobwebTree:  14%|        | 1483/10788 [00:32<03:36, 42.90it/s]Training CobwebTree:  14%|        | 1488/10788 [00:32<03:42, 41.85it/s]Training CobwebTree:  14%|        | 1493/10788 [00:33<03:51, 40.23it/s]Training CobwebTree:  14%|        | 1498/10788 [00:33<03:55, 39.38it/s]Training CobwebTree:  14%|        | 1502/10788 [00:33<03:55, 39.36it/s]Training CobwebTree:  14%|        | 1506/10788 [00:33<03:56, 39.21it/s]Training CobwebTree:  14%|        | 1510/10788 [00:33<04:05, 37.86it/s]Training CobwebTree:  14%|        | 1514/10788 [00:33<04:04, 37.99it/s]Training CobwebTree:  14%|        | 1518/10788 [00:33<04:07, 37.48it/s]Training CobwebTree:  14%|        | 1522/10788 [00:33<04:02, 38.16it/s]Training CobwebTree:  14%|        | 1526/10788 [00:33<04:08, 37.26it/s]Training CobwebTree:  14%|        | 1531/10788 [00:34<04:05, 37.64it/s]Training CobwebTree:  14%|        | 1535/10788 [00:34<04:07, 37.31it/s]Training CobwebTree:  14%|        | 1540/10788 [00:34<04:03, 37.91it/s]Training CobwebTree:  14%|        | 1544/10788 [00:34<04:11, 36.78it/s]Training CobwebTree:  14%|        | 1548/10788 [00:34<04:10, 36.92it/s]Training CobwebTree:  14%|        | 1552/10788 [00:34<04:04, 37.74it/s]Training CobwebTree:  14%|        | 1557/10788 [00:34<03:57, 38.86it/s]Training CobwebTree:  14%|        | 1562/10788 [00:34<03:51, 39.90it/s]Training CobwebTree:  15%|        | 1566/10788 [00:35<04:06, 37.49it/s]Training CobwebTree:  15%|        | 1571/10788 [00:35<03:56, 39.02it/s]Training CobwebTree:  15%|        | 1576/10788 [00:35<03:49, 40.11it/s]Training CobwebTree:  15%|        | 1581/10788 [00:35<03:53, 39.48it/s]Training CobwebTree:  15%|        | 1586/10788 [00:35<03:52, 39.64it/s]Training CobwebTree:  15%|        | 1591/10788 [00:35<03:53, 39.46it/s]Training CobwebTree:  15%|        | 1596/10788 [00:35<03:41, 41.42it/s]Training CobwebTree:  15%|        | 1601/10788 [00:35<03:44, 40.91it/s]Training CobwebTree:  15%|        | 1606/10788 [00:35<03:45, 40.79it/s]Training CobwebTree:  15%|        | 1611/10788 [00:36<03:55, 38.92it/s]Training CobwebTree:  15%|        | 1616/10788 [00:36<03:45, 40.67it/s]Training CobwebTree:  15%|        | 1621/10788 [00:36<03:46, 40.40it/s]Training CobwebTree:  15%|        | 1626/10788 [00:36<03:55, 38.83it/s]Training CobwebTree:  15%|        | 1630/10788 [00:36<03:58, 38.33it/s]Training CobwebTree:  15%|        | 1635/10788 [00:36<03:53, 39.26it/s]Training CobwebTree:  15%|        | 1640/10788 [00:36<03:48, 40.07it/s]Training CobwebTree:  15%|        | 1645/10788 [00:36<03:49, 39.90it/s]Training CobwebTree:  15%|        | 1650/10788 [00:37<03:49, 39.87it/s]Training CobwebTree:  15%|        | 1654/10788 [00:37<04:04, 37.40it/s]Training CobwebTree:  15%|        | 1659/10788 [00:37<03:55, 38.69it/s]Training CobwebTree:  15%|        | 1664/10788 [00:37<03:51, 39.48it/s]Training CobwebTree:  15%|        | 1669/10788 [00:37<03:42, 41.03it/s]Training CobwebTree:  16%|        | 1674/10788 [00:37<03:50, 39.62it/s]Training CobwebTree:  16%|        | 1679/10788 [00:37<03:42, 40.94it/s]Training CobwebTree:  16%|        | 1684/10788 [00:37<03:53, 39.05it/s]Training CobwebTree:  16%|        | 1689/10788 [00:38<03:41, 41.00it/s]Training CobwebTree:  16%|        | 1694/10788 [00:38<03:44, 40.49it/s]Training CobwebTree:  16%|        | 1699/10788 [00:38<03:57, 38.27it/s]Training CobwebTree:  16%|        | 1703/10788 [00:38<03:55, 38.54it/s]Training CobwebTree:  16%|        | 1708/10788 [00:38<03:47, 39.93it/s]Training CobwebTree:  16%|        | 1713/10788 [00:38<03:40, 41.24it/s]Training CobwebTree:  16%|        | 1718/10788 [00:38<03:34, 42.20it/s]Training CobwebTree:  16%|        | 1723/10788 [00:38<03:44, 40.38it/s]Training CobwebTree:  16%|        | 1728/10788 [00:39<03:41, 40.90it/s]Training CobwebTree:  16%|        | 1733/10788 [00:39<03:41, 40.93it/s]Training CobwebTree:  16%|        | 1738/10788 [00:39<03:52, 38.87it/s]Training CobwebTree:  16%|        | 1742/10788 [00:39<03:51, 39.12it/s]Training CobwebTree:  16%|        | 1747/10788 [00:39<03:47, 39.78it/s]Training CobwebTree:  16%|        | 1751/10788 [00:39<03:57, 38.04it/s]Training CobwebTree:  16%|        | 1755/10788 [00:39<04:04, 36.87it/s]Training CobwebTree:  16%|        | 1759/10788 [00:39<04:01, 37.36it/s]Training CobwebTree:  16%|        | 1763/10788 [00:40<04:15, 35.37it/s]Training CobwebTree:  16%|        | 1767/10788 [00:40<04:21, 34.47it/s]Training CobwebTree:  16%|        | 1772/10788 [00:40<04:07, 36.40it/s]Training CobwebTree:  16%|        | 1776/10788 [00:40<04:12, 35.68it/s]Training CobwebTree:  17%|        | 1781/10788 [00:40<03:57, 37.92it/s]Training CobwebTree:  17%|        | 1786/10788 [00:40<03:54, 38.35it/s]Training CobwebTree:  17%|        | 1790/10788 [00:40<03:57, 37.90it/s]Training CobwebTree:  17%|        | 1794/10788 [00:40<03:54, 38.40it/s]Training CobwebTree:  17%|        | 1798/10788 [00:40<03:57, 37.88it/s]Training CobwebTree:  17%|        | 1802/10788 [00:41<03:56, 38.01it/s]Training CobwebTree:  17%|        | 1806/10788 [00:41<03:59, 37.49it/s]Training CobwebTree:  17%|        | 1810/10788 [00:41<04:07, 36.25it/s]Training CobwebTree:  17%|        | 1815/10788 [00:41<04:01, 37.13it/s]Training CobwebTree:  17%|        | 1819/10788 [00:41<03:57, 37.69it/s]Training CobwebTree:  17%|        | 1823/10788 [00:41<03:58, 37.61it/s]Training CobwebTree:  17%|        | 1827/10788 [00:41<04:03, 36.85it/s]Training CobwebTree:  17%|        | 1832/10788 [00:41<03:55, 38.06it/s]Training CobwebTree:  17%|        | 1836/10788 [00:41<03:56, 37.84it/s]Training CobwebTree:  17%|        | 1840/10788 [00:42<04:01, 37.12it/s]Training CobwebTree:  17%|        | 1845/10788 [00:42<03:51, 38.66it/s]Training CobwebTree:  17%|        | 1850/10788 [00:42<03:42, 40.17it/s]Training CobwebTree:  17%|        | 1855/10788 [00:42<03:40, 40.49it/s]Training CobwebTree:  17%|        | 1860/10788 [00:42<03:42, 40.08it/s]Training CobwebTree:  17%|        | 1865/10788 [00:42<03:56, 37.69it/s]Training CobwebTree:  17%|        | 1870/10788 [00:42<03:45, 39.59it/s]Training CobwebTree:  17%|        | 1875/10788 [00:42<03:40, 40.35it/s]Training CobwebTree:  17%|        | 1880/10788 [00:43<03:43, 39.90it/s]Training CobwebTree:  17%|        | 1885/10788 [00:43<03:48, 38.94it/s]Training CobwebTree:  18%|        | 1889/10788 [00:43<03:57, 37.46it/s]Training CobwebTree:  18%|        | 1893/10788 [00:43<03:57, 37.51it/s]Training CobwebTree:  18%|        | 1897/10788 [00:43<04:09, 35.70it/s]Training CobwebTree:  18%|        | 1901/10788 [00:43<04:11, 35.31it/s]Training CobwebTree:  18%|        | 1906/10788 [00:43<04:01, 36.77it/s]Training CobwebTree:  18%|        | 1911/10788 [00:43<03:51, 38.29it/s]Training CobwebTree:  18%|        | 1915/10788 [00:44<03:50, 38.55it/s]Training CobwebTree:  18%|        | 1919/10788 [00:44<03:53, 37.92it/s]Training CobwebTree:  18%|        | 1923/10788 [00:44<04:04, 36.25it/s]Training CobwebTree:  18%|        | 1928/10788 [00:44<03:51, 38.22it/s]Training CobwebTree:  18%|        | 1932/10788 [00:44<03:49, 38.59it/s]Training CobwebTree:  18%|        | 1937/10788 [00:44<03:44, 39.46it/s]Training CobwebTree:  18%|        | 1941/10788 [00:44<03:48, 38.64it/s]Training CobwebTree:  18%|        | 1945/10788 [00:44<03:53, 37.91it/s]Training CobwebTree:  18%|        | 1949/10788 [00:44<03:56, 37.34it/s]Training CobwebTree:  18%|        | 1953/10788 [00:45<04:08, 35.59it/s]Training CobwebTree:  18%|        | 1957/10788 [00:45<04:03, 36.30it/s]Training CobwebTree:  18%|        | 1961/10788 [00:45<03:58, 37.06it/s]Training CobwebTree:  18%|        | 1965/10788 [00:45<03:57, 37.15it/s]Training CobwebTree:  18%|        | 1969/10788 [00:45<03:58, 36.94it/s]Training CobwebTree:  18%|        | 1973/10788 [00:45<04:07, 35.57it/s]Training CobwebTree:  18%|        | 1977/10788 [00:45<04:05, 35.96it/s]Training CobwebTree:  18%|        | 1981/10788 [00:45<04:00, 36.56it/s]Training CobwebTree:  18%|        | 1985/10788 [00:45<03:55, 37.36it/s]Training CobwebTree:  18%|        | 1990/10788 [00:46<03:46, 38.77it/s]Training CobwebTree:  18%|        | 1994/10788 [00:46<04:03, 36.10it/s]Training CobwebTree:  19%|        | 1998/10788 [00:46<04:04, 35.91it/s]Training CobwebTree:  19%|        | 2002/10788 [00:46<04:08, 35.37it/s]Training CobwebTree:  19%|        | 2007/10788 [00:46<03:57, 36.99it/s]Training CobwebTree:  19%|        | 2012/10788 [00:46<03:50, 38.05it/s]Training CobwebTree:  19%|        | 2016/10788 [00:46<03:49, 38.29it/s]Training CobwebTree:  19%|        | 2020/10788 [00:46<03:51, 37.86it/s]Training CobwebTree:  19%|        | 2024/10788 [00:46<03:52, 37.77it/s]Training CobwebTree:  19%|        | 2029/10788 [00:47<03:44, 38.97it/s]Training CobwebTree:  19%|        | 2033/10788 [00:47<03:48, 38.27it/s]Training CobwebTree:  19%|        | 2037/10788 [00:47<03:56, 36.99it/s]Training CobwebTree:  19%|        | 2041/10788 [00:47<03:54, 37.38it/s]Training CobwebTree:  19%|        | 2045/10788 [00:47<03:50, 37.99it/s]Training CobwebTree:  19%|        | 2050/10788 [00:47<03:45, 38.79it/s]Training CobwebTree:  19%|        | 2055/10788 [00:47<03:43, 39.00it/s]Training CobwebTree:  19%|        | 2059/10788 [00:47<03:49, 38.03it/s]Training CobwebTree:  19%|        | 2064/10788 [00:47<03:42, 39.30it/s]Training CobwebTree:  19%|        | 2068/10788 [00:48<03:42, 39.18it/s]Training CobwebTree:  19%|        | 2072/10788 [00:48<03:43, 38.97it/s]Training CobwebTree:  19%|        | 2076/10788 [00:48<03:51, 37.57it/s]Training CobwebTree:  19%|        | 2081/10788 [00:48<03:48, 38.06it/s]Training CobwebTree:  19%|        | 2085/10788 [00:48<03:50, 37.79it/s]Training CobwebTree:  19%|        | 2089/10788 [00:48<03:54, 37.12it/s]Training CobwebTree:  19%|        | 2093/10788 [00:48<03:50, 37.68it/s]Training CobwebTree:  19%|        | 2097/10788 [00:48<03:51, 37.59it/s]Training CobwebTree:  19%|        | 2102/10788 [00:48<03:44, 38.73it/s]Training CobwebTree:  20%|        | 2106/10788 [00:49<03:49, 37.78it/s]Training CobwebTree:  20%|        | 2110/10788 [00:49<03:46, 38.24it/s]Training CobwebTree:  20%|        | 2114/10788 [00:49<03:45, 38.44it/s]Training CobwebTree:  20%|        | 2119/10788 [00:49<03:46, 38.27it/s]Training CobwebTree:  20%|        | 2123/10788 [00:49<03:56, 36.62it/s]Training CobwebTree:  20%|        | 2127/10788 [00:49<03:59, 36.23it/s]Training CobwebTree:  20%|        | 2131/10788 [00:50<08:07, 17.75it/s]Training CobwebTree:  20%|        | 2135/10788 [00:50<06:56, 20.77it/s]Training CobwebTree:  20%|        | 2139/10788 [00:50<06:04, 23.75it/s]Training CobwebTree:  20%|        | 2143/10788 [00:50<05:33, 25.96it/s]Training CobwebTree:  20%|        | 2147/10788 [00:50<05:17, 27.19it/s]Training CobwebTree:  20%|        | 2151/10788 [00:50<04:48, 29.92it/s]Training CobwebTree:  20%|        | 2155/10788 [00:50<04:28, 32.17it/s]Training CobwebTree:  20%|        | 2159/10788 [00:50<04:18, 33.32it/s]Training CobwebTree:  20%|        | 2164/10788 [00:51<04:05, 35.12it/s]Training CobwebTree:  20%|        | 2168/10788 [00:51<04:12, 34.13it/s]Training CobwebTree:  20%|        | 2172/10788 [00:51<04:09, 34.53it/s]Training CobwebTree:  20%|        | 2176/10788 [00:51<04:11, 34.30it/s]Training CobwebTree:  20%|        | 2180/10788 [00:51<04:04, 35.14it/s]Training CobwebTree:  20%|        | 2184/10788 [00:51<04:07, 34.81it/s]Training CobwebTree:  20%|        | 2188/10788 [00:51<04:07, 34.73it/s]Training CobwebTree:  20%|        | 2192/10788 [00:51<04:07, 34.69it/s]Training CobwebTree:  20%|        | 2196/10788 [00:52<04:04, 35.19it/s]Training CobwebTree:  20%|        | 2200/10788 [00:52<04:06, 34.78it/s]Training CobwebTree:  20%|        | 2204/10788 [00:52<04:06, 34.85it/s]Training CobwebTree:  20%|        | 2209/10788 [00:52<03:51, 36.98it/s]Training CobwebTree:  21%|        | 2213/10788 [00:52<03:55, 36.47it/s]Training CobwebTree:  21%|        | 2217/10788 [00:52<03:51, 36.97it/s]Training CobwebTree:  21%|        | 2222/10788 [00:52<03:43, 38.36it/s]Training CobwebTree:  21%|        | 2226/10788 [00:52<03:49, 37.24it/s]Training CobwebTree:  21%|        | 2231/10788 [00:52<03:42, 38.40it/s]Training CobwebTree:  21%|        | 2235/10788 [00:53<03:51, 36.91it/s]Training CobwebTree:  21%|        | 2240/10788 [00:53<03:41, 38.62it/s]Training CobwebTree:  21%|        | 2244/10788 [00:53<03:46, 37.70it/s]Training CobwebTree:  21%|        | 2249/10788 [00:53<03:42, 38.34it/s]Training CobwebTree:  21%|        | 2253/10788 [00:53<03:40, 38.73it/s]Training CobwebTree:  21%|        | 2257/10788 [00:53<03:42, 38.41it/s]Training CobwebTree:  21%|        | 2262/10788 [00:53<03:37, 39.20it/s]Training CobwebTree:  21%|        | 2266/10788 [00:53<03:36, 39.30it/s]Training CobwebTree:  21%|        | 2270/10788 [00:53<03:45, 37.76it/s]Training CobwebTree:  21%|        | 2274/10788 [00:54<03:49, 37.10it/s]Training CobwebTree:  21%|        | 2279/10788 [00:54<03:47, 37.45it/s]Training CobwebTree:  21%|        | 2283/10788 [00:54<03:53, 36.47it/s]Training CobwebTree:  21%|        | 2288/10788 [00:54<03:50, 36.87it/s]Training CobwebTree:  21%|        | 2292/10788 [00:54<03:58, 35.63it/s]Training CobwebTree:  21%|       | 2296/10788 [00:54<04:09, 34.05it/s]Training CobwebTree:  21%|       | 2300/10788 [00:54<04:05, 34.63it/s]Training CobwebTree:  21%|       | 2304/10788 [00:54<04:05, 34.52it/s]Training CobwebTree:  21%|       | 2308/10788 [00:55<03:58, 35.63it/s]Training CobwebTree:  21%|       | 2312/10788 [00:55<03:57, 35.66it/s]Training CobwebTree:  21%|       | 2316/10788 [00:55<04:01, 35.04it/s]Training CobwebTree:  22%|       | 2320/10788 [00:55<03:54, 36.04it/s]Training CobwebTree:  22%|       | 2324/10788 [00:55<04:00, 35.25it/s]Training CobwebTree:  22%|       | 2329/10788 [00:55<03:48, 37.04it/s]Training CobwebTree:  22%|       | 2333/10788 [00:55<03:53, 36.26it/s]Training CobwebTree:  22%|       | 2337/10788 [00:55<03:52, 36.34it/s]Training CobwebTree:  22%|       | 2341/10788 [00:55<03:51, 36.44it/s]Training CobwebTree:  22%|       | 2345/10788 [00:56<03:58, 35.34it/s]Training CobwebTree:  22%|       | 2349/10788 [00:56<04:04, 34.57it/s]Training CobwebTree:  22%|       | 2354/10788 [00:56<03:50, 36.54it/s]Training CobwebTree:  22%|       | 2358/10788 [00:56<03:50, 36.52it/s]Training CobwebTree:  22%|       | 2362/10788 [00:56<03:57, 35.53it/s]Training CobwebTree:  22%|       | 2366/10788 [00:56<03:59, 35.23it/s]Training CobwebTree:  22%|       | 2371/10788 [00:56<03:48, 36.77it/s]Training CobwebTree:  22%|       | 2375/10788 [00:56<03:56, 35.51it/s]Training CobwebTree:  22%|       | 2379/10788 [00:57<03:53, 35.98it/s]Training CobwebTree:  22%|       | 2383/10788 [00:57<04:07, 33.95it/s]Training CobwebTree:  22%|       | 2387/10788 [00:57<03:59, 35.02it/s]Training CobwebTree:  22%|       | 2391/10788 [00:57<04:06, 34.11it/s]Training CobwebTree:  22%|       | 2395/10788 [00:57<03:55, 35.59it/s]Training CobwebTree:  22%|       | 2399/10788 [00:57<03:56, 35.44it/s]Training CobwebTree:  22%|       | 2403/10788 [00:57<03:51, 36.16it/s]Training CobwebTree:  22%|       | 2407/10788 [00:57<03:57, 35.26it/s]Training CobwebTree:  22%|       | 2411/10788 [00:57<04:04, 34.20it/s]Training CobwebTree:  22%|       | 2415/10788 [00:58<04:01, 34.72it/s]Training CobwebTree:  22%|       | 2419/10788 [00:58<04:09, 33.49it/s]Training CobwebTree:  22%|       | 2423/10788 [00:58<04:09, 33.59it/s]Training CobwebTree:  22%|       | 2427/10788 [00:58<04:14, 32.87it/s]Training CobwebTree:  23%|       | 2431/10788 [00:58<04:04, 34.12it/s]Training CobwebTree:  23%|       | 2435/10788 [00:58<04:01, 34.66it/s]Training CobwebTree:  23%|       | 2439/10788 [00:58<03:59, 34.91it/s]Training CobwebTree:  23%|       | 2443/10788 [00:58<03:57, 35.15it/s]Training CobwebTree:  23%|       | 2447/10788 [00:58<03:55, 35.38it/s]Training CobwebTree:  23%|       | 2451/10788 [00:59<04:08, 33.60it/s]Training CobwebTree:  23%|       | 2455/10788 [00:59<04:03, 34.19it/s]Training CobwebTree:  23%|       | 2459/10788 [00:59<04:01, 34.49it/s]Training CobwebTree:  23%|       | 2463/10788 [00:59<04:00, 34.57it/s]Training CobwebTree:  23%|       | 2467/10788 [00:59<04:03, 34.20it/s]Training CobwebTree:  23%|       | 2471/10788 [00:59<03:57, 35.02it/s]Training CobwebTree:  23%|       | 2475/10788 [00:59<03:56, 35.08it/s]Training CobwebTree:  23%|       | 2479/10788 [00:59<03:53, 35.55it/s]Training CobwebTree:  23%|       | 2483/10788 [01:00<03:51, 35.92it/s]Training CobwebTree:  23%|       | 2487/10788 [01:00<03:53, 35.54it/s]Training CobwebTree:  23%|       | 2491/10788 [01:00<03:56, 35.04it/s]Training CobwebTree:  23%|       | 2495/10788 [01:00<04:00, 34.51it/s]Training CobwebTree:  23%|       | 2499/10788 [01:00<04:02, 34.13it/s]Training CobwebTree:  23%|       | 2503/10788 [01:00<04:11, 32.98it/s]Training CobwebTree:  23%|       | 2507/10788 [01:00<03:58, 34.68it/s]Training CobwebTree:  23%|       | 2512/10788 [01:00<03:48, 36.24it/s]Training CobwebTree:  23%|       | 2516/10788 [01:00<03:56, 34.98it/s]Training CobwebTree:  23%|       | 2520/10788 [01:01<04:01, 34.29it/s]Training CobwebTree:  23%|       | 2524/10788 [01:01<03:54, 35.23it/s]Training CobwebTree:  23%|       | 2528/10788 [01:01<03:55, 35.09it/s]Training CobwebTree:  23%|       | 2532/10788 [01:01<03:50, 35.78it/s]Training CobwebTree:  24%|       | 2536/10788 [01:01<04:01, 34.19it/s]Training CobwebTree:  24%|       | 2540/10788 [01:01<03:59, 34.48it/s]Training CobwebTree:  24%|       | 2544/10788 [01:01<04:00, 34.27it/s]Training CobwebTree:  24%|       | 2548/10788 [01:01<04:01, 34.15it/s]Training CobwebTree:  24%|       | 2552/10788 [01:02<04:01, 34.15it/s]Training CobwebTree:  24%|       | 2556/10788 [01:02<03:59, 34.43it/s]Training CobwebTree:  24%|       | 2560/10788 [01:02<03:56, 34.77it/s]Training CobwebTree:  24%|       | 2564/10788 [01:02<04:01, 34.01it/s]Training CobwebTree:  24%|       | 2568/10788 [01:02<04:03, 33.72it/s]Training CobwebTree:  24%|       | 2572/10788 [01:02<03:54, 34.97it/s]Training CobwebTree:  24%|       | 2577/10788 [01:02<03:48, 35.90it/s]Training CobwebTree:  24%|       | 2581/10788 [01:02<03:50, 35.67it/s]Training CobwebTree:  24%|       | 2585/10788 [01:02<03:52, 35.35it/s]Training CobwebTree:  24%|       | 2589/10788 [01:03<03:57, 34.52it/s]Training CobwebTree:  24%|       | 2593/10788 [01:03<03:51, 35.39it/s]Training CobwebTree:  24%|       | 2597/10788 [01:03<03:53, 35.07it/s]Training CobwebTree:  24%|       | 2601/10788 [01:03<03:51, 35.32it/s]Training CobwebTree:  24%|       | 2605/10788 [01:03<03:50, 35.52it/s]Training CobwebTree:  24%|       | 2609/10788 [01:03<03:47, 35.94it/s]Training CobwebTree:  24%|       | 2613/10788 [01:03<03:41, 36.94it/s]Training CobwebTree:  24%|       | 2617/10788 [01:03<03:43, 36.62it/s]Training CobwebTree:  24%|       | 2621/10788 [01:03<03:46, 36.04it/s]Training CobwebTree:  24%|       | 2625/10788 [01:04<03:49, 35.57it/s]Training CobwebTree:  24%|       | 2629/10788 [01:04<03:54, 34.72it/s]Training CobwebTree:  24%|       | 2633/10788 [01:04<03:54, 34.80it/s]Training CobwebTree:  24%|       | 2638/10788 [01:04<03:47, 35.87it/s]Training CobwebTree:  24%|       | 2642/10788 [01:04<03:43, 36.48it/s]Training CobwebTree:  25%|       | 2646/10788 [01:04<03:56, 34.41it/s]Training CobwebTree:  25%|       | 2650/10788 [01:04<04:00, 33.87it/s]Training CobwebTree:  25%|       | 2654/10788 [01:04<04:09, 32.54it/s]Training CobwebTree:  25%|       | 2658/10788 [01:05<04:05, 33.13it/s]Training CobwebTree:  25%|       | 2662/10788 [01:05<04:07, 32.84it/s]Training CobwebTree:  25%|       | 2666/10788 [01:05<04:05, 33.15it/s]Training CobwebTree:  25%|       | 2670/10788 [01:05<04:00, 33.70it/s]Training CobwebTree:  25%|       | 2674/10788 [01:05<03:56, 34.25it/s]Training CobwebTree:  25%|       | 2678/10788 [01:05<03:58, 33.98it/s]Training CobwebTree:  25%|       | 2682/10788 [01:05<04:01, 33.63it/s]Training CobwebTree:  25%|       | 2687/10788 [01:05<03:46, 35.79it/s]Training CobwebTree:  25%|       | 2691/10788 [01:06<03:58, 33.97it/s]Training CobwebTree:  25%|       | 2695/10788 [01:06<03:52, 34.87it/s]Training CobwebTree:  25%|       | 2699/10788 [01:06<03:54, 34.47it/s]Training CobwebTree:  25%|       | 2703/10788 [01:06<03:59, 33.69it/s]Training CobwebTree:  25%|       | 2707/10788 [01:06<03:54, 34.47it/s]Training CobwebTree:  25%|       | 2711/10788 [01:06<03:52, 34.73it/s]Training CobwebTree:  25%|       | 2715/10788 [01:06<03:55, 34.31it/s]Training CobwebTree:  25%|       | 2719/10788 [01:06<04:01, 33.44it/s]Training CobwebTree:  25%|       | 2723/10788 [01:06<04:00, 33.57it/s]Training CobwebTree:  25%|       | 2727/10788 [01:07<04:01, 33.31it/s]Training CobwebTree:  25%|       | 2732/10788 [01:07<03:48, 35.30it/s]Training CobwebTree:  25%|       | 2736/10788 [01:07<03:50, 34.96it/s]Training CobwebTree:  25%|       | 2740/10788 [01:07<03:42, 36.13it/s]Training CobwebTree:  25%|       | 2744/10788 [01:07<03:36, 37.15it/s]Training CobwebTree:  25%|       | 2748/10788 [01:07<03:36, 37.09it/s]Training CobwebTree:  26%|       | 2752/10788 [01:07<03:42, 36.15it/s]Training CobwebTree:  26%|       | 2756/10788 [01:07<03:51, 34.72it/s]Training CobwebTree:  26%|       | 2760/10788 [01:07<03:46, 35.50it/s]Training CobwebTree:  26%|       | 2764/10788 [01:08<03:38, 36.66it/s]Training CobwebTree:  26%|       | 2768/10788 [01:08<03:44, 35.78it/s]Training CobwebTree:  26%|       | 2772/10788 [01:08<03:46, 35.46it/s]Training CobwebTree:  26%|       | 2777/10788 [01:08<03:32, 37.62it/s]Training CobwebTree:  26%|       | 2781/10788 [01:08<03:49, 34.82it/s]Training CobwebTree:  26%|       | 2785/10788 [01:08<03:52, 34.42it/s]Training CobwebTree:  26%|       | 2789/10788 [01:08<03:58, 33.59it/s]Training CobwebTree:  26%|       | 2793/10788 [01:08<04:04, 32.66it/s]Training CobwebTree:  26%|       | 2797/10788 [01:09<03:59, 33.34it/s]Training CobwebTree:  26%|       | 2801/10788 [01:09<03:51, 34.43it/s]Training CobwebTree:  26%|       | 2805/10788 [01:09<03:53, 34.16it/s]Training CobwebTree:  26%|       | 2809/10788 [01:09<04:03, 32.71it/s]Training CobwebTree:  26%|       | 2813/10788 [01:09<03:54, 33.98it/s]Training CobwebTree:  26%|       | 2817/10788 [01:09<03:50, 34.56it/s]Training CobwebTree:  26%|       | 2821/10788 [01:09<03:59, 33.20it/s]Training CobwebTree:  26%|       | 2825/10788 [01:09<03:59, 33.22it/s]Training CobwebTree:  26%|       | 2829/10788 [01:10<03:57, 33.53it/s]Training CobwebTree:  26%|       | 2833/10788 [01:10<04:12, 31.54it/s]Training CobwebTree:  26%|       | 2837/10788 [01:10<04:03, 32.71it/s]Training CobwebTree:  26%|       | 2841/10788 [01:10<04:01, 32.91it/s]Training CobwebTree:  26%|       | 2845/10788 [01:10<04:07, 32.15it/s]Training CobwebTree:  26%|       | 2849/10788 [01:10<04:27, 29.71it/s]Training CobwebTree:  26%|       | 2853/10788 [01:10<04:18, 30.65it/s]Training CobwebTree:  26%|       | 2857/10788 [01:10<04:10, 31.66it/s]Training CobwebTree:  27%|       | 2861/10788 [01:11<03:56, 33.51it/s]Training CobwebTree:  27%|       | 2865/10788 [01:11<03:46, 34.92it/s]Training CobwebTree:  27%|       | 2869/10788 [01:11<03:53, 33.87it/s]Training CobwebTree:  27%|       | 2873/10788 [01:11<03:51, 34.24it/s]Training CobwebTree:  27%|       | 2877/10788 [01:11<03:50, 34.26it/s]Training CobwebTree:  27%|       | 2881/10788 [01:11<03:52, 34.04it/s]Training CobwebTree:  27%|       | 2885/10788 [01:11<03:51, 34.16it/s]Training CobwebTree:  27%|       | 2889/10788 [01:11<03:52, 34.02it/s]Training CobwebTree:  27%|       | 2893/10788 [01:11<03:46, 34.82it/s]Training CobwebTree:  27%|       | 2897/10788 [01:12<03:49, 34.40it/s]Training CobwebTree:  27%|       | 2901/10788 [01:12<03:55, 33.42it/s]Training CobwebTree:  27%|       | 2905/10788 [01:12<03:47, 34.71it/s]Training CobwebTree:  27%|       | 2909/10788 [01:12<03:44, 35.13it/s]Training CobwebTree:  27%|       | 2913/10788 [01:12<03:40, 35.64it/s]Training CobwebTree:  27%|       | 2917/10788 [01:12<03:40, 35.63it/s]Training CobwebTree:  27%|       | 2921/10788 [01:12<03:43, 35.21it/s]Training CobwebTree:  27%|       | 2925/10788 [01:12<03:45, 34.82it/s]Training CobwebTree:  27%|       | 2929/10788 [01:12<03:38, 35.99it/s]Training CobwebTree:  27%|       | 2933/10788 [01:13<03:55, 33.35it/s]Training CobwebTree:  27%|       | 2937/10788 [01:13<03:55, 33.29it/s]Training CobwebTree:  27%|       | 2941/10788 [01:13<03:56, 33.24it/s]Training CobwebTree:  27%|       | 2945/10788 [01:13<04:03, 32.27it/s]Training CobwebTree:  27%|       | 2949/10788 [01:13<03:55, 33.27it/s]Training CobwebTree:  27%|       | 2953/10788 [01:13<04:03, 32.23it/s]Training CobwebTree:  27%|       | 2957/10788 [01:13<04:06, 31.80it/s]Training CobwebTree:  27%|       | 2961/10788 [01:13<03:57, 32.97it/s]Training CobwebTree:  27%|       | 2965/10788 [01:14<03:55, 33.17it/s]Training CobwebTree:  28%|       | 2969/10788 [01:14<04:06, 31.71it/s]Training CobwebTree:  28%|       | 2974/10788 [01:14<03:55, 33.22it/s]Training CobwebTree:  28%|       | 2978/10788 [01:14<03:54, 33.25it/s]Training CobwebTree:  28%|       | 2982/10788 [01:14<03:48, 34.10it/s]Training CobwebTree:  28%|       | 2986/10788 [01:14<03:41, 35.20it/s]Training CobwebTree:  28%|       | 2991/10788 [01:14<03:33, 36.58it/s]Training CobwebTree:  28%|       | 2995/10788 [01:14<03:38, 35.61it/s]Training CobwebTree:  28%|       | 2999/10788 [01:15<03:32, 36.65it/s]Training CobwebTree:  28%|       | 3003/10788 [01:15<03:38, 35.64it/s]Training CobwebTree:  28%|       | 3007/10788 [01:15<03:37, 35.81it/s]Training CobwebTree:  28%|       | 3011/10788 [01:15<03:40, 35.21it/s]Training CobwebTree:  28%|       | 3015/10788 [01:15<03:51, 33.60it/s]Training CobwebTree:  28%|       | 3020/10788 [01:15<03:42, 34.92it/s]Training CobwebTree:  28%|       | 3024/10788 [01:15<03:47, 34.08it/s]Training CobwebTree:  28%|       | 3028/10788 [01:15<03:46, 34.19it/s]Training CobwebTree:  28%|       | 3032/10788 [01:16<03:47, 34.13it/s]Training CobwebTree:  28%|       | 3036/10788 [01:16<03:46, 34.23it/s]Training CobwebTree:  28%|       | 3040/10788 [01:16<03:42, 34.75it/s]Training CobwebTree:  28%|       | 3044/10788 [01:16<03:44, 34.56it/s]Training CobwebTree:  28%|       | 3048/10788 [01:16<03:39, 35.18it/s]Training CobwebTree:  28%|       | 3052/10788 [01:16<03:32, 36.38it/s]Training CobwebTree:  28%|       | 3057/10788 [01:16<03:26, 37.49it/s]Training CobwebTree:  28%|       | 3061/10788 [01:16<03:24, 37.84it/s]Training CobwebTree:  28%|       | 3066/10788 [01:16<03:15, 39.56it/s]Training CobwebTree:  28%|       | 3070/10788 [01:17<03:24, 37.69it/s]Training CobwebTree:  28%|       | 3074/10788 [01:17<03:27, 37.23it/s]Training CobwebTree:  29%|       | 3078/10788 [01:17<03:31, 36.42it/s]Training CobwebTree:  29%|       | 3082/10788 [01:17<03:33, 36.09it/s]Training CobwebTree:  29%|       | 3086/10788 [01:17<03:31, 36.37it/s]Training CobwebTree:  29%|       | 3090/10788 [01:17<03:39, 35.00it/s]Training CobwebTree:  29%|       | 3094/10788 [01:17<03:32, 36.15it/s]Training CobwebTree:  29%|       | 3098/10788 [01:17<03:35, 35.69it/s]Training CobwebTree:  29%|       | 3102/10788 [01:17<03:45, 34.09it/s]Training CobwebTree:  29%|       | 3106/10788 [01:18<03:45, 34.01it/s]Training CobwebTree:  29%|       | 3110/10788 [01:18<03:38, 35.15it/s]Training CobwebTree:  29%|       | 3114/10788 [01:18<03:45, 34.06it/s]Training CobwebTree:  29%|       | 3119/10788 [01:18<03:34, 35.75it/s]Training CobwebTree:  29%|       | 3123/10788 [01:18<03:30, 36.43it/s]Training CobwebTree:  29%|       | 3127/10788 [01:18<03:35, 35.52it/s]Training CobwebTree:  29%|       | 3131/10788 [01:18<03:35, 35.61it/s]Training CobwebTree:  29%|       | 3135/10788 [01:18<03:29, 36.52it/s]Training CobwebTree:  29%|       | 3140/10788 [01:18<03:21, 37.91it/s]Training CobwebTree:  29%|       | 3144/10788 [01:19<03:19, 38.28it/s]Training CobwebTree:  29%|       | 3148/10788 [01:19<03:18, 38.57it/s]Training CobwebTree:  29%|       | 3152/10788 [01:19<03:25, 37.18it/s]Training CobwebTree:  29%|       | 3156/10788 [01:19<03:24, 37.27it/s]Training CobwebTree:  29%|       | 3160/10788 [01:19<03:26, 36.89it/s]Training CobwebTree:  29%|       | 3164/10788 [01:19<03:33, 35.72it/s]Training CobwebTree:  29%|       | 3169/10788 [01:19<03:26, 36.92it/s]Training CobwebTree:  29%|       | 3174/10788 [01:19<03:18, 38.37it/s]Training CobwebTree:  29%|       | 3178/10788 [01:19<03:20, 38.04it/s]Training CobwebTree:  29%|       | 3182/10788 [01:20<03:24, 37.26it/s]Training CobwebTree:  30%|       | 3186/10788 [01:20<03:24, 37.22it/s]Training CobwebTree:  30%|       | 3190/10788 [01:20<03:30, 36.18it/s]Training CobwebTree:  30%|       | 3195/10788 [01:20<03:19, 38.03it/s]Training CobwebTree:  30%|       | 3199/10788 [01:20<03:23, 37.33it/s]Training CobwebTree:  30%|       | 3203/10788 [01:20<03:22, 37.50it/s]Training CobwebTree:  30%|       | 3207/10788 [01:20<03:20, 37.83it/s]Training CobwebTree:  30%|       | 3211/10788 [01:20<03:31, 35.81it/s]Training CobwebTree:  30%|       | 3215/10788 [01:21<03:30, 35.98it/s]Training CobwebTree:  30%|       | 3220/10788 [01:21<03:27, 36.46it/s]Training CobwebTree:  30%|       | 3224/10788 [01:21<03:24, 36.98it/s]Training CobwebTree:  30%|       | 3228/10788 [01:21<03:27, 36.38it/s]Training CobwebTree:  30%|       | 3232/10788 [01:21<03:32, 35.64it/s]Training CobwebTree:  30%|       | 3236/10788 [01:21<03:30, 35.85it/s]Training CobwebTree:  30%|       | 3240/10788 [01:21<03:29, 36.03it/s]Training CobwebTree:  30%|       | 3244/10788 [01:21<03:30, 35.80it/s]Training CobwebTree:  30%|       | 3248/10788 [01:21<03:32, 35.54it/s]Training CobwebTree:  30%|       | 3252/10788 [01:22<03:30, 35.76it/s]Training CobwebTree:  30%|       | 3256/10788 [01:22<03:28, 36.13it/s]Training CobwebTree:  30%|       | 3260/10788 [01:22<03:30, 35.71it/s]Training CobwebTree:  30%|       | 3264/10788 [01:22<03:32, 35.34it/s]Training CobwebTree:  30%|       | 3268/10788 [01:22<03:26, 36.43it/s]Training CobwebTree:  30%|       | 3272/10788 [01:22<03:27, 36.25it/s]Training CobwebTree:  30%|       | 3277/10788 [01:22<03:19, 37.73it/s]Training CobwebTree:  30%|       | 3282/10788 [01:22<03:14, 38.61it/s]Training CobwebTree:  30%|       | 3286/10788 [01:22<03:19, 37.67it/s]Training CobwebTree:  30%|       | 3290/10788 [01:23<03:21, 37.25it/s]Training CobwebTree:  31%|       | 3294/10788 [01:23<03:31, 35.46it/s]Training CobwebTree:  31%|       | 3298/10788 [01:23<03:40, 33.89it/s]Training CobwebTree:  31%|       | 3302/10788 [01:23<03:45, 33.26it/s]Training CobwebTree:  31%|       | 3306/10788 [01:23<03:44, 33.28it/s]Training CobwebTree:  31%|       | 3310/10788 [01:23<03:36, 34.54it/s]Training CobwebTree:  31%|       | 3314/10788 [01:23<03:38, 34.28it/s]Training CobwebTree:  31%|       | 3318/10788 [01:23<03:34, 34.77it/s]Training CobwebTree:  31%|       | 3322/10788 [01:24<03:40, 33.87it/s]Training CobwebTree:  31%|       | 3326/10788 [01:24<03:38, 34.13it/s]Training CobwebTree:  31%|       | 3330/10788 [01:24<03:38, 34.20it/s]Training CobwebTree:  31%|       | 3334/10788 [01:24<03:37, 34.35it/s]Training CobwebTree:  31%|       | 3338/10788 [01:24<03:29, 35.63it/s]Training CobwebTree:  31%|       | 3342/10788 [01:24<03:32, 35.07it/s]Training CobwebTree:  31%|       | 3346/10788 [01:24<03:30, 35.41it/s]Training CobwebTree:  31%|       | 3350/10788 [01:24<03:26, 36.05it/s]Training CobwebTree:  31%|       | 3354/10788 [01:24<03:30, 35.30it/s]Training CobwebTree:  31%|       | 3359/10788 [01:25<03:20, 37.07it/s]Training CobwebTree:  31%|       | 3364/10788 [01:25<03:14, 38.09it/s]Training CobwebTree:  31%|       | 3368/10788 [01:25<03:15, 37.98it/s]Training CobwebTree:  31%|      | 3372/10788 [01:25<03:20, 37.00it/s]Training CobwebTree:  31%|      | 3376/10788 [01:25<03:18, 37.42it/s]Training CobwebTree:  31%|      | 3380/10788 [01:25<03:26, 35.94it/s]Training CobwebTree:  31%|      | 3384/10788 [01:25<03:38, 33.91it/s]Training CobwebTree:  31%|      | 3388/10788 [01:25<03:39, 33.73it/s]Training CobwebTree:  31%|      | 3392/10788 [01:25<03:38, 33.84it/s]Training CobwebTree:  31%|      | 3396/10788 [01:26<03:34, 34.43it/s]Training CobwebTree:  32%|      | 3400/10788 [01:26<03:36, 34.18it/s]Training CobwebTree:  32%|      | 3404/10788 [01:26<03:32, 34.79it/s]Training CobwebTree:  32%|      | 3408/10788 [01:26<03:40, 33.50it/s]Training CobwebTree:  32%|      | 3412/10788 [01:26<03:43, 32.97it/s]Training CobwebTree:  32%|      | 3417/10788 [01:26<03:30, 34.97it/s]Training CobwebTree:  32%|      | 3421/10788 [01:26<03:24, 36.01it/s]Training CobwebTree:  32%|      | 3426/10788 [01:26<03:16, 37.42it/s]Training CobwebTree:  32%|      | 3430/10788 [01:27<03:17, 37.35it/s]Training CobwebTree:  32%|      | 3434/10788 [01:27<03:19, 36.93it/s]Training CobwebTree:  32%|      | 3438/10788 [01:27<03:14, 37.71it/s]Training CobwebTree:  32%|      | 3442/10788 [01:27<03:15, 37.50it/s]Training CobwebTree:  32%|      | 3446/10788 [01:27<03:24, 35.83it/s]Training CobwebTree:  32%|      | 3450/10788 [01:27<03:29, 35.09it/s]Training CobwebTree:  32%|      | 3454/10788 [01:27<03:30, 34.81it/s]Training CobwebTree:  32%|      | 3458/10788 [01:27<03:27, 35.25it/s]Training CobwebTree:  32%|      | 3462/10788 [01:27<03:25, 35.64it/s]Training CobwebTree:  32%|      | 3466/10788 [01:28<03:31, 34.56it/s]Training CobwebTree:  32%|      | 3470/10788 [01:28<03:33, 34.28it/s]Training CobwebTree:  32%|      | 3474/10788 [01:28<03:25, 35.59it/s]Training CobwebTree:  32%|      | 3478/10788 [01:28<03:20, 36.47it/s]Training CobwebTree:  32%|      | 3482/10788 [01:28<03:15, 37.29it/s]Training CobwebTree:  32%|      | 3486/10788 [01:28<03:25, 35.58it/s]Training CobwebTree:  32%|      | 3491/10788 [01:28<03:17, 37.02it/s]Training CobwebTree:  32%|      | 3495/10788 [01:28<03:25, 35.52it/s]Training CobwebTree:  32%|      | 3499/10788 [01:28<03:18, 36.68it/s]Training CobwebTree:  32%|      | 3503/10788 [01:29<03:17, 36.83it/s]Training CobwebTree:  33%|      | 3507/10788 [01:29<03:16, 36.96it/s]Training CobwebTree:  33%|      | 3511/10788 [01:29<03:18, 36.65it/s]Training CobwebTree:  33%|      | 3515/10788 [01:29<03:15, 37.11it/s]Training CobwebTree:  33%|      | 3519/10788 [01:29<03:13, 37.63it/s]Training CobwebTree:  33%|      | 3523/10788 [01:29<03:25, 35.32it/s]Training CobwebTree:  33%|      | 3527/10788 [01:29<03:25, 35.34it/s]Training CobwebTree:  33%|      | 3531/10788 [01:29<03:23, 35.73it/s]Training CobwebTree:  33%|      | 3535/10788 [01:29<03:19, 36.44it/s]Training CobwebTree:  33%|      | 3540/10788 [01:30<03:08, 38.55it/s]Training CobwebTree:  33%|      | 3544/10788 [01:30<03:11, 37.82it/s]Training CobwebTree:  33%|      | 3548/10788 [01:30<03:10, 37.97it/s]Training CobwebTree:  33%|      | 3552/10788 [01:30<03:13, 37.41it/s]Training CobwebTree:  33%|      | 3556/10788 [01:30<03:18, 36.50it/s]Training CobwebTree:  33%|      | 3560/10788 [01:30<03:27, 34.75it/s]Training CobwebTree:  33%|      | 3564/10788 [01:30<03:21, 35.81it/s]Training CobwebTree:  33%|      | 3568/10788 [01:30<03:31, 34.11it/s]Training CobwebTree:  33%|      | 3572/10788 [01:30<03:29, 34.48it/s]Training CobwebTree:  33%|      | 3576/10788 [01:31<03:29, 34.42it/s]Training CobwebTree:  33%|      | 3580/10788 [01:31<03:32, 33.99it/s]Training CobwebTree:  33%|      | 3584/10788 [01:31<03:29, 34.37it/s]Training CobwebTree:  33%|      | 3588/10788 [01:31<03:27, 34.71it/s]Training CobwebTree:  33%|      | 3592/10788 [01:31<03:32, 33.89it/s]Training CobwebTree:  33%|      | 3596/10788 [01:31<03:31, 33.94it/s]Training CobwebTree:  33%|      | 3600/10788 [01:31<03:29, 34.25it/s]Training CobwebTree:  33%|      | 3604/10788 [01:31<03:33, 33.70it/s]Training CobwebTree:  33%|      | 3608/10788 [01:32<03:37, 33.00it/s]Training CobwebTree:  33%|      | 3612/10788 [01:32<03:33, 33.65it/s]Training CobwebTree:  34%|      | 3616/10788 [01:32<03:37, 32.96it/s]Training CobwebTree:  34%|      | 3620/10788 [01:32<03:40, 32.46it/s]Training CobwebTree:  34%|      | 3624/10788 [01:32<03:33, 33.59it/s]Training CobwebTree:  34%|      | 3628/10788 [01:32<03:23, 35.23it/s]Training CobwebTree:  34%|      | 3632/10788 [01:32<03:30, 34.01it/s]Training CobwebTree:  34%|      | 3636/10788 [01:32<03:33, 33.49it/s]Training CobwebTree:  34%|      | 3640/10788 [01:33<03:36, 32.98it/s]Training CobwebTree:  34%|      | 3644/10788 [01:33<03:39, 32.60it/s]Training CobwebTree:  34%|      | 3648/10788 [01:33<03:30, 33.88it/s]Training CobwebTree:  34%|      | 3652/10788 [01:33<03:30, 33.85it/s]Training CobwebTree:  34%|      | 3656/10788 [01:33<03:30, 33.93it/s]Training CobwebTree:  34%|      | 3660/10788 [01:33<03:23, 34.99it/s]Training CobwebTree:  34%|      | 3664/10788 [01:33<03:31, 33.63it/s]Training CobwebTree:  34%|      | 3668/10788 [01:33<03:27, 34.29it/s]Training CobwebTree:  34%|      | 3672/10788 [01:33<03:24, 34.78it/s]Training CobwebTree:  34%|      | 3676/10788 [01:34<03:28, 34.18it/s]Training CobwebTree:  34%|      | 3680/10788 [01:34<03:26, 34.36it/s]Training CobwebTree:  34%|      | 3684/10788 [01:34<03:32, 33.41it/s]Training CobwebTree:  34%|      | 3688/10788 [01:34<03:25, 34.59it/s]Training CobwebTree:  34%|      | 3692/10788 [01:34<03:20, 35.45it/s]Training CobwebTree:  34%|      | 3696/10788 [01:34<03:27, 34.10it/s]Training CobwebTree:  34%|      | 3700/10788 [01:34<03:19, 35.50it/s]Training CobwebTree:  34%|      | 3704/10788 [01:34<03:15, 36.30it/s]Training CobwebTree:  34%|      | 3708/10788 [01:34<03:10, 37.17it/s]Training CobwebTree:  34%|      | 3712/10788 [01:35<03:06, 37.95it/s]Training CobwebTree:  34%|      | 3717/10788 [01:35<03:04, 38.42it/s]Training CobwebTree:  34%|      | 3721/10788 [01:35<03:05, 38.15it/s]Training CobwebTree:  35%|      | 3725/10788 [01:35<03:12, 36.74it/s]Training CobwebTree:  35%|      | 3729/10788 [01:35<03:13, 36.45it/s]Training CobwebTree:  35%|      | 3733/10788 [01:35<03:17, 35.68it/s]Training CobwebTree:  35%|      | 3737/10788 [01:35<03:18, 35.59it/s]Training CobwebTree:  35%|      | 3741/10788 [01:35<03:14, 36.27it/s]Training CobwebTree:  35%|      | 3745/10788 [01:35<03:30, 33.51it/s]Training CobwebTree:  35%|      | 3749/10788 [01:36<03:23, 34.57it/s]Training CobwebTree:  35%|      | 3753/10788 [01:36<03:18, 35.51it/s]Training CobwebTree:  35%|      | 3758/10788 [01:36<03:08, 37.39it/s]Training CobwebTree:  35%|      | 3762/10788 [01:36<03:23, 34.55it/s]Training CobwebTree:  35%|      | 3766/10788 [01:36<03:20, 35.10it/s]Training CobwebTree:  35%|      | 3770/10788 [01:36<03:20, 35.02it/s]Training CobwebTree:  35%|      | 3774/10788 [01:36<03:25, 34.14it/s]Training CobwebTree:  35%|      | 3778/10788 [01:36<03:25, 34.05it/s]Training CobwebTree:  35%|      | 3782/10788 [01:37<03:20, 35.01it/s]Training CobwebTree:  35%|      | 3786/10788 [01:37<03:16, 35.59it/s]Training CobwebTree:  35%|      | 3790/10788 [01:37<03:22, 34.51it/s]Training CobwebTree:  35%|      | 3794/10788 [01:37<03:14, 35.95it/s]Training CobwebTree:  35%|      | 3798/10788 [01:37<03:17, 35.36it/s]Training CobwebTree:  35%|      | 3802/10788 [01:37<03:26, 33.86it/s]Training CobwebTree:  35%|      | 3806/10788 [01:37<03:25, 33.99it/s]Training CobwebTree:  35%|      | 3810/10788 [01:37<03:25, 33.90it/s]Training CobwebTree:  35%|      | 3814/10788 [01:37<03:24, 34.06it/s]Training CobwebTree:  35%|      | 3818/10788 [01:38<03:25, 33.89it/s]Training CobwebTree:  35%|      | 3822/10788 [01:38<03:19, 34.86it/s]Training CobwebTree:  35%|      | 3826/10788 [01:38<03:22, 34.39it/s]Training CobwebTree:  36%|      | 3830/10788 [01:38<03:21, 34.61it/s]Training CobwebTree:  36%|      | 3834/10788 [01:38<03:21, 34.50it/s]Training CobwebTree:  36%|      | 3838/10788 [01:38<03:27, 33.49it/s]Training CobwebTree:  36%|      | 3842/10788 [01:38<03:22, 34.31it/s]Training CobwebTree:  36%|      | 3846/10788 [01:38<03:28, 33.34it/s]Training CobwebTree:  36%|      | 3850/10788 [01:39<03:26, 33.66it/s]Training CobwebTree:  36%|      | 3854/10788 [01:39<03:23, 34.00it/s]Training CobwebTree:  36%|      | 3858/10788 [01:39<03:27, 33.44it/s]Training CobwebTree:  36%|      | 3862/10788 [01:39<03:36, 32.02it/s]Training CobwebTree:  36%|      | 3866/10788 [01:39<03:31, 32.80it/s]Training CobwebTree:  36%|      | 3870/10788 [01:39<03:24, 33.89it/s]Training CobwebTree:  36%|      | 3874/10788 [01:39<03:23, 34.04it/s]Training CobwebTree:  36%|      | 3878/10788 [01:39<03:33, 32.32it/s]Training CobwebTree:  36%|      | 3882/10788 [01:39<03:26, 33.44it/s]Training CobwebTree:  36%|      | 3886/10788 [01:40<03:20, 34.47it/s]Training CobwebTree:  36%|      | 3890/10788 [01:40<03:19, 34.51it/s]Training CobwebTree:  36%|      | 3894/10788 [01:40<03:14, 35.42it/s]Training CobwebTree:  36%|      | 3898/10788 [01:40<03:08, 36.54it/s]Training CobwebTree:  36%|      | 3902/10788 [01:40<03:08, 36.60it/s]Training CobwebTree:  36%|      | 3906/10788 [01:40<03:06, 36.83it/s]Training CobwebTree:  36%|      | 3910/10788 [01:40<03:05, 37.00it/s]Training CobwebTree:  36%|      | 3914/10788 [01:40<03:09, 36.30it/s]Training CobwebTree:  36%|      | 3918/10788 [01:40<03:06, 36.82it/s]Training CobwebTree:  36%|      | 3922/10788 [01:41<03:12, 35.71it/s]Training CobwebTree:  36%|      | 3926/10788 [01:41<03:14, 35.31it/s]Training CobwebTree:  36%|      | 3930/10788 [01:41<03:21, 33.95it/s]Training CobwebTree:  36%|      | 3934/10788 [01:41<03:15, 34.99it/s]Training CobwebTree:  37%|      | 3938/10788 [01:41<03:18, 34.43it/s]Training CobwebTree:  37%|      | 3942/10788 [01:41<03:10, 35.91it/s]Training CobwebTree:  37%|      | 3946/10788 [01:41<03:12, 35.55it/s]Training CobwebTree:  37%|      | 3950/10788 [01:41<03:14, 35.12it/s]Training CobwebTree:  37%|      | 3954/10788 [01:42<03:11, 35.70it/s]Training CobwebTree:  37%|      | 3958/10788 [01:42<03:12, 35.53it/s]Training CobwebTree:  37%|      | 3962/10788 [01:42<03:16, 34.71it/s]Training CobwebTree:  37%|      | 3966/10788 [01:42<03:25, 33.27it/s]Training CobwebTree:  37%|      | 3970/10788 [01:42<03:17, 34.56it/s]Training CobwebTree:  37%|      | 3974/10788 [01:42<03:18, 34.39it/s]Training CobwebTree:  37%|      | 3978/10788 [01:42<03:22, 33.59it/s]Training CobwebTree:  37%|      | 3982/10788 [01:42<03:31, 32.20it/s]Training CobwebTree:  37%|      | 3986/10788 [01:42<03:29, 32.39it/s]Training CobwebTree:  37%|      | 3990/10788 [01:43<03:35, 31.60it/s]Training CobwebTree:  37%|      | 3994/10788 [01:43<03:26, 32.92it/s]Training CobwebTree:  37%|      | 3998/10788 [01:43<03:26, 32.82it/s]Training CobwebTree:  37%|      | 4002/10788 [01:43<03:25, 32.96it/s]Training CobwebTree:  37%|      | 4006/10788 [01:43<03:34, 31.63it/s]Training CobwebTree:  37%|      | 4010/10788 [01:43<03:29, 32.34it/s]Training CobwebTree:  37%|      | 4014/10788 [01:43<03:34, 31.61it/s]Training CobwebTree:  37%|      | 4018/10788 [01:43<03:35, 31.46it/s]Training CobwebTree:  37%|      | 4022/10788 [01:44<03:35, 31.33it/s]Training CobwebTree:  37%|      | 4026/10788 [01:44<03:38, 30.91it/s]Training CobwebTree:  37%|      | 4030/10788 [01:44<03:36, 31.15it/s]Training CobwebTree:  37%|      | 4034/10788 [01:44<03:33, 31.56it/s]Training CobwebTree:  37%|      | 4038/10788 [01:44<03:22, 33.37it/s]Training CobwebTree:  37%|      | 4043/10788 [01:44<03:09, 35.51it/s]Training CobwebTree:  38%|      | 4047/10788 [01:44<03:12, 35.03it/s]Training CobwebTree:  38%|      | 4051/10788 [01:44<03:19, 33.85it/s]Training CobwebTree:  38%|      | 4055/10788 [01:45<03:15, 34.37it/s]Training CobwebTree:  38%|      | 4059/10788 [01:45<03:09, 35.45it/s]Training CobwebTree:  38%|      | 4063/10788 [01:45<03:15, 34.38it/s]Training CobwebTree:  38%|      | 4067/10788 [01:45<03:22, 33.18it/s]Training CobwebTree:  38%|      | 4071/10788 [01:45<03:21, 33.31it/s]Training CobwebTree:  38%|      | 4075/10788 [01:45<03:18, 33.77it/s]Training CobwebTree:  38%|      | 4079/10788 [01:45<03:18, 33.74it/s]Training CobwebTree:  38%|      | 4083/10788 [01:45<03:14, 34.40it/s]Training CobwebTree:  38%|      | 4087/10788 [01:46<03:16, 34.10it/s]Training CobwebTree:  38%|      | 4091/10788 [01:46<03:15, 34.26it/s]Training CobwebTree:  38%|      | 4095/10788 [01:46<03:15, 34.25it/s]Training CobwebTree:  38%|      | 4099/10788 [01:46<03:10, 35.08it/s]Training CobwebTree:  38%|      | 4103/10788 [01:46<03:19, 33.54it/s]Training CobwebTree:  38%|      | 4107/10788 [01:46<03:22, 33.04it/s]Training CobwebTree:  38%|      | 4111/10788 [01:46<03:20, 33.35it/s]Training CobwebTree:  38%|      | 4115/10788 [01:46<03:16, 34.00it/s]Training CobwebTree:  38%|      | 4119/10788 [01:46<03:19, 33.40it/s]Training CobwebTree:  38%|      | 4123/10788 [01:47<03:25, 32.39it/s]Training CobwebTree:  38%|      | 4128/10788 [01:47<03:11, 34.80it/s]Training CobwebTree:  38%|      | 4132/10788 [01:47<03:21, 33.10it/s]Training CobwebTree:  38%|      | 4136/10788 [01:47<03:17, 33.76it/s]Training CobwebTree:  38%|      | 4140/10788 [01:47<03:18, 33.47it/s]Training CobwebTree:  38%|      | 4144/10788 [01:47<03:14, 34.20it/s]Training CobwebTree:  38%|      | 4148/10788 [01:47<03:09, 35.12it/s]Training CobwebTree:  38%|      | 4152/10788 [01:47<03:12, 34.47it/s]Training CobwebTree:  39%|      | 4156/10788 [01:48<03:08, 35.22it/s]Training CobwebTree:  39%|      | 4160/10788 [01:48<03:11, 34.60it/s]Training CobwebTree:  39%|      | 4164/10788 [01:48<03:09, 34.87it/s]Training CobwebTree:  39%|      | 4168/10788 [01:48<03:07, 35.40it/s]Training CobwebTree:  39%|      | 4172/10788 [01:48<03:08, 35.02it/s]Training CobwebTree:  39%|      | 4176/10788 [01:48<03:06, 35.45it/s]Training CobwebTree:  39%|      | 4180/10788 [01:48<03:00, 36.68it/s]Training CobwebTree:  39%|      | 4184/10788 [01:48<03:03, 35.99it/s]Training CobwebTree:  39%|      | 4188/10788 [01:48<03:12, 34.30it/s]Training CobwebTree:  39%|      | 4192/10788 [01:49<03:14, 34.00it/s]Training CobwebTree:  39%|      | 4196/10788 [01:49<03:09, 34.75it/s]Training CobwebTree:  39%|      | 4200/10788 [01:49<03:11, 34.40it/s]Training CobwebTree:  39%|      | 4204/10788 [01:49<03:15, 33.60it/s]Training CobwebTree:  39%|      | 4208/10788 [01:49<03:14, 33.83it/s]Training CobwebTree:  39%|      | 4212/10788 [01:49<03:13, 33.91it/s]Training CobwebTree:  39%|      | 4216/10788 [01:49<03:09, 34.68it/s]Training CobwebTree:  39%|      | 4220/10788 [01:49<03:05, 35.42it/s]Training CobwebTree:  39%|      | 4224/10788 [01:50<03:09, 34.67it/s]Training CobwebTree:  39%|      | 4228/10788 [01:50<03:13, 33.86it/s]Training CobwebTree:  39%|      | 4232/10788 [01:50<03:05, 35.34it/s]Training CobwebTree:  39%|      | 4236/10788 [01:50<03:15, 33.47it/s]Training CobwebTree:  39%|      | 4240/10788 [01:50<03:12, 34.01it/s]Training CobwebTree:  39%|      | 4244/10788 [01:50<03:08, 34.63it/s]Training CobwebTree:  39%|      | 4248/10788 [01:50<03:09, 34.50it/s]Training CobwebTree:  39%|      | 4252/10788 [01:50<03:13, 33.75it/s]Training CobwebTree:  39%|      | 4256/10788 [01:50<03:12, 33.95it/s]Training CobwebTree:  39%|      | 4260/10788 [01:51<03:10, 34.33it/s]Training CobwebTree:  40%|      | 4264/10788 [01:51<03:09, 34.44it/s]Training CobwebTree:  40%|      | 4268/10788 [01:51<03:07, 34.84it/s]Training CobwebTree:  40%|      | 4272/10788 [01:51<03:08, 34.56it/s]Training CobwebTree:  40%|      | 4276/10788 [01:51<03:02, 35.70it/s]Training CobwebTree:  40%|      | 4280/10788 [01:51<03:01, 35.78it/s]Training CobwebTree:  40%|      | 4285/10788 [01:51<02:55, 37.02it/s]Training CobwebTree:  40%|      | 4289/10788 [01:51<03:04, 35.14it/s]Training CobwebTree:  40%|      | 4293/10788 [01:51<03:05, 34.99it/s]Training CobwebTree:  40%|      | 4297/10788 [01:52<03:13, 33.54it/s]Training CobwebTree:  40%|      | 4301/10788 [01:52<03:10, 33.99it/s]Training CobwebTree:  40%|      | 4305/10788 [01:52<03:17, 32.80it/s]Training CobwebTree:  40%|      | 4309/10788 [01:52<03:23, 31.80it/s]Training CobwebTree:  40%|      | 4313/10788 [01:52<03:14, 33.34it/s]Training CobwebTree:  40%|      | 4317/10788 [01:52<03:13, 33.46it/s]Training CobwebTree:  40%|      | 4321/10788 [01:52<03:13, 33.40it/s]Training CobwebTree:  40%|      | 4325/10788 [01:52<03:09, 34.17it/s]Training CobwebTree:  40%|      | 4329/10788 [01:53<03:25, 31.44it/s]Training CobwebTree:  40%|      | 4333/10788 [01:53<03:18, 32.55it/s]Training CobwebTree:  40%|      | 4337/10788 [01:53<03:20, 32.20it/s]Training CobwebTree:  40%|      | 4341/10788 [01:53<03:17, 32.57it/s]Training CobwebTree:  40%|      | 4345/10788 [01:53<03:15, 33.02it/s]Training CobwebTree:  40%|      | 4349/10788 [01:53<03:16, 32.69it/s]Training CobwebTree:  40%|      | 4353/10788 [01:53<03:17, 32.63it/s]Training CobwebTree:  40%|      | 4357/10788 [01:53<03:15, 32.87it/s]Training CobwebTree:  40%|      | 4361/10788 [01:54<03:11, 33.50it/s]Training CobwebTree:  40%|      | 4365/10788 [01:54<03:16, 32.71it/s]Training CobwebTree:  40%|      | 4369/10788 [01:54<03:24, 31.45it/s]Training CobwebTree:  41%|      | 4373/10788 [01:54<03:14, 32.99it/s]Training CobwebTree:  41%|      | 4377/10788 [01:54<03:07, 34.22it/s]Training CobwebTree:  41%|      | 4381/10788 [01:54<03:10, 33.59it/s]Training CobwebTree:  41%|      | 4385/10788 [01:54<03:14, 32.96it/s]Training CobwebTree:  41%|      | 4389/10788 [01:54<03:18, 32.17it/s]Training CobwebTree:  41%|      | 4393/10788 [01:55<03:14, 32.82it/s]Training CobwebTree:  41%|      | 4398/10788 [01:55<02:58, 35.83it/s]Training CobwebTree:  41%|      | 4402/10788 [01:55<03:04, 34.59it/s]Training CobwebTree:  41%|      | 4406/10788 [01:55<03:08, 33.89it/s]Training CobwebTree:  41%|      | 4410/10788 [01:55<03:00, 35.39it/s]Training CobwebTree:  41%|      | 4414/10788 [01:55<03:00, 35.33it/s]Training CobwebTree:  41%|      | 4418/10788 [01:55<03:01, 35.16it/s]Training CobwebTree:  41%|      | 4422/10788 [01:55<03:07, 33.96it/s]Training CobwebTree:  41%|      | 4426/10788 [01:55<03:14, 32.77it/s]Training CobwebTree:  41%|      | 4430/10788 [01:56<03:05, 34.21it/s]Training CobwebTree:  41%|      | 4434/10788 [01:56<02:59, 35.47it/s]Training CobwebTree:  41%|      | 4438/10788 [01:56<02:54, 36.43it/s]Training CobwebTree:  41%|      | 4442/10788 [01:56<03:02, 34.73it/s]Training CobwebTree:  41%|      | 4446/10788 [01:56<02:59, 35.28it/s]Training CobwebTree:  41%|      | 4450/10788 [01:56<03:06, 33.92it/s]Training CobwebTree:  41%|     | 4454/10788 [01:56<03:10, 33.20it/s]Training CobwebTree:  41%|     | 4458/10788 [01:56<03:14, 32.61it/s]Training CobwebTree:  41%|     | 4462/10788 [01:57<03:10, 33.23it/s]Training CobwebTree:  41%|     | 4466/10788 [01:57<03:12, 32.88it/s]Training CobwebTree:  41%|     | 4470/10788 [01:57<03:06, 33.86it/s]Training CobwebTree:  41%|     | 4474/10788 [01:57<03:06, 33.78it/s]Training CobwebTree:  42%|     | 4478/10788 [01:57<03:07, 33.62it/s]Training CobwebTree:  42%|     | 4482/10788 [01:57<03:02, 34.48it/s]Training CobwebTree:  42%|     | 4486/10788 [01:57<03:06, 33.84it/s]Training CobwebTree:  42%|     | 4490/10788 [01:57<03:05, 33.93it/s]Training CobwebTree:  42%|     | 4494/10788 [01:57<03:00, 34.92it/s]Training CobwebTree:  42%|     | 4498/10788 [01:58<02:53, 36.27it/s]Training CobwebTree:  42%|     | 4502/10788 [01:58<02:51, 36.57it/s]Training CobwebTree:  42%|     | 4506/10788 [01:58<02:53, 36.15it/s]Training CobwebTree:  42%|     | 4510/10788 [01:58<02:58, 35.20it/s]Training CobwebTree:  42%|     | 4514/10788 [01:58<02:56, 35.60it/s]Training CobwebTree:  42%|     | 4518/10788 [01:58<03:01, 34.49it/s]Training CobwebTree:  42%|     | 4522/10788 [01:58<02:55, 35.65it/s]Training CobwebTree:  42%|     | 4526/10788 [01:58<03:00, 34.73it/s]Training CobwebTree:  42%|     | 4530/10788 [01:58<03:00, 34.65it/s]Training CobwebTree:  42%|     | 4534/10788 [01:59<02:59, 34.80it/s]Training CobwebTree:  42%|     | 4538/10788 [01:59<02:57, 35.16it/s]Training CobwebTree:  42%|     | 4542/10788 [01:59<03:04, 33.83it/s]Training CobwebTree:  42%|     | 4546/10788 [01:59<03:06, 33.54it/s]Training CobwebTree:  42%|     | 4550/10788 [01:59<03:08, 33.10it/s]Training CobwebTree:  42%|     | 4554/10788 [01:59<03:14, 32.06it/s]Training CobwebTree:  42%|     | 4558/10788 [01:59<03:11, 32.61it/s]Training CobwebTree:  42%|     | 4562/10788 [01:59<03:08, 33.02it/s]Training CobwebTree:  42%|     | 4566/10788 [02:00<03:09, 32.88it/s]Training CobwebTree:  42%|     | 4570/10788 [02:00<03:07, 33.11it/s]Training CobwebTree:  42%|     | 4574/10788 [02:00<03:01, 34.20it/s]Training CobwebTree:  42%|     | 4578/10788 [02:00<03:07, 33.06it/s]Training CobwebTree:  42%|     | 4582/10788 [02:00<03:05, 33.50it/s]Training CobwebTree:  43%|     | 4587/10788 [02:00<02:54, 35.48it/s]Training CobwebTree:  43%|     | 4591/10788 [02:00<02:56, 35.03it/s]Training CobwebTree:  43%|     | 4595/10788 [02:00<02:54, 35.48it/s]Training CobwebTree:  43%|     | 4599/10788 [02:01<02:54, 35.53it/s]Training CobwebTree:  43%|     | 4603/10788 [02:01<03:01, 34.05it/s]Training CobwebTree:  43%|     | 4608/10788 [02:01<02:54, 35.47it/s]Training CobwebTree:  43%|     | 4613/10788 [02:01<02:48, 36.66it/s]Training CobwebTree:  43%|     | 4617/10788 [02:01<02:47, 36.86it/s]Training CobwebTree:  43%|     | 4621/10788 [02:01<02:47, 36.72it/s]Training CobwebTree:  43%|     | 4625/10788 [02:01<02:48, 36.67it/s]Training CobwebTree:  43%|     | 4629/10788 [02:01<02:50, 36.19it/s]Training CobwebTree:  43%|     | 4633/10788 [02:01<02:59, 34.34it/s]Training CobwebTree:  43%|     | 4637/10788 [02:02<02:53, 35.42it/s]Training CobwebTree:  43%|     | 4641/10788 [02:02<02:49, 36.27it/s]Training CobwebTree:  43%|     | 4645/10788 [02:02<02:55, 34.95it/s]Training CobwebTree:  43%|     | 4649/10788 [02:02<02:53, 35.48it/s]Training CobwebTree:  43%|     | 4653/10788 [02:02<02:48, 36.50it/s]Training CobwebTree:  43%|     | 4658/10788 [02:02<02:41, 38.04it/s]Training CobwebTree:  43%|     | 4662/10788 [02:02<02:44, 37.26it/s]Training CobwebTree:  43%|     | 4666/10788 [02:02<02:55, 34.83it/s]Training CobwebTree:  43%|     | 4670/10788 [02:03<02:56, 34.75it/s]Training CobwebTree:  43%|     | 4674/10788 [02:03<02:56, 34.73it/s]Training CobwebTree:  43%|     | 4678/10788 [02:03<02:58, 34.21it/s]Training CobwebTree:  43%|     | 4682/10788 [02:03<02:54, 35.02it/s]Training CobwebTree:  43%|     | 4686/10788 [02:03<02:59, 34.01it/s]Training CobwebTree:  43%|     | 4690/10788 [02:03<03:03, 33.30it/s]Training CobwebTree:  44%|     | 4694/10788 [02:03<03:08, 32.34it/s]Training CobwebTree:  44%|     | 4698/10788 [02:03<03:00, 33.66it/s]Training CobwebTree:  44%|     | 4702/10788 [02:03<03:00, 33.72it/s]Training CobwebTree:  44%|     | 4706/10788 [02:04<03:04, 32.91it/s]Training CobwebTree:  44%|     | 4711/10788 [02:04<02:54, 34.91it/s]Training CobwebTree:  44%|     | 4715/10788 [02:04<02:55, 34.70it/s]Training CobwebTree:  44%|     | 4719/10788 [02:04<02:52, 35.09it/s]Training CobwebTree:  44%|     | 4723/10788 [02:04<02:49, 35.76it/s]Training CobwebTree:  44%|     | 4727/10788 [02:04<02:51, 35.26it/s]Training CobwebTree:  44%|     | 4731/10788 [02:04<02:52, 35.10it/s]Training CobwebTree:  44%|     | 4735/10788 [02:04<02:58, 33.97it/s]Training CobwebTree:  44%|     | 4739/10788 [02:05<02:58, 33.82it/s]Training CobwebTree:  44%|     | 4743/10788 [02:05<02:57, 34.11it/s]Training CobwebTree:  44%|     | 4747/10788 [02:05<03:00, 33.47it/s]Training CobwebTree:  44%|     | 4751/10788 [02:05<03:05, 32.51it/s]Training CobwebTree:  44%|     | 4755/10788 [02:05<02:58, 33.89it/s]Training CobwebTree:  44%|     | 4759/10788 [02:05<03:06, 32.29it/s]Training CobwebTree:  44%|     | 4763/10788 [02:05<03:11, 31.50it/s]Training CobwebTree:  44%|     | 4767/10788 [02:05<03:05, 32.39it/s]Training CobwebTree:  44%|     | 4772/10788 [02:06<02:54, 34.52it/s]Training CobwebTree:  44%|     | 4776/10788 [02:06<03:00, 33.37it/s]Training CobwebTree:  44%|     | 4781/10788 [02:06<02:49, 35.54it/s]Training CobwebTree:  44%|     | 4785/10788 [02:06<02:46, 36.03it/s]Training CobwebTree:  44%|     | 4789/10788 [02:06<02:51, 34.90it/s]Training CobwebTree:  44%|     | 4793/10788 [02:06<02:50, 35.12it/s]Training CobwebTree:  44%|     | 4797/10788 [02:06<02:50, 35.07it/s]Training CobwebTree:  45%|     | 4801/10788 [02:06<02:55, 34.04it/s]Training CobwebTree:  45%|     | 4805/10788 [02:06<03:04, 32.46it/s]Training CobwebTree:  45%|     | 4809/10788 [02:07<02:57, 33.66it/s]Training CobwebTree:  45%|     | 4813/10788 [02:07<02:57, 33.65it/s]Training CobwebTree:  45%|     | 4817/10788 [02:07<03:00, 33.12it/s]Training CobwebTree:  45%|     | 4821/10788 [02:07<02:56, 33.75it/s]Training CobwebTree:  45%|     | 4825/10788 [02:07<02:55, 34.02it/s]Training CobwebTree:  45%|     | 4829/10788 [02:07<02:54, 34.23it/s]Training CobwebTree:  45%|     | 4833/10788 [02:07<02:58, 33.30it/s]Training CobwebTree:  45%|     | 4837/10788 [02:07<02:58, 33.27it/s]Training CobwebTree:  45%|     | 4841/10788 [02:08<02:51, 34.61it/s]Training CobwebTree:  45%|     | 4845/10788 [02:08<02:55, 33.86it/s]Training CobwebTree:  45%|     | 4849/10788 [02:08<02:51, 34.73it/s]Training CobwebTree:  45%|     | 4853/10788 [02:08<02:50, 34.84it/s]Training CobwebTree:  45%|     | 4858/10788 [02:08<02:43, 36.22it/s]Training CobwebTree:  45%|     | 4862/10788 [02:08<02:45, 35.86it/s]Training CobwebTree:  45%|     | 4866/10788 [02:08<02:46, 35.66it/s]Training CobwebTree:  45%|     | 4870/10788 [02:08<02:46, 35.49it/s]Training CobwebTree:  45%|     | 4874/10788 [02:08<02:51, 34.52it/s]Training CobwebTree:  45%|     | 4878/10788 [02:09<02:55, 33.71it/s]Training CobwebTree:  45%|     | 4882/10788 [02:09<02:59, 32.86it/s]Training CobwebTree:  45%|     | 4887/10788 [02:09<02:46, 35.49it/s]Training CobwebTree:  45%|     | 4891/10788 [02:09<02:49, 34.80it/s]Training CobwebTree:  45%|     | 4895/10788 [02:09<02:55, 33.64it/s]Training CobwebTree:  45%|     | 4899/10788 [02:09<03:02, 32.26it/s]Training CobwebTree:  45%|     | 4903/10788 [02:09<03:06, 31.49it/s]Training CobwebTree:  45%|     | 4907/10788 [02:09<03:05, 31.75it/s]Training CobwebTree:  46%|     | 4911/10788 [02:10<02:56, 33.21it/s]Training CobwebTree:  46%|     | 4915/10788 [02:10<02:56, 33.26it/s]Training CobwebTree:  46%|     | 4919/10788 [02:10<02:58, 32.93it/s]Training CobwebTree:  46%|     | 4923/10788 [02:10<02:59, 32.64it/s]Training CobwebTree:  46%|     | 4927/10788 [02:10<02:57, 33.03it/s]Training CobwebTree:  46%|     | 4931/10788 [02:10<03:05, 31.64it/s]Training CobwebTree:  46%|     | 4935/10788 [02:10<02:59, 32.55it/s]Training CobwebTree:  46%|     | 4939/10788 [02:10<02:54, 33.49it/s]Training CobwebTree:  46%|     | 4943/10788 [02:11<03:02, 32.11it/s]Training CobwebTree:  46%|     | 4947/10788 [02:11<03:02, 31.98it/s]Training CobwebTree:  46%|     | 4951/10788 [02:11<02:53, 33.60it/s]Training CobwebTree:  46%|     | 4955/10788 [02:11<02:46, 35.03it/s]Training CobwebTree:  46%|     | 4959/10788 [02:11<02:52, 33.75it/s]Training CobwebTree:  46%|     | 4963/10788 [02:11<02:56, 33.09it/s]Training CobwebTree:  46%|     | 4967/10788 [02:11<02:54, 33.31it/s]Training CobwebTree:  46%|     | 4971/10788 [02:11<03:01, 32.09it/s]Training CobwebTree:  46%|     | 4975/10788 [02:12<03:00, 32.20it/s]Training CobwebTree:  46%|     | 4979/10788 [02:12<03:02, 31.76it/s]Training CobwebTree:  46%|     | 4983/10788 [02:12<03:01, 31.90it/s]Training CobwebTree:  46%|     | 4987/10788 [02:12<02:58, 32.55it/s]Training CobwebTree:  46%|     | 4991/10788 [02:12<03:01, 31.86it/s]Training CobwebTree:  46%|     | 4995/10788 [02:12<02:59, 32.22it/s]Training CobwebTree:  46%|     | 4999/10788 [02:12<03:03, 31.60it/s]Training CobwebTree:  46%|     | 5003/10788 [02:12<02:58, 32.36it/s]Training CobwebTree:  46%|     | 5007/10788 [02:13<02:50, 33.83it/s]Training CobwebTree:  46%|     | 5011/10788 [02:13<02:46, 34.67it/s]Training CobwebTree:  46%|     | 5015/10788 [02:13<02:44, 35.19it/s]Training CobwebTree:  47%|     | 5019/10788 [02:13<02:47, 34.51it/s]Training CobwebTree:  47%|     | 5023/10788 [02:13<02:40, 35.85it/s]Training CobwebTree:  47%|     | 5027/10788 [02:13<02:38, 36.36it/s]Training CobwebTree:  47%|     | 5031/10788 [02:13<02:44, 34.94it/s]Training CobwebTree:  47%|     | 5035/10788 [02:13<02:57, 32.32it/s]Training CobwebTree:  47%|     | 5039/10788 [02:13<02:58, 32.12it/s]Training CobwebTree:  47%|     | 5043/10788 [02:14<02:56, 32.61it/s]Training CobwebTree:  47%|     | 5047/10788 [02:14<02:54, 32.99it/s]Training CobwebTree:  47%|     | 5051/10788 [02:14<02:50, 33.73it/s]Training CobwebTree:  47%|     | 5055/10788 [02:14<03:06, 30.81it/s]Training CobwebTree:  47%|     | 5059/10788 [02:14<03:01, 31.65it/s]Training CobwebTree:  47%|     | 5063/10788 [02:14<02:53, 33.01it/s]Training CobwebTree:  47%|     | 5067/10788 [02:14<02:54, 32.73it/s]Training CobwebTree:  47%|     | 5071/10788 [02:14<02:49, 33.65it/s]Training CobwebTree:  47%|     | 5075/10788 [02:15<02:51, 33.34it/s]Training CobwebTree:  47%|     | 5079/10788 [02:15<02:49, 33.71it/s]Training CobwebTree:  47%|     | 5083/10788 [02:15<02:53, 32.90it/s]Training CobwebTree:  47%|     | 5087/10788 [02:15<02:52, 33.12it/s]Training CobwebTree:  47%|     | 5091/10788 [02:15<02:43, 34.91it/s]Training CobwebTree:  47%|     | 5095/10788 [02:15<02:50, 33.35it/s]Training CobwebTree:  47%|     | 5099/10788 [02:15<02:45, 34.27it/s]Training CobwebTree:  47%|     | 5104/10788 [02:15<02:36, 36.26it/s]Training CobwebTree:  47%|     | 5109/10788 [02:16<02:31, 37.46it/s]Training CobwebTree:  47%|     | 5113/10788 [02:16<02:36, 36.20it/s]Training CobwebTree:  47%|     | 5117/10788 [02:16<02:42, 34.86it/s]Training CobwebTree:  47%|     | 5121/10788 [02:16<02:46, 34.09it/s]Training CobwebTree:  48%|     | 5125/10788 [02:16<02:48, 33.60it/s]Training CobwebTree:  48%|     | 5129/10788 [02:16<02:53, 32.63it/s]Training CobwebTree:  48%|     | 5133/10788 [02:16<02:45, 34.08it/s]Training CobwebTree:  48%|     | 5138/10788 [02:16<02:35, 36.40it/s]Training CobwebTree:  48%|     | 5142/10788 [02:16<02:35, 36.27it/s]Training CobwebTree:  48%|     | 5146/10788 [02:17<02:46, 33.89it/s]Training CobwebTree:  48%|     | 5150/10788 [02:17<02:50, 32.98it/s]Training CobwebTree:  48%|     | 5154/10788 [02:17<02:48, 33.49it/s]Training CobwebTree:  48%|     | 5158/10788 [02:17<02:52, 32.64it/s]Training CobwebTree:  48%|     | 5162/10788 [02:17<02:47, 33.50it/s]Training CobwebTree:  48%|     | 5166/10788 [02:17<02:49, 33.15it/s]Training CobwebTree:  48%|     | 5170/10788 [02:17<02:53, 32.38it/s]Training CobwebTree:  48%|     | 5174/10788 [02:17<02:50, 32.89it/s]Training CobwebTree:  48%|     | 5178/10788 [02:18<02:51, 32.76it/s]Training CobwebTree:  48%|     | 5182/10788 [02:18<02:55, 31.99it/s]Training CobwebTree:  48%|     | 5186/10788 [02:18<02:51, 32.69it/s]Training CobwebTree:  48%|     | 5190/10788 [02:18<02:48, 33.19it/s]Training CobwebTree:  48%|     | 5194/10788 [02:18<02:51, 32.68it/s]Training CobwebTree:  48%|     | 5198/10788 [02:18<02:44, 33.91it/s]Training CobwebTree:  48%|     | 5202/10788 [02:18<02:43, 34.08it/s]Training CobwebTree:  48%|     | 5206/10788 [02:18<02:46, 33.60it/s]Training CobwebTree:  48%|     | 5210/10788 [02:19<02:44, 33.82it/s]Training CobwebTree:  48%|     | 5214/10788 [02:19<02:44, 33.98it/s]Training CobwebTree:  48%|     | 5219/10788 [02:19<02:34, 36.11it/s]Training CobwebTree:  48%|     | 5223/10788 [02:19<02:42, 34.35it/s]Training CobwebTree:  48%|     | 5227/10788 [02:19<02:36, 35.51it/s]Training CobwebTree:  48%|     | 5231/10788 [02:19<02:43, 34.05it/s]Training CobwebTree:  49%|     | 5235/10788 [02:19<02:44, 33.86it/s]Training CobwebTree:  49%|     | 5239/10788 [02:19<02:51, 32.45it/s]Training CobwebTree:  49%|     | 5243/10788 [02:20<02:47, 33.07it/s]Training CobwebTree:  49%|     | 5247/10788 [02:20<02:50, 32.55it/s]Training CobwebTree:  49%|     | 5251/10788 [02:20<02:51, 32.35it/s]Training CobwebTree:  49%|     | 5255/10788 [02:20<02:47, 32.94it/s]Training CobwebTree:  49%|     | 5259/10788 [02:20<02:52, 32.14it/s]Training CobwebTree:  49%|     | 5263/10788 [02:20<02:54, 31.69it/s]Training CobwebTree:  49%|     | 5267/10788 [02:20<02:50, 32.30it/s]Training CobwebTree:  49%|     | 5271/10788 [02:20<02:53, 31.84it/s]Training CobwebTree:  49%|     | 5275/10788 [02:21<02:51, 32.20it/s]Training CobwebTree:  49%|     | 5279/10788 [02:21<02:53, 31.66it/s]Training CobwebTree:  49%|     | 5283/10788 [02:21<02:54, 31.60it/s]Training CobwebTree:  49%|     | 5288/10788 [02:21<02:38, 34.67it/s]Training CobwebTree:  49%|     | 5292/10788 [02:21<02:34, 35.50it/s]Training CobwebTree:  49%|     | 5296/10788 [02:21<02:34, 35.44it/s]Training CobwebTree:  49%|     | 5300/10788 [02:21<02:33, 35.78it/s]Training CobwebTree:  49%|     | 5304/10788 [02:21<02:35, 35.25it/s]Training CobwebTree:  49%|     | 5308/10788 [02:21<02:40, 34.12it/s]Training CobwebTree:  49%|     | 5312/10788 [02:22<02:40, 34.10it/s]Training CobwebTree:  49%|     | 5316/10788 [02:22<02:39, 34.27it/s]Training CobwebTree:  49%|     | 5320/10788 [02:22<02:47, 32.63it/s]Training CobwebTree:  49%|     | 5324/10788 [02:22<02:51, 31.93it/s]Training CobwebTree:  49%|     | 5328/10788 [02:22<02:49, 32.31it/s]Training CobwebTree:  49%|     | 5332/10788 [02:22<02:48, 32.38it/s]Training CobwebTree:  49%|     | 5336/10788 [02:22<02:48, 32.28it/s]Training CobwebTree:  49%|     | 5340/10788 [02:22<02:49, 32.19it/s]Training CobwebTree:  50%|     | 5344/10788 [02:23<02:51, 31.66it/s]Training CobwebTree:  50%|     | 5348/10788 [02:23<02:48, 32.23it/s]Training CobwebTree:  50%|     | 5352/10788 [02:23<02:53, 31.40it/s]Training CobwebTree:  50%|     | 5356/10788 [02:23<02:46, 32.54it/s]Training CobwebTree:  50%|     | 5360/10788 [02:23<02:48, 32.22it/s]Training CobwebTree:  50%|     | 5364/10788 [02:23<02:43, 33.15it/s]Training CobwebTree:  50%|     | 5368/10788 [02:23<02:42, 33.29it/s]Training CobwebTree:  50%|     | 5372/10788 [02:23<02:39, 33.98it/s]Training CobwebTree:  50%|     | 5376/10788 [02:24<02:41, 33.48it/s]Training CobwebTree:  50%|     | 5380/10788 [02:24<02:45, 32.78it/s]Training CobwebTree:  50%|     | 5384/10788 [02:24<02:42, 33.24it/s]Training CobwebTree:  50%|     | 5388/10788 [02:24<02:39, 33.88it/s]Training CobwebTree:  50%|     | 5392/10788 [02:24<02:40, 33.65it/s]Training CobwebTree:  50%|     | 5396/10788 [02:24<02:39, 33.80it/s]Training CobwebTree:  50%|     | 5400/10788 [02:24<02:40, 33.56it/s]Training CobwebTree:  50%|     | 5405/10788 [02:24<02:31, 35.48it/s]Training CobwebTree:  50%|     | 5409/10788 [02:25<02:32, 35.17it/s]Training CobwebTree:  50%|     | 5413/10788 [02:25<02:35, 34.60it/s]Training CobwebTree:  50%|     | 5417/10788 [02:25<02:36, 34.36it/s]Training CobwebTree:  50%|     | 5421/10788 [02:25<02:45, 32.37it/s]Training CobwebTree:  50%|     | 5425/10788 [02:25<02:49, 31.61it/s]Training CobwebTree:  50%|     | 5429/10788 [02:25<02:43, 32.77it/s]Training CobwebTree:  50%|     | 5433/10788 [02:25<02:49, 31.67it/s]Training CobwebTree:  50%|     | 5437/10788 [02:25<02:50, 31.31it/s]Training CobwebTree:  50%|     | 5441/10788 [02:26<02:47, 31.98it/s]Training CobwebTree:  50%|     | 5445/10788 [02:26<02:47, 31.96it/s]Training CobwebTree:  51%|     | 5449/10788 [02:26<02:48, 31.63it/s]Training CobwebTree:  51%|     | 5453/10788 [02:26<02:48, 31.72it/s]Training CobwebTree:  51%|     | 5457/10788 [02:26<02:56, 30.20it/s]Training CobwebTree:  51%|     | 5461/10788 [02:26<02:46, 31.94it/s]Training CobwebTree:  51%|     | 5465/10788 [02:26<02:45, 32.16it/s]Training CobwebTree:  51%|     | 5469/10788 [02:26<02:44, 32.28it/s]Training CobwebTree:  51%|     | 5473/10788 [02:27<02:43, 32.46it/s]Training CobwebTree:  51%|     | 5477/10788 [02:27<02:43, 32.55it/s]Training CobwebTree:  51%|     | 5481/10788 [02:27<02:44, 32.23it/s]Training CobwebTree:  51%|     | 5485/10788 [02:27<02:41, 32.93it/s]Training CobwebTree:  51%|     | 5489/10788 [02:27<02:39, 33.31it/s]Training CobwebTree:  51%|     | 5493/10788 [02:27<02:37, 33.53it/s]Training CobwebTree:  51%|     | 5497/10788 [02:27<02:38, 33.39it/s]Training CobwebTree:  51%|     | 5501/10788 [02:27<02:35, 34.08it/s]Training CobwebTree:  51%|     | 5505/10788 [02:27<02:35, 33.92it/s]Training CobwebTree:  51%|     | 5509/10788 [02:28<02:42, 32.55it/s]Training CobwebTree:  51%|     | 5513/10788 [02:28<02:42, 32.39it/s]Training CobwebTree:  51%|     | 5517/10788 [02:28<02:38, 33.33it/s]Training CobwebTree:  51%|     | 5521/10788 [02:28<02:39, 32.93it/s]Training CobwebTree:  51%|     | 5525/10788 [02:28<02:36, 33.54it/s]Training CobwebTree:  51%|    | 5529/10788 [02:28<02:39, 32.88it/s]Training CobwebTree:  51%|    | 5533/10788 [02:28<02:35, 33.88it/s]Training CobwebTree:  51%|    | 5537/10788 [02:28<02:38, 33.17it/s]Training CobwebTree:  51%|    | 5541/10788 [02:29<02:46, 31.46it/s]Training CobwebTree:  51%|    | 5545/10788 [02:29<02:41, 32.46it/s]Training CobwebTree:  51%|    | 5549/10788 [02:29<02:36, 33.55it/s]Training CobwebTree:  51%|    | 5553/10788 [02:29<02:38, 33.12it/s]Training CobwebTree:  52%|    | 5557/10788 [02:29<02:34, 33.82it/s]Training CobwebTree:  52%|    | 5561/10788 [02:29<02:35, 33.61it/s]Training CobwebTree:  52%|    | 5565/10788 [02:29<02:38, 32.89it/s]Training CobwebTree:  52%|    | 5569/10788 [02:29<02:41, 32.34it/s]Training CobwebTree:  52%|    | 5573/10788 [02:30<02:35, 33.58it/s]Training CobwebTree:  52%|    | 5577/10788 [02:30<02:35, 33.48it/s]Training CobwebTree:  52%|    | 5581/10788 [02:30<02:37, 33.08it/s]Training CobwebTree:  52%|    | 5585/10788 [02:30<02:38, 32.81it/s]Training CobwebTree:  52%|    | 5589/10788 [02:30<02:44, 31.66it/s]Training CobwebTree:  52%|    | 5593/10788 [02:30<02:39, 32.50it/s]Training CobwebTree:  52%|    | 5597/10788 [02:30<02:35, 33.29it/s]Training CobwebTree:  52%|    | 5601/10788 [02:30<02:39, 32.55it/s]Training CobwebTree:  52%|    | 5605/10788 [02:31<02:40, 32.20it/s]Training CobwebTree:  52%|    | 5609/10788 [02:31<02:36, 33.01it/s]Training CobwebTree:  52%|    | 5613/10788 [02:31<02:35, 33.34it/s]Training CobwebTree:  52%|    | 5617/10788 [02:31<02:38, 32.57it/s]Training CobwebTree:  52%|    | 5621/10788 [02:31<02:40, 32.21it/s]Training CobwebTree:  52%|    | 5625/10788 [02:31<02:37, 32.81it/s]Training CobwebTree:  52%|    | 5629/10788 [02:31<02:32, 33.89it/s]Training CobwebTree:  52%|    | 5633/10788 [02:31<02:33, 33.51it/s]Training CobwebTree:  52%|    | 5637/10788 [02:32<02:37, 32.72it/s]Training CobwebTree:  52%|    | 5641/10788 [02:32<02:41, 31.97it/s]Training CobwebTree:  52%|    | 5645/10788 [02:32<02:37, 32.67it/s]Training CobwebTree:  52%|    | 5649/10788 [02:32<02:32, 33.67it/s]Training CobwebTree:  52%|    | 5653/10788 [02:32<02:28, 34.59it/s]Training CobwebTree:  52%|    | 5657/10788 [02:32<02:23, 35.72it/s]Training CobwebTree:  52%|    | 5661/10788 [02:32<02:25, 35.19it/s]Training CobwebTree:  53%|    | 5665/10788 [02:32<02:31, 33.76it/s]Training CobwebTree:  53%|    | 5669/10788 [02:32<02:35, 32.95it/s]Training CobwebTree:  53%|    | 5673/10788 [02:33<02:33, 33.35it/s]Training CobwebTree:  53%|    | 5677/10788 [02:33<02:34, 33.00it/s]Training CobwebTree:  53%|    | 5681/10788 [02:33<02:36, 32.61it/s]Training CobwebTree:  53%|    | 5685/10788 [02:33<02:40, 31.83it/s]Training CobwebTree:  53%|    | 5689/10788 [02:33<02:44, 31.03it/s]Training CobwebTree:  53%|    | 5693/10788 [02:33<02:39, 31.87it/s]Training CobwebTree:  53%|    | 5697/10788 [02:33<02:36, 32.62it/s]Training CobwebTree:  53%|    | 5701/10788 [02:33<02:35, 32.67it/s]Training CobwebTree:  53%|    | 5705/10788 [02:34<02:37, 32.25it/s]Training CobwebTree:  53%|    | 5709/10788 [02:34<02:39, 31.78it/s]Training CobwebTree:  53%|    | 5713/10788 [02:34<02:35, 32.72it/s]Training CobwebTree:  53%|    | 5717/10788 [02:34<02:35, 32.71it/s]Training CobwebTree:  53%|    | 5721/10788 [02:34<02:33, 33.08it/s]Training CobwebTree:  53%|    | 5725/10788 [02:34<02:26, 34.47it/s]Training CobwebTree:  53%|    | 5729/10788 [02:34<02:26, 34.57it/s]Training CobwebTree:  53%|    | 5733/10788 [02:34<02:28, 33.98it/s]Training CobwebTree:  53%|    | 5737/10788 [02:35<02:26, 34.48it/s]Training CobwebTree:  53%|    | 5741/10788 [02:35<02:23, 35.21it/s]Training CobwebTree:  53%|    | 5745/10788 [02:35<02:29, 33.69it/s]Training CobwebTree:  53%|    | 5749/10788 [02:35<02:30, 33.41it/s]Training CobwebTree:  53%|    | 5753/10788 [02:35<02:26, 34.49it/s]Training CobwebTree:  53%|    | 5757/10788 [02:35<02:27, 34.06it/s]Training CobwebTree:  53%|    | 5761/10788 [02:35<02:28, 33.86it/s]Training CobwebTree:  53%|    | 5765/10788 [02:35<02:23, 35.06it/s]Training CobwebTree:  53%|    | 5769/10788 [02:35<02:28, 33.72it/s]Training CobwebTree:  54%|    | 5773/10788 [02:36<02:28, 33.72it/s]Training CobwebTree:  54%|    | 5777/10788 [02:36<02:22, 35.20it/s]Training CobwebTree:  54%|    | 5781/10788 [02:36<02:21, 35.30it/s]Training CobwebTree:  54%|    | 5785/10788 [02:36<02:24, 34.71it/s]Training CobwebTree:  54%|    | 5789/10788 [02:36<02:23, 34.87it/s]Training CobwebTree:  54%|    | 5793/10788 [02:36<02:29, 33.49it/s]Training CobwebTree:  54%|    | 5797/10788 [02:36<02:26, 33.98it/s]Training CobwebTree:  54%|    | 5801/10788 [02:36<02:26, 34.13it/s]Training CobwebTree:  54%|    | 5805/10788 [02:37<02:29, 33.33it/s]Training CobwebTree:  54%|    | 5809/10788 [02:37<02:27, 33.76it/s]Training CobwebTree:  54%|    | 5813/10788 [02:37<02:29, 33.17it/s]Training CobwebTree:  54%|    | 5817/10788 [02:37<02:35, 31.88it/s]Training CobwebTree:  54%|    | 5821/10788 [02:37<02:33, 32.42it/s]Training CobwebTree:  54%|    | 5825/10788 [02:37<02:32, 32.58it/s]Training CobwebTree:  54%|    | 5829/10788 [02:37<02:27, 33.69it/s]Training CobwebTree:  54%|    | 5833/10788 [02:37<02:33, 32.22it/s]Training CobwebTree:  54%|    | 5837/10788 [02:37<02:31, 32.59it/s]Training CobwebTree:  54%|    | 5841/10788 [02:38<02:34, 32.08it/s]Training CobwebTree:  54%|    | 5845/10788 [02:38<02:36, 31.53it/s]Training CobwebTree:  54%|    | 5849/10788 [02:38<02:28, 33.37it/s]Training CobwebTree:  54%|    | 5853/10788 [02:38<02:32, 32.46it/s]Training CobwebTree:  54%|    | 5857/10788 [02:38<02:26, 33.69it/s]Training CobwebTree:  54%|    | 5861/10788 [02:38<02:28, 33.19it/s]Training CobwebTree:  54%|    | 5865/10788 [02:38<02:37, 31.29it/s]Training CobwebTree:  54%|    | 5869/10788 [02:38<02:32, 32.34it/s]Training CobwebTree:  54%|    | 5873/10788 [02:39<02:32, 32.23it/s]Training CobwebTree:  54%|    | 5877/10788 [02:39<02:24, 33.97it/s]Training CobwebTree:  55%|    | 5881/10788 [02:39<02:27, 33.22it/s]Training CobwebTree:  55%|    | 5885/10788 [02:39<02:28, 33.04it/s]Training CobwebTree:  55%|    | 5889/10788 [02:39<02:31, 32.34it/s]Training CobwebTree:  55%|    | 5893/10788 [02:39<02:37, 31.13it/s]Training CobwebTree:  55%|    | 5897/10788 [02:39<02:35, 31.49it/s]Training CobwebTree:  55%|    | 5901/10788 [02:39<02:36, 31.22it/s]Training CobwebTree:  55%|    | 5905/10788 [02:40<02:37, 30.96it/s]Training CobwebTree:  55%|    | 5909/10788 [02:40<02:37, 30.95it/s]Training CobwebTree:  55%|    | 5913/10788 [02:40<02:41, 30.17it/s]Training CobwebTree:  55%|    | 5917/10788 [02:40<02:39, 30.57it/s]Training CobwebTree:  55%|    | 5921/10788 [02:40<02:33, 31.80it/s]Training CobwebTree:  55%|    | 5925/10788 [02:40<02:37, 30.80it/s]Training CobwebTree:  55%|    | 5929/10788 [02:40<02:31, 32.09it/s]Training CobwebTree:  55%|    | 5933/10788 [02:40<02:28, 32.79it/s]Training CobwebTree:  55%|    | 5937/10788 [02:41<02:30, 32.33it/s]Training CobwebTree:  55%|    | 5941/10788 [02:41<02:28, 32.73it/s]Training CobwebTree:  55%|    | 5945/10788 [02:41<02:31, 31.94it/s]Training CobwebTree:  55%|    | 5949/10788 [02:41<02:31, 31.95it/s]Training CobwebTree:  55%|    | 5953/10788 [02:41<02:27, 32.74it/s]Training CobwebTree:  55%|    | 5957/10788 [02:41<02:21, 34.12it/s]Training CobwebTree:  55%|    | 5961/10788 [02:41<02:25, 33.13it/s]Training CobwebTree:  55%|    | 5965/10788 [02:41<02:21, 34.07it/s]Training CobwebTree:  55%|    | 5969/10788 [02:42<02:23, 33.49it/s]Training CobwebTree:  55%|    | 5973/10788 [02:42<02:28, 32.38it/s]Training CobwebTree:  55%|    | 5977/10788 [02:42<02:21, 34.00it/s]Training CobwebTree:  55%|    | 5981/10788 [02:42<02:29, 32.11it/s]Training CobwebTree:  55%|    | 5985/10788 [02:42<02:28, 32.43it/s]Training CobwebTree:  56%|    | 5989/10788 [02:42<02:30, 31.89it/s]Training CobwebTree:  56%|    | 5993/10788 [02:42<02:27, 32.53it/s]Training CobwebTree:  56%|    | 5997/10788 [02:42<02:26, 32.69it/s]Training CobwebTree:  56%|    | 6001/10788 [02:43<02:28, 32.29it/s]Training CobwebTree:  56%|    | 6005/10788 [02:43<02:27, 32.48it/s]Training CobwebTree:  56%|    | 6009/10788 [02:43<02:33, 31.16it/s]Training CobwebTree:  56%|    | 6013/10788 [02:43<02:35, 30.74it/s]Training CobwebTree:  56%|    | 6017/10788 [02:43<02:35, 30.72it/s]Training CobwebTree:  56%|    | 6021/10788 [02:43<02:37, 30.29it/s]Training CobwebTree:  56%|    | 6025/10788 [02:43<02:42, 29.35it/s]Training CobwebTree:  56%|    | 6029/10788 [02:43<02:33, 31.10it/s]Training CobwebTree:  56%|    | 6033/10788 [02:44<02:29, 31.81it/s]Training CobwebTree:  56%|    | 6037/10788 [02:44<02:22, 33.43it/s]Training CobwebTree:  56%|    | 6041/10788 [02:44<02:22, 33.34it/s]Training CobwebTree:  56%|    | 6045/10788 [02:44<02:21, 33.60it/s]Training CobwebTree:  56%|    | 6049/10788 [02:44<02:27, 32.11it/s]Training CobwebTree:  56%|    | 6053/10788 [02:44<02:23, 32.88it/s]Training CobwebTree:  56%|    | 6057/10788 [02:44<02:27, 32.13it/s]Training CobwebTree:  56%|    | 6061/10788 [02:44<02:28, 31.93it/s]Training CobwebTree:  56%|    | 6065/10788 [02:45<02:27, 32.05it/s]Training CobwebTree:  56%|    | 6069/10788 [02:45<02:29, 31.59it/s]Training CobwebTree:  56%|    | 6073/10788 [02:45<02:23, 32.95it/s]Training CobwebTree:  56%|    | 6077/10788 [02:45<02:30, 31.39it/s]Training CobwebTree:  56%|    | 6081/10788 [02:45<02:29, 31.52it/s]Training CobwebTree:  56%|    | 6085/10788 [02:45<02:26, 32.07it/s]Training CobwebTree:  56%|    | 6089/10788 [02:45<02:31, 31.05it/s]Training CobwebTree:  56%|    | 6093/10788 [02:45<02:30, 31.23it/s]Training CobwebTree:  57%|    | 6097/10788 [02:46<02:28, 31.53it/s]Training CobwebTree:  57%|    | 6101/10788 [02:46<02:21, 33.10it/s]Training CobwebTree:  57%|    | 6105/10788 [02:46<02:17, 33.97it/s]Training CobwebTree:  57%|    | 6109/10788 [02:46<02:25, 32.14it/s]Training CobwebTree:  57%|    | 6113/10788 [02:46<02:26, 31.93it/s]Training CobwebTree:  57%|    | 6117/10788 [02:46<02:32, 30.67it/s]Training CobwebTree:  57%|    | 6121/10788 [02:46<02:33, 30.40it/s]Training CobwebTree:  57%|    | 6125/10788 [02:46<02:29, 31.26it/s]Training CobwebTree:  57%|    | 6129/10788 [02:47<02:26, 31.79it/s]Training CobwebTree:  57%|    | 6133/10788 [02:47<02:22, 32.69it/s]Training CobwebTree:  57%|    | 6137/10788 [02:47<02:21, 32.95it/s]Training CobwebTree:  57%|    | 6141/10788 [02:47<02:23, 32.42it/s]Training CobwebTree:  57%|    | 6145/10788 [02:47<02:24, 32.22it/s]Training CobwebTree:  57%|    | 6149/10788 [02:47<02:21, 32.69it/s]Training CobwebTree:  57%|    | 6153/10788 [02:47<02:21, 32.76it/s]Training CobwebTree:  57%|    | 6157/10788 [02:47<02:24, 32.01it/s]Training CobwebTree:  57%|    | 6161/10788 [02:48<02:25, 31.75it/s]Training CobwebTree:  57%|    | 6165/10788 [02:48<02:23, 32.29it/s]Training CobwebTree:  57%|    | 6169/10788 [02:48<02:21, 32.61it/s]Training CobwebTree:  57%|    | 6173/10788 [02:48<02:24, 31.97it/s]Training CobwebTree:  57%|    | 6177/10788 [02:48<02:26, 31.43it/s]Training CobwebTree:  57%|    | 6181/10788 [02:48<02:25, 31.77it/s]Training CobwebTree:  57%|    | 6185/10788 [02:48<02:24, 31.95it/s]Training CobwebTree:  57%|    | 6189/10788 [02:48<02:29, 30.77it/s]Training CobwebTree:  57%|    | 6193/10788 [02:49<02:30, 30.51it/s]Training CobwebTree:  57%|    | 6197/10788 [02:49<02:26, 31.36it/s]Training CobwebTree:  57%|    | 6201/10788 [02:49<02:28, 30.88it/s]Training CobwebTree:  58%|    | 6205/10788 [02:49<02:20, 32.55it/s]Training CobwebTree:  58%|    | 6209/10788 [02:49<02:19, 32.90it/s]Training CobwebTree:  58%|    | 6213/10788 [02:49<02:18, 33.07it/s]Training CobwebTree:  58%|    | 6217/10788 [02:49<02:21, 32.28it/s]Training CobwebTree:  58%|    | 6221/10788 [02:49<02:23, 31.74it/s]Training CobwebTree:  58%|    | 6225/10788 [02:50<02:31, 30.06it/s]Training CobwebTree:  58%|    | 6229/10788 [02:50<02:27, 30.81it/s]Training CobwebTree:  58%|    | 6233/10788 [02:50<02:32, 29.86it/s]Training CobwebTree:  58%|    | 6237/10788 [02:50<02:34, 29.53it/s]Training CobwebTree:  58%|    | 6240/10788 [02:50<02:38, 28.75it/s]Training CobwebTree:  58%|    | 6243/10788 [02:50<02:44, 27.65it/s]Training CobwebTree:  58%|    | 6247/10788 [02:50<02:36, 29.05it/s]Training CobwebTree:  58%|    | 6250/10788 [02:50<02:40, 28.20it/s]Training CobwebTree:  58%|    | 6253/10788 [02:51<02:43, 27.78it/s]Training CobwebTree:  58%|    | 6256/10788 [02:51<02:42, 27.87it/s]Training CobwebTree:  58%|    | 6260/10788 [02:51<02:35, 29.09it/s]Training CobwebTree:  58%|    | 6263/10788 [02:51<02:35, 29.16it/s]Training CobwebTree:  58%|    | 6267/10788 [02:51<02:29, 30.18it/s]Training CobwebTree:  58%|    | 6271/10788 [02:51<02:23, 31.51it/s]Training CobwebTree:  58%|    | 6275/10788 [02:51<02:20, 32.13it/s]Training CobwebTree:  58%|    | 6279/10788 [02:51<02:19, 32.33it/s]Training CobwebTree:  58%|    | 6283/10788 [02:52<02:18, 32.57it/s]Training CobwebTree:  58%|    | 6287/10788 [02:52<02:19, 32.31it/s]Training CobwebTree:  58%|    | 6291/10788 [02:52<02:21, 31.83it/s]Training CobwebTree:  58%|    | 6295/10788 [02:52<02:24, 31.17it/s]Training CobwebTree:  58%|    | 6299/10788 [02:52<02:29, 30.12it/s]Training CobwebTree:  58%|    | 6303/10788 [02:52<02:33, 29.31it/s]Training CobwebTree:  58%|    | 6307/10788 [02:52<02:27, 30.37it/s]Training CobwebTree:  59%|    | 6311/10788 [02:52<02:27, 30.45it/s]Training CobwebTree:  59%|    | 6315/10788 [02:53<02:23, 31.17it/s]Training CobwebTree:  59%|    | 6319/10788 [02:53<02:22, 31.31it/s]Training CobwebTree:  59%|    | 6323/10788 [02:53<02:18, 32.22it/s]Training CobwebTree:  59%|    | 6327/10788 [02:53<02:18, 32.14it/s]Training CobwebTree:  59%|    | 6331/10788 [02:53<02:12, 33.72it/s]Training CobwebTree:  59%|    | 6335/10788 [02:53<02:20, 31.59it/s]Training CobwebTree:  59%|    | 6339/10788 [02:53<02:15, 32.92it/s]Training CobwebTree:  59%|    | 6343/10788 [02:53<02:15, 32.92it/s]Training CobwebTree:  59%|    | 6347/10788 [02:54<02:16, 32.61it/s]Training CobwebTree:  59%|    | 6351/10788 [02:54<02:19, 31.80it/s]Training CobwebTree:  59%|    | 6355/10788 [02:54<02:23, 30.99it/s]Training CobwebTree:  59%|    | 6359/10788 [02:54<02:21, 31.38it/s]Training CobwebTree:  59%|    | 6363/10788 [02:54<02:24, 30.55it/s]Training CobwebTree:  59%|    | 6367/10788 [02:54<02:24, 30.69it/s]Training CobwebTree:  59%|    | 6371/10788 [02:54<02:23, 30.79it/s]Training CobwebTree:  59%|    | 6375/10788 [02:54<02:25, 30.38it/s]Training CobwebTree:  59%|    | 6379/10788 [02:55<02:23, 30.83it/s]Training CobwebTree:  59%|    | 6383/10788 [02:55<02:19, 31.63it/s]Training CobwebTree:  59%|    | 6387/10788 [02:55<02:16, 32.30it/s]Training CobwebTree:  59%|    | 6391/10788 [02:55<02:18, 31.86it/s]Training CobwebTree:  59%|    | 6395/10788 [02:55<02:14, 32.56it/s]Training CobwebTree:  59%|    | 6399/10788 [02:55<02:15, 32.42it/s]Training CobwebTree:  59%|    | 6403/10788 [02:55<02:20, 31.22it/s]Training CobwebTree:  59%|    | 6407/10788 [02:55<02:21, 31.05it/s]Training CobwebTree:  59%|    | 6411/10788 [02:56<02:24, 30.38it/s]Training CobwebTree:  59%|    | 6415/10788 [02:56<02:22, 30.74it/s]Training CobwebTree:  60%|    | 6419/10788 [02:56<02:21, 30.88it/s]Training CobwebTree:  60%|    | 6423/10788 [02:56<02:24, 30.23it/s]Training CobwebTree:  60%|    | 6427/10788 [02:56<02:19, 31.18it/s]Training CobwebTree:  60%|    | 6431/10788 [02:56<02:20, 30.99it/s]Training CobwebTree:  60%|    | 6435/10788 [02:56<02:24, 30.16it/s]Training CobwebTree:  60%|    | 6439/10788 [02:57<02:20, 30.94it/s]Training CobwebTree:  60%|    | 6443/10788 [02:57<02:17, 31.51it/s]Training CobwebTree:  60%|    | 6447/10788 [02:57<02:15, 32.00it/s]Training CobwebTree:  60%|    | 6451/10788 [02:57<02:14, 32.30it/s]Training CobwebTree:  60%|    | 6455/10788 [02:57<02:24, 29.92it/s]Training CobwebTree:  60%|    | 6459/10788 [02:57<02:17, 31.48it/s]Training CobwebTree:  60%|    | 6463/10788 [02:57<02:17, 31.56it/s]Training CobwebTree:  60%|    | 6467/10788 [02:57<02:22, 30.26it/s]Training CobwebTree:  60%|    | 6471/10788 [02:58<02:16, 31.54it/s]Training CobwebTree:  60%|    | 6475/10788 [02:58<02:25, 29.67it/s]Training CobwebTree:  60%|    | 6479/10788 [02:58<02:22, 30.23it/s]Training CobwebTree:  60%|    | 6483/10788 [02:58<02:21, 30.32it/s]Training CobwebTree:  60%|    | 6487/10788 [02:58<02:25, 29.63it/s]Training CobwebTree:  60%|    | 6491/10788 [02:58<02:26, 29.41it/s]Training CobwebTree:  60%|    | 6494/10788 [02:58<02:27, 29.06it/s]Training CobwebTree:  60%|    | 6497/10788 [02:58<02:32, 28.07it/s]Training CobwebTree:  60%|    | 6501/10788 [02:59<02:27, 28.99it/s]Training CobwebTree:  60%|    | 6505/10788 [02:59<02:19, 30.73it/s]Training CobwebTree:  60%|    | 6510/10788 [02:59<02:07, 33.67it/s]Training CobwebTree:  60%|    | 6514/10788 [02:59<02:07, 33.47it/s]Training CobwebTree:  60%|    | 6518/10788 [02:59<02:07, 33.46it/s]Training CobwebTree:  60%|    | 6522/10788 [02:59<02:05, 34.05it/s]Training CobwebTree:  60%|    | 6526/10788 [02:59<02:07, 33.44it/s]Training CobwebTree:  61%|    | 6530/10788 [02:59<02:07, 33.33it/s]Training CobwebTree:  61%|    | 6534/10788 [03:00<02:08, 33.08it/s]Training CobwebTree:  61%|    | 6538/10788 [03:00<02:08, 33.08it/s]Training CobwebTree:  61%|    | 6542/10788 [03:00<02:14, 31.56it/s]Training CobwebTree:  61%|    | 6546/10788 [03:00<02:14, 31.53it/s]Training CobwebTree:  61%|    | 6550/10788 [03:00<02:14, 31.44it/s]Training CobwebTree:  61%|    | 6554/10788 [03:00<02:14, 31.59it/s]Training CobwebTree:  61%|    | 6558/10788 [03:00<02:11, 32.22it/s]Training CobwebTree:  61%|    | 6562/10788 [03:00<02:18, 30.55it/s]Training CobwebTree:  61%|    | 6566/10788 [03:01<02:15, 31.14it/s]Training CobwebTree:  61%|    | 6570/10788 [03:01<02:18, 30.36it/s]Training CobwebTree:  61%|    | 6574/10788 [03:01<02:20, 30.06it/s]Training CobwebTree:  61%|    | 6578/10788 [03:01<02:21, 29.84it/s]Training CobwebTree:  61%|    | 6582/10788 [03:01<02:21, 29.74it/s]Training CobwebTree:  61%|    | 6586/10788 [03:01<02:18, 30.40it/s]Training CobwebTree:  61%|    | 6590/10788 [03:01<02:17, 30.51it/s]Training CobwebTree:  61%|    | 6594/10788 [03:02<02:13, 31.37it/s]Training CobwebTree:  61%|    | 6598/10788 [03:02<02:08, 32.52it/s]Training CobwebTree:  61%|    | 6602/10788 [03:02<02:14, 31.23it/s]Training CobwebTree:  61%|    | 6606/10788 [03:02<02:13, 31.34it/s]Training CobwebTree:  61%|   | 6610/10788 [03:02<02:05, 33.19it/s]Training CobwebTree:  61%|   | 6614/10788 [03:02<02:14, 31.09it/s]Training CobwebTree:  61%|   | 6618/10788 [03:02<02:13, 31.22it/s]Training CobwebTree:  61%|   | 6622/10788 [03:02<02:14, 31.08it/s]Training CobwebTree:  61%|   | 6626/10788 [03:03<02:12, 31.52it/s]Training CobwebTree:  61%|   | 6630/10788 [03:03<02:11, 31.54it/s]Training CobwebTree:  61%|   | 6634/10788 [03:03<02:16, 30.43it/s]Training CobwebTree:  62%|   | 6638/10788 [03:03<02:21, 29.24it/s]Training CobwebTree:  62%|   | 6642/10788 [03:03<02:13, 30.95it/s]Training CobwebTree:  62%|   | 6646/10788 [03:03<02:15, 30.50it/s]Training CobwebTree:  62%|   | 6650/10788 [03:03<02:15, 30.59it/s]Training CobwebTree:  62%|   | 6654/10788 [03:03<02:14, 30.81it/s]Training CobwebTree:  62%|   | 6658/10788 [03:04<02:14, 30.78it/s]Training CobwebTree:  62%|   | 6662/10788 [03:04<02:11, 31.47it/s]Training CobwebTree:  62%|   | 6666/10788 [03:04<02:17, 30.02it/s]Training CobwebTree:  62%|   | 6670/10788 [03:04<02:10, 31.64it/s]Training CobwebTree:  62%|   | 6674/10788 [03:04<02:13, 30.87it/s]Training CobwebTree:  62%|   | 6678/10788 [03:04<02:13, 30.86it/s]Training CobwebTree:  62%|   | 6682/10788 [03:04<02:08, 31.84it/s]Training CobwebTree:  62%|   | 6686/10788 [03:04<02:04, 32.85it/s]Training CobwebTree:  62%|   | 6690/10788 [03:05<02:06, 32.44it/s]Training CobwebTree:  62%|   | 6694/10788 [03:05<02:08, 31.89it/s]Training CobwebTree:  62%|   | 6698/10788 [03:05<02:10, 31.35it/s]Training CobwebTree:  62%|   | 6702/10788 [03:05<02:13, 30.64it/s]Training CobwebTree:  62%|   | 6706/10788 [03:05<02:10, 31.20it/s]Training CobwebTree:  62%|   | 6710/10788 [03:05<02:07, 31.96it/s]Training CobwebTree:  62%|   | 6714/10788 [03:05<02:07, 32.05it/s]Training CobwebTree:  62%|   | 6718/10788 [03:05<02:09, 31.49it/s]Training CobwebTree:  62%|   | 6722/10788 [03:06<02:06, 32.11it/s]Training CobwebTree:  62%|   | 6726/10788 [03:06<02:03, 32.92it/s]Training CobwebTree:  62%|   | 6730/10788 [03:06<02:07, 31.95it/s]Training CobwebTree:  62%|   | 6734/10788 [03:06<02:06, 32.06it/s]Training CobwebTree:  62%|   | 6738/10788 [03:06<02:02, 32.97it/s]Training CobwebTree:  62%|   | 6742/10788 [03:06<02:03, 32.63it/s]Training CobwebTree:  63%|   | 6746/10788 [03:06<02:01, 33.16it/s]Training CobwebTree:  63%|   | 6750/10788 [03:06<02:00, 33.42it/s]Training CobwebTree:  63%|   | 6754/10788 [03:07<02:04, 32.43it/s]Training CobwebTree:  63%|   | 6758/10788 [03:07<02:06, 31.92it/s]Training CobwebTree:  63%|   | 6762/10788 [03:07<02:02, 32.96it/s]Training CobwebTree:  63%|   | 6766/10788 [03:07<02:01, 33.02it/s]Training CobwebTree:  63%|   | 6770/10788 [03:07<02:01, 33.01it/s]Training CobwebTree:  63%|   | 6774/10788 [03:07<02:04, 32.32it/s]Training CobwebTree:  63%|   | 6778/10788 [03:07<02:04, 32.33it/s]Training CobwebTree:  63%|   | 6782/10788 [03:07<02:09, 30.99it/s]Training CobwebTree:  63%|   | 6786/10788 [03:08<02:10, 30.56it/s]Training CobwebTree:  63%|   | 6790/10788 [03:08<02:09, 30.76it/s]Training CobwebTree:  63%|   | 6794/10788 [03:08<02:04, 32.17it/s]Training CobwebTree:  63%|   | 6798/10788 [03:08<02:01, 32.89it/s]Training CobwebTree:  63%|   | 6802/10788 [03:08<01:56, 34.08it/s]Training CobwebTree:  63%|   | 6806/10788 [03:08<01:59, 33.34it/s]Training CobwebTree:  63%|   | 6810/10788 [03:08<02:00, 32.95it/s]Training CobwebTree:  63%|   | 6814/10788 [03:08<02:05, 31.64it/s]Training CobwebTree:  63%|   | 6818/10788 [03:09<02:08, 30.96it/s]Training CobwebTree:  63%|   | 6822/10788 [03:09<02:01, 32.77it/s]Training CobwebTree:  63%|   | 6826/10788 [03:09<02:04, 31.79it/s]Training CobwebTree:  63%|   | 6830/10788 [03:09<02:07, 31.14it/s]Training CobwebTree:  63%|   | 6834/10788 [03:09<02:07, 30.98it/s]Training CobwebTree:  63%|   | 6838/10788 [03:09<02:09, 30.55it/s]Training CobwebTree:  63%|   | 6842/10788 [03:09<02:08, 30.64it/s]Training CobwebTree:  63%|   | 6846/10788 [03:09<02:06, 31.14it/s]Training CobwebTree:  63%|   | 6850/10788 [03:10<02:08, 30.58it/s]Training CobwebTree:  64%|   | 6854/10788 [03:10<02:04, 31.61it/s]Training CobwebTree:  64%|   | 6858/10788 [03:10<02:06, 31.13it/s]Training CobwebTree:  64%|   | 6862/10788 [03:10<02:06, 31.11it/s]Training CobwebTree:  64%|   | 6866/10788 [03:10<02:10, 30.16it/s]Training CobwebTree:  64%|   | 6870/10788 [03:10<02:01, 32.21it/s]Training CobwebTree:  64%|   | 6874/10788 [03:10<02:02, 32.08it/s]Training CobwebTree:  64%|   | 6878/10788 [03:10<02:02, 31.92it/s]Training CobwebTree:  64%|   | 6882/10788 [03:11<02:06, 30.96it/s]Training CobwebTree:  64%|   | 6886/10788 [03:11<02:08, 30.40it/s]Training CobwebTree:  64%|   | 6890/10788 [03:11<02:07, 30.63it/s]Training CobwebTree:  64%|   | 6894/10788 [03:11<02:11, 29.64it/s]Training CobwebTree:  64%|   | 6898/10788 [03:11<02:09, 29.94it/s]Training CobwebTree:  64%|   | 6902/10788 [03:11<02:06, 30.80it/s]Training CobwebTree:  64%|   | 6906/10788 [03:11<01:59, 32.46it/s]Training CobwebTree:  64%|   | 6910/10788 [03:12<02:02, 31.72it/s]Training CobwebTree:  64%|   | 6914/10788 [03:12<01:58, 32.62it/s]Training CobwebTree:  64%|   | 6918/10788 [03:12<02:03, 31.39it/s]Training CobwebTree:  64%|   | 6922/10788 [03:12<02:06, 30.54it/s]Training CobwebTree:  64%|   | 6926/10788 [03:12<02:06, 30.64it/s]Training CobwebTree:  64%|   | 6930/10788 [03:12<02:05, 30.83it/s]Training CobwebTree:  64%|   | 6935/10788 [03:12<01:55, 33.43it/s]Training CobwebTree:  64%|   | 6939/10788 [03:12<01:53, 33.76it/s]Training CobwebTree:  64%|   | 6943/10788 [03:13<01:57, 32.74it/s]Training CobwebTree:  64%|   | 6947/10788 [03:13<01:54, 33.44it/s]Training CobwebTree:  64%|   | 6951/10788 [03:13<01:57, 32.65it/s]Training CobwebTree:  64%|   | 6955/10788 [03:13<01:54, 33.44it/s]Training CobwebTree:  65%|   | 6959/10788 [03:13<01:50, 34.60it/s]Training CobwebTree:  65%|   | 6963/10788 [03:13<01:53, 33.74it/s]Training CobwebTree:  65%|   | 6967/10788 [03:13<01:58, 32.26it/s]Training CobwebTree:  65%|   | 6971/10788 [03:13<02:01, 31.37it/s]Training CobwebTree:  65%|   | 6975/10788 [03:14<02:00, 31.67it/s]Training CobwebTree:  65%|   | 6979/10788 [03:14<02:01, 31.29it/s]Training CobwebTree:  65%|   | 6983/10788 [03:14<02:01, 31.41it/s]Training CobwebTree:  65%|   | 6987/10788 [03:14<01:57, 32.36it/s]Training CobwebTree:  65%|   | 6992/10788 [03:14<01:49, 34.62it/s]Training CobwebTree:  65%|   | 6996/10788 [03:14<01:57, 32.18it/s]Training CobwebTree:  65%|   | 7000/10788 [03:14<02:03, 30.65it/s]Training CobwebTree:  65%|   | 7004/10788 [03:14<02:05, 30.07it/s]Training CobwebTree:  65%|   | 7008/10788 [03:15<02:02, 30.81it/s]Training CobwebTree:  65%|   | 7012/10788 [03:15<02:00, 31.37it/s]Training CobwebTree:  65%|   | 7016/10788 [03:15<01:58, 31.77it/s]Training CobwebTree:  65%|   | 7020/10788 [03:15<01:57, 31.94it/s]Training CobwebTree:  65%|   | 7024/10788 [03:15<01:58, 31.66it/s]Training CobwebTree:  65%|   | 7028/10788 [03:15<02:01, 30.83it/s]Training CobwebTree:  65%|   | 7032/10788 [03:15<01:55, 32.48it/s]Training CobwebTree:  65%|   | 7036/10788 [03:15<01:54, 32.69it/s]Training CobwebTree:  65%|   | 7040/10788 [03:16<01:54, 32.60it/s]Training CobwebTree:  65%|   | 7044/10788 [03:16<01:52, 33.18it/s]Training CobwebTree:  65%|   | 7048/10788 [03:16<01:54, 32.64it/s]Training CobwebTree:  65%|   | 7052/10788 [03:16<01:58, 31.48it/s]Training CobwebTree:  65%|   | 7056/10788 [03:16<01:59, 31.36it/s]Training CobwebTree:  65%|   | 7060/10788 [03:16<01:59, 31.21it/s]Training CobwebTree:  65%|   | 7064/10788 [03:16<01:56, 32.07it/s]Training CobwebTree:  66%|   | 7068/10788 [03:16<01:53, 32.91it/s]Training CobwebTree:  66%|   | 7072/10788 [03:17<01:53, 32.78it/s]Training CobwebTree:  66%|   | 7076/10788 [03:17<01:52, 32.90it/s]Training CobwebTree:  66%|   | 7080/10788 [03:17<01:55, 32.06it/s]Training CobwebTree:  66%|   | 7084/10788 [03:17<01:54, 32.34it/s]Training CobwebTree:  66%|   | 7088/10788 [03:17<01:57, 31.45it/s]Training CobwebTree:  66%|   | 7092/10788 [03:17<01:53, 32.65it/s]Training CobwebTree:  66%|   | 7096/10788 [03:17<01:51, 32.97it/s]Training CobwebTree:  66%|   | 7100/10788 [03:17<01:51, 33.08it/s]Training CobwebTree:  66%|   | 7104/10788 [03:18<01:50, 33.48it/s]Training CobwebTree:  66%|   | 7108/10788 [03:18<01:44, 35.06it/s]Training CobwebTree:  66%|   | 7112/10788 [03:18<01:49, 33.68it/s]Training CobwebTree:  66%|   | 7116/10788 [03:18<01:50, 33.35it/s]Training CobwebTree:  66%|   | 7120/10788 [03:18<01:51, 33.01it/s]Training CobwebTree:  66%|   | 7124/10788 [03:18<01:48, 33.75it/s]Training CobwebTree:  66%|   | 7128/10788 [03:18<01:51, 32.73it/s]Training CobwebTree:  66%|   | 7132/10788 [03:18<01:54, 32.06it/s]Training CobwebTree:  66%|   | 7136/10788 [03:18<01:49, 33.32it/s]Training CobwebTree:  66%|   | 7140/10788 [03:19<01:55, 31.49it/s]Training CobwebTree:  66%|   | 7144/10788 [03:19<01:54, 31.80it/s]Training CobwebTree:  66%|   | 7148/10788 [03:19<01:54, 31.72it/s]Training CobwebTree:  66%|   | 7152/10788 [03:19<02:00, 30.28it/s]Training CobwebTree:  66%|   | 7156/10788 [03:19<01:56, 31.27it/s]Training CobwebTree:  66%|   | 7160/10788 [03:19<01:54, 31.77it/s]Training CobwebTree:  66%|   | 7164/10788 [03:19<01:53, 31.96it/s]Training CobwebTree:  66%|   | 7168/10788 [03:20<01:57, 30.75it/s]Training CobwebTree:  66%|   | 7172/10788 [03:20<02:03, 29.33it/s]Training CobwebTree:  67%|   | 7176/10788 [03:20<01:58, 30.41it/s]Training CobwebTree:  67%|   | 7180/10788 [03:20<01:54, 31.47it/s]Training CobwebTree:  67%|   | 7184/10788 [03:20<01:53, 31.77it/s]Training CobwebTree:  67%|   | 7188/10788 [03:20<01:47, 33.54it/s]Training CobwebTree:  67%|   | 7192/10788 [03:20<01:48, 33.22it/s]Training CobwebTree:  67%|   | 7196/10788 [03:20<01:47, 33.46it/s]Training CobwebTree:  67%|   | 7200/10788 [03:20<01:44, 34.48it/s]Training CobwebTree:  67%|   | 7204/10788 [03:21<01:50, 32.32it/s]Training CobwebTree:  67%|   | 7208/10788 [03:21<01:50, 32.40it/s]Training CobwebTree:  67%|   | 7212/10788 [03:21<01:54, 31.35it/s]Training CobwebTree:  67%|   | 7216/10788 [03:21<01:54, 31.28it/s]Training CobwebTree:  67%|   | 7220/10788 [03:21<01:49, 32.53it/s]Training CobwebTree:  67%|   | 7224/10788 [03:21<01:49, 32.56it/s]Training CobwebTree:  67%|   | 7228/10788 [03:21<01:50, 32.08it/s]Training CobwebTree:  67%|   | 7232/10788 [03:22<01:50, 32.21it/s]Training CobwebTree:  67%|   | 7236/10788 [03:22<01:52, 31.44it/s]Training CobwebTree:  67%|   | 7240/10788 [03:22<01:48, 32.61it/s]Training CobwebTree:  67%|   | 7244/10788 [03:22<01:50, 32.13it/s]Training CobwebTree:  67%|   | 7248/10788 [03:22<01:53, 31.16it/s]Training CobwebTree:  67%|   | 7252/10788 [03:22<01:57, 30.04it/s]Training CobwebTree:  67%|   | 7256/10788 [03:22<01:56, 30.40it/s]Training CobwebTree:  67%|   | 7260/10788 [03:22<01:52, 31.42it/s]Training CobwebTree:  67%|   | 7264/10788 [03:23<01:51, 31.59it/s]Training CobwebTree:  67%|   | 7268/10788 [03:23<01:49, 32.05it/s]Training CobwebTree:  67%|   | 7272/10788 [03:23<01:45, 33.43it/s]Training CobwebTree:  67%|   | 7276/10788 [03:23<01:46, 33.03it/s]Training CobwebTree:  67%|   | 7280/10788 [03:23<01:45, 33.27it/s]Training CobwebTree:  68%|   | 7284/10788 [03:23<01:54, 30.69it/s]Training CobwebTree:  68%|   | 7288/10788 [03:23<01:48, 32.22it/s]Training CobwebTree:  68%|   | 7292/10788 [03:23<01:44, 33.58it/s]Training CobwebTree:  68%|   | 7296/10788 [03:23<01:42, 34.19it/s]Training CobwebTree:  68%|   | 7300/10788 [03:24<01:41, 34.20it/s]Training CobwebTree:  68%|   | 7304/10788 [03:24<01:44, 33.46it/s]Training CobwebTree:  68%|   | 7308/10788 [03:24<01:46, 32.67it/s]Training CobwebTree:  68%|   | 7312/10788 [03:24<01:44, 33.42it/s]Training CobwebTree:  68%|   | 7316/10788 [03:24<01:46, 32.66it/s]Training CobwebTree:  68%|   | 7320/10788 [03:24<01:47, 32.16it/s]Training CobwebTree:  68%|   | 7324/10788 [03:24<01:53, 30.49it/s]Training CobwebTree:  68%|   | 7328/10788 [03:25<01:50, 31.33it/s]Training CobwebTree:  68%|   | 7332/10788 [03:25<01:50, 31.30it/s]Training CobwebTree:  68%|   | 7336/10788 [03:25<01:50, 31.36it/s]Training CobwebTree:  68%|   | 7340/10788 [03:25<01:51, 30.88it/s]Training CobwebTree:  68%|   | 7344/10788 [03:25<01:50, 31.20it/s]Training CobwebTree:  68%|   | 7348/10788 [03:25<01:51, 30.87it/s]Training CobwebTree:  68%|   | 7352/10788 [03:25<01:48, 31.68it/s]Training CobwebTree:  68%|   | 7356/10788 [03:25<01:47, 31.79it/s]Training CobwebTree:  68%|   | 7360/10788 [03:26<01:48, 31.61it/s]Training CobwebTree:  68%|   | 7364/10788 [03:26<01:44, 32.67it/s]Training CobwebTree:  68%|   | 7368/10788 [03:26<01:40, 34.10it/s]Training CobwebTree:  68%|   | 7372/10788 [03:26<01:38, 34.68it/s]Training CobwebTree:  68%|   | 7376/10788 [03:26<01:40, 34.02it/s]Training CobwebTree:  68%|   | 7380/10788 [03:26<01:44, 32.48it/s]Training CobwebTree:  68%|   | 7384/10788 [03:26<01:41, 33.65it/s]Training CobwebTree:  68%|   | 7388/10788 [03:26<01:45, 32.21it/s]Training CobwebTree:  69%|   | 7392/10788 [03:26<01:43, 32.96it/s]Training CobwebTree:  69%|   | 7396/10788 [03:27<01:43, 32.84it/s]Training CobwebTree:  69%|   | 7400/10788 [03:27<01:45, 32.02it/s]Training CobwebTree:  69%|   | 7404/10788 [03:27<01:45, 32.13it/s]Training CobwebTree:  69%|   | 7408/10788 [03:27<01:48, 31.11it/s]Training CobwebTree:  69%|   | 7412/10788 [03:27<01:49, 30.91it/s]Training CobwebTree:  69%|   | 7416/10788 [03:27<01:43, 32.73it/s]Training CobwebTree:  69%|   | 7420/10788 [03:27<01:51, 30.10it/s]Training CobwebTree:  69%|   | 7424/10788 [03:27<01:45, 31.78it/s]Training CobwebTree:  69%|   | 7428/10788 [03:28<01:41, 33.06it/s]Training CobwebTree:  69%|   | 7432/10788 [03:28<01:40, 33.28it/s]Training CobwebTree:  69%|   | 7436/10788 [03:28<01:42, 32.75it/s]Training CobwebTree:  69%|   | 7440/10788 [03:28<01:45, 31.84it/s]Training CobwebTree:  69%|   | 7444/10788 [03:28<01:45, 31.74it/s]Training CobwebTree:  69%|   | 7448/10788 [03:28<01:45, 31.72it/s]Training CobwebTree:  69%|   | 7452/10788 [03:28<01:45, 31.58it/s]Training CobwebTree:  69%|   | 7456/10788 [03:28<01:46, 31.32it/s]Training CobwebTree:  69%|   | 7460/10788 [03:29<01:46, 31.27it/s]Training CobwebTree:  69%|   | 7464/10788 [03:29<01:47, 30.89it/s]Training CobwebTree:  69%|   | 7468/10788 [03:29<01:47, 30.82it/s]Training CobwebTree:  69%|   | 7472/10788 [03:29<01:53, 29.26it/s]Training CobwebTree:  69%|   | 7476/10788 [03:29<01:51, 29.59it/s]Training CobwebTree:  69%|   | 7480/10788 [03:29<01:53, 29.15it/s]Training CobwebTree:  69%|   | 7483/10788 [03:29<01:54, 28.91it/s]Training CobwebTree:  69%|   | 7487/10788 [03:30<01:46, 31.04it/s]Training CobwebTree:  69%|   | 7491/10788 [03:30<01:47, 30.70it/s]Training CobwebTree:  69%|   | 7495/10788 [03:30<01:46, 30.92it/s]Training CobwebTree:  70%|   | 7499/10788 [03:30<01:48, 30.37it/s]Training CobwebTree:  70%|   | 7503/10788 [03:30<01:51, 29.55it/s]Training CobwebTree:  70%|   | 7507/10788 [03:30<01:47, 30.54it/s]Training CobwebTree:  70%|   | 7511/10788 [03:30<01:44, 31.31it/s]Training CobwebTree:  70%|   | 7515/10788 [03:30<01:43, 31.66it/s]Training CobwebTree:  70%|   | 7519/10788 [03:31<01:45, 30.87it/s]Training CobwebTree:  70%|   | 7523/10788 [03:31<01:40, 32.37it/s]Training CobwebTree:  70%|   | 7527/10788 [03:31<01:46, 30.48it/s]Training CobwebTree:  70%|   | 7531/10788 [03:31<01:44, 31.28it/s]Training CobwebTree:  70%|   | 7535/10788 [03:31<01:41, 32.06it/s]Training CobwebTree:  70%|   | 7539/10788 [03:31<01:43, 31.47it/s]Training CobwebTree:  70%|   | 7543/10788 [03:31<01:41, 31.91it/s]Training CobwebTree:  70%|   | 7547/10788 [03:31<01:46, 30.49it/s]Training CobwebTree:  70%|   | 7551/10788 [03:32<01:46, 30.42it/s]Training CobwebTree:  70%|   | 7555/10788 [03:32<01:42, 31.61it/s]Training CobwebTree:  70%|   | 7559/10788 [03:32<01:41, 31.90it/s]Training CobwebTree:  70%|   | 7563/10788 [03:32<01:40, 31.99it/s]Training CobwebTree:  70%|   | 7567/10788 [03:32<01:36, 33.21it/s]Training CobwebTree:  70%|   | 7571/10788 [03:32<01:39, 32.47it/s]Training CobwebTree:  70%|   | 7575/10788 [03:32<01:37, 32.81it/s]Training CobwebTree:  70%|   | 7579/10788 [03:32<01:35, 33.68it/s]Training CobwebTree:  70%|   | 7583/10788 [03:33<01:35, 33.55it/s]Training CobwebTree:  70%|   | 7587/10788 [03:33<01:37, 32.92it/s]Training CobwebTree:  70%|   | 7591/10788 [03:33<01:34, 33.82it/s]Training CobwebTree:  70%|   | 7595/10788 [03:33<01:39, 32.04it/s]Training CobwebTree:  70%|   | 7599/10788 [03:33<01:36, 33.11it/s]Training CobwebTree:  70%|   | 7603/10788 [03:33<01:35, 33.21it/s]Training CobwebTree:  71%|   | 7607/10788 [03:33<01:37, 32.53it/s]Training CobwebTree:  71%|   | 7611/10788 [03:33<01:34, 33.56it/s]Training CobwebTree:  71%|   | 7615/10788 [03:34<01:33, 33.90it/s]Training CobwebTree:  71%|   | 7619/10788 [03:34<01:37, 32.55it/s]Training CobwebTree:  71%|   | 7623/10788 [03:34<01:39, 31.83it/s]Training CobwebTree:  71%|   | 7627/10788 [03:34<01:35, 33.06it/s]Training CobwebTree:  71%|   | 7631/10788 [03:34<01:38, 32.15it/s]Training CobwebTree:  71%|   | 7635/10788 [03:34<01:38, 32.15it/s]Training CobwebTree:  71%|   | 7639/10788 [03:34<01:40, 31.39it/s]Training CobwebTree:  71%|   | 7643/10788 [03:34<01:39, 31.49it/s]Training CobwebTree:  71%|   | 7647/10788 [03:35<01:37, 32.34it/s]Training CobwebTree:  71%|   | 7651/10788 [03:35<01:40, 31.12it/s]Training CobwebTree:  71%|   | 7655/10788 [03:35<01:41, 30.87it/s]Training CobwebTree:  71%|   | 7659/10788 [03:35<01:38, 31.83it/s]Training CobwebTree:  71%|   | 7663/10788 [03:35<01:38, 31.83it/s]Training CobwebTree:  71%|   | 7667/10788 [03:35<01:38, 31.83it/s]Training CobwebTree:  71%|   | 7671/10788 [03:35<01:42, 30.52it/s]Training CobwebTree:  71%|   | 7675/10788 [03:35<01:41, 30.67it/s]Training CobwebTree:  71%|   | 7679/10788 [03:36<01:42, 30.27it/s]Training CobwebTree:  71%|   | 7683/10788 [03:36<01:42, 30.32it/s]Training CobwebTree:  71%|  | 7687/10788 [03:36<01:41, 30.62it/s]Training CobwebTree:  71%|  | 7691/10788 [03:36<01:39, 31.25it/s]Training CobwebTree:  71%|  | 7695/10788 [03:36<01:37, 31.60it/s]Training CobwebTree:  71%|  | 7699/10788 [03:36<01:36, 31.89it/s]Training CobwebTree:  71%|  | 7703/10788 [03:36<01:41, 30.33it/s]Training CobwebTree:  71%|  | 7707/10788 [03:36<01:39, 31.05it/s]Training CobwebTree:  71%|  | 7711/10788 [03:37<01:40, 30.75it/s]Training CobwebTree:  72%|  | 7715/10788 [03:37<01:36, 31.76it/s]Training CobwebTree:  72%|  | 7719/10788 [03:37<01:35, 32.03it/s]Training CobwebTree:  72%|  | 7723/10788 [03:37<01:35, 32.04it/s]Training CobwebTree:  72%|  | 7727/10788 [03:37<01:34, 32.48it/s]Training CobwebTree:  72%|  | 7731/10788 [03:37<01:37, 31.35it/s]Training CobwebTree:  72%|  | 7735/10788 [03:37<01:38, 30.95it/s]Training CobwebTree:  72%|  | 7739/10788 [03:37<01:35, 32.05it/s]Training CobwebTree:  72%|  | 7743/10788 [03:38<01:36, 31.49it/s]Training CobwebTree:  72%|  | 7747/10788 [03:38<01:38, 31.02it/s]Training CobwebTree:  72%|  | 7751/10788 [03:38<01:38, 30.88it/s]Training CobwebTree:  72%|  | 7755/10788 [03:38<01:37, 30.99it/s]Training CobwebTree:  72%|  | 7759/10788 [03:38<01:39, 30.54it/s]Training CobwebTree:  72%|  | 7763/10788 [03:38<01:35, 31.73it/s]Training CobwebTree:  72%|  | 7767/10788 [03:38<01:35, 31.56it/s]Training CobwebTree:  72%|  | 7771/10788 [03:38<01:33, 32.26it/s]Training CobwebTree:  72%|  | 7775/10788 [03:39<01:34, 31.93it/s]Training CobwebTree:  72%|  | 7779/10788 [03:39<01:36, 31.15it/s]Training CobwebTree:  72%|  | 7783/10788 [03:39<01:35, 31.52it/s]Training CobwebTree:  72%|  | 7787/10788 [03:39<01:37, 30.77it/s]Training CobwebTree:  72%|  | 7791/10788 [03:39<01:45, 28.53it/s]Training CobwebTree:  72%|  | 7795/10788 [03:39<01:43, 29.05it/s]Training CobwebTree:  72%|  | 7799/10788 [03:39<01:39, 30.17it/s]Training CobwebTree:  72%|  | 7803/10788 [03:40<01:37, 30.76it/s]Training CobwebTree:  72%|  | 7807/10788 [03:40<01:38, 30.29it/s]Training CobwebTree:  72%|  | 7811/10788 [03:40<01:39, 30.00it/s]Training CobwebTree:  72%|  | 7815/10788 [03:40<01:40, 29.45it/s]Training CobwebTree:  72%|  | 7819/10788 [03:40<01:36, 30.78it/s]Training CobwebTree:  73%|  | 7823/10788 [03:40<01:33, 31.62it/s]Training CobwebTree:  73%|  | 7827/10788 [03:40<01:34, 31.38it/s]Training CobwebTree:  73%|  | 7831/10788 [03:40<01:29, 33.19it/s]Training CobwebTree:  73%|  | 7835/10788 [03:41<01:32, 31.92it/s]Training CobwebTree:  73%|  | 7839/10788 [03:41<01:33, 31.68it/s]Training CobwebTree:  73%|  | 7843/10788 [03:41<01:37, 30.36it/s]Training CobwebTree:  73%|  | 7847/10788 [03:41<01:31, 32.07it/s]Training CobwebTree:  73%|  | 7851/10788 [03:41<01:34, 31.04it/s]Training CobwebTree:  73%|  | 7855/10788 [03:41<01:35, 30.83it/s]Training CobwebTree:  73%|  | 7859/10788 [03:41<01:33, 31.48it/s]Training CobwebTree:  73%|  | 7863/10788 [03:41<01:35, 30.59it/s]Training CobwebTree:  73%|  | 7867/10788 [03:42<01:31, 31.85it/s]Training CobwebTree:  73%|  | 7871/10788 [03:42<01:31, 31.90it/s]Training CobwebTree:  73%|  | 7875/10788 [03:42<01:32, 31.43it/s]Training CobwebTree:  73%|  | 7879/10788 [03:42<01:32, 31.40it/s]Training CobwebTree:  73%|  | 7883/10788 [03:42<01:32, 31.45it/s]Training CobwebTree:  73%|  | 7887/10788 [03:42<01:36, 30.16it/s]Training CobwebTree:  73%|  | 7891/10788 [03:42<01:33, 30.95it/s]Training CobwebTree:  73%|  | 7895/10788 [03:42<01:30, 31.98it/s]Training CobwebTree:  73%|  | 7899/10788 [03:43<01:27, 32.90it/s]Training CobwebTree:  73%|  | 7903/10788 [03:43<01:28, 32.50it/s]Training CobwebTree:  73%|  | 7907/10788 [03:43<01:29, 32.17it/s]Training CobwebTree:  73%|  | 7911/10788 [03:43<01:28, 32.65it/s]Training CobwebTree:  73%|  | 7915/10788 [03:43<01:29, 32.10it/s]Training CobwebTree:  73%|  | 7919/10788 [03:43<01:27, 32.84it/s]Training CobwebTree:  73%|  | 7923/10788 [03:43<01:26, 33.10it/s]Training CobwebTree:  73%|  | 7927/10788 [03:43<01:23, 34.28it/s]Training CobwebTree:  74%|  | 7931/10788 [03:44<01:25, 33.39it/s]Training CobwebTree:  74%|  | 7935/10788 [03:44<01:27, 32.59it/s]Training CobwebTree:  74%|  | 7939/10788 [03:44<01:26, 32.76it/s]Training CobwebTree:  74%|  | 7943/10788 [03:44<01:32, 30.69it/s]Training CobwebTree:  74%|  | 7947/10788 [03:44<01:30, 31.47it/s]Training CobwebTree:  74%|  | 7951/10788 [03:44<01:29, 31.70it/s]Training CobwebTree:  74%|  | 7955/10788 [03:44<01:29, 31.64it/s]Training CobwebTree:  74%|  | 7959/10788 [03:44<01:29, 31.53it/s]Training CobwebTree:  74%|  | 7963/10788 [03:45<01:28, 31.98it/s]Training CobwebTree:  74%|  | 7967/10788 [03:45<01:27, 32.25it/s]Training CobwebTree:  74%|  | 7971/10788 [03:45<01:28, 31.83it/s]Training CobwebTree:  74%|  | 7975/10788 [03:45<01:29, 31.35it/s]Training CobwebTree:  74%|  | 7979/10788 [03:45<01:30, 31.03it/s]Training CobwebTree:  74%|  | 7983/10788 [03:45<01:29, 31.22it/s]Training CobwebTree:  74%|  | 7987/10788 [03:45<01:29, 31.29it/s]Training CobwebTree:  74%|  | 7991/10788 [03:45<01:29, 31.37it/s]Training CobwebTree:  74%|  | 7995/10788 [03:46<01:27, 31.86it/s]Training CobwebTree:  74%|  | 7999/10788 [03:46<01:25, 32.75it/s]Training CobwebTree:  74%|  | 8003/10788 [03:46<01:23, 33.28it/s]Training CobwebTree:  74%|  | 8007/10788 [03:46<01:24, 32.84it/s]Training CobwebTree:  74%|  | 8011/10788 [03:46<01:24, 32.80it/s]Training CobwebTree:  74%|  | 8015/10788 [03:46<01:21, 34.03it/s]Training CobwebTree:  74%|  | 8019/10788 [03:46<01:22, 33.53it/s]Training CobwebTree:  74%|  | 8023/10788 [03:46<01:22, 33.57it/s]Training CobwebTree:  74%|  | 8027/10788 [03:47<01:23, 33.26it/s]Training CobwebTree:  74%|  | 8031/10788 [03:47<01:23, 33.17it/s]Training CobwebTree:  74%|  | 8035/10788 [03:47<01:24, 32.70it/s]Training CobwebTree:  75%|  | 8039/10788 [03:47<01:25, 32.21it/s]Training CobwebTree:  75%|  | 8043/10788 [03:47<01:28, 31.02it/s]Training CobwebTree:  75%|  | 8047/10788 [03:47<01:27, 31.31it/s]Training CobwebTree:  75%|  | 8051/10788 [03:47<01:26, 31.70it/s]Training CobwebTree:  75%|  | 8055/10788 [03:47<01:25, 32.13it/s]Training CobwebTree:  75%|  | 8059/10788 [03:48<01:23, 32.56it/s]Training CobwebTree:  75%|  | 8063/10788 [03:48<01:24, 32.34it/s]Training CobwebTree:  75%|  | 8067/10788 [03:48<01:24, 32.33it/s]Training CobwebTree:  75%|  | 8071/10788 [03:48<01:30, 30.06it/s]Training CobwebTree:  75%|  | 8075/10788 [03:48<01:27, 30.88it/s]Training CobwebTree:  75%|  | 8079/10788 [03:48<01:30, 30.01it/s]Training CobwebTree:  75%|  | 8083/10788 [03:48<01:32, 29.23it/s]Training CobwebTree:  75%|  | 8086/10788 [03:48<01:33, 29.04it/s]Training CobwebTree:  75%|  | 8090/10788 [03:49<01:28, 30.54it/s]Training CobwebTree:  75%|  | 8094/10788 [03:49<01:25, 31.41it/s]Training CobwebTree:  75%|  | 8098/10788 [03:49<01:26, 30.97it/s]Training CobwebTree:  75%|  | 8102/10788 [03:49<01:25, 31.48it/s]Training CobwebTree:  75%|  | 8106/10788 [03:49<01:25, 31.53it/s]Training CobwebTree:  75%|  | 8110/10788 [03:49<01:25, 31.37it/s]Training CobwebTree:  75%|  | 8114/10788 [03:49<01:23, 31.86it/s]Training CobwebTree:  75%|  | 8118/10788 [03:49<01:22, 32.43it/s]Training CobwebTree:  75%|  | 8122/10788 [03:50<01:21, 32.72it/s]Training CobwebTree:  75%|  | 8126/10788 [03:50<01:25, 31.06it/s]Training CobwebTree:  75%|  | 8130/10788 [03:50<01:30, 29.40it/s]Training CobwebTree:  75%|  | 8134/10788 [03:50<01:26, 30.72it/s]Training CobwebTree:  75%|  | 8138/10788 [03:50<01:25, 31.07it/s]Training CobwebTree:  75%|  | 8142/10788 [03:50<01:26, 30.73it/s]Training CobwebTree:  76%|  | 8146/10788 [03:50<01:24, 31.35it/s]Training CobwebTree:  76%|  | 8150/10788 [03:51<01:24, 31.21it/s]Training CobwebTree:  76%|  | 8154/10788 [03:51<01:23, 31.61it/s]Training CobwebTree:  76%|  | 8158/10788 [03:51<01:24, 31.10it/s]Training CobwebTree:  76%|  | 8162/10788 [03:51<01:26, 30.35it/s]Training CobwebTree:  76%|  | 8166/10788 [03:51<01:25, 30.62it/s]Training CobwebTree:  76%|  | 8170/10788 [03:51<01:24, 30.88it/s]Training CobwebTree:  76%|  | 8174/10788 [03:51<01:24, 30.76it/s]Training CobwebTree:  76%|  | 8178/10788 [03:51<01:24, 30.99it/s]Training CobwebTree:  76%|  | 8182/10788 [03:52<01:25, 30.60it/s]Training CobwebTree:  76%|  | 8186/10788 [03:52<01:25, 30.35it/s]Training CobwebTree:  76%|  | 8190/10788 [03:52<01:22, 31.47it/s]Training CobwebTree:  76%|  | 8194/10788 [03:52<01:23, 30.96it/s]Training CobwebTree:  76%|  | 8198/10788 [03:52<01:24, 30.82it/s]Training CobwebTree:  76%|  | 8202/10788 [03:52<01:23, 30.81it/s]Training CobwebTree:  76%|  | 8206/10788 [03:52<01:24, 30.42it/s]Training CobwebTree:  76%|  | 8210/10788 [03:52<01:22, 31.28it/s]Training CobwebTree:  76%|  | 8214/10788 [03:53<01:20, 31.83it/s]Training CobwebTree:  76%|  | 8218/10788 [03:53<01:20, 32.05it/s]Training CobwebTree:  76%|  | 8222/10788 [03:53<01:25, 30.00it/s]Training CobwebTree:  76%|  | 8226/10788 [03:53<01:24, 30.39it/s]Training CobwebTree:  76%|  | 8230/10788 [03:53<01:23, 30.60it/s]Training CobwebTree:  76%|  | 8234/10788 [03:53<01:21, 31.25it/s]Training CobwebTree:  76%|  | 8238/10788 [03:53<01:20, 31.52it/s]Training CobwebTree:  76%|  | 8242/10788 [03:53<01:19, 32.21it/s]Training CobwebTree:  76%|  | 8246/10788 [03:54<01:16, 33.08it/s]Training CobwebTree:  76%|  | 8250/10788 [03:54<01:21, 31.20it/s]Training CobwebTree:  77%|  | 8254/10788 [03:54<01:20, 31.38it/s]Training CobwebTree:  77%|  | 8258/10788 [03:54<01:23, 30.33it/s]Training CobwebTree:  77%|  | 8262/10788 [03:54<01:17, 32.44it/s]Training CobwebTree:  77%|  | 8266/10788 [03:54<01:18, 32.26it/s]Training CobwebTree:  77%|  | 8270/10788 [03:54<01:17, 32.43it/s]Training CobwebTree:  77%|  | 8274/10788 [03:54<01:21, 30.93it/s]Training CobwebTree:  77%|  | 8278/10788 [03:55<01:18, 31.88it/s]Training CobwebTree:  77%|  | 8282/10788 [03:55<01:19, 31.45it/s]Training CobwebTree:  77%|  | 8286/10788 [03:55<01:22, 30.45it/s]Training CobwebTree:  77%|  | 8290/10788 [03:55<01:22, 30.43it/s]Training CobwebTree:  77%|  | 8294/10788 [03:55<01:24, 29.44it/s]Training CobwebTree:  77%|  | 8298/10788 [03:55<01:23, 29.97it/s]Training CobwebTree:  77%|  | 8302/10788 [03:55<01:22, 30.20it/s]Training CobwebTree:  77%|  | 8306/10788 [03:56<01:21, 30.31it/s]Training CobwebTree:  77%|  | 8310/10788 [03:56<01:24, 29.43it/s]Training CobwebTree:  77%|  | 8313/10788 [03:56<01:24, 29.35it/s]Training CobwebTree:  77%|  | 8317/10788 [03:56<01:24, 29.38it/s]Training CobwebTree:  77%|  | 8320/10788 [03:56<01:23, 29.52it/s]Training CobwebTree:  77%|  | 8323/10788 [03:56<01:24, 29.18it/s]Training CobwebTree:  77%|  | 8327/10788 [03:56<01:18, 31.20it/s]Training CobwebTree:  77%|  | 8331/10788 [03:56<01:20, 30.66it/s]Training CobwebTree:  77%|  | 8335/10788 [03:56<01:17, 31.72it/s]Training CobwebTree:  77%|  | 8339/10788 [03:57<01:16, 31.95it/s]Training CobwebTree:  77%|  | 8343/10788 [03:57<01:18, 31.27it/s]Training CobwebTree:  77%|  | 8347/10788 [03:57<01:20, 30.31it/s]Training CobwebTree:  77%|  | 8351/10788 [03:57<01:21, 29.89it/s]Training CobwebTree:  77%|  | 8355/10788 [03:57<01:22, 29.44it/s]Training CobwebTree:  77%|  | 8359/10788 [03:57<01:19, 30.60it/s]Training CobwebTree:  78%|  | 8363/10788 [03:57<01:21, 29.75it/s]Training CobwebTree:  78%|  | 8366/10788 [03:58<01:23, 28.94it/s]Training CobwebTree:  78%|  | 8369/10788 [03:58<01:23, 28.84it/s]Training CobwebTree:  78%|  | 8373/10788 [03:58<01:20, 30.01it/s]Training CobwebTree:  78%|  | 8377/10788 [03:58<01:17, 31.27it/s]Training CobwebTree:  78%|  | 8381/10788 [03:58<01:14, 32.19it/s]Training CobwebTree:  78%|  | 8385/10788 [03:58<01:18, 30.61it/s]Training CobwebTree:  78%|  | 8389/10788 [03:58<01:19, 30.34it/s]Training CobwebTree:  78%|  | 8393/10788 [03:58<01:18, 30.37it/s]Training CobwebTree:  78%|  | 8397/10788 [03:59<01:17, 30.82it/s]Training CobwebTree:  78%|  | 8401/10788 [03:59<01:17, 30.94it/s]Training CobwebTree:  78%|  | 8405/10788 [03:59<01:16, 31.18it/s]Training CobwebTree:  78%|  | 8409/10788 [03:59<01:14, 31.86it/s]Training CobwebTree:  78%|  | 8413/10788 [03:59<01:14, 32.04it/s]Training CobwebTree:  78%|  | 8417/10788 [03:59<01:17, 30.58it/s]Training CobwebTree:  78%|  | 8421/10788 [03:59<01:15, 31.24it/s]Training CobwebTree:  78%|  | 8425/10788 [03:59<01:16, 30.83it/s]Training CobwebTree:  78%|  | 8429/10788 [04:00<01:15, 31.42it/s]Training CobwebTree:  78%|  | 8433/10788 [04:00<01:15, 31.35it/s]Training CobwebTree:  78%|  | 8437/10788 [04:00<01:10, 33.41it/s]Training CobwebTree:  78%|  | 8441/10788 [04:00<01:09, 33.74it/s]Training CobwebTree:  78%|  | 8445/10788 [04:00<01:08, 34.45it/s]Training CobwebTree:  78%|  | 8449/10788 [04:00<01:11, 32.76it/s]Training CobwebTree:  78%|  | 8453/10788 [04:00<01:08, 33.92it/s]Training CobwebTree:  78%|  | 8457/10788 [04:00<01:10, 32.89it/s]Training CobwebTree:  78%|  | 8461/10788 [04:01<01:13, 31.66it/s]Training CobwebTree:  78%|  | 8465/10788 [04:01<01:13, 31.78it/s]Training CobwebTree:  79%|  | 8469/10788 [04:01<01:13, 31.40it/s]Training CobwebTree:  79%|  | 8473/10788 [04:01<01:12, 31.90it/s]Training CobwebTree:  79%|  | 8477/10788 [04:01<01:10, 33.00it/s]Training CobwebTree:  79%|  | 8481/10788 [04:01<01:11, 32.27it/s]Training CobwebTree:  79%|  | 8485/10788 [04:01<01:14, 30.86it/s]Training CobwebTree:  79%|  | 8489/10788 [04:01<01:13, 31.40it/s]Training CobwebTree:  79%|  | 8493/10788 [04:02<01:10, 32.51it/s]Training CobwebTree:  79%|  | 8497/10788 [04:02<01:08, 33.48it/s]Training CobwebTree:  79%|  | 8501/10788 [04:02<01:09, 32.88it/s]Training CobwebTree:  79%|  | 8505/10788 [04:02<01:07, 33.89it/s]Training CobwebTree:  79%|  | 8509/10788 [04:02<01:09, 32.67it/s]Training CobwebTree:  79%|  | 8513/10788 [04:02<01:09, 32.90it/s]Training CobwebTree:  79%|  | 8517/10788 [04:02<01:09, 32.47it/s]Training CobwebTree:  79%|  | 8521/10788 [04:02<01:07, 33.45it/s]Training CobwebTree:  79%|  | 8525/10788 [04:02<01:07, 33.32it/s]Training CobwebTree:  79%|  | 8529/10788 [04:03<01:07, 33.48it/s]Training CobwebTree:  79%|  | 8533/10788 [04:03<01:06, 33.95it/s]Training CobwebTree:  79%|  | 8537/10788 [04:03<01:07, 33.20it/s]Training CobwebTree:  79%|  | 8541/10788 [04:03<01:11, 31.39it/s]Training CobwebTree:  79%|  | 8545/10788 [04:03<01:09, 32.48it/s]Training CobwebTree:  79%|  | 8549/10788 [04:03<01:07, 33.33it/s]Training CobwebTree:  79%|  | 8553/10788 [04:03<01:07, 33.19it/s]Training CobwebTree:  79%|  | 8557/10788 [04:03<01:09, 31.89it/s]Training CobwebTree:  79%|  | 8561/10788 [04:04<01:09, 32.00it/s]Training CobwebTree:  79%|  | 8565/10788 [04:04<01:09, 31.91it/s]Training CobwebTree:  79%|  | 8569/10788 [04:04<01:12, 30.74it/s]Training CobwebTree:  79%|  | 8573/10788 [04:04<01:13, 30.27it/s]Training CobwebTree:  80%|  | 8577/10788 [04:04<01:12, 30.36it/s]Training CobwebTree:  80%|  | 8581/10788 [04:04<01:10, 31.37it/s]Training CobwebTree:  80%|  | 8585/10788 [04:04<01:08, 32.14it/s]Training CobwebTree:  80%|  | 8589/10788 [04:04<01:09, 31.69it/s]Training CobwebTree:  80%|  | 8593/10788 [04:05<01:13, 30.02it/s]Training CobwebTree:  80%|  | 8597/10788 [04:05<01:11, 30.76it/s]Training CobwebTree:  80%|  | 8601/10788 [04:05<01:08, 31.96it/s]Training CobwebTree:  80%|  | 8605/10788 [04:05<01:07, 32.38it/s]Training CobwebTree:  80%|  | 8609/10788 [04:05<01:09, 31.53it/s]Training CobwebTree:  80%|  | 8613/10788 [04:05<01:10, 31.05it/s]Training CobwebTree:  80%|  | 8617/10788 [04:05<01:10, 30.84it/s]Training CobwebTree:  80%|  | 8621/10788 [04:06<01:12, 29.97it/s]Training CobwebTree:  80%|  | 8625/10788 [04:06<01:08, 31.75it/s]Training CobwebTree:  80%|  | 8629/10788 [04:06<01:07, 31.96it/s]Training CobwebTree:  80%|  | 8633/10788 [04:06<01:08, 31.63it/s]Training CobwebTree:  80%|  | 8637/10788 [04:06<01:09, 31.10it/s]Training CobwebTree:  80%|  | 8641/10788 [04:06<01:08, 31.35it/s]Training CobwebTree:  80%|  | 8645/10788 [04:06<01:09, 31.03it/s]Training CobwebTree:  80%|  | 8649/10788 [04:06<01:06, 32.19it/s]Training CobwebTree:  80%|  | 8653/10788 [04:07<01:04, 32.94it/s]Training CobwebTree:  80%|  | 8657/10788 [04:07<01:02, 33.85it/s]Training CobwebTree:  80%|  | 8661/10788 [04:07<01:02, 33.85it/s]Training CobwebTree:  80%|  | 8665/10788 [04:07<01:01, 34.60it/s]Training CobwebTree:  80%|  | 8669/10788 [04:07<01:06, 31.65it/s]Training CobwebTree:  80%|  | 8673/10788 [04:07<01:06, 31.60it/s]Training CobwebTree:  80%|  | 8677/10788 [04:07<01:06, 31.95it/s]Training CobwebTree:  80%|  | 8681/10788 [04:07<01:04, 32.80it/s]Training CobwebTree:  81%|  | 8685/10788 [04:08<01:05, 32.23it/s]Training CobwebTree:  81%|  | 8689/10788 [04:08<01:09, 30.12it/s]Training CobwebTree:  81%|  | 8693/10788 [04:08<01:06, 31.72it/s]Training CobwebTree:  81%|  | 8697/10788 [04:08<01:08, 30.44it/s]Training CobwebTree:  81%|  | 8701/10788 [04:08<01:05, 31.99it/s]Training CobwebTree:  81%|  | 8705/10788 [04:08<01:03, 32.85it/s]Training CobwebTree:  81%|  | 8709/10788 [04:08<01:04, 32.31it/s]Training CobwebTree:  81%|  | 8713/10788 [04:08<01:02, 33.33it/s]Training CobwebTree:  81%|  | 8717/10788 [04:08<01:01, 33.86it/s]Training CobwebTree:  81%|  | 8721/10788 [04:09<01:03, 32.61it/s]Training CobwebTree:  81%|  | 8725/10788 [04:09<01:04, 32.16it/s]Training CobwebTree:  81%|  | 8729/10788 [04:09<01:04, 31.91it/s]Training CobwebTree:  81%|  | 8733/10788 [04:09<01:06, 31.11it/s]Training CobwebTree:  81%|  | 8737/10788 [04:09<01:07, 30.45it/s]Training CobwebTree:  81%|  | 8741/10788 [04:09<01:04, 31.55it/s]Training CobwebTree:  81%|  | 8745/10788 [04:09<01:04, 31.73it/s]Training CobwebTree:  81%|  | 8749/10788 [04:10<01:05, 31.13it/s]Training CobwebTree:  81%|  | 8753/10788 [04:10<01:04, 31.52it/s]Training CobwebTree:  81%|  | 8757/10788 [04:10<01:04, 31.50it/s]Training CobwebTree:  81%|  | 8761/10788 [04:10<01:05, 31.04it/s]Training CobwebTree:  81%|  | 8765/10788 [04:10<01:04, 31.34it/s]Training CobwebTree:  81%| | 8769/10788 [04:10<01:03, 31.67it/s]Training CobwebTree:  81%| | 8773/10788 [04:10<01:02, 32.32it/s]Training CobwebTree:  81%| | 8777/10788 [04:10<01:02, 32.00it/s]Training CobwebTree:  81%| | 8781/10788 [04:11<00:59, 33.46it/s]Training CobwebTree:  81%| | 8785/10788 [04:11<00:59, 33.81it/s]Training CobwebTree:  81%| | 8789/10788 [04:11<01:00, 33.13it/s]Training CobwebTree:  82%| | 8793/10788 [04:11<01:03, 31.56it/s]Training CobwebTree:  82%| | 8797/10788 [04:11<01:02, 31.68it/s]Training CobwebTree:  82%| | 8801/10788 [04:11<01:05, 30.49it/s]Training CobwebTree:  82%| | 8805/10788 [04:11<01:03, 31.37it/s]Training CobwebTree:  82%| | 8809/10788 [04:11<01:02, 31.42it/s]Training CobwebTree:  82%| | 8813/10788 [04:12<01:01, 32.25it/s]Training CobwebTree:  82%| | 8817/10788 [04:12<01:01, 31.87it/s]Training CobwebTree:  82%| | 8821/10788 [04:12<00:59, 33.30it/s]Training CobwebTree:  82%| | 8825/10788 [04:12<00:59, 33.05it/s]Training CobwebTree:  82%| | 8829/10788 [04:12<01:01, 32.09it/s]Training CobwebTree:  82%| | 8833/10788 [04:12<01:00, 32.21it/s]Training CobwebTree:  82%| | 8837/10788 [04:12<00:59, 32.90it/s]Training CobwebTree:  82%| | 8841/10788 [04:12<00:56, 34.18it/s]Training CobwebTree:  82%| | 8845/10788 [04:13<00:59, 32.40it/s]Training CobwebTree:  82%| | 8849/10788 [04:13<00:58, 33.04it/s]Training CobwebTree:  82%| | 8853/10788 [04:13<00:59, 32.42it/s]Training CobwebTree:  82%| | 8857/10788 [04:13<01:02, 31.14it/s]Training CobwebTree:  82%| | 8861/10788 [04:13<00:59, 32.17it/s]Training CobwebTree:  82%| | 8865/10788 [04:13<01:02, 30.98it/s]Training CobwebTree:  82%| | 8869/10788 [04:13<00:59, 32.52it/s]Training CobwebTree:  82%| | 8873/10788 [04:13<01:01, 31.02it/s]Training CobwebTree:  82%| | 8877/10788 [04:14<00:59, 32.08it/s]Training CobwebTree:  82%| | 8881/10788 [04:14<01:00, 31.29it/s]Training CobwebTree:  82%| | 8885/10788 [04:14<00:57, 32.83it/s]Training CobwebTree:  82%| | 8889/10788 [04:14<00:55, 34.01it/s]Training CobwebTree:  82%| | 8893/10788 [04:14<00:54, 34.88it/s]Training CobwebTree:  82%| | 8897/10788 [04:14<00:53, 35.21it/s]Training CobwebTree:  83%| | 8901/10788 [04:14<00:54, 34.68it/s]Training CobwebTree:  83%| | 8905/10788 [04:14<00:55, 34.02it/s]Training CobwebTree:  83%| | 8909/10788 [04:14<00:56, 33.22it/s]Training CobwebTree:  83%| | 8913/10788 [04:15<00:55, 33.82it/s]Training CobwebTree:  83%| | 8917/10788 [04:15<00:57, 32.29it/s]Training CobwebTree:  83%| | 8921/10788 [04:15<00:57, 32.45it/s]Training CobwebTree:  83%| | 8925/10788 [04:15<00:57, 32.58it/s]Training CobwebTree:  83%| | 8929/10788 [04:15<00:58, 31.77it/s]Training CobwebTree:  83%| | 8933/10788 [04:15<01:00, 30.76it/s]Training CobwebTree:  83%| | 8937/10788 [04:15<01:01, 29.98it/s]Training CobwebTree:  83%| | 8941/10788 [04:15<01:01, 30.04it/s]Training CobwebTree:  83%| | 8945/10788 [04:16<00:59, 31.00it/s]Training CobwebTree:  83%| | 8949/10788 [04:16<00:57, 31.97it/s]Training CobwebTree:  83%| | 8953/10788 [04:16<01:00, 30.30it/s]Training CobwebTree:  83%| | 8957/10788 [04:16<00:59, 30.86it/s]Training CobwebTree:  83%| | 8961/10788 [04:16<00:58, 31.38it/s]Training CobwebTree:  83%| | 8965/10788 [04:16<00:57, 31.69it/s]Training CobwebTree:  83%| | 8969/10788 [04:16<00:55, 32.59it/s]Training CobwebTree:  83%| | 8973/10788 [04:16<00:57, 31.67it/s]Training CobwebTree:  83%| | 8977/10788 [04:17<00:57, 31.63it/s]Training CobwebTree:  83%| | 8981/10788 [04:17<00:57, 31.65it/s]Training CobwebTree:  83%| | 8985/10788 [04:17<00:56, 31.88it/s]Training CobwebTree:  83%| | 8989/10788 [04:17<00:56, 32.12it/s]Training CobwebTree:  83%| | 8993/10788 [04:17<00:55, 32.17it/s]Training CobwebTree:  83%| | 8997/10788 [04:17<00:56, 31.74it/s]Training CobwebTree:  83%| | 9001/10788 [04:17<00:58, 30.74it/s]Training CobwebTree:  83%| | 9005/10788 [04:17<00:55, 32.05it/s]Training CobwebTree:  84%| | 9009/10788 [04:18<00:55, 32.05it/s]Training CobwebTree:  84%| | 9013/10788 [04:18<00:56, 31.33it/s]Training CobwebTree:  84%| | 9017/10788 [04:18<00:57, 30.83it/s]Training CobwebTree:  84%| | 9021/10788 [04:18<00:56, 31.12it/s]Training CobwebTree:  84%| | 9025/10788 [04:18<00:54, 32.19it/s]Training CobwebTree:  84%| | 9029/10788 [04:18<00:54, 32.35it/s]Training CobwebTree:  84%| | 9033/10788 [04:18<00:52, 33.54it/s]Training CobwebTree:  84%| | 9037/10788 [04:18<00:53, 32.53it/s]Training CobwebTree:  84%| | 9041/10788 [04:19<00:53, 32.95it/s]Training CobwebTree:  84%| | 9045/10788 [04:19<00:53, 32.85it/s]Training CobwebTree:  84%| | 9049/10788 [04:19<00:54, 31.70it/s]Training CobwebTree:  84%| | 9053/10788 [04:19<00:56, 30.50it/s]Training CobwebTree:  84%| | 9057/10788 [04:19<00:57, 30.16it/s]Training CobwebTree:  84%| | 9061/10788 [04:19<00:55, 30.90it/s]Training CobwebTree:  84%| | 9065/10788 [04:19<00:53, 32.35it/s]Training CobwebTree:  84%| | 9069/10788 [04:20<00:53, 32.38it/s]Training CobwebTree:  84%| | 9073/10788 [04:20<00:50, 33.63it/s]Training CobwebTree:  84%| | 9077/10788 [04:20<00:51, 32.92it/s]Training CobwebTree:  84%| | 9081/10788 [04:20<00:51, 32.93it/s]Training CobwebTree:  84%| | 9085/10788 [04:20<00:52, 32.63it/s]Training CobwebTree:  84%| | 9089/10788 [04:20<00:53, 31.84it/s]Training CobwebTree:  84%| | 9093/10788 [04:20<00:53, 31.89it/s]Training CobwebTree:  84%| | 9097/10788 [04:20<00:54, 30.90it/s]Training CobwebTree:  84%| | 9101/10788 [04:21<00:53, 31.40it/s]Training CobwebTree:  84%| | 9105/10788 [04:21<00:51, 32.85it/s]Training CobwebTree:  84%| | 9109/10788 [04:21<00:56, 29.72it/s]Training CobwebTree:  84%| | 9113/10788 [04:21<00:56, 29.89it/s]Training CobwebTree:  85%| | 9117/10788 [04:21<00:53, 31.21it/s]Training CobwebTree:  85%| | 9121/10788 [04:21<00:52, 31.97it/s]Training CobwebTree:  85%| | 9125/10788 [04:21<00:54, 30.62it/s]Training CobwebTree:  85%| | 9129/10788 [04:21<00:55, 29.73it/s]Training CobwebTree:  85%| | 9133/10788 [04:22<00:53, 30.70it/s]Training CobwebTree:  85%| | 9137/10788 [04:22<00:54, 30.28it/s]Training CobwebTree:  85%| | 9141/10788 [04:22<00:53, 30.72it/s]Training CobwebTree:  85%| | 9145/10788 [04:22<00:52, 31.53it/s]Training CobwebTree:  85%| | 9149/10788 [04:22<00:52, 31.39it/s]Training CobwebTree:  85%| | 9153/10788 [04:22<00:52, 30.96it/s]Training CobwebTree:  85%| | 9157/10788 [04:22<00:51, 31.67it/s]Training CobwebTree:  85%| | 9161/10788 [04:22<00:52, 31.27it/s]Training CobwebTree:  85%| | 9165/10788 [04:23<00:51, 31.56it/s]Training CobwebTree:  85%| | 9169/10788 [04:23<00:50, 32.32it/s]Training CobwebTree:  85%| | 9173/10788 [04:23<00:51, 31.58it/s]Training CobwebTree:  85%| | 9177/10788 [04:23<00:54, 29.62it/s]Training CobwebTree:  85%| | 9181/10788 [04:23<00:52, 30.52it/s]Training CobwebTree:  85%| | 9185/10788 [04:23<00:52, 30.58it/s]Training CobwebTree:  85%| | 9189/10788 [04:23<00:53, 29.62it/s]Training CobwebTree:  85%| | 9193/10788 [04:23<00:51, 30.67it/s]Training CobwebTree:  85%| | 9197/10788 [04:24<00:50, 31.80it/s]Training CobwebTree:  85%| | 9201/10788 [04:24<00:49, 32.14it/s]Training CobwebTree:  85%| | 9205/10788 [04:24<00:49, 31.78it/s]Training CobwebTree:  85%| | 9209/10788 [04:24<00:50, 31.55it/s]Training CobwebTree:  85%| | 9213/10788 [04:24<00:52, 30.27it/s]Training CobwebTree:  85%| | 9217/10788 [04:24<00:51, 30.71it/s]Training CobwebTree:  85%| | 9221/10788 [04:24<00:48, 32.27it/s]Training CobwebTree:  86%| | 9225/10788 [04:24<00:49, 31.54it/s]Training CobwebTree:  86%| | 9229/10788 [04:25<00:48, 32.05it/s]Training CobwebTree:  86%| | 9233/10788 [04:25<00:51, 30.40it/s]Training CobwebTree:  86%| | 9237/10788 [04:25<00:47, 32.45it/s]Training CobwebTree:  86%| | 9241/10788 [04:25<00:48, 31.97it/s]Training CobwebTree:  86%| | 9245/10788 [04:25<00:48, 31.67it/s]Training CobwebTree:  86%| | 9249/10788 [04:25<00:50, 30.59it/s]Training CobwebTree:  86%| | 9253/10788 [04:25<00:50, 30.67it/s]Training CobwebTree:  86%| | 9257/10788 [04:26<00:48, 31.27it/s]Training CobwebTree:  86%| | 9261/10788 [04:26<00:50, 30.42it/s]Training CobwebTree:  86%| | 9265/10788 [04:26<00:49, 30.74it/s]Training CobwebTree:  86%| | 9269/10788 [04:26<00:49, 30.66it/s]Training CobwebTree:  86%| | 9273/10788 [04:26<00:50, 29.74it/s]Training CobwebTree:  86%| | 9277/10788 [04:26<00:50, 29.99it/s]Training CobwebTree:  86%| | 9281/10788 [04:26<00:50, 30.00it/s]Training CobwebTree:  86%| | 9285/10788 [04:26<00:49, 30.24it/s]Training CobwebTree:  86%| | 9289/10788 [04:27<00:48, 30.69it/s]Training CobwebTree:  86%| | 9293/10788 [04:27<00:48, 30.67it/s]Training CobwebTree:  86%| | 9297/10788 [04:27<00:48, 30.79it/s]Training CobwebTree:  86%| | 9301/10788 [04:27<00:47, 31.53it/s]Training CobwebTree:  86%| | 9305/10788 [04:27<00:47, 31.29it/s]Training CobwebTree:  86%| | 9309/10788 [04:27<00:46, 31.98it/s]Training CobwebTree:  86%| | 9313/10788 [04:27<00:45, 32.62it/s]Training CobwebTree:  86%| | 9317/10788 [04:27<00:45, 32.66it/s]Training CobwebTree:  86%| | 9321/10788 [04:28<00:45, 32.46it/s]Training CobwebTree:  86%| | 9325/10788 [04:28<00:46, 31.73it/s]Training CobwebTree:  86%| | 9329/10788 [04:28<00:46, 31.41it/s]Training CobwebTree:  87%| | 9333/10788 [04:28<00:46, 30.98it/s]Training CobwebTree:  87%| | 9337/10788 [04:28<00:48, 29.94it/s]Training CobwebTree:  87%| | 9341/10788 [04:28<00:47, 30.26it/s]Training CobwebTree:  87%| | 9345/10788 [04:28<00:47, 30.32it/s]Training CobwebTree:  87%| | 9349/10788 [04:28<00:46, 31.02it/s]Training CobwebTree:  87%| | 9353/10788 [04:29<00:45, 31.42it/s]Training CobwebTree:  87%| | 9357/10788 [04:29<00:46, 30.84it/s]Training CobwebTree:  87%| | 9361/10788 [04:29<00:45, 31.57it/s]Training CobwebTree:  87%| | 9365/10788 [04:29<00:45, 31.35it/s]Training CobwebTree:  87%| | 9369/10788 [04:29<00:45, 31.32it/s]Training CobwebTree:  87%| | 9373/10788 [04:29<00:43, 32.20it/s]Training CobwebTree:  87%| | 9377/10788 [04:29<00:43, 32.59it/s]Training CobwebTree:  87%| | 9381/10788 [04:30<00:45, 31.02it/s]Training CobwebTree:  87%| | 9385/10788 [04:30<00:46, 29.98it/s]Training CobwebTree:  87%| | 9389/10788 [04:30<00:45, 30.54it/s]Training CobwebTree:  87%| | 9393/10788 [04:30<00:44, 31.42it/s]Training CobwebTree:  87%| | 9397/10788 [04:30<00:42, 32.67it/s]Training CobwebTree:  87%| | 9401/10788 [04:30<00:43, 31.99it/s]Training CobwebTree:  87%| | 9405/10788 [04:30<00:43, 31.90it/s]Training CobwebTree:  87%| | 9409/10788 [04:30<00:41, 33.09it/s]Training CobwebTree:  87%| | 9413/10788 [04:30<00:40, 33.59it/s]Training CobwebTree:  87%| | 9417/10788 [04:31<00:41, 33.16it/s]Training CobwebTree:  87%| | 9421/10788 [04:31<00:42, 32.32it/s]Training CobwebTree:  87%| | 9425/10788 [04:31<00:42, 32.30it/s]Training CobwebTree:  87%| | 9429/10788 [04:31<00:41, 32.73it/s]Training CobwebTree:  87%| | 9433/10788 [04:31<00:41, 32.55it/s]Training CobwebTree:  87%| | 9437/10788 [04:31<00:42, 31.93it/s]Training CobwebTree:  88%| | 9441/10788 [04:31<00:42, 31.56it/s]Training CobwebTree:  88%| | 9445/10788 [04:31<00:41, 32.56it/s]Training CobwebTree:  88%| | 9449/10788 [04:32<00:42, 31.36it/s]Training CobwebTree:  88%| | 9453/10788 [04:32<00:42, 31.45it/s]Training CobwebTree:  88%| | 9457/10788 [04:32<00:43, 30.71it/s]Training CobwebTree:  88%| | 9461/10788 [04:32<00:42, 30.97it/s]Training CobwebTree:  88%| | 9465/10788 [04:32<00:43, 30.09it/s]Training CobwebTree:  88%| | 9469/10788 [04:32<00:42, 31.00it/s]Training CobwebTree:  88%| | 9473/10788 [04:32<00:43, 30.32it/s]Training CobwebTree:  88%| | 9477/10788 [04:33<00:42, 30.72it/s]Training CobwebTree:  88%| | 9481/10788 [04:33<00:41, 31.49it/s]Training CobwebTree:  88%| | 9485/10788 [04:33<00:41, 31.70it/s]Training CobwebTree:  88%| | 9489/10788 [04:33<00:42, 30.88it/s]Training CobwebTree:  88%| | 9493/10788 [04:33<00:40, 31.70it/s]Training CobwebTree:  88%| | 9497/10788 [04:33<00:41, 30.82it/s]Training CobwebTree:  88%| | 9501/10788 [04:33<00:44, 29.02it/s]Training CobwebTree:  88%| | 9505/10788 [04:33<00:45, 28.50it/s]Training CobwebTree:  88%| | 9509/10788 [04:34<00:41, 30.76it/s]Training CobwebTree:  88%| | 9513/10788 [04:34<00:44, 28.93it/s]Training CobwebTree:  88%| | 9516/10788 [04:34<00:43, 28.99it/s]Training CobwebTree:  88%| | 9520/10788 [04:34<00:42, 29.93it/s]Training CobwebTree:  88%| | 9524/10788 [04:34<00:41, 30.75it/s]Training CobwebTree:  88%| | 9528/10788 [04:34<00:41, 30.57it/s]Training CobwebTree:  88%| | 9532/10788 [04:34<00:42, 29.65it/s]Training CobwebTree:  88%| | 9535/10788 [04:34<00:42, 29.71it/s]Training CobwebTree:  88%| | 9538/10788 [04:35<00:42, 29.30it/s]Training CobwebTree:  88%| | 9541/10788 [04:35<00:43, 28.83it/s]Training CobwebTree:  88%| | 9544/10788 [04:35<00:42, 29.11it/s]Training CobwebTree:  89%| | 9548/10788 [04:35<00:41, 30.10it/s]Training CobwebTree:  89%| | 9552/10788 [04:35<00:39, 31.08it/s]Training CobwebTree:  89%| | 9556/10788 [04:35<00:38, 32.16it/s]Training CobwebTree:  89%| | 9560/10788 [04:35<00:39, 30.93it/s]Training CobwebTree:  89%| | 9564/10788 [04:35<00:41, 29.81it/s]Training CobwebTree:  89%| | 9568/10788 [04:36<00:37, 32.15it/s]Training CobwebTree:  89%| | 9572/10788 [04:36<00:37, 32.01it/s]Training CobwebTree:  89%| | 9576/10788 [04:36<00:37, 32.24it/s]Training CobwebTree:  89%| | 9580/10788 [04:36<00:38, 31.31it/s]Training CobwebTree:  89%| | 9584/10788 [04:36<00:36, 33.32it/s]Training CobwebTree:  89%| | 9588/10788 [04:36<00:37, 31.72it/s]Training CobwebTree:  89%| | 9592/10788 [04:36<00:39, 30.23it/s]Training CobwebTree:  89%| | 9596/10788 [04:36<00:39, 30.28it/s]Training CobwebTree:  89%| | 9600/10788 [04:37<00:37, 31.65it/s]Training CobwebTree:  89%| | 9604/10788 [04:37<00:38, 30.53it/s]Training CobwebTree:  89%| | 9608/10788 [04:37<00:40, 28.83it/s]Training CobwebTree:  89%| | 9612/10788 [04:37<00:39, 29.60it/s]Training CobwebTree:  89%| | 9615/10788 [04:37<00:41, 28.34it/s]Training CobwebTree:  89%| | 9618/10788 [04:37<00:40, 28.58it/s]Training CobwebTree:  89%| | 9622/10788 [04:37<00:39, 29.51it/s]Training CobwebTree:  89%| | 9626/10788 [04:37<00:37, 31.15it/s]Training CobwebTree:  89%| | 9630/10788 [04:38<00:37, 31.28it/s]Training CobwebTree:  89%| | 9634/10788 [04:38<00:37, 31.18it/s]Training CobwebTree:  89%| | 9638/10788 [04:38<00:38, 29.69it/s]Training CobwebTree:  89%| | 9642/10788 [04:38<00:37, 30.20it/s]Training CobwebTree:  89%| | 9646/10788 [04:38<00:39, 29.02it/s]Training CobwebTree:  89%| | 9650/10788 [04:38<00:37, 30.29it/s]Training CobwebTree:  89%| | 9654/10788 [04:38<00:35, 31.63it/s]Training CobwebTree:  90%| | 9658/10788 [04:38<00:36, 31.30it/s]Training CobwebTree:  90%| | 9662/10788 [04:39<00:35, 32.00it/s]Training CobwebTree:  90%| | 9666/10788 [04:39<00:35, 32.00it/s]Training CobwebTree:  90%| | 9670/10788 [04:39<00:34, 32.28it/s]Training CobwebTree:  90%| | 9674/10788 [04:39<00:35, 31.17it/s]Training CobwebTree:  90%| | 9678/10788 [04:39<00:35, 31.30it/s]Training CobwebTree:  90%| | 9682/10788 [04:39<00:33, 32.76it/s]Training CobwebTree:  90%| | 9686/10788 [04:39<00:34, 31.79it/s]Training CobwebTree:  90%| | 9690/10788 [04:40<00:35, 30.78it/s]Training CobwebTree:  90%| | 9694/10788 [04:40<00:35, 31.05it/s]Training CobwebTree:  90%| | 9698/10788 [04:40<00:34, 31.44it/s]Training CobwebTree:  90%| | 9702/10788 [04:40<00:35, 30.80it/s]Training CobwebTree:  90%| | 9706/10788 [04:40<00:34, 31.07it/s]Training CobwebTree:  90%| | 9710/10788 [04:40<00:33, 31.82it/s]Training CobwebTree:  90%| | 9714/10788 [04:40<00:32, 32.63it/s]Training CobwebTree:  90%| | 9718/10788 [04:40<00:35, 30.15it/s]Training CobwebTree:  90%| | 9722/10788 [04:41<00:34, 30.46it/s]Training CobwebTree:  90%| | 9726/10788 [04:41<00:34, 31.08it/s]Training CobwebTree:  90%| | 9730/10788 [04:41<00:32, 32.32it/s]Training CobwebTree:  90%| | 9734/10788 [04:41<00:31, 32.96it/s]Training CobwebTree:  90%| | 9738/10788 [04:41<00:31, 33.78it/s]Training CobwebTree:  90%| | 9742/10788 [04:41<00:31, 32.78it/s]Training CobwebTree:  90%| | 9746/10788 [04:41<00:31, 32.70it/s]Training CobwebTree:  90%| | 9750/10788 [04:41<00:34, 30.31it/s]Training CobwebTree:  90%| | 9754/10788 [04:42<00:34, 30.27it/s]Training CobwebTree:  90%| | 9758/10788 [04:42<00:33, 31.21it/s]Training CobwebTree:  90%| | 9762/10788 [04:42<00:34, 30.06it/s]Training CobwebTree:  91%| | 9766/10788 [04:42<00:33, 30.12it/s]Training CobwebTree:  91%| | 9770/10788 [04:42<00:32, 31.24it/s]Training CobwebTree:  91%| | 9774/10788 [04:42<00:33, 30.46it/s]Training CobwebTree:  91%| | 9778/10788 [04:42<00:33, 30.48it/s]Training CobwebTree:  91%| | 9782/10788 [04:42<00:31, 31.47it/s]Training CobwebTree:  91%| | 9786/10788 [04:43<00:31, 31.47it/s]Training CobwebTree:  91%| | 9790/10788 [04:43<00:33, 29.72it/s]Training CobwebTree:  91%| | 9794/10788 [04:43<00:34, 28.87it/s]Training CobwebTree:  91%| | 9797/10788 [04:43<00:34, 28.41it/s]Training CobwebTree:  91%| | 9801/10788 [04:43<00:33, 29.77it/s]Training CobwebTree:  91%| | 9805/10788 [04:43<00:30, 31.78it/s]Training CobwebTree:  91%| | 9809/10788 [04:43<00:29, 32.83it/s]Training CobwebTree:  91%| | 9813/10788 [04:43<00:29, 33.45it/s]Training CobwebTree:  91%| | 9817/10788 [04:44<00:29, 32.53it/s]Training CobwebTree:  91%| | 9821/10788 [04:44<00:29, 32.67it/s]Training CobwebTree:  91%| | 9825/10788 [04:44<00:31, 30.80it/s]Training CobwebTree:  91%| | 9829/10788 [04:44<00:30, 31.15it/s]Training CobwebTree:  91%| | 9833/10788 [04:44<00:30, 31.17it/s]Training CobwebTree:  91%| | 9837/10788 [04:44<00:30, 30.74it/s]Training CobwebTree:  91%| | 9841/10788 [04:44<00:30, 30.95it/s]Training CobwebTree:  91%|| 9845/10788 [04:44<00:30, 30.69it/s]Training CobwebTree:  91%|| 9849/10788 [04:45<00:31, 29.85it/s]Training CobwebTree:  91%|| 9853/10788 [04:45<00:29, 31.23it/s]Training CobwebTree:  91%|| 9857/10788 [04:45<00:31, 29.93it/s]Training CobwebTree:  91%|| 9861/10788 [04:45<00:29, 31.42it/s]Training CobwebTree:  91%|| 9865/10788 [04:45<00:27, 33.10it/s]Training CobwebTree:  91%|| 9869/10788 [04:45<00:28, 32.46it/s]Training CobwebTree:  92%|| 9873/10788 [04:45<00:28, 31.82it/s]Training CobwebTree:  92%|| 9877/10788 [04:45<00:28, 31.75it/s]Training CobwebTree:  92%|| 9881/10788 [04:46<00:28, 31.92it/s]Training CobwebTree:  92%|| 9885/10788 [04:46<00:29, 30.68it/s]Training CobwebTree:  92%|| 9889/10788 [04:46<00:28, 31.13it/s]Training CobwebTree:  92%|| 9893/10788 [04:46<00:28, 31.40it/s]Training CobwebTree:  92%|| 9897/10788 [04:46<00:28, 31.58it/s]Training CobwebTree:  92%|| 9901/10788 [04:46<00:29, 30.31it/s]Training CobwebTree:  92%|| 9905/10788 [04:46<00:29, 29.72it/s]Training CobwebTree:  92%|| 9908/10788 [04:47<00:29, 29.49it/s]Training CobwebTree:  92%|| 9911/10788 [04:47<00:30, 28.37it/s]Training CobwebTree:  92%|| 9915/10788 [04:47<00:29, 29.94it/s]Training CobwebTree:  92%|| 9918/10788 [04:47<00:30, 28.89it/s]Training CobwebTree:  92%|| 9922/10788 [04:47<00:28, 30.04it/s]Training CobwebTree:  92%|| 9926/10788 [04:47<00:28, 29.76it/s]Training CobwebTree:  92%|| 9930/10788 [04:47<00:28, 30.39it/s]Training CobwebTree:  92%|| 9934/10788 [04:47<00:27, 30.90it/s]Training CobwebTree:  92%|| 9938/10788 [04:47<00:26, 31.81it/s]Training CobwebTree:  92%|| 9942/10788 [04:48<00:28, 29.29it/s]Training CobwebTree:  92%|| 9946/10788 [04:48<00:27, 30.74it/s]Training CobwebTree:  92%|| 9950/10788 [04:48<00:26, 31.72it/s]Training CobwebTree:  92%|| 9954/10788 [04:48<00:25, 33.17it/s]Training CobwebTree:  92%|| 9958/10788 [04:48<00:26, 31.92it/s]Training CobwebTree:  92%|| 9962/10788 [04:48<00:26, 30.72it/s]Training CobwebTree:  92%|| 9966/10788 [04:48<00:26, 30.46it/s]Training CobwebTree:  92%|| 9970/10788 [04:49<00:26, 30.45it/s]Training CobwebTree:  92%|| 9974/10788 [04:49<00:26, 30.64it/s]Training CobwebTree:  92%|| 9978/10788 [04:49<00:26, 31.04it/s]Training CobwebTree:  93%|| 9982/10788 [04:49<00:24, 32.55it/s]Training CobwebTree:  93%|| 9986/10788 [04:49<00:24, 32.53it/s]Training CobwebTree:  93%|| 9990/10788 [04:49<00:25, 30.97it/s]Training CobwebTree:  93%|| 9994/10788 [04:49<00:25, 31.04it/s]Training CobwebTree:  93%|| 9998/10788 [04:49<00:26, 29.98it/s]Training CobwebTree:  93%|| 10002/10788 [04:50<00:25, 30.74it/s]Training CobwebTree:  93%|| 10006/10788 [04:50<00:25, 30.38it/s]Training CobwebTree:  93%|| 10010/10788 [04:50<00:24, 31.32it/s]Training CobwebTree:  93%|| 10014/10788 [04:50<00:24, 32.22it/s]Training CobwebTree:  93%|| 10018/10788 [04:50<00:24, 31.85it/s]Training CobwebTree:  93%|| 10022/10788 [04:50<00:23, 32.92it/s]Training CobwebTree:  93%|| 10026/10788 [04:50<00:22, 33.70it/s]Training CobwebTree:  93%|| 10030/10788 [04:50<00:22, 33.28it/s]Training CobwebTree:  93%|| 10034/10788 [04:51<00:23, 32.07it/s]Training CobwebTree:  93%|| 10038/10788 [04:51<00:22, 32.99it/s]Training CobwebTree:  93%|| 10042/10788 [04:51<00:22, 32.82it/s]Training CobwebTree:  93%|| 10046/10788 [04:51<00:22, 33.39it/s]Training CobwebTree:  93%|| 10050/10788 [04:51<00:23, 31.68it/s]Training CobwebTree:  93%|| 10054/10788 [04:51<00:23, 31.91it/s]Training CobwebTree:  93%|| 10058/10788 [04:51<00:22, 32.34it/s]Training CobwebTree:  93%|| 10062/10788 [04:51<00:23, 31.22it/s]Training CobwebTree:  93%|| 10066/10788 [04:52<00:24, 29.99it/s]Training CobwebTree:  93%|| 10070/10788 [04:52<00:23, 30.51it/s]Training CobwebTree:  93%|| 10074/10788 [04:52<00:23, 30.65it/s]Training CobwebTree:  93%|| 10078/10788 [04:52<00:23, 30.54it/s]Training CobwebTree:  93%|| 10082/10788 [04:52<00:23, 30.06it/s]Training CobwebTree:  93%|| 10086/10788 [04:52<00:22, 31.41it/s]Training CobwebTree:  94%|| 10090/10788 [04:52<00:22, 31.57it/s]Training CobwebTree:  94%|| 10094/10788 [04:52<00:21, 32.62it/s]Training CobwebTree:  94%|| 10098/10788 [04:53<00:22, 30.26it/s]Training CobwebTree:  94%|| 10102/10788 [04:53<00:22, 30.57it/s]Training CobwebTree:  94%|| 10106/10788 [04:53<00:22, 30.40it/s]Training CobwebTree:  94%|| 10110/10788 [04:53<00:22, 30.15it/s]Training CobwebTree:  94%|| 10114/10788 [04:53<00:23, 29.16it/s]Training CobwebTree:  94%|| 10118/10788 [04:53<00:22, 29.74it/s]Training CobwebTree:  94%|| 10122/10788 [04:53<00:21, 31.21it/s]Training CobwebTree:  94%|| 10126/10788 [04:54<00:21, 30.34it/s]Training CobwebTree:  94%|| 10130/10788 [04:54<00:21, 30.08it/s]Training CobwebTree:  94%|| 10134/10788 [04:54<00:22, 29.60it/s]Training CobwebTree:  94%|| 10138/10788 [04:54<00:21, 29.94it/s]Training CobwebTree:  94%|| 10142/10788 [04:54<00:22, 29.06it/s]Training CobwebTree:  94%|| 10146/10788 [04:54<00:21, 30.49it/s]Training CobwebTree:  94%|| 10150/10788 [04:54<00:20, 31.00it/s]Training CobwebTree:  94%|| 10154/10788 [04:54<00:19, 31.71it/s]Training CobwebTree:  94%|| 10158/10788 [04:55<00:20, 31.39it/s]Training CobwebTree:  94%|| 10162/10788 [04:55<00:19, 32.24it/s]Training CobwebTree:  94%|| 10166/10788 [04:55<00:20, 30.95it/s]Training CobwebTree:  94%|| 10170/10788 [04:55<00:19, 31.99it/s]Training CobwebTree:  94%|| 10174/10788 [04:55<00:19, 31.84it/s]Training CobwebTree:  94%|| 10178/10788 [04:55<00:19, 31.28it/s]Training CobwebTree:  94%|| 10182/10788 [04:55<00:19, 30.31it/s]Training CobwebTree:  94%|| 10186/10788 [04:55<00:19, 31.38it/s]Training CobwebTree:  94%|| 10190/10788 [04:56<00:19, 31.38it/s]Training CobwebTree:  94%|| 10194/10788 [04:56<00:18, 31.56it/s]Training CobwebTree:  95%|| 10198/10788 [04:56<00:18, 31.23it/s]Training CobwebTree:  95%|| 10202/10788 [04:56<00:18, 30.98it/s]Training CobwebTree:  95%|| 10206/10788 [04:56<00:19, 30.29it/s]Training CobwebTree:  95%|| 10210/10788 [04:56<00:18, 30.75it/s]Training CobwebTree:  95%|| 10214/10788 [04:56<00:18, 31.22it/s]Training CobwebTree:  95%|| 10218/10788 [04:56<00:18, 31.03it/s]Training CobwebTree:  95%|| 10222/10788 [04:57<00:18, 30.76it/s]Training CobwebTree:  95%|| 10226/10788 [04:57<00:18, 31.08it/s]Training CobwebTree:  95%|| 10230/10788 [04:57<00:17, 31.48it/s]Training CobwebTree:  95%|| 10234/10788 [04:57<00:17, 30.79it/s]Training CobwebTree:  95%|| 10238/10788 [04:57<00:18, 30.28it/s]Training CobwebTree:  95%|| 10242/10788 [04:57<00:17, 31.65it/s]Training CobwebTree:  95%|| 10246/10788 [04:57<00:17, 30.95it/s]Training CobwebTree:  95%|| 10250/10788 [04:58<00:17, 31.26it/s]Training CobwebTree:  95%|| 10254/10788 [04:58<00:17, 30.96it/s]Training CobwebTree:  95%|| 10258/10788 [04:58<00:17, 31.13it/s]Training CobwebTree:  95%|| 10262/10788 [04:58<00:16, 31.76it/s]Training CobwebTree:  95%|| 10266/10788 [04:58<00:16, 32.24it/s]Training CobwebTree:  95%|| 10270/10788 [04:58<00:16, 31.64it/s]Training CobwebTree:  95%|| 10274/10788 [04:58<00:16, 30.45it/s]Training CobwebTree:  95%|| 10278/10788 [04:58<00:16, 31.32it/s]Training CobwebTree:  95%|| 10282/10788 [04:59<00:16, 30.20it/s]Training CobwebTree:  95%|| 10286/10788 [04:59<00:16, 30.78it/s]Training CobwebTree:  95%|| 10290/10788 [04:59<00:16, 30.51it/s]Training CobwebTree:  95%|| 10294/10788 [04:59<00:15, 31.00it/s]Training CobwebTree:  95%|| 10298/10788 [04:59<00:16, 29.34it/s]Training CobwebTree:  95%|| 10302/10788 [04:59<00:16, 29.59it/s]Training CobwebTree:  96%|| 10305/10788 [04:59<00:17, 28.40it/s]Training CobwebTree:  96%|| 10309/10788 [04:59<00:15, 30.33it/s]Training CobwebTree:  96%|| 10313/10788 [05:00<00:15, 30.76it/s]Training CobwebTree:  96%|| 10317/10788 [05:00<00:14, 32.57it/s]Training CobwebTree:  96%|| 10321/10788 [05:00<00:14, 33.24it/s]Training CobwebTree:  96%|| 10325/10788 [05:00<00:14, 31.95it/s]Training CobwebTree:  96%|| 10329/10788 [05:00<00:13, 33.30it/s]Training CobwebTree:  96%|| 10333/10788 [05:00<00:13, 33.77it/s]Training CobwebTree:  96%|| 10337/10788 [05:00<00:14, 31.79it/s]Training CobwebTree:  96%|| 10341/10788 [05:00<00:14, 31.71it/s]Training CobwebTree:  96%|| 10345/10788 [05:01<00:14, 30.86it/s]Training CobwebTree:  96%|| 10349/10788 [05:01<00:14, 30.70it/s]Training CobwebTree:  96%|| 10353/10788 [05:01<00:14, 30.94it/s]Training CobwebTree:  96%|| 10357/10788 [05:01<00:13, 31.56it/s]Training CobwebTree:  96%|| 10361/10788 [05:01<00:13, 31.79it/s]Training CobwebTree:  96%|| 10365/10788 [05:01<00:13, 31.68it/s]Training CobwebTree:  96%|| 10369/10788 [05:01<00:13, 32.11it/s]Training CobwebTree:  96%|| 10373/10788 [05:01<00:13, 31.07it/s]Training CobwebTree:  96%|| 10377/10788 [05:02<00:12, 32.12it/s]Training CobwebTree:  96%|| 10381/10788 [05:02<00:13, 29.20it/s]Training CobwebTree:  96%|| 10385/10788 [05:02<00:13, 30.62it/s]Training CobwebTree:  96%|| 10389/10788 [05:02<00:12, 32.32it/s]Training CobwebTree:  96%|| 10393/10788 [05:02<00:12, 30.92it/s]Training CobwebTree:  96%|| 10397/10788 [05:02<00:12, 31.20it/s]Training CobwebTree:  96%|| 10401/10788 [05:02<00:12, 30.29it/s]Training CobwebTree:  96%|| 10405/10788 [05:02<00:12, 30.99it/s]Training CobwebTree:  96%|| 10409/10788 [05:03<00:12, 31.27it/s]Training CobwebTree:  97%|| 10413/10788 [05:03<00:12, 31.11it/s]Training CobwebTree:  97%|| 10417/10788 [05:03<00:11, 31.56it/s]Training CobwebTree:  97%|| 10421/10788 [05:03<00:11, 31.96it/s]Training CobwebTree:  97%|| 10425/10788 [05:03<00:11, 31.88it/s]Training CobwebTree:  97%|| 10429/10788 [05:03<00:11, 32.40it/s]Training CobwebTree:  97%|| 10433/10788 [05:03<00:10, 33.51it/s]Training CobwebTree:  97%|| 10437/10788 [05:03<00:10, 33.08it/s]Training CobwebTree:  97%|| 10441/10788 [05:04<00:10, 32.85it/s]Training CobwebTree:  97%|| 10445/10788 [05:04<00:10, 33.11it/s]Training CobwebTree:  97%|| 10449/10788 [05:04<00:10, 31.78it/s]Training CobwebTree:  97%|| 10453/10788 [05:04<00:10, 32.76it/s]Training CobwebTree:  97%|| 10457/10788 [05:04<00:09, 33.17it/s]Training CobwebTree:  97%|| 10461/10788 [05:04<00:10, 31.98it/s]Training CobwebTree:  97%|| 10465/10788 [05:04<00:10, 29.64it/s]Training CobwebTree:  97%|| 10469/10788 [05:04<00:10, 31.07it/s]Training CobwebTree:  97%|| 10473/10788 [05:05<00:10, 29.16it/s]Training CobwebTree:  97%|| 10477/10788 [05:05<00:10, 29.96it/s]Training CobwebTree:  97%|| 10481/10788 [05:05<00:10, 30.49it/s]Training CobwebTree:  97%|| 10485/10788 [05:05<00:09, 30.94it/s]Training CobwebTree:  97%|| 10489/10788 [05:05<00:09, 30.29it/s]Training CobwebTree:  97%|| 10493/10788 [05:05<00:09, 31.63it/s]Training CobwebTree:  97%|| 10497/10788 [05:05<00:09, 30.41it/s]Training CobwebTree:  97%|| 10501/10788 [05:06<00:09, 31.88it/s]Training CobwebTree:  97%|| 10505/10788 [05:06<00:08, 31.88it/s]Training CobwebTree:  97%|| 10509/10788 [05:06<00:09, 30.64it/s]Training CobwebTree:  97%|| 10513/10788 [05:06<00:09, 29.79it/s]Training CobwebTree:  97%|| 10517/10788 [05:06<00:08, 30.32it/s]Training CobwebTree:  98%|| 10521/10788 [05:06<00:08, 30.31it/s]Training CobwebTree:  98%|| 10525/10788 [05:06<00:09, 29.15it/s]Training CobwebTree:  98%|| 10529/10788 [05:06<00:08, 30.90it/s]Training CobwebTree:  98%|| 10533/10788 [05:07<00:07, 32.38it/s]Training CobwebTree:  98%|| 10537/10788 [05:07<00:07, 32.12it/s]Training CobwebTree:  98%|| 10541/10788 [05:07<00:07, 31.34it/s]Training CobwebTree:  98%|| 10545/10788 [05:07<00:07, 31.83it/s]Training CobwebTree:  98%|| 10549/10788 [05:07<00:07, 31.19it/s]Training CobwebTree:  98%|| 10553/10788 [05:07<00:07, 30.52it/s]Training CobwebTree:  98%|| 10557/10788 [05:07<00:07, 30.46it/s]Training CobwebTree:  98%|| 10561/10788 [05:07<00:07, 31.22it/s]Training CobwebTree:  98%|| 10565/10788 [05:08<00:07, 30.45it/s]Training CobwebTree:  98%|| 10569/10788 [05:08<00:07, 30.26it/s]Training CobwebTree:  98%|| 10573/10788 [05:08<00:07, 29.83it/s]Training CobwebTree:  98%|| 10577/10788 [05:08<00:06, 30.81it/s]Training CobwebTree:  98%|| 10581/10788 [05:08<00:06, 30.72it/s]Training CobwebTree:  98%|| 10585/10788 [05:08<00:06, 30.68it/s]Training CobwebTree:  98%|| 10589/10788 [05:08<00:06, 31.38it/s]Training CobwebTree:  98%|| 10593/10788 [05:09<00:06, 30.16it/s]Training CobwebTree:  98%|| 10597/10788 [05:09<00:06, 29.54it/s]Training CobwebTree:  98%|| 10601/10788 [05:09<00:05, 31.51it/s]Training CobwebTree:  98%|| 10605/10788 [05:09<00:05, 32.52it/s]Training CobwebTree:  98%|| 10609/10788 [05:09<00:05, 32.12it/s]Training CobwebTree:  98%|| 10613/10788 [05:09<00:05, 32.28it/s]Training CobwebTree:  98%|| 10617/10788 [05:09<00:05, 32.25it/s]Training CobwebTree:  98%|| 10621/10788 [05:09<00:05, 31.48it/s]Training CobwebTree:  98%|| 10625/10788 [05:10<00:05, 32.37it/s]Training CobwebTree:  99%|| 10629/10788 [05:10<00:05, 30.82it/s]Training CobwebTree:  99%|| 10633/10788 [05:10<00:05, 30.61it/s]Training CobwebTree:  99%|| 10637/10788 [05:10<00:04, 30.72it/s]Training CobwebTree:  99%|| 10641/10788 [05:10<00:04, 30.61it/s]Training CobwebTree:  99%|| 10645/10788 [05:10<00:04, 31.05it/s]Training CobwebTree:  99%|| 10649/10788 [05:10<00:04, 31.95it/s]Training CobwebTree:  99%|| 10653/10788 [05:10<00:04, 32.14it/s]Training CobwebTree:  99%|| 10657/10788 [05:11<00:04, 31.13it/s]Training CobwebTree:  99%|| 10661/10788 [05:11<00:04, 29.99it/s]Training CobwebTree:  99%|| 10665/10788 [05:11<00:03, 31.41it/s]Training CobwebTree:  99%|| 10669/10788 [05:11<00:03, 33.20it/s]Training CobwebTree:  99%|| 10673/10788 [05:11<00:03, 34.37it/s]Training CobwebTree:  99%|| 10677/10788 [05:11<00:03, 35.15it/s]Training CobwebTree:  99%|| 10681/10788 [05:11<00:03, 34.32it/s]Training CobwebTree:  99%|| 10685/10788 [05:11<00:03, 33.40it/s]Training CobwebTree:  99%|| 10689/10788 [05:12<00:02, 33.65it/s]Training CobwebTree:  99%|| 10693/10788 [05:12<00:02, 33.83it/s]Training CobwebTree:  99%|| 10697/10788 [05:12<00:02, 33.44it/s]Training CobwebTree:  99%|| 10701/10788 [05:12<00:02, 33.87it/s]Training CobwebTree:  99%|| 10705/10788 [05:12<00:02, 33.02it/s]Training CobwebTree:  99%|| 10709/10788 [05:12<00:02, 33.82it/s]Training CobwebTree:  99%|| 10713/10788 [05:12<00:02, 33.30it/s]Training CobwebTree:  99%|| 10717/10788 [05:12<00:02, 32.51it/s]Training CobwebTree:  99%|| 10721/10788 [05:13<00:02, 30.56it/s]Training CobwebTree:  99%|| 10725/10788 [05:13<00:02, 31.37it/s]Training CobwebTree:  99%|| 10729/10788 [05:13<00:01, 31.86it/s]Training CobwebTree:  99%|| 10733/10788 [05:13<00:01, 32.15it/s]Training CobwebTree: 100%|| 10737/10788 [05:13<00:01, 32.25it/s]Training CobwebTree: 100%|| 10741/10788 [05:13<00:01, 33.04it/s]Training CobwebTree: 100%|| 10745/10788 [05:13<00:01, 32.89it/s]Training CobwebTree: 100%|| 10749/10788 [05:13<00:01, 32.49it/s]Training CobwebTree: 100%|| 10753/10788 [05:13<00:01, 32.61it/s]Training CobwebTree: 100%|| 10757/10788 [05:14<00:00, 32.56it/s]Training CobwebTree: 100%|| 10761/10788 [05:14<00:00, 32.88it/s]Training CobwebTree: 100%|| 10765/10788 [05:14<00:00, 31.92it/s]Training CobwebTree: 100%|| 10769/10788 [05:14<00:00, 31.80it/s]Training CobwebTree: 100%|| 10773/10788 [05:14<00:00, 31.03it/s]Training CobwebTree: 100%|| 10777/10788 [05:14<00:00, 31.53it/s]Training CobwebTree: 100%|| 10781/10788 [05:14<00:00, 30.91it/s]Training CobwebTree: 100%|| 10785/10788 [05:14<00:00, 31.14it/s]Training CobwebTree: 100%|| 10788/10788 [05:15<00:00, 34.24it/s]
2025-12-21 10:39:12,833 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 10:39:17,302 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (633 virtual)
2025-12-21 10:39:17,312 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (-11972 virtual)
2025-12-21 10:39:17,317 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (-14019 virtual)
2025-12-21 10:39:17,334 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (-34777 virtual)
2025-12-21 10:39:17,455 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (-88525 virtual)
2025-12-21 10:39:17,625 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (-92686 virtual)
2025-12-21 10:39:17,729 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (-98730 virtual)
2025-12-21 10:39:18,376 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (-171391 virtual)
2025-12-21 10:39:18,385 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (-179399 virtual)
2025-12-21 10:39:18,497 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (-199278 virtual)
2025-12-21 10:39:18,546 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (-198754 virtual)
2025-12-21 10:39:18,618 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (-209223 virtual)
2025-12-21 10:39:18,855 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (-246783 virtual)
2025-12-21 10:39:18,934 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (-260622 virtual)
2025-12-21 10:39:19,055 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,055 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,055 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,055 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,056 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,056 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,057 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,058 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,058 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,058 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,058 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,059 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,060 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,060 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,061 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,061 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,061 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,061 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,062 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,062 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,063 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,063 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,063 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,064 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,064 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,065 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,067 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,069 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,070 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,071 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,072 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,073 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,074 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,075 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,075 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,076 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,077 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,078 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,078 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,079 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,080 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,080 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,080 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,081 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,081 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,082 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,083 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,083 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,083 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,083 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,084 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,085 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,086 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,087 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,089 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,089 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,090 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,091 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,103 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,103 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,103 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,103 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,104 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,105 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,111 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,115 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,119 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,123 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,127 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,130 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,131 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,134 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,135 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,139 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,142 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,142 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,143 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,145 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,147 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,151 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,155 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,159 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,167 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,175 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,176 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,179 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,183 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,187 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,191 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,199 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,203 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,207 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,215 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,219 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,221 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,231 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,233 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,234 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,262 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,268 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,286 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,287 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,324 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,325 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,337 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,360 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,361 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,369 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,379 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,403 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,404 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,405 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,419 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,423 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,430 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,445 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,455 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,486 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,491 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,503 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,506 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,515 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,519 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,546 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,434 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,613 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,638 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,659 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,667 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,671 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,782 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,904 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,918 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:19,926 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:19,933 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:20,003 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:20,055 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:20,115 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:20,195 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:20,256 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:20,327 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:20,421 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:20,511 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:20,551 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:20,663 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:20,582 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:20,807 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:24,483 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 10:39:24,538 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 216990 virtual documents
2025-12-21 10:39:24,810 INFO gensim.topic_coherence.probability_estimation: using ParallelWordOccurrenceAccumulator<processes=127, batch_size=64> to estimate probabilities from sliding windows
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-12-21 10:39:29,817 INFO gensim.topic_coherence.text_analysis: 1 batches submitted to accumulate stats from 64 documents (7033 virtual)
2025-12-21 10:39:29,820 INFO gensim.topic_coherence.text_analysis: 2 batches submitted to accumulate stats from 128 documents (11049 virtual)
2025-12-21 10:39:29,821 INFO gensim.topic_coherence.text_analysis: 3 batches submitted to accumulate stats from 192 documents (15227 virtual)
2025-12-21 10:39:29,823 INFO gensim.topic_coherence.text_analysis: 4 batches submitted to accumulate stats from 256 documents (19846 virtual)
2025-12-21 10:39:29,825 INFO gensim.topic_coherence.text_analysis: 5 batches submitted to accumulate stats from 320 documents (25371 virtual)
2025-12-21 10:39:29,827 INFO gensim.topic_coherence.text_analysis: 6 batches submitted to accumulate stats from 384 documents (30552 virtual)
2025-12-21 10:39:29,828 INFO gensim.topic_coherence.text_analysis: 7 batches submitted to accumulate stats from 448 documents (35238 virtual)
2025-12-21 10:39:29,830 INFO gensim.topic_coherence.text_analysis: 8 batches submitted to accumulate stats from 512 documents (38594 virtual)
2025-12-21 10:39:29,832 INFO gensim.topic_coherence.text_analysis: 9 batches submitted to accumulate stats from 576 documents (45628 virtual)
2025-12-21 10:39:29,834 INFO gensim.topic_coherence.text_analysis: 10 batches submitted to accumulate stats from 640 documents (51017 virtual)
2025-12-21 10:39:29,836 INFO gensim.topic_coherence.text_analysis: 11 batches submitted to accumulate stats from 704 documents (55998 virtual)
2025-12-21 10:39:29,840 INFO gensim.topic_coherence.text_analysis: 12 batches submitted to accumulate stats from 768 documents (62781 virtual)
2025-12-21 10:39:29,842 INFO gensim.topic_coherence.text_analysis: 13 batches submitted to accumulate stats from 832 documents (67458 virtual)
2025-12-21 10:39:29,843 INFO gensim.topic_coherence.text_analysis: 14 batches submitted to accumulate stats from 896 documents (71162 virtual)
2025-12-21 10:39:29,845 INFO gensim.topic_coherence.text_analysis: 15 batches submitted to accumulate stats from 960 documents (74038 virtual)
2025-12-21 10:39:29,847 INFO gensim.topic_coherence.text_analysis: 16 batches submitted to accumulate stats from 1024 documents (77917 virtual)
2025-12-21 10:39:29,848 INFO gensim.topic_coherence.text_analysis: 17 batches submitted to accumulate stats from 1088 documents (82019 virtual)
2025-12-21 10:39:29,850 INFO gensim.topic_coherence.text_analysis: 18 batches submitted to accumulate stats from 1152 documents (86513 virtual)
2025-12-21 10:39:29,852 INFO gensim.topic_coherence.text_analysis: 19 batches submitted to accumulate stats from 1216 documents (90676 virtual)
2025-12-21 10:39:29,853 INFO gensim.topic_coherence.text_analysis: 20 batches submitted to accumulate stats from 1280 documents (95804 virtual)
2025-12-21 10:39:29,855 INFO gensim.topic_coherence.text_analysis: 21 batches submitted to accumulate stats from 1344 documents (100030 virtual)
2025-12-21 10:39:29,857 INFO gensim.topic_coherence.text_analysis: 22 batches submitted to accumulate stats from 1408 documents (104493 virtual)
2025-12-21 10:39:29,859 INFO gensim.topic_coherence.text_analysis: 23 batches submitted to accumulate stats from 1472 documents (112423 virtual)
2025-12-21 10:39:29,861 INFO gensim.topic_coherence.text_analysis: 24 batches submitted to accumulate stats from 1536 documents (118102 virtual)
2025-12-21 10:39:29,863 INFO gensim.topic_coherence.text_analysis: 25 batches submitted to accumulate stats from 1600 documents (122752 virtual)
2025-12-21 10:39:29,865 INFO gensim.topic_coherence.text_analysis: 26 batches submitted to accumulate stats from 1664 documents (127775 virtual)
2025-12-21 10:39:29,867 INFO gensim.topic_coherence.text_analysis: 27 batches submitted to accumulate stats from 1728 documents (132364 virtual)
2025-12-21 10:39:29,869 INFO gensim.topic_coherence.text_analysis: 28 batches submitted to accumulate stats from 1792 documents (136415 virtual)
2025-12-21 10:39:29,871 INFO gensim.topic_coherence.text_analysis: 29 batches submitted to accumulate stats from 1856 documents (142314 virtual)
2025-12-21 10:39:29,872 INFO gensim.topic_coherence.text_analysis: 30 batches submitted to accumulate stats from 1920 documents (147004 virtual)
2025-12-21 10:39:29,874 INFO gensim.topic_coherence.text_analysis: 31 batches submitted to accumulate stats from 1984 documents (151194 virtual)
2025-12-21 10:39:29,876 INFO gensim.topic_coherence.text_analysis: 32 batches submitted to accumulate stats from 2048 documents (156488 virtual)
2025-12-21 10:39:29,878 INFO gensim.topic_coherence.text_analysis: 33 batches submitted to accumulate stats from 2112 documents (160446 virtual)
2025-12-21 10:39:29,879 INFO gensim.topic_coherence.text_analysis: 34 batches submitted to accumulate stats from 2176 documents (164353 virtual)
2025-12-21 10:39:29,928 INFO gensim.topic_coherence.text_analysis: 35 batches submitted to accumulate stats from 2240 documents (168859 virtual)
2025-12-21 10:39:29,930 INFO gensim.topic_coherence.text_analysis: 36 batches submitted to accumulate stats from 2304 documents (173213 virtual)
2025-12-21 10:39:29,931 INFO gensim.topic_coherence.text_analysis: 37 batches submitted to accumulate stats from 2368 documents (177453 virtual)
2025-12-21 10:39:29,933 INFO gensim.topic_coherence.text_analysis: 38 batches submitted to accumulate stats from 2432 documents (181087 virtual)
2025-12-21 10:39:29,944 INFO gensim.topic_coherence.text_analysis: 39 batches submitted to accumulate stats from 2496 documents (185494 virtual)
2025-12-21 10:39:29,956 INFO gensim.topic_coherence.text_analysis: 40 batches submitted to accumulate stats from 2560 documents (189174 virtual)
2025-12-21 10:39:29,957 INFO gensim.topic_coherence.text_analysis: 41 batches submitted to accumulate stats from 2624 documents (193319 virtual)
2025-12-21 10:39:29,959 INFO gensim.topic_coherence.text_analysis: 42 batches submitted to accumulate stats from 2688 documents (197199 virtual)
2025-12-21 10:39:29,960 INFO gensim.topic_coherence.text_analysis: 43 batches submitted to accumulate stats from 2752 documents (200764 virtual)
2025-12-21 10:39:29,962 INFO gensim.topic_coherence.text_analysis: 44 batches submitted to accumulate stats from 2816 documents (203944 virtual)
2025-12-21 10:39:29,963 INFO gensim.topic_coherence.text_analysis: 45 batches submitted to accumulate stats from 2880 documents (207762 virtual)
2025-12-21 10:39:30,108 INFO gensim.topic_coherence.text_analysis: 46 batches submitted to accumulate stats from 2944 documents (211840 virtual)
2025-12-21 10:39:30,110 INFO gensim.topic_coherence.text_analysis: 47 batches submitted to accumulate stats from 3008 documents (217347 virtual)
2025-12-21 10:39:30,112 INFO gensim.topic_coherence.text_analysis: 48 batches submitted to accumulate stats from 3072 documents (222097 virtual)
2025-12-21 10:39:30,113 INFO gensim.topic_coherence.text_analysis: 49 batches submitted to accumulate stats from 3136 documents (225835 virtual)
2025-12-21 10:39:30,176 INFO gensim.topic_coherence.text_analysis: 50 batches submitted to accumulate stats from 3200 documents (231280 virtual)
2025-12-21 10:39:30,178 INFO gensim.topic_coherence.text_analysis: 51 batches submitted to accumulate stats from 3264 documents (237875 virtual)
2025-12-21 10:39:30,180 INFO gensim.topic_coherence.text_analysis: 52 batches submitted to accumulate stats from 3328 documents (242428 virtual)
2025-12-21 10:39:30,216 INFO gensim.topic_coherence.text_analysis: 53 batches submitted to accumulate stats from 3392 documents (247165 virtual)
2025-12-21 10:39:30,376 INFO gensim.topic_coherence.text_analysis: 54 batches submitted to accumulate stats from 3456 documents (252287 virtual)
2025-12-21 10:39:30,405 INFO gensim.topic_coherence.text_analysis: 55 batches submitted to accumulate stats from 3520 documents (259314 virtual)
2025-12-21 10:39:30,476 INFO gensim.topic_coherence.text_analysis: 56 batches submitted to accumulate stats from 3584 documents (264059 virtual)
2025-12-21 10:39:30,504 INFO gensim.topic_coherence.text_analysis: 57 batches submitted to accumulate stats from 3648 documents (268758 virtual)
2025-12-21 10:39:30,506 INFO gensim.topic_coherence.text_analysis: 58 batches submitted to accumulate stats from 3712 documents (273407 virtual)
2025-12-21 10:39:30,598 INFO gensim.topic_coherence.text_analysis: 59 batches submitted to accumulate stats from 3776 documents (278679 virtual)
2025-12-21 10:39:30,601 INFO gensim.topic_coherence.text_analysis: 60 batches submitted to accumulate stats from 3840 documents (285270 virtual)
2025-12-21 10:39:30,603 INFO gensim.topic_coherence.text_analysis: 61 batches submitted to accumulate stats from 3904 documents (289977 virtual)
2025-12-21 10:39:30,675 INFO gensim.topic_coherence.text_analysis: 62 batches submitted to accumulate stats from 3968 documents (293768 virtual)
2025-12-21 10:39:30,677 INFO gensim.topic_coherence.text_analysis: 63 batches submitted to accumulate stats from 4032 documents (298491 virtual)
2025-12-21 10:39:30,679 INFO gensim.topic_coherence.text_analysis: 64 batches submitted to accumulate stats from 4096 documents (301882 virtual)
2025-12-21 10:39:30,741 INFO gensim.topic_coherence.text_analysis: 65 batches submitted to accumulate stats from 4160 documents (306644 virtual)
2025-12-21 10:39:30,744 INFO gensim.topic_coherence.text_analysis: 66 batches submitted to accumulate stats from 4224 documents (311933 virtual)
2025-12-21 10:39:30,746 INFO gensim.topic_coherence.text_analysis: 67 batches submitted to accumulate stats from 4288 documents (316416 virtual)
2025-12-21 10:39:30,813 INFO gensim.topic_coherence.text_analysis: 68 batches submitted to accumulate stats from 4352 documents (320059 virtual)
2025-12-21 10:39:30,816 INFO gensim.topic_coherence.text_analysis: 69 batches submitted to accumulate stats from 4416 documents (324555 virtual)
2025-12-21 10:39:30,818 INFO gensim.topic_coherence.text_analysis: 70 batches submitted to accumulate stats from 4480 documents (329143 virtual)
2025-12-21 10:39:30,822 INFO gensim.topic_coherence.text_analysis: 71 batches submitted to accumulate stats from 4544 documents (332491 virtual)
2025-12-21 10:39:30,906 INFO gensim.topic_coherence.text_analysis: 72 batches submitted to accumulate stats from 4608 documents (338223 virtual)
2025-12-21 10:39:30,908 INFO gensim.topic_coherence.text_analysis: 73 batches submitted to accumulate stats from 4672 documents (343280 virtual)
2025-12-21 10:39:30,910 INFO gensim.topic_coherence.text_analysis: 74 batches submitted to accumulate stats from 4736 documents (346924 virtual)
2025-12-21 10:39:30,966 INFO gensim.topic_coherence.text_analysis: 75 batches submitted to accumulate stats from 4800 documents (353231 virtual)
2025-12-21 10:39:30,969 INFO gensim.topic_coherence.text_analysis: 76 batches submitted to accumulate stats from 4864 documents (357642 virtual)
2025-12-21 10:39:30,971 INFO gensim.topic_coherence.text_analysis: 77 batches submitted to accumulate stats from 4928 documents (362825 virtual)
2025-12-21 10:39:31,024 INFO gensim.topic_coherence.text_analysis: 78 batches submitted to accumulate stats from 4992 documents (367426 virtual)
2025-12-21 10:39:31,044 INFO gensim.topic_coherence.text_analysis: 79 batches submitted to accumulate stats from 5056 documents (372262 virtual)
2025-12-21 10:39:31,082 INFO gensim.topic_coherence.text_analysis: 80 batches submitted to accumulate stats from 5120 documents (376603 virtual)
2025-12-21 10:39:31,084 INFO gensim.topic_coherence.text_analysis: 81 batches submitted to accumulate stats from 5184 documents (380893 virtual)
2025-12-21 10:39:31,096 INFO gensim.topic_coherence.text_analysis: 82 batches submitted to accumulate stats from 5248 documents (384908 virtual)
2025-12-21 10:39:31,140 INFO gensim.topic_coherence.text_analysis: 83 batches submitted to accumulate stats from 5312 documents (388317 virtual)
2025-12-21 10:39:31,177 INFO gensim.topic_coherence.text_analysis: 84 batches submitted to accumulate stats from 5376 documents (392507 virtual)
2025-12-21 10:39:31,214 INFO gensim.topic_coherence.text_analysis: 85 batches submitted to accumulate stats from 5440 documents (397284 virtual)
2025-12-21 10:39:31,216 INFO gensim.topic_coherence.text_analysis: 86 batches submitted to accumulate stats from 5504 documents (401417 virtual)
2025-12-21 10:39:31,218 INFO gensim.topic_coherence.text_analysis: 87 batches submitted to accumulate stats from 5568 documents (406497 virtual)
2025-12-21 10:39:31,232 INFO gensim.topic_coherence.text_analysis: 88 batches submitted to accumulate stats from 5632 documents (412148 virtual)
2025-12-21 10:39:31,286 INFO gensim.topic_coherence.text_analysis: 89 batches submitted to accumulate stats from 5696 documents (417629 virtual)
2025-12-21 10:39:31,300 INFO gensim.topic_coherence.text_analysis: 90 batches submitted to accumulate stats from 5760 documents (421741 virtual)
2025-12-21 10:39:31,363 INFO gensim.topic_coherence.text_analysis: 91 batches submitted to accumulate stats from 5824 documents (427516 virtual)
2025-12-21 10:39:31,370 INFO gensim.topic_coherence.text_analysis: 92 batches submitted to accumulate stats from 5888 documents (433042 virtual)
2025-12-21 10:39:31,382 INFO gensim.topic_coherence.text_analysis: 93 batches submitted to accumulate stats from 5952 documents (437780 virtual)
2025-12-21 10:39:31,384 INFO gensim.topic_coherence.text_analysis: 94 batches submitted to accumulate stats from 6016 documents (442733 virtual)
2025-12-21 10:39:31,390 INFO gensim.topic_coherence.text_analysis: 95 batches submitted to accumulate stats from 6080 documents (446639 virtual)
2025-12-21 10:39:31,410 INFO gensim.topic_coherence.text_analysis: 96 batches submitted to accumulate stats from 6144 documents (452005 virtual)
2025-12-21 10:39:31,424 INFO gensim.topic_coherence.text_analysis: 97 batches submitted to accumulate stats from 6208 documents (455878 virtual)
2025-12-21 10:39:31,426 INFO gensim.topic_coherence.text_analysis: 98 batches submitted to accumulate stats from 6272 documents (460870 virtual)
2025-12-21 10:39:31,462 INFO gensim.topic_coherence.text_analysis: 99 batches submitted to accumulate stats from 6336 documents (467201 virtual)
2025-12-21 10:39:31,465 INFO gensim.topic_coherence.text_analysis: 100 batches submitted to accumulate stats from 6400 documents (472256 virtual)
2025-12-21 10:39:31,467 INFO gensim.topic_coherence.text_analysis: 101 batches submitted to accumulate stats from 6464 documents (476990 virtual)
2025-12-21 10:39:31,468 INFO gensim.topic_coherence.text_analysis: 102 batches submitted to accumulate stats from 6528 documents (481054 virtual)
2025-12-21 10:39:31,494 INFO gensim.topic_coherence.text_analysis: 103 batches submitted to accumulate stats from 6592 documents (486886 virtual)
2025-12-21 10:39:31,497 INFO gensim.topic_coherence.text_analysis: 104 batches submitted to accumulate stats from 6656 documents (494209 virtual)
2025-12-21 10:39:31,499 INFO gensim.topic_coherence.text_analysis: 105 batches submitted to accumulate stats from 6720 documents (498785 virtual)
2025-12-21 10:39:31,531 INFO gensim.topic_coherence.text_analysis: 106 batches submitted to accumulate stats from 6784 documents (502864 virtual)
2025-12-21 10:39:31,538 INFO gensim.topic_coherence.text_analysis: 107 batches submitted to accumulate stats from 6848 documents (507353 virtual)
2025-12-21 10:39:31,556 INFO gensim.topic_coherence.text_analysis: 108 batches submitted to accumulate stats from 6912 documents (511741 virtual)
2025-12-21 10:39:31,558 INFO gensim.topic_coherence.text_analysis: 109 batches submitted to accumulate stats from 6976 documents (518201 virtual)
2025-12-21 10:39:31,560 INFO gensim.topic_coherence.text_analysis: 110 batches submitted to accumulate stats from 7040 documents (523593 virtual)
2025-12-21 10:39:31,566 INFO gensim.topic_coherence.text_analysis: 111 batches submitted to accumulate stats from 7104 documents (528115 virtual)
2025-12-21 10:39:31,604 INFO gensim.topic_coherence.text_analysis: 112 batches submitted to accumulate stats from 7168 documents (532670 virtual)
2025-12-21 10:39:31,620 INFO gensim.topic_coherence.text_analysis: 113 batches submitted to accumulate stats from 7232 documents (535931 virtual)
2025-12-21 10:39:31,632 INFO gensim.topic_coherence.text_analysis: 114 batches submitted to accumulate stats from 7296 documents (540124 virtual)
2025-12-21 10:39:31,698 INFO gensim.topic_coherence.text_analysis: 115 batches submitted to accumulate stats from 7360 documents (545546 virtual)
2025-12-21 10:39:31,701 INFO gensim.topic_coherence.text_analysis: 116 batches submitted to accumulate stats from 7424 documents (549958 virtual)
2025-12-21 10:39:31,703 INFO gensim.topic_coherence.text_analysis: 117 batches submitted to accumulate stats from 7488 documents (553616 virtual)
2025-12-21 10:39:31,704 INFO gensim.topic_coherence.text_analysis: 118 batches submitted to accumulate stats from 7552 documents (558729 virtual)
2025-12-21 10:39:31,706 INFO gensim.topic_coherence.text_analysis: 119 batches submitted to accumulate stats from 7616 documents (562314 virtual)
2025-12-21 10:39:31,758 INFO gensim.topic_coherence.text_analysis: 120 batches submitted to accumulate stats from 7680 documents (568722 virtual)
2025-12-21 10:39:31,761 INFO gensim.topic_coherence.text_analysis: 121 batches submitted to accumulate stats from 7744 documents (575646 virtual)
2025-12-21 10:39:31,763 INFO gensim.topic_coherence.text_analysis: 122 batches submitted to accumulate stats from 7808 documents (580285 virtual)
2025-12-21 10:39:31,811 INFO gensim.topic_coherence.text_analysis: 123 batches submitted to accumulate stats from 7872 documents (584834 virtual)
2025-12-21 10:39:31,813 INFO gensim.topic_coherence.text_analysis: 124 batches submitted to accumulate stats from 7936 documents (589188 virtual)
2025-12-21 10:39:31,815 INFO gensim.topic_coherence.text_analysis: 125 batches submitted to accumulate stats from 8000 documents (593040 virtual)
2025-12-21 10:39:31,857 INFO gensim.topic_coherence.text_analysis: 126 batches submitted to accumulate stats from 8064 documents (596996 virtual)
2025-12-21 10:39:31,894 INFO gensim.topic_coherence.text_analysis: 127 batches submitted to accumulate stats from 8128 documents (603577 virtual)
2025-12-21 10:39:31,896 INFO gensim.topic_coherence.text_analysis: 128 batches submitted to accumulate stats from 8192 documents (608442 virtual)
2025-12-21 10:39:31,898 INFO gensim.topic_coherence.text_analysis: 129 batches submitted to accumulate stats from 8256 documents (612220 virtual)
2025-12-21 10:39:31,900 INFO gensim.topic_coherence.text_analysis: 130 batches submitted to accumulate stats from 8320 documents (617959 virtual)
2025-12-21 10:39:31,942 INFO gensim.topic_coherence.text_analysis: 131 batches submitted to accumulate stats from 8384 documents (622807 virtual)
2025-12-21 10:39:31,944 INFO gensim.topic_coherence.text_analysis: 132 batches submitted to accumulate stats from 8448 documents (627658 virtual)
2025-12-21 10:39:31,946 INFO gensim.topic_coherence.text_analysis: 133 batches submitted to accumulate stats from 8512 documents (633394 virtual)
2025-12-21 10:39:32,026 INFO gensim.topic_coherence.text_analysis: 134 batches submitted to accumulate stats from 8576 documents (637501 virtual)
2025-12-21 10:39:32,040 INFO gensim.topic_coherence.text_analysis: 135 batches submitted to accumulate stats from 8640 documents (642933 virtual)
2025-12-21 10:39:32,042 INFO gensim.topic_coherence.text_analysis: 136 batches submitted to accumulate stats from 8704 documents (647435 virtual)
2025-12-21 10:39:32,094 INFO gensim.topic_coherence.text_analysis: 137 batches submitted to accumulate stats from 8768 documents (651383 virtual)
2025-12-21 10:39:32,097 INFO gensim.topic_coherence.text_analysis: 138 batches submitted to accumulate stats from 8832 documents (656121 virtual)
2025-12-21 10:39:32,098 INFO gensim.topic_coherence.text_analysis: 139 batches submitted to accumulate stats from 8896 documents (660956 virtual)
2025-12-21 10:39:32,159 INFO gensim.topic_coherence.text_analysis: 140 batches submitted to accumulate stats from 8960 documents (665804 virtual)
2025-12-21 10:39:32,172 INFO gensim.topic_coherence.text_analysis: 141 batches submitted to accumulate stats from 9024 documents (670439 virtual)
2025-12-21 10:39:32,178 INFO gensim.topic_coherence.text_analysis: 142 batches submitted to accumulate stats from 9088 documents (674788 virtual)
2025-12-21 10:39:32,246 INFO gensim.topic_coherence.text_analysis: 143 batches submitted to accumulate stats from 9152 documents (679648 virtual)
2025-12-21 10:39:32,249 INFO gensim.topic_coherence.text_analysis: 144 batches submitted to accumulate stats from 9216 documents (684466 virtual)
2025-12-21 10:39:32,251 INFO gensim.topic_coherence.text_analysis: 145 batches submitted to accumulate stats from 9280 documents (689125 virtual)
2025-12-21 10:39:32,293 INFO gensim.topic_coherence.text_analysis: 146 batches submitted to accumulate stats from 9344 documents (692658 virtual)
2025-12-21 10:39:32,295 INFO gensim.topic_coherence.text_analysis: 147 batches submitted to accumulate stats from 9408 documents (696175 virtual)
2025-12-21 10:39:32,297 INFO gensim.topic_coherence.text_analysis: 148 batches submitted to accumulate stats from 9472 documents (700675 virtual)
2025-12-21 10:39:32,342 INFO gensim.topic_coherence.text_analysis: 149 batches submitted to accumulate stats from 9536 documents (704909 virtual)
2025-12-21 10:39:32,357 INFO gensim.topic_coherence.text_analysis: 150 batches submitted to accumulate stats from 9600 documents (713217 virtual)
2025-12-21 10:39:32,418 INFO gensim.topic_coherence.text_analysis: 151 batches submitted to accumulate stats from 9664 documents (718538 virtual)
2025-12-21 10:39:32,432 INFO gensim.topic_coherence.text_analysis: 152 batches submitted to accumulate stats from 9728 documents (722110 virtual)
2025-12-21 10:39:32,490 INFO gensim.topic_coherence.text_analysis: 153 batches submitted to accumulate stats from 9792 documents (726636 virtual)
2025-12-21 10:39:32,492 INFO gensim.topic_coherence.text_analysis: 154 batches submitted to accumulate stats from 9856 documents (731183 virtual)
2025-12-21 10:39:32,494 INFO gensim.topic_coherence.text_analysis: 155 batches submitted to accumulate stats from 9920 documents (737384 virtual)
2025-12-21 10:39:32,496 INFO gensim.topic_coherence.text_analysis: 156 batches submitted to accumulate stats from 9984 documents (743164 virtual)
2025-12-21 10:39:32,574 INFO gensim.topic_coherence.text_analysis: 157 batches submitted to accumulate stats from 10048 documents (748551 virtual)
2025-12-21 10:39:32,580 INFO gensim.topic_coherence.text_analysis: 158 batches submitted to accumulate stats from 10112 documents (753920 virtual)
2025-12-21 10:39:32,588 INFO gensim.topic_coherence.text_analysis: 159 batches submitted to accumulate stats from 10176 documents (758664 virtual)
2025-12-21 10:39:32,596 INFO gensim.topic_coherence.text_analysis: 160 batches submitted to accumulate stats from 10240 documents (763347 virtual)
2025-12-21 10:39:32,598 INFO gensim.topic_coherence.text_analysis: 161 batches submitted to accumulate stats from 10304 documents (769778 virtual)
2025-12-21 10:39:32,628 INFO gensim.topic_coherence.text_analysis: 162 batches submitted to accumulate stats from 10368 documents (773386 virtual)
2025-12-21 10:39:32,654 INFO gensim.topic_coherence.text_analysis: 163 batches submitted to accumulate stats from 10432 documents (777174 virtual)
2025-12-21 10:39:32,668 INFO gensim.topic_coherence.text_analysis: 164 batches submitted to accumulate stats from 10496 documents (781439 virtual)
2025-12-21 10:39:32,714 INFO gensim.topic_coherence.text_analysis: 165 batches submitted to accumulate stats from 10560 documents (786411 virtual)
2025-12-21 10:39:32,717 INFO gensim.topic_coherence.text_analysis: 166 batches submitted to accumulate stats from 10624 documents (792215 virtual)
2025-12-21 10:39:32,719 INFO gensim.topic_coherence.text_analysis: 167 batches submitted to accumulate stats from 10688 documents (798374 virtual)
2025-12-21 10:39:32,778 INFO gensim.topic_coherence.text_analysis: 168 batches submitted to accumulate stats from 10752 documents (802455 virtual)
2025-12-21 10:39:32,783 INFO gensim.topic_coherence.text_analysis: 169 batches submitted to accumulate stats from 10816 documents (805216 virtual)
2025-12-21 10:39:32,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,788 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,789 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,789 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,789 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,795 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,796 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,796 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,796 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,797 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,798 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,799 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,800 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,801 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,802 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,803 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,804 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,805 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,806 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,807 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,808 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,809 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,810 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,811 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,812 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,813 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,814 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,815 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,816 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,816 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,817 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,818 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,819 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,820 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,821 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,822 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,823 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,824 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,825 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,827 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,828 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,830 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,831 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,831 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,835 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,839 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,843 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,847 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,851 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,855 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,859 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,863 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,867 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,871 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,875 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,879 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,883 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,887 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,891 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,893 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,895 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,898 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,899 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,903 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,907 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,910 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,911 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,913 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,915 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,918 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,919 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,923 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,927 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,931 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,935 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,939 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,943 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,947 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,951 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,955 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:32,961 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:32,995 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,068 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,095 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,099 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,132 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,163 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,066 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,211 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,266 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,292 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,307 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,315 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,336 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,359 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,387 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,387 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,422 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,451 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,483 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,609 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,719 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,846 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,861 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,874 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,919 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,942 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,946 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:33,967 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:33,999 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,001 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,043 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,075 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,088 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,091 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,107 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,171 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,173 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,179 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,193 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,287 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,291 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,328 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,186 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,411 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,427 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,529 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,599 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,651 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,679 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,685 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,687 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,707 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,708 INFO gensim.topic_coherence.text_analysis: serializing accumulator to return to master...
2025-12-21 10:39:34,715 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:34,739 INFO gensim.topic_coherence.text_analysis: accumulator serialized
2025-12-21 10:39:37,921 INFO gensim.topic_coherence.text_analysis: 127 accumulators retrieved from output queue
2025-12-21 10:39:37,984 INFO gensim.topic_coherence.text_analysis: accumulated word occurrence stats for 805366 virtual documents
2025-12-21 10:39:38,231 INFO __main__: Model 0 (HDBSCAN) metrics: {'coherence_c_v': 0.6533630932243453, 'coherence_npmi': 0.17104114076684643, 'topic_diversity': 0.571, 'inter_topic_similarity': 0.49742600321769714}
2025-12-21 10:39:38,231 INFO __main__: Model 1 (KMeans) metrics: {'coherence_c_v': 0.6635075367545704, 'coherence_npmi': 0.1598037363740759, 'topic_diversity': 0.54, 'inter_topic_similarity': 0.5181266069412231}
2025-12-21 10:39:38,231 INFO __main__: Model 2 (BERTopicCobwebWrapper) metrics: {'coherence_c_v': 0.6286422937550609, 'coherence_npmi': 0.14850335039864923, 'topic_diversity': 0.64375, 'inter_topic_similarity': 0.5720689296722412}
